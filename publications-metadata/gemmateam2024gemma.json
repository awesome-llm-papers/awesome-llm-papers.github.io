[["kim2016sequence", "Sequence-level Knowledge Distillation"], ["yang2023baichuan", "Baichuan 2: Open Large-scale Language Models"], ["wang2020minilm", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"], ["liu2019improving", "Improving Multi-task Deep Neural Networks Via Knowledge Distillation For Natural Language Understanding"]]