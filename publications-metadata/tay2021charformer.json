[["heinzerling2017bpemb", "Bpemb: Tokenization-free Pre-trained Subword Embeddings In 275 Languages"], ["kudo2018sentencepiece", "Sentencepiece: A Simple And Language Independent Subword Tokenizer And Detokenizer For Neural Text Processing"], ["clark2021canine", "CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation"], ["ma2020charbert", "Charbert: Character-aware Pre-trained Language Model"]]