[["chefer2021generic", "Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers"], ["bai2021are", "Are Transformers More Robust Than Cnns?"], ["albert2023mamba", "Mamba: Linear-time Sequence Modeling With Selective State Spaces"], ["yuan2021incorporating", "Incorporating Convolution Designs Into Visual Transformers"]]