[["correia2019adaptively", "Adaptively Sparse Transformers"], ["raganato2020fixed", "Fixed Encoder Self-attention Patterns In Transformer-based Machine Translation"], ["kovaleva2019revealing", "Revealing The Dark Secrets Of BERT"], ["wang2020spatten", "Spatten: Efficient Sparse Attention Architecture With Cascade Token And Head Pruning"]]