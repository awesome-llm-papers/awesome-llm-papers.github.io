[["derose2020attention", "Attention Flows: Analyzing And Comparing Attention Mechanisms In Language Models"], ["htut2019do", "Do Attention Heads In BERT Track Syntactic Dependencies?"], ["zhang2018accelerating", "Accelerating Neural Transformer Via An Average Attention Network"], ["mrini2019rethinking", "Rethinking Self-attention: Towards Interpretability In Neural Parsing"]]