[["he2020deberta", "Deberta: Decoding-enhanced BERT With Disentangled Attention"], ["sachan2018parameter", "Parameter Sharing Methods For Multilingual Self-attentional Translation Models"], ["chi2021xlm", "XLM-E: Cross-lingual Language Model Pre-training Via ELECTRA"], ["chung2020rethinking", "Rethinking Embedding Coupling In Pre-trained Language Models"]]