[["ke2020rethinking", "Rethinking Positional Encoding In Language Pre-training"], ["wang2020what", "What Do Position Embeddings Learn? An Empirical Study Of Pre-trained Language Model Positional Encoding"], ["su2021roformer", "Roformer: Enhanced Transformer With Rotary Position Embedding"], ["michael2019evaluating", "Evaluating Sequence-to-sequence Models For Handwritten Text Recognition"]]