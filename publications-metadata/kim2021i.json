[["bhandare2019efficient", "Efficient 8-bit Quantization Of Transformer Neural Machine Language Translation Model"], ["zadeh2020gobo", "GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference"], ["frantar2022gptq", "GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers"], ["chen2016enhanced", "Enhanced LSTM For Natural Language Inference"]]