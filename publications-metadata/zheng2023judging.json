[["amanda2021general", "A General Language Assistant As A Laboratory For Alignment"], ["bakker2022fine", "Fine-tuning Language Models To Find Agreement Among Humans With Diverse Preferences"], ["rafael2023direct", "Direct Preference Optimization: Your Language Model Is Secretly A Reward Model"], ["chiang2023can", "Can Large Language Models Be An Alternative To Human Evaluations?"]]