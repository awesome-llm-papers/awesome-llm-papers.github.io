[["bai2021are", "Are Transformers More Robust Than Cnns?"], ["yuan2021tokens", "Tokens-to-token Vit: Training Vision Transformers From Scratch On Imagenet"], ["hassani2021escaping", "Escaping The Big Data Paradigm With Compact Transformers"], ["chefer2021generic", "Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers"]]