[["kim2021i", "I-BERT: Integer-only BERT Quantization"], ["frantar2022gptq", "GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers"], ["zadeh2020gobo", "GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference"], ["zafrir2019q8bert", "Q8BERT: Quantized 8bit BERT"]]