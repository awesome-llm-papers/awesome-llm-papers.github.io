[["rafael2023direct", "Direct Preference Optimization: Your Language Model Is Secretly A Reward Model"], ["amanda2021general", "A General Language Assistant As A Laboratory For Alignment"], ["amelia2022improving", "Improving Alignment Of Dialogue Agents Via Targeted Human Judgements"], ["k\u00f6pf2023openassistant", "Openassistant Conversations -- Democratizing Large Language Model Alignment"]]