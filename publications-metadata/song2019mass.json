[["zhang2019pretraining", "Pretraining-based Natural Language Generation For Text Summarization"], ["dong2022maskclip", "Maskclip: Masked Self-distillation Advances Contrastive Language-image Pretraining"], ["ramachandran2016unsupervised", "Unsupervised Pretraining For Sequence To Sequence Learning"], ["chen2019distilling", "Distilling Knowledge Learned In BERT For Text Generation"]]