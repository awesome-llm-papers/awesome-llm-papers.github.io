[["narayanan2021efficient", "Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm"], ["mohammad2019megatron", "Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism"], ["fedus2021switch", "Switch Transformers: Scaling To Trillion Parameter Models With Simple And Efficient Sparsity"], ["wei2021pangu", "Pangu-\\(\u03b1\\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation"]]