[["tambe2020edgebert", "Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference"], ["kim2021i", "I-BERT: Integer-only BERT Quantization"], ["frantar2022gptq", "GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers"], ["zhang2020ternarybert", "Ternarybert: Distillation-aware Ultra-low Bit BERT"]]