[["yu2020fine", "Fine-tuning Pre-trained Language Model With Weak Supervision: A Contrastive-regularized Self-training Approach"], ["jiang2020x", "X-FACTR: Multilingual Factual Knowledge Retrieval From Pretrained Language Models"], ["mallen2022when", "When Not To Trust Language Models: Investigating Effectiveness Of Parametric And Non-parametric Memories"], ["aman2022language", "Language Models Of Code Are Few-shot Commonsense Learners"]]