[["prakash2016neural", "Neural Paraphrase Generation With Stacked Residual LSTM Networks"], ["kitaev2020reformer", "Reformer: The Efficient Transformer"], ["xiong2017dcn", "DCN+: Mixed Objective And Deep Residual Coattention For Question Answering"], ["he2020realformer", "Realformer: Transformer Likes Residual Attention"]]