[["wu2021fastformer", "Fastformer: Additive Attention Can Be All You Need"], ["dao2023flashattention", "Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning"], ["roy2020efficient", "Efficient Content-based Sparse Attention With Routing Transformers"], ["child2019generating", "Generating Long Sequences With Sparse Transformers"]]