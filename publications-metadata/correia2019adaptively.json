[["michel2019are", "Are Sixteen Heads Really Better Than One?"], ["kovaleva2019revealing", "Revealing The Dark Secrets Of BERT"], ["raganato2020fixed", "Fixed Encoder Self-attention Patterns In Transformer-based Machine Translation"], ["wang2020spatten", "Spatten: Efficient Sparse Attention Architecture With Cascade Token And Head Pruning"]]