[["narayanan2021efficient", "Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm"], ["wei2021pangu", "Pangu-\\(\u03b1\\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation"], ["hassani2021escaping", "Escaping The Big Data Paradigm With Compact Transformers"], ["rajbhandari2019zero", "Zero: Memory Optimizations Toward Training Trillion Parameter Models"]]