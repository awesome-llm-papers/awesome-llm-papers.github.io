[["he2019revisiting", "Revisiting Self-training For Neural Sequence Generation"], ["meng2021coco", "COCO-LM: Correcting And Contrasting Text Sequences For Language Model Pretraining"], ["lu2021pretrained", "Pretrained Transformers As Universal Computation Engines"], ["fan2019using", "Using Local Knowledge Graph Construction To Scale Seq2seq Models To Multi-document Inputs"]]