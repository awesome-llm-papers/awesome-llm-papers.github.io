[["su2021roformer", "Roformer: Enhanced Transformer With Rotary Position Embedding"], ["wang2020what", "What Do Position Embeddings Learn? An Empirical Study Of Pre-trained Language Model Positional Encoding"], ["gu2022xylayoutlm", "Xylayoutlm: Towards Layout-aware Multimodal Networks For Visually-rich Document Understanding"], ["devlin2018bert", "BERT: Pre-training Of Deep Bidirectional Transformers For Language Understanding"]]