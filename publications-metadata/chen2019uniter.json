[["qi2020imagebert", "Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data"], ["wettig2022should", "Should You Mask 15% In Masked Language Modeling?"], ["li2019unicoder", "Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training"], ["xiao2020ernie", "Ernie-gram: Pre-training With Explicitly N-gram Masked Language Modeling For Natural Language Understanding"]]