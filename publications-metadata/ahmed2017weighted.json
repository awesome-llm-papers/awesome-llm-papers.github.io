[["ashish2017attention", "Attention Is All You Need"], ["hao2019modeling", "Modeling Recurrence For Transformer"], ["liu2020very", "Very Deep Transformers For Neural Machine Translation"], ["yu2018qanet", "Qanet: Combining Local Convolution With Global Self-attention For Reading Comprehension"]]