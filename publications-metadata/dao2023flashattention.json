[["dao2022flashattention", "Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness"], ["ye2019bp", "Bp-transformer: Modelling Long-range Context Via Binary Partitioning"], ["qiu2019blockwise", "Blockwise Self-attention For Long Document Understanding"], ["narayanan2021efficient", "Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm"]]