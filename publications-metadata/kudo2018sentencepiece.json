[["heinzerling2017bpemb", "Bpemb: Tokenization-free Pre-trained Subword Embeddings In 275 Languages"], ["tay2021charformer", "Charformer: Fast Character Transformers Via Gradient-based Subword Tokenization"], ["ma2020charbert", "Charbert: Character-aware Pre-trained Language Model"], ["zeyer2018improved", "Improved Training Of End-to-end Attention Models For Speech Recognition"]]