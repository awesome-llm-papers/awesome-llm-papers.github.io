[["wang2020hat", "HAT: Hardware-aware Transformers For Efficient Natural Language Processing"], ["zadeh2020gobo", "GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference"], ["popel2018training", "Training Tips For The Transformer Model"], ["tambe2020edgebert", "Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference"]]