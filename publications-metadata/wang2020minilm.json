[["wang2020minilmv2", "Minilmv2: Multi-head Self-attention Relation Distillation For Compressing Pretrained Transformers"], ["sun2019patient", "Patient Knowledge Distillation For BERT Model Compression"], ["yuan2020reinforced", "Reinforced Multi-teacher Selection For Knowledge Distillation"], ["kim2016sequence", "Sequence-level Knowledge Distillation"]]