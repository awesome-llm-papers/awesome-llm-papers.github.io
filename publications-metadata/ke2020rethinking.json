[["irie2019language", "Language Modeling With Deep Transformers"], ["wang2020what", "What Do Position Embeddings Learn? An Empirical Study Of Pre-trained Language Model Positional Encoding"], ["su2021roformer", "Roformer: Enhanced Transformer With Rotary Position Embedding"], ["takase2019positional", "Positional Encoding To Control Output Sequence Length"]]