[["kassner2021multilingual", "Multilingual LAMA: Investigating Knowledge In Multilingual Pretrained Language Models"], ["wu2020are", "Are All Languages Created Equal In Multilingual BERT?"], ["nozza2020what", "What The [MASK]? Making Sense Of Language-specific BERT Models"], ["lan2020empirical", "An Empirical Study Of Pre-trained Transformers For Arabic Information Extraction"]]