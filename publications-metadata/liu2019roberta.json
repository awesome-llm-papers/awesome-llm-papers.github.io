[["baevski2019cloze", "Cloze-driven Pretraining Of Self-attention Networks"], ["gu2020domain", "Domain-specific Language Model Pretraining For Biomedical Natural Language Processing"], ["wang2018can", "Can You Tell Me How To Get Past Sesame Street? Sentence-level Pretraining Beyond Language Modeling"], ["lan2019albert", "ALBERT: A Lite BERT For Self-supervised Learning Of Language Representations"]]