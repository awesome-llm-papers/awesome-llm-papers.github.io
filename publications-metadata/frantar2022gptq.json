[["bai2020binarybert", "Binarybert: Pushing The Limit Of BERT Quantization"], ["kim2021i", "I-BERT: Integer-only BERT Quantization"], ["zadeh2020gobo", "GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference"], ["bhandare2019efficient", "Efficient 8-bit Quantization Of Transformer Neural Machine Language Translation Model"]]