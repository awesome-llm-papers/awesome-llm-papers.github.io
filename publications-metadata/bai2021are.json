[["yuan2021incorporating", "Incorporating Convolution Designs Into Visual Transformers"], ["tang2018why", "Why Self-attention? A Targeted Evaluation Of Neural Machine Translation Architectures"], ["liu2021survey", "A Survey Of Visual Transformers"], ["chefer2021generic", "Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers"]]