[["kim2016sequence", "Sequence-level Knowledge Distillation"], ["freitag2017ensemble", "Ensemble Distillation For Neural Machine Translation"], ["wang2020minilm", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"], ["sun2019patient", "Patient Knowledge Distillation For BERT Model Compression"]]