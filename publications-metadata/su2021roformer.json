[["shaw2018self", "Self-attention With Relative Position Representations"], ["wang2020what", "What Do Position Embeddings Learn? An Empirical Study Of Pre-trained Language Model Positional Encoding"], ["ke2020rethinking", "Rethinking Positional Encoding In Language Pre-training"], ["wang2019r", "R-transformer: Recurrent Neural Network Enhanced Transformer"]]