[["yang2022vision", "Vision-language Pre-training With Triple Contrastive Learning"], ["qi2020imagebert", "Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data"], ["li2019unicoder", "Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training"], ["chung2021w2v", "W2v-bert: Combining Contrastive Learning And Masked Language Modeling For Self-supervised Speech Pre-training"]]