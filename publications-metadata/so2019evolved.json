[["wang2020hat", "HAT: Hardware-aware Transformers For Efficient Natural Language Processing"], ["wu2020lite", "Lite Transformer With Long-short Range Attention"], ["hassani2021escaping", "Escaping The Big Data Paradigm With Compact Transformers"], ["ashish2017attention", "Attention Is All You Need"]]