[["wang2020minilm", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"], ["sun2019patient", "Patient Knowledge Distillation For BERT Model Compression"], ["yuan2020reinforced", "Reinforced Multi-teacher Selection For Knowledge Distillation"], ["kim2016sequence", "Sequence-level Knowledge Distillation"]]