[["sun2019patient", "Patient Knowledge Distillation For BERT Model Compression"], ["freitag2017ensemble", "Ensemble Distillation For Neural Machine Translation"], ["kim2016sequence", "Sequence-level Knowledge Distillation"], ["wang2020minilm", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"]]