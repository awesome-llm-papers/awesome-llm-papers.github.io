[["frantar2022gptq", "GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers"], ["kim2021i", "I-BERT: Integer-only BERT Quantization"], ["mahabadi2021compacter", "Compacter: Efficient Low-rank Hypercomplex Adapter Layers"], ["child2019generating", "Generating Long Sequences With Sparse Transformers"]]