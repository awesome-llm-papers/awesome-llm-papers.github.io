[["yang2018convolutional", "Convolutional Self-attention Network"], ["correia2019adaptively", "Adaptively Sparse Transformers"], ["raganato2020fixed", "Fixed Encoder Self-attention Patterns In Transformer-based Machine Translation"], ["michel2019are", "Are Sixteen Heads Really Better Than One?"]]