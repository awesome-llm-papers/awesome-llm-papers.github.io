[["bai2021are", "Are Transformers More Robust Than Cnns?"], ["chefer2021generic", "Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers"], ["yuan2021incorporating", "Incorporating Convolution Designs Into Visual Transformers"], ["du2022survey", "A Survey Of Vision-language Pre-trained Models"]]