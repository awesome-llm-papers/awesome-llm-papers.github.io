[["xin2020deebert", "Deebert: Dynamic Early Exiting For Accelerating BERT Inference"], ["tambe2020edgebert", "Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference"], ["geva2022transformer", "Transformer Feed-forward Layers Build Predictions By Promoting Concepts In The Vocabulary Space"], ["r\u00fcckl\u00e92020adapterdrop", "Adapterdrop: On The Efficiency Of Adapters In Transformers"]]