[["sun2019patient", "Patient Knowledge Distillation For BERT Model Compression"], ["wang2020minilm", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"], ["kim2016sequence", "Sequence-level Knowledge Distillation"], ["yuan2020reinforced", "Reinforced Multi-teacher Selection For Knowledge Distillation"]]