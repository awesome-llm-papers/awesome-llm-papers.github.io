[["he2021debertav3", "Debertav3: Improving Deberta Using Electra-style Pre-training With Gradient-disentangled Embedding Sharing"], ["edward2021lora", "Lora: Low-rank Adaptation Of Large Language Models"], ["shaw2018self", "Self-attention With Relative Position Representations"], ["chen2019semantically", "Semantically Conditioned Dialog Response Generation Via Hierarchical Disentangled Self-attention"]]