[["xiao2020ernie", "Ernie-gram: Pre-training With Explicitly N-gram Masked Language Modeling For Natural Language Understanding"], ["wiedemann2020uhh", "UHH-LT At Semeval-2020 Task 12: Fine-tuning Of Pre-trained Transformer Networks For Offensive Language Detection"], ["chung2021w2v", "W2v-bert: Combining Contrastive Learning And Masked Language Modeling For Self-supervised Speech Pre-training"], ["vanaken2019how", "How Does BERT Answer Questions? A Layer-wise Analysis Of Transformer Representations"]]