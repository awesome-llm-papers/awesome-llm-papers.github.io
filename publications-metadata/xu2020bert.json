[["xu2022beyond", "Beyond Preserved Accuracy: Evaluating Loyalty And Robustness Of BERT Compression"], ["chen2020adabert", "Adabert: Task-adaptive BERT Compression With Differentiable Neural Architecture Search"], ["turc2019well", "Well-read Students Learn Better: On The Importance Of Pre-training Compact Models"], ["wang2020minilm", "Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers"]]