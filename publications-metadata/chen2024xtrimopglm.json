[["born2022regression", "Regression Transformer: Concurrent Sequence Regression And Generation For Molecular Language Modeling"], ["lu2021pretrained", "Pretrained Transformers As Universal Computation Engines"], ["bao2020unilmv2", "Unilmv2: Pseudo-masked Language Models For Unified Language Model Pre-training"], ["dehghani2018universal", "Universal Transformers"]]