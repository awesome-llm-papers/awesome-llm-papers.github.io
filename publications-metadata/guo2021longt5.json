[["ainslie2020etc", "ETC: Encoding Long And Structured Inputs In Transformers"], ["zhang2019pegasus", "PEGASUS: Pre-training With Extracted Gap-sentences For Abstractive Summarization"], ["takase2019positional", "Positional Encoding To Control Output Sequence Length"], ["he2020realformer", "Realformer: Transformer Likes Residual Attention"]]