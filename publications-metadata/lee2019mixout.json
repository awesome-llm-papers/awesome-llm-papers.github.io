[["lu2021pretrained", "Pretrained Transformers As Universal Computation Engines"], ["chung2022scaling", "Scaling Instruction-finetuned Language Models"], ["logan2021cutting", "Cutting Down On Prompts And Parameters: Simple Few-shot Learning With Language Models"], ["tang2020multilingual", "Multilingual Translation With Extensible Multilingual Pretraining And Finetuning"]]