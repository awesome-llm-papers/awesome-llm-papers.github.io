[["rafael2023direct", "Direct Preference Optimization: Your Language Model Is Secretly A Reward Model"], ["bai2022training", "Training A Helpful And Harmless Assistant With Reinforcement Learning From Human Feedback"], ["garg2019jointly", "Jointly Learning To Align And Translate With Transformer Models"], ["liu2023what", "What Makes Good Data For Alignment? A Comprehensive Study Of Automatic Data Selection In Instruction Tuning"]]