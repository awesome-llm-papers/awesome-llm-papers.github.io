[["mahabadi2021parameter", "Parameter-efficient Multi-task Fine-tuning For Transformers Via Shared Hypernetworks"], ["sachan2018parameter", "Parameter Sharing Methods For Multilingual Self-attentional Translation Models"], ["r\u00fcckl\u00e92020adapterdrop", "Adapterdrop: On The Efficiency Of Adapters In Transformers"], ["wang2020k", "K-adapter: Infusing Knowledge Into Pre-trained Models With Adapters"]]