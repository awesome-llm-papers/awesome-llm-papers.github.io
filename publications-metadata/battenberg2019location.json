[["choi2018fine", "Fine-grained Attention Mechanism For Neural Machine Translation"], ["wu2021fastformer", "Fastformer: Additive Attention Can Be All You Need"], ["derose2020attention", "Attention Flows: Analyzing And Comparing Attention Mechanisms In Language Models"], ["deng2018latent", "Latent Alignment And Variational Attention"]]