[["vig2019analyzing", "Analyzing The Structure Of Attention In A Transformer Language Model"], ["htut2019do", "Do Attention Heads In BERT Track Syntactic Dependencies?"], ["derose2020attention", "Attention Flows: Analyzing And Comparing Attention Mechanisms In Language Models"], ["correia2019adaptively", "Adaptively Sparse Transformers"]]