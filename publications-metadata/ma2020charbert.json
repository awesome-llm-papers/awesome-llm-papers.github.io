[["heinzerling2017bpemb", "Bpemb: Tokenization-free Pre-trained Subword Embeddings In 275 Languages"], ["kudo2018sentencepiece", "Sentencepiece: A Simple And Language Independent Subword Tokenizer And Detokenizer For Neural Text Processing"], ["tay2021charformer", "Charformer: Fast Character Transformers Via Gradient-based Subword Tokenization"], ["bostrom2020byte", "Byte Pair Encoding Is Suboptimal For Language Model Pretraining"]]