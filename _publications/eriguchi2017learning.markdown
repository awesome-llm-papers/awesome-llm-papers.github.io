---
layout: publication
title: Learning To Parse And Translate Improves Neural Machine Translation
authors: Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho
conference: 'Proceedings of the 55th Annual Meeting of the Association for Computational
  Linguistics (Volume 2: Short Papers)'
year: 2017
bibkey: eriguchi2017learning
citations: 152
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1702.03525'}]
tags: ["Training Techniques"]
short_authors: Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho
---
There has been relatively little attention to incorporating linguistic prior
to neural machine translation. Much of the previous work was further
constrained to considering linguistic prior on the source side. In this paper,
we propose a hybrid model, called NMT+RNNG, that learns to parse and translate
by combining the recurrent neural network grammar into the attention-based
neural machine translation. Our approach encourages the neural machine
translation model to incorporate linguistic prior during training, and lets it
translate on its own afterward. Extensive experiments with four language pairs
show the effectiveness of the proposed NMT+RNNG.