---
layout: publication
title: Studying The Effect Of AI Code Generators On Supporting Novice Learners In
  Introductory Programming
authors: Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David
  Weintrop, Tovi Grossman
conference: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems
year: 2023
bibkey: kazemitabaar2023studying
citations: 175
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2302.07427'}]
tags: ["Evaluation"]
short_authors: Kazemitabaar et al.
---
AI code generators like OpenAI Codex have the potential to assist novice
programmers by generating code from natural language descriptions, however,
over-reliance might negatively impact learning and retention. To explore the
implications that AI code generators have on introductory programming, we
conducted a controlled experiment with 69 novices (ages 10-17). Learners worked
on 45 Python code-authoring tasks, for which half of the learners had access to
Codex, each followed by a code-modification task. Our results show that using
Codex significantly increased code-authoring performance (1.15x increased
completion rate and 1.8x higher scores) while not decreasing performance on
manual code-modification tasks. Additionally, learners with access to Codex
during the training phase performed slightly better on the evaluation
post-tests conducted one week later, although this difference did not reach
statistical significance. Of interest, learners with higher Scratch pre-test
scores performed significantly better on retention post-tests, if they had
prior access to Codex.