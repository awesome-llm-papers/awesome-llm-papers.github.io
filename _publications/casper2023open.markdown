---
layout: publication
title: Open Problems And Fundamental Limitations Of Reinforcement Learning From Human
  Feedback
authors: "Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\xE9\
  r\xE9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro\
  \ Freire, Tony Wang, Samuel Marks, Charbel-rapha\xEBl Segerie, Micah Carroll, Andi\
  \ Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand\
  \ Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov,\
  \ Xin Chen, Lauro Langosco, Peter Hase, Erdem B\u0131y\u0131k, Anca Dragan, David\
  \ Krueger, Dorsa Sadigh, Dylan Hadfield-menell"
conference: No Venue
year: 2023
bibkey: casper2023open
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/hf2307.15217'}]
tags: ["Reinforcement Learning", "Survey Paper"]
short_authors: Casper et al.
---
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.

https://huggingface.co/discussions/paper/64c74567589826641bcfcd02