---
layout: publication
title: Relevance Of Unsupervised Metrics In Task-oriented Dialogue For Evaluating
  Natural Language Generation
authors: Shikhar Sharma, Layla El Asri, Hannes Schulz, Jeremie Zumer
conference: Arxiv
year: 2017
bibkey: sharma2017relevance
citations: 206
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1706.09799'}]
tags: ["Evaluation"]
short_authors: Sharma et al.
---
Automated metrics such as BLEU are widely used in the machine translation
literature. They have also been used recently in the dialogue community for
evaluating dialogue response generation. However, previous work in dialogue
response generation has shown that these metrics do not correlate strongly with
human judgment in the non task-oriented dialogue setting. Task-oriented
dialogue responses are expressed on narrower domains and exhibit lower
diversity. It is thus reasonable to think that these automated metrics would
correlate well with human judgment in the task-oriented setting where the
generation task consists of translating dialogue acts into a sentence. We
conduct an empirical study to confirm whether this is the case. Our findings
indicate that these automated metrics have stronger correlation with human
judgments in the task-oriented setting compared to what has been observed in
the non task-oriented setting. We also observe that these metrics correlate
even better for datasets which provide multiple ground truth reference
sentences. In addition, we show that some of the currently available corpora
for task-oriented language generation can be solved with simple models and
advocate for more challenging datasets.