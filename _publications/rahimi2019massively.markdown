---
layout: publication
title: Massively Multilingual Transfer For NER
authors: Afshin Rahimi, Yuan Li, Trevor Cohn
conference: Proceedings of the 57th Annual Meeting of the Association for Computational
  Linguistics
year: 2019
bibkey: rahimi2019massively
citations: 183
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1902.00193'}]
tags: ["Few-Shot"]
short_authors: Afshin Rahimi, Yuan Li, Trevor Cohn
---
In cross-lingual transfer, NLP models over one or more source languages are
applied to a low-resource target language. While most prior work has used a
single source model or a few carefully selected models, here we consider a
`massive' setting with many such models. This setting raises the problem of
poor transfer, particularly from distant languages. We propose two techniques
for modulating the transfer, suitable for zero-shot or few-shot learning,
respectively. Evaluating on named entity recognition, we show that our
techniques are much more effective than strong baselines, including standard
ensembling, and our unsupervised method rivals oracle selection of the single
best individual model.