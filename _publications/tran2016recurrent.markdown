---
layout: publication
title: Recurrent Memory Networks For Language Modeling
authors: Ke Tran, Arianna Bisazza, Christof Monz
conference: 'Proceedings of the 2016 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies'
year: 2016
bibkey: tran2016recurrent
citations: 78
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1601.01272'}]
tags: ["Model Architecture", "NAACL"]
short_authors: Ke Tran, Arianna Bisazza, Christof Monz
---
Recurrent Neural Networks (RNN) have obtained excellent result in many
natural language processing (NLP) tasks. However, understanding and
interpreting the source of this success remains a challenge. In this paper, we
propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only
amplifies the power of RNN but also facilitates our understanding of its
internal functioning and allows us to discover underlying patterns in data. We
demonstrate the power of RMN on language modeling and sentence completion
tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)
network on three large German, Italian, and English dataset. Additionally we
perform in-depth analysis of various linguistic dimensions that RMN captures.
On Sentence Completion Challenge, for which it is essential to capture sentence
coherence, our RMN obtains 69.2% accuracy, surpassing the previous
state-of-the-art by a large margin.