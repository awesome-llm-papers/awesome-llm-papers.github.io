---
layout: publication
title: 'Lexglue: A Benchmark Dataset For Legal Language Understanding In English'
authors: Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos,
  Daniel Martin Katz, Nikolaos Aletras
conference: 'Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2022
bibkey: chalkidis2021lexglue
citations: 81
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2110.00976'}]
tags: ["Datasets", "Evaluation"]
short_authors: Chalkidis et al.
---
Laws and their interpretations, legal arguments and agreements\ are typically
expressed in writing, leading to the production of vast corpora of legal text.
Their analysis, which is at the center of legal practice, becomes increasingly
elaborate as these collections grow in size. Natural language understanding
(NLU) technologies can be a valuable tool to support legal practitioners in
these endeavors. Their usefulness, however, largely depends on whether current
state-of-the-art models can generalize across various tasks in the legal
domain. To answer this currently open question, we introduce the Legal General
Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets
for evaluating model performance across a diverse set of legal NLU tasks in a
standardized way. We also provide an evaluation and analysis of several generic
and legal-oriented models demonstrating that the latter consistently offer
performance improvements across multiple tasks.