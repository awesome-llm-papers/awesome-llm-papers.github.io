---
layout: publication
title: Massively Multilingual Adversarial Speech Recognition
authors: Oliver Adams, Matthew Wiesner, Shinji Watanabe, David Yarowsky
conference: Proceedings of the 2019 Conference of the North
year: 2019
bibkey: adams2019massively
citations: 76
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1904.02210'}]
tags: ["Security"]
short_authors: Adams et al.
---
We report on adaptation of multilingual end-to-end speech recognition models
trained on as many as 100 languages. Our findings shed light on the relative
importance of similarity between the target and pretraining languages along the
dimensions of phonetics, phonology, language family, geographical location, and
orthography. In this context, experiments demonstrate the effectiveness of two
additional pretraining objectives in encouraging language-independent encoder
representations: a context-independent phoneme objective paired with a
language-adversarial classification objective.