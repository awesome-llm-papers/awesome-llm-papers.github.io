---
layout: publication
title: Learning Language-visual Embedding For Movie Understanding With Natural-language
authors: Atousa Torabi, Niket Tandon, Leonid Sigal
conference: Arxiv
year: 2016
bibkey: torabi2016learning
citations: 79
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1609.08124'}]
tags: ["Datasets", "Evaluation", "Model Architecture"]
short_authors: Atousa Torabi, Niket Tandon, Leonid Sigal
---
Learning a joint language-visual embedding has a number of very appealing
properties and can result in variety of practical application, including
natural language image/video annotation and search. In this work, we study
three different joint language-visual neural network model architectures. We
evaluate our models on large scale LSMDC16 movie dataset for two tasks: 1)
Standard Ranking for video annotation and retrieval 2) Our proposed movie
multiple-choice test. This test facilitate automatic evaluation of
visual-language models for natural language video annotation based on human
activities. In addition to original Audio Description (AD) captions, provided
as part of LSMDC16, we collected and will make available a) manually generated
re-phrasings of those captions obtained using Amazon MTurk b) automatically
generated human activity elements in "Predicate + Object" (PO) phrases based on
"Knowlywood", an activity knowledge mining model. Our best model archives
Recall@10 of 19.2% on annotation and 18.9% on video retrieval tasks for subset
of 1000 samples. For multiple-choice test, our best model achieve accuracy
58.11% over whole LSMDC16 public test-set.