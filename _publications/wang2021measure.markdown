---
layout: publication
title: 'Measure And Improve Robustness In NLP Models: A Survey'
authors: Xuezhi Wang, Haohan Wang, Diyi Yang
conference: 'Proceedings of the 2022 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies'
year: 2022
bibkey: wang2021measure
citations: 66
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2112.08313'}]
tags: ["Evaluation", "Survey Paper"]
short_authors: Xuezhi Wang, Haohan Wang, Diyi Yang
---
As NLP models achieved state-of-the-art performances over benchmarks and
gained wide applications, it has been increasingly important to ensure the safe
deployment of these models in the real world, e.g., making sure the models are
robust against unseen or challenging scenarios. Despite robustness being an
increasingly studied topic, it has been separately explored in applications
like vision and NLP, with various definitions, evaluation and mitigation
strategies in multiple lines of research. In this paper, we aim to provide a
unifying survey of how to define, measure and improve robustness in NLP. We
first connect multiple definitions of robustness, then unify various lines of
work on identifying robustness failures and evaluating models' robustness.
Correspondingly, we present mitigation strategies that are data-driven,
model-driven, and inductive-prior-based, with a more systematic view of how to
effectively improve robustness in NLP models. Finally, we conclude by outlining
open challenges and future directions to motivate further research in this
area.