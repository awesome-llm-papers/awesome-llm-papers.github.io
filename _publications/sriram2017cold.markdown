---
layout: publication
title: 'Cold Fusion: Training Seq2seq Models Together With Language Models'
authors: Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, Adam Coates
conference: Interspeech 2018
year: 2018
bibkey: sriram2017cold
citations: 249
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1708.06426'}]
tags: ["INTERSPEECH", "Model Architecture", "Training Techniques"]
short_authors: Sriram et al.
---
Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks
which involve generating natural language sentences such as machine
translation, image captioning and speech recognition. Performance has further
been improved by leveraging unlabeled data, often in the form of a language
model. In this work, we present the Cold Fusion method, which leverages a
pre-trained language model during training, and show its effectiveness on the
speech recognition task. We show that Seq2Seq models with Cold Fusion are able
to better utilize language information enjoying i) faster convergence and
better generalization, and ii) almost complete transfer to a new domain while
using less than 10% of the labeled training data.