---
layout: publication
title: 'Bertweet: A Pre-trained Language Model For English Tweets'
authors: Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen
conference: 'Proceedings of the 2020 Conference on Empirical Methods in Natural Language
  Processing: System Demonstrations'
year: 2020
bibkey: nguyen2020bertweet
citations: 688
additional_links: [{name: Code, url: 'https://github.com/VinAIResearch/BERTweet'},
  {name: Paper, url: 'https://arxiv.org/abs/2005.10200'}]
tags: ["Applications", "Has Code", "Model Architecture", "Training Techniques"]
short_authors: Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen
---
We present BERTweet, the first public large-scale pre-trained language model
for English Tweets. Our BERTweet, having the same architecture as BERT-base
(Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu
et al., 2019). Experiments show that BERTweet outperforms strong baselines
RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better
performance results than the previous state-of-the-art models on three Tweet
NLP tasks: Part-of-speech tagging, Named-entity recognition and text
classification. We release BERTweet under the MIT License to facilitate future
research and applications on Tweet data. Our BERTweet is available at
https://github.com/VinAIResearch/BERTweet