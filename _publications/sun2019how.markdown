---
layout: publication
title: How To Fine-tune BERT For Text Classification?
authors: Chi Sun, Xipeng Qiu, Yige Xu, Xuanjing Huang
conference: Lecture Notes in Computer Science
year: 2019
bibkey: sun2019how
citations: 1187
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1905.05583'}]
tags: ["Fine-Tuning", "Model Architecture", "Training Techniques"]
short_authors: Sun et al.
---
Language model pre-training has proven to be useful in learning universal
language representations. As a state-of-the-art language model pre-training
model, BERT (Bidirectional Encoder Representations from Transformers) has
achieved amazing results in many language understanding tasks. In this paper,
we conduct exhaustive experiments to investigate different fine-tuning methods
of BERT on text classification task and provide a general solution for BERT
fine-tuning. Finally, the proposed solution obtains new state-of-the-art
results on eight widely-studied text classification datasets.