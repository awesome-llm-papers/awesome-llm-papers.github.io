---
layout: publication
title: A Survey Of Small Language Models
authors: Chien van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian
  Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, Junda Wu, Ashish Singh,
  Yu Wang, Jiuxiang Gu, Franck Dernoncourt, Nesreen K. Ahmed, Nedim Lipka, Ruiyi Zhang,
  Xiang Chen, Tong Yu, Sungchul Kim, Hanieh Deilamsalehy, Namyong Park, Mike Rimer,
  Zhehao Zhang, Huanrui Yang, Ryan A. Rossi, Thien Huu Nguyen
conference: No Venue
year: 2024
bibkey: nguyen2024survey
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/hf2410.20011'}]
tags: ["Survey Paper"]
short_authors: Nguyen et al.
---
Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.

https://huggingface.co/discussions/paper/672043886e75d23f3e214230