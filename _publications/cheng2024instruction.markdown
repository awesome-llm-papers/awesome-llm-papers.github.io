---
layout: publication
title: 'Instruction Pre-training: Language Models Are Supervised Multitask Learners'
authors: Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, Furu Wei
conference: No Venue
year: 2024
bibkey: cheng2024instruction
additional_links: [{name: Code, url: 'https://github.com/microsoft/LMOps'}, {name: Code,
    url: 'https://huggingface.co/discussions/paper/6674ef915f7d5c8af70b566d'}, {name: Paper,
    url: 'https://arxiv.org/abs/hf2406.14491'}]
tags: ["Training Techniques"]
short_authors: Cheng et al.
---
Unsupervised multitask pre-training has been the critical method behind the recent success of language models (LMs). However, supervised multitask learning still holds significant promise, as scaling it in the post-training stage trends towards better generalization. In this paper, we explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train LMs. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. In our experiments, we synthesize 200M instruction-response pairs covering 40+ task categories to verify the effectiveness of Instruction Pre-Training. In pre-training from scratch, Instruction Pre-Training not only consistently enhances pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3-70B. Our model, code, and data are available at https://github.com/microsoft/LMOps.

https://huggingface.co/discussions/paper/6674ef915f7d5c8af70b566d