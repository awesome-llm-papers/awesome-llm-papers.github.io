---
layout: publication
title: Input Combination Strategies For Multi-source Transformer Decoder
authors: "Jind\u0159ich Libovick\xFD, Jind\u0159ich Helcl, David Mare\u010Dek"
conference: 'Proceedings of the Third Conference on Machine Translation: Research
  Papers'
year: 2018
bibkey: "libovick\xFD2018input"
citations: 65
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1811.04716'}]
tags: ["Model Architecture"]
short_authors: "Jind\u0159ich Libovick\xFD, Jind\u0159ich Helcl, David Mare\u010D\
  ek"
---
In multi-source sequence-to-sequence tasks, the attention mechanism can be
modeled in several ways. This topic has been thoroughly studied on recurrent
architectures. In this paper, we extend the previous work to the
encoder-decoder attention in the Transformer architecture. We propose four
different input combination strategies for the encoder-decoder attention:
serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of
multimodal translation and translation with multiple source languages. The
experiments show that the models are able to use multiple sources and improve
over single source baselines.