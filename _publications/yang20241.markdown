---
layout: publication
title: 1.58-bit FLUX
authors: Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim, Xing Mei, Xiaohui Shen,
  Liang-chieh Chen
conference: No Venue
year: 2024
bibkey: yang20241
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/hf2412.18653'}]
tags: ["Efficiency"]
short_authors: Yang et al.
---
We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in \{-1, 0, +1\}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction in model storage, a 5.1x reduction in inference memory, and improved inference latency. Extensive evaluations on the GenEval and T2I Compbench benchmarks demonstrate the effectiveness of 1.58-bit FLUX in maintaining generation quality while significantly enhancing computational efficiency.

https://huggingface.co/discussions/paper/6772d70133efe31653f02bde