---
layout: publication
title: 'On The Origin Of Hallucinations In Conversational Models: Is It The Datasets
  Or The Models?'
authors: Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, Siva Reddy
conference: 'Proceedings of the 2022 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies'
year: 2022
bibkey: dziri2022origin
citations: 83
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2204.07931'}]
tags: ["Datasets", "NAACL"]
short_authors: Dziri et al.
---
Knowledge-grounded conversational models are known to suffer from producing
factually invalid statements, a phenomenon commonly called hallucination. In
this work, we investigate the underlying causes of this phenomenon: is
hallucination due to the training data, or to the models? We conduct a
comprehensive human study on both existing knowledge-grounded conversational
benchmarks and several state-of-the-art models. Our study reveals that the
standard benchmarks consist of >60% hallucinated responses, leading to models
that not only hallucinate but even amplify hallucinations. Our findings raise
important questions on the quality of existing datasets and models trained
using them. We make our annotations publicly available for future research.