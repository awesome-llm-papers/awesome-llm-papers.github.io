---
layout: publication
title: Does Multimodality Help Human And Machine For Translation And Image Captioning?
authors: "Ozan Caglayan, Walid Aransa, Yaxing Wang, Marc Masana, Mercedes Garc\xED\
  a-mart\xEDnez, Fethi Bougares, Lo\xEFc Barrault, Joost van de Weijer"
conference: 'Proceedings of the First Conference on Machine Translation: Volume 2,
  Shared Task Papers'
year: 2016
bibkey: caglayan2016does
citations: 77
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1605.09186'}]
tags: ["Evaluation"]
short_authors: Caglayan et al.
---
This paper presents the systems developed by LIUM and CVC for the WMT16
Multimodal Machine Translation challenge. We explored various comparative
methods, namely phrase-based systems and attentional recurrent neural networks
models trained using monomodal or multimodal data. We also performed a human
evaluation in order to estimate the usefulness of multimodal data for human
machine translation and image description generation. Our systems obtained the
best results for both tasks according to the automatic evaluation metrics BLEU
and METEOR.