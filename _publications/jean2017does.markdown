---
layout: publication
title: Does Neural Machine Translation Benefit From Larger Context?
authors: Sebastien Jean, Stanislas Lauly, Orhan Firat, Kyunghyun Cho
conference: Arxiv
year: 2017
bibkey: jean2017does
citations: 138
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1704.05135'}]
tags: ["Model Architecture"]
short_authors: Jean et al.
---
We propose a neural machine translation architecture that models the
surrounding text in addition to the source sentence. These models lead to
better performance, both in terms of general translation quality and pronoun
prediction, when trained on small corpora, although this improvement largely
disappears when trained with a larger corpus. We also discover that
attention-based neural machine translation is well suited for pronoun
prediction and compares favorably with other approaches that were specifically
designed for this task.