---
layout: publication
title: Vision Language Models Are Blind
authors: Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen
conference: No Venue
year: 2024
bibkey: rahmanzadehgervi2024vision
additional_links: [{name: Code, url: 'https://vlmsareblind.github.io/'}, {name: Code,
    url: 'https://huggingface.co/discussions/paper/668de4e91c1fe11f8c1d3d0d'}, {name: Paper,
    url: 'https://arxiv.org/abs/hf2407.06581'}]
tags: ["Model Architecture"]
short_authors: Rahmanzadehgervi et al.
---
Large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini 1.5 Pro are powering countless image-text applications and scoring high on many vision-understanding benchmarks. Yet, we find that VLMs fail on 7 visual tasks absurdly easy to humans such as identifying (a) whether two circles overlap; (b) whether two lines intersect; (c) which letter is being circled in a word; and (d) counting the number of circles in a Olympic-like logo. The shockingly poor performance of four state-of-the-art VLMs suggests their vision is, at best, like of a person with myopia seeing fine details as blurry, and at worst, like an intelligent person that is blind making educated guesses. Code is available at: https://vlmsareblind.github.io/

https://huggingface.co/discussions/paper/668de4e91c1fe11f8c1d3d0d