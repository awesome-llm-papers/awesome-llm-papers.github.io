---
layout: publication
title: 'Alignment Studio: Aligning Large Language Models To Particular Contextual
  Regulations'
authors: Swapnaja Achintalwar, Ioana Baldini, Djallel Bouneffouf, Joan Byamugisha,
  Maria Chang, Pierre Dognin, Eitan Farchi, Ndivhuwo Makondo, Aleksandra Mojsilovic,
  Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Inkit Padhi, Orna Raz, Jesus Rios,
  Prasanna Sattigeri, Moninder Singh, Siphiwe Thwala, Rosario A. Uceda-sosa, Kush
  R. Varshney
conference: No Venue
year: 2024
bibkey: achintalwar2024alignment
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/hf2403.09704'}]
tags: ["Applications", "Model Architecture"]
short_authors: Achintalwar et al.
---
The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.

https://huggingface.co/discussions/paper/65f7a456fdb0e12d2c46dc9e