---
layout: publication
title: Enhancing Machine Translation With Dependency-aware Self-attention
authors: Emanuele Bugliarello, Naoaki Okazaki
conference: Proceedings of the 58th Annual Meeting of the Association for Computational
  Linguistics
year: 2020
bibkey: bugliarello2019enhancing
citations: 68
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1909.03149'}]
tags: ["Model Architecture"]
short_authors: Emanuele Bugliarello, Naoaki Okazaki
---
Most neural machine translation models only rely on pairs of parallel
sentences, assuming syntactic information is automatically learned by an
attention mechanism. In this work, we investigate different approaches to
incorporate syntactic knowledge in the Transformer model and also propose a
novel, parameter-free, dependency-aware self-attention mechanism that improves
its translation quality, especially for long sentences and in low-resource
scenarios. We show the efficacy of each approach on WMT English-German and
English-Turkish, and WAT English-Japanese translation tasks.