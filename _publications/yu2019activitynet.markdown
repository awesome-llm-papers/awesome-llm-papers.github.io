---
layout: publication
title: 'Activitynet-qa: A Dataset For Understanding Complex Web Videos Via Question
  Answering'
authors: Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, Dacheng Tao
conference: Proceedings of the AAAI Conference on Artificial Intelligence
year: 2019
bibkey: yu2019activitynet
citations: 166
additional_links: [{name: Code, url: 'https://github.com/MILVLG/activitynet-qa'},
  {name: Paper, url: 'https://arxiv.org/abs/1906.02467'}]
tags: ["AAAI", "Datasets"]
short_authors: Yu et al.
---
Recent developments in modeling language and vision have been successfully
applied to image question answering. It is both crucial and natural to extend
this research direction to the video domain for video question answering
(VideoQA). Compared to the image domain where large scale and fully annotated
benchmark datasets exists, VideoQA datasets are limited to small scale and are
automatically generated, etc. These limitations restrict their applicability in
practice. Here we introduce ActivityNet-QA, a fully annotated and large scale
VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web
videos derived from the popular ActivityNet dataset. We present a statistical
analysis of our ActivityNet-QA dataset and conduct extensive experiments on it
by comparing existing VideoQA baselines. Moreover, we explore various video
representation strategies to improve VideoQA performance, especially for long
videos. The dataset is available at https://github.com/MILVLG/activitynet-qa