---
layout: publication
title: Knowledge-grounded Dialogue Generation With Pre-trained Language Models
authors: Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan
conference: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
  Processing (EMNLP)
year: 2020
bibkey: zhao2020knowledge
citations: 149
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2010.08824'}]
tags: ["EMNLP"]
short_authors: Zhao et al.
---
We study knowledge-grounded dialogue generation with pre-trained language
models. To leverage the redundant external knowledge under capacity constraint,
we propose equipping response generation defined by a pre-trained language
model with a knowledge selection module, and an unsupervised approach to
jointly optimizing knowledge selection and response generation with unlabeled
dialogues. Empirical results on two benchmarks indicate that our model can
significantly outperform state-of-the-art methods in both automatic evaluation
and human judgment.