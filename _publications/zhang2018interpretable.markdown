---
layout: publication
title: Interpretable Visual Question Answering By Visual Grounding From Attention
  Supervision Mining
authors: Yundong Zhang, Juan Carlos Niebles, Alvaro Soto
conference: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV)
year: 2019
bibkey: zhang2018interpretable
citations: 68
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1808.00265'}]
tags: ["Applications", "Model Architecture"]
short_authors: Yundong Zhang, Juan Carlos Niebles, Alvaro Soto
---
A key aspect of VQA models that are interpretable is their ability to ground
their answers to relevant regions in the image. Current approaches with this
capability rely on supervised learning and human annotated groundings to train
attention mechanisms inside the VQA architecture. Unfortunately, obtaining
human annotations specific for visual grounding is difficult and expensive. In
this work, we demonstrate that we can effectively train a VQA architecture with
grounding supervision that can be automatically obtained from available region
descriptions and object annotations. We also show that our model trained with
this mined supervision generates visual groundings that achieve a higher
correlation with respect to manually-annotated groundings, meanwhile achieving
state-of-the-art VQA accuracy.