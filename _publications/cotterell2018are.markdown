---
layout: publication
title: Are All Languages Equally Hard To Language-model?
authors: Ryan Cotterell, Sabrina J. Mielke, Jason Eisner, Brian Roark
conference: 'Proceedings of the 2018 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies, Volume 2
  (Short Papers)'
year: 2018
bibkey: cotterell2018are
citations: 95
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1806.03743'}]
tags: ["NAACL"]
short_authors: Cotterell et al.
---
For general modeling methods applied to diverse languages, a natural question
is: how well should we expect our models to work on languages with differing
typological profiles? In this work, we develop an evaluation framework for fair
cross-linguistic comparison of language models, using translated text so that
all models are asked to predict approximately the same information. We then
conduct a study on 21 languages, demonstrating that in some languages, the
textual expression of the information is harder to predict with both \\(n\\)-gram
and LSTM language models. We show complex inflectional morphology to be a cause
of performance differences among languages.