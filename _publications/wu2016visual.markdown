---
layout: publication
title: 'Visual Question Answering: A Survey Of Methods And Datasets'
authors: Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, Anton van Den
  Hengel
conference: Computer Vision and Image Understanding
year: 2017
bibkey: wu2016visual
citations: 401
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1607.05910'}]
tags: ["Datasets", "Survey Paper"]
short_authors: Wu et al.
---
Visual Question Answering (VQA) is a challenging task that has received
increasing attention from both the computer vision and the natural language
processing communities. Given an image and a question in natural language, it
requires reasoning over visual elements of the image and general knowledge to
infer the correct answer. In the first part of this survey, we examine the
state of the art by comparing modern approaches to the problem. We classify
methods by their mechanism to connect the visual and textual modalities. In
particular, we examine the common approach of combining convolutional and
recurrent neural networks to map images and questions to a common feature
space. We also discuss memory-augmented and modular architectures that
interface with structured knowledge bases. In the second part of this survey,
we review the datasets available for training and evaluating VQA systems. The
various datatsets contain questions at different levels of complexity, which
require different capabilities and types of reasoning. We examine in depth the
question/answer pairs from the Visual Genome project, and evaluate the
relevance of the structured annotations of images with scene graphs for VQA.
Finally, we discuss promising future directions for the field, in particular
the connection to structured knowledge bases and the use of natural language
processing models.