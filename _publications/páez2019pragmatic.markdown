---
layout: publication
title: The Pragmatic Turn In Explainable Artificial Intelligence (XAI)
authors: "Andr\xE9s P\xE1ez"
conference: Minds and Machines
year: 2019
bibkey: "p\xE1ez2019pragmatic"
citations: 85
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2002.09595'}]
tags: ["Agentic"]
short_authors: "Andr\xE9s P\xE1ez"
---
In this paper I argue that the search for explainable models and
interpretable decisions in AI must be reformulated in terms of the broader
project of offering a pragmatic and naturalistic account of understanding in
AI. Intuitively, the purpose of providing an explanation of a model or a
decision is to make it understandable to its stakeholders. But without a
previous grasp of what it means to say that an agent understands a model or a
decision, the explanatory strategies will lack a well-defined goal. Aside from
providing a clearer objective for XAI, focusing on understanding also allows us
to relax the factivity condition on explanation, which is impossible to fulfill
in many machine learning models, and to focus instead on the pragmatic
conditions that determine the best fit between a model and the methods and
devices deployed to understand it. After an examination of the different types
of understanding discussed in the philosophical and psychological literature, I
conclude that interpretative or approximation models not only provide the best
way to achieve the objectual understanding of a machine learning model, but are
also a necessary condition to achieve post-hoc interpretability. This
conclusion is partly based on the shortcomings of the purely functionalist
approach to post-hoc interpretability that seems to be predominant in most
recent literature.