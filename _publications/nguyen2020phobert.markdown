---
layout: publication
title: 'Phobert: Pre-trained Language Models For Vietnamese'
authors: Dat Quoc Nguyen, Anh Tuan Nguyen
conference: 'Findings of the Association for Computational Linguistics: EMNLP 2020'
year: 2020
bibkey: nguyen2020phobert
citations: 310
additional_links: [{name: Code, url: 'https://github.com/VinAIResearch/PhoBERT'},
  {name: Paper, url: 'https://arxiv.org/abs/2003.00744'}]
tags: ["EMNLP", "Has Code"]
short_authors: Dat Quoc Nguyen, Anh Tuan Nguyen
---
We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the
first public large-scale monolingual language models pre-trained for
Vietnamese. Experimental results show that PhoBERT consistently outperforms the
recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and
improves the state-of-the-art in multiple Vietnamese-specific NLP tasks
including Part-of-speech tagging, Dependency parsing, Named-entity recognition
and Natural language inference. We release PhoBERT to facilitate future
research and downstream applications for Vietnamese NLP. Our PhoBERT models are
available at https://github.com/VinAIResearch/PhoBERT