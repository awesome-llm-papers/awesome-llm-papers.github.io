---
layout: publication
title: 'Llama: Open And Efficient Foundation Language Models'
authors: [hugo Touvron, thibaut Lavril, gautier Izacard, xavier Martinet, marie-anne
    Lachaux, "timoth\xE9e Lacroix", "baptiste Rozi\xE8re", naman Goyal, eric Hambro,
  faisal Azhar, aurelien Rodriguez, armand Joulin, edouard Grave, guillaume Lample]
conference: Arxiv
year: 2023
bibkey: hugo2023llama
citations: 1805
additional_links: [{name: Paper, url: 'http://arxiv.org/abs/2302.13971v1'}]
tags: ["Model Architecture"]
short_authors: Touvron et al.
---
We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.