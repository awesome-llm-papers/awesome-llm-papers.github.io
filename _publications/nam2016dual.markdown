---
layout: publication
title: Dual Attention Networks For Multimodal Reasoning And Matching
authors: Hyeonseob Nam, Jung-woo Ha, Jeonghee Kim
conference: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
bibkey: nam2016dual
citations: 732
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1611.00471'}]
tags: ["CVPR", "Model Architecture"]
short_authors: Hyeonseob Nam, Jung-woo Ha, Jeonghee Kim
---
We propose Dual Attention Networks (DANs) which jointly leverage visual and
textual attention mechanisms to capture fine-grained interplay between vision
and language. DANs attend to specific regions in images and words in text
through multiple steps and gather essential information from both modalities.
Based on this framework, we introduce two types of DANs for multimodal
reasoning and matching, respectively. The reasoning model allows visual and
textual attentions to steer each other during collaborative inference, which is
useful for tasks such as Visual Question Answering (VQA). In addition, the
matching model exploits the two attention mechanisms to estimate the similarity
between images and sentences by focusing on their shared semantics. Our
extensive experiments validate the effectiveness of DANs in combining vision
and language, achieving the state-of-the-art performance on public benchmarks
for VQA and image-text matching.