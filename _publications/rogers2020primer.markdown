---
layout: publication
title: 'A Primer In Bertology: What We Know About How BERT Works'
authors: Anna Rogers, Olga Kovaleva, Anna Rumshisky
conference: Transactions of the Association for Computational Linguistics
year: 2020
bibkey: rogers2020primer
citations: 1280
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2002.12327'}]
tags: ["Model Architecture", "Survey Paper", "TACL"]
short_authors: Anna Rogers, Olga Kovaleva, Anna Rumshisky
---
Transformer-based models have pushed state of the art in many areas of NLP,
but our understanding of what is behind their success is still limited. This
paper is the first survey of over 150 studies of the popular BERT model. We
review the current state of knowledge about how BERT works, what kind of
information it learns and how it is represented, common modifications to its
training objectives and architecture, the overparameterization issue and
approaches to compression. We then outline directions for future research.