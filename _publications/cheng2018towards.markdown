---
layout: publication
title: Towards Robust Neural Machine Translation
authors: Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai, Yang Liu
conference: 'Proceedings of the 56th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2018
bibkey: cheng2018towards
citations: 182
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1805.06130'}]
tags: ["Training Techniques"]
short_authors: Cheng et al.
---
Small perturbations in the input can severely distort intermediate
representations and thus impact translation quality of neural machine
translation (NMT) models. In this paper, we propose to improve the robustness
of NMT models with adversarial stability training. The basic idea is to make
both the encoder and decoder in NMT models robust against input perturbations
by enabling them to behave similarly for the original input and its perturbed
counterpart. Experimental results on Chinese-English, English-German and
English-French translation tasks show that our approaches can not only achieve
significant improvements over strong NMT systems but also improve the
robustness of NMT models.