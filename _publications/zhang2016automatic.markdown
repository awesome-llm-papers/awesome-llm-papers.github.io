---
layout: publication
title: Automatic Generation Of Grounded Visual Questions
authors: Shijie Zhang, Lizhen Qu, Shaodi You, Zhenglu Yang, Jiawan Zhang
conference: Proceedings of the Twenty-Sixth International Joint Conference on Artificial
  Intelligence
year: 2017
bibkey: zhang2016automatic
citations: 72
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1612.06530'}]
tags: ["IJCAI"]
short_authors: Zhang et al.
---
In this paper, we propose the first model to be able to generate visually
grounded questions with diverse types for a single image. Visual question
generation is an emerging topic which aims to ask questions in natural language
based on visual input. To the best of our knowledge, it lacks automatic methods
to generate meaningful questions with various types for the same visual input.
To circumvent the problem, we propose a model that automatically generates
visually grounded questions with varying types. Our model takes as input both
images and the captions generated by a dense caption model, samples the most
probable question types, and generates the questions in sequel. The
experimental results on two real world datasets show that our model outperforms
the strongest baseline in terms of both correctness and diversity with a wide
margin.