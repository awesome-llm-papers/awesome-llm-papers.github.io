---
layout: publication
title: 'Harnessing The Power Of Llms In Practice: A Survey On Chatgpt And Beyond'
authors: Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming
  Jiang, Bing Yin, Xia Hu
conference: ACM Transactions on Knowledge Discovery from Data
year: 2024
bibkey: yang2023harnessing
citations: 221
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2304.13712'}]
tags: ["Applications", "Survey Paper"]
short_authors: Yang et al.
---
This paper presents a comprehensive and practical guide for practitioners and
end-users working with Large Language Models (LLMs) in their downstream natural
language processing (NLP) tasks. We provide discussions and insights into the
usage of LLMs from the perspectives of models, data, and downstream tasks.
Firstly, we offer an introduction and brief summary of current GPT- and
BERT-style LLMs. Then, we discuss the influence of pre-training data, training
data, and test data. Most importantly, we provide a detailed discussion about
the use and non-use cases of large language models for various natural language
processing tasks, such as knowledge-intensive tasks, traditional natural
language understanding tasks, natural language generation tasks, emergent
abilities, and considerations for specific tasks.We present various use cases
and non-use cases to illustrate the practical applications and limitations of
LLMs in real-world scenarios. We also try to understand the importance of data
and the specific challenges associated with each NLP task. Furthermore, we
explore the impact of spurious biases on LLMs and delve into other essential
considerations, such as efficiency, cost, and latency, to ensure a
comprehensive understanding of deploying LLMs in practice. This comprehensive
guide aims to provide researchers and practitioners with valuable insights and
best practices for working with LLMs, thereby enabling the successful
implementation of these models in a wide range of NLP tasks. A curated list of
practical guide resources of LLMs, regularly updated, can be found at
https://github.com/Mooler0410/LLMsPracticalGuide.