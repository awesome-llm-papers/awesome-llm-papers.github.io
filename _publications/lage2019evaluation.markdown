---
layout: publication
title: An Evaluation Of The Human-interpretability Of Explanation
authors: Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman,
  Finale Doshi-velez
conference: Arxiv
year: 2019
bibkey: lage2019evaluation
citations: 125
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1902.00006'}]
tags: ["Evaluation"]
short_authors: Lage et al.
---
Recent years have seen a boom in interest in machine learning systems that
can provide a human-understandable rationale for their predictions or
decisions. However, exactly what kinds of explanation are truly
human-interpretable remains poorly understood. This work advances our
understanding of what makes explanations interpretable under three specific
tasks that users may perform with machine learning systems: simulation of the
response, verification of a suggested response, and determining whether the
correctness of a suggested response changes under a change to the inputs.
Through carefully controlled human-subject experiments, we identify
regularizers that can be used to optimize for the interpretability of machine
learning systems. Our results show that the type of complexity matters:
cognitive chunks (newly defined concepts) affect performance more than variable
repetitions, and these trends are consistent across tasks and domains. This
suggests that there may exist some common design principles for explanation
systems.