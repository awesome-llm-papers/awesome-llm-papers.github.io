---
layout: publication
title: Language To Logical Form With Neural Attention
authors: Li Dong, Mirella Lapata
conference: 'Proceedings of the 54th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2016
bibkey: dong2016language
citations: 667
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1601.01280'}]
tags: ["Datasets"]
short_authors: Li Dong, Mirella Lapata
---
Semantic parsing aims at mapping natural language to machine interpretable
meaning representations. Traditional approaches rely on high-quality lexicons,
manually-built templates, and linguistic features which are either domain- or
representation-specific. In this paper we present a general method based on an
attention-enhanced encoder-decoder model. We encode input utterances into
vector representations, and generate their logical forms by conditioning the
output sequences or trees on the encoding vectors. Experimental results on four
datasets show that our approach performs competitively without using
hand-engineered features and is easy to adapt across domains and meaning
representations.