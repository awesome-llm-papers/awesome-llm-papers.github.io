---
layout: publication
title: 'Large Language Models Meet Nl2code: A Survey'
authors: Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan,
  Yongji Wang, Jian-guang Lou
conference: 'Proceedings of the 61st Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2023
bibkey: zan2022large
citations: 60
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2212.09420'}]
tags: ["Llm For Code", "Survey Paper"]
short_authors: Zan et al.
---
The task of generating code from a natural language description, or NL2Code,
is considered a pressing and significant challenge in code intelligence. Thanks
to the rapid development of pre-training techniques, surging large language
models are being proposed for code, sparking the advances in NL2Code. To
facilitate further research and applications in this field, in this paper, we
present a comprehensive survey of 27 existing large language models for
NL2Code, and also review benchmarks and metrics. We provide an intuitive
comparison of all existing models on the HumanEval benchmark. Through in-depth
observation and analysis, we provide some insights and conclude that the key
factors contributing to the success of large language models for NL2Code are
"Large Size, Premium Data, Expert Tuning". In addition, we discuss challenges
and opportunities regarding the gap between models and humans. We also create a
website https://nl2code.github.io to track the latest progress through
crowd-sourcing. To the best of our knowledge, this is the first survey of large
language models for NL2Code, and we believe it will contribute to the ongoing
development of the field.