---
layout: publication
title: Context And Attribute Grounded Dense Captioning
authors: Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, Jing Shao
conference: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2019
bibkey: yin2019context
citations: 61
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1904.01410'}]
tags: ["CVPR", "Datasets"]
short_authors: Yin et al.
---
Dense captioning aims at simultaneously localizing semantic regions and
describing these regions-of-interest (ROIs) with short phrases or sentences in
natural language. Previous studies have shown remarkable progresses, but they
are often vulnerable to the aperture problem that a caption generated by the
features inside one ROI lacks contextual coherence with its surrounding context
in the input image. In this work, we investigate contextual reasoning based on
multi-scale message propagations from the neighboring contents to the target
ROIs. To this end, we design a novel end-to-end context and attribute grounded
dense captioning framework consisting of 1) a contextual visual mining module
and 2) a multi-level attribute grounded description generation module. Knowing
that captions often co-occur with the linguistic attributes (such as who, what
and where), we also incorporate an auxiliary supervision from hierarchical
linguistic attributes to augment the distinctiveness of the learned captions.
Extensive experiments and ablation studies on Visual Genome dataset demonstrate
the superiority of the proposed model in comparison to state-of-the-art
methods.