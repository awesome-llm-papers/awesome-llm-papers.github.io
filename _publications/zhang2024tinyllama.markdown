---
layout: publication
title: 'Tinyllama: An Open-source Small Language Model'
authors: Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu
conference: No Venue
year: 2024
bibkey: zhang2024tinyllama
additional_links: [{name: Code, url: 'https://github.com/jzhang38/TinyLlama'}, {name: Code,
    url: 'https://huggingface.co/discussions/paper/6597627202a265cc802c016c'}, {name: Paper,
    url: 'https://arxiv.org/abs/hf2401.02385'}]
tags: ["Efficiency", "Has Code", "Model Architecture"]
short_authors: Zhang et al.
---
We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.

https://huggingface.co/discussions/paper/6597627202a265cc802c016c