---
layout: publication
title: Exploiting BERT For Multimodal Target Sentiment Classification Through Input
  Space Translation
authors: Zaid Khan, Yun Fu
conference: Proceedings of the 29th ACM International Conference on Multimedia
year: 2021
bibkey: khan2021exploiting
citations: 111
additional_links: [{name: Code, url: 'https://github.com/codezakh/exploiting-BERT-thru-translation\'},
  {name: Paper, url: 'https://arxiv.org/abs/2108.01682'}]
tags: ["Model Architecture"]
short_authors: Zaid Khan, Yun Fu
---
Multimodal target/aspect sentiment classification combines multimodal
sentiment analysis and aspect/target sentiment classification. The goal of the
task is to combine vision and language to understand the sentiment towards a
target entity in a sentence. Twitter is an ideal setting for the task because
it is inherently multimodal, highly emotional, and affects real world events.
However, multimodal tweets are short and accompanied by complex, possibly
irrelevant images. We introduce a two-stream model that translates images in
input space using an object-aware transformer followed by a single-pass
non-autoregressive text generation approach. We then leverage the translation
to construct an auxiliary sentence that provides multimodal information to a
language model. Our approach increases the amount of text available to the
language model and distills the object-level information in complex images. We
achieve state-of-the-art performance on two multimodal Twitter datasets without
modifying the internals of the language model to accept multimodal data,
demonstrating the effectiveness of our translation. In addition, we explain a
failure mode of a popular approach for aspect sentiment analysis when applied
to tweets. Our code is available at
\textcolor\{blue\}\{https://github.com/codezakh/exploiting-BERT-thru-translation\}.