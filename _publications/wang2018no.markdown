---
layout: publication
title: 'No Metrics Are Perfect: Adversarial Reward Learning For Visual Storytelling'
authors: Xin Wang, Wenhu Chen, Yuan-fang Wang, William Yang Wang
conference: 'Proceedings of the 56th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2018
bibkey: wang2018no
citations: 178
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1804.09160'}]
tags: ["Evaluation", "Reinforcement Learning"]
short_authors: Wang et al.
---
Though impressive results have been achieved in visual captioning, the task
of generating abstract stories from photo streams is still a little-tapped
problem. Different from captions, stories have more expressive language styles
and contain many imaginary concepts that do not appear in the images. Thus it
poses challenges to behavioral cloning algorithms. Furthermore, due to the
limitations of automatic metrics on evaluating story quality, reinforcement
learning methods with hand-crafted rewards also face difficulties in gaining an
overall performance boost. Therefore, we propose an Adversarial REward Learning
(AREL) framework to learn an implicit reward function from human
demonstrations, and then optimize policy search with the learned reward
function. Though automatic eval- uation indicates slight performance boost over
state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation
shows that our approach achieves significant improvement in generating more
human-like stories than SOTA systems.