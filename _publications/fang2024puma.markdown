---
layout: publication
title: 'PUMA: Empowering Unified MLLM With Multi-granular Visual Generation'
authors: Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui
  Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu
conference: No Venue
year: 2024
bibkey: fang2024puma
additional_links: [{name: Code, url: 'https://github.com/rongyaofang/PUMA'}, {name: Code,
    url: 'https://huggingface.co/discussions/paper/671715667892af397b9a94ab'}, {name: Paper,
    url: 'https://arxiv.org/abs/hf2410.13861'}]
tags: ["Has Code", "Tools"]
short_authors: Fang et al.
---
Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.

https://huggingface.co/discussions/paper/671715667892af397b9a94ab