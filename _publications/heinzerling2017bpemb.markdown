---
layout: publication
title: 'Bpemb: Tokenization-free Pre-trained Subword Embeddings In 275 Languages'
authors: Benjamin Heinzerling, Michael Strube
conference: Arxiv
year: 2017
bibkey: heinzerling2017bpemb
citations: 129
additional_links: [{name: Code, url: 'https://github.com/bheinzerling/bpemb'}, {name: Paper,
    url: 'https://arxiv.org/abs/1710.02187'}]
tags: ["Evaluation", "Has Code"]
short_authors: Benjamin Heinzerling, Michael Strube
---
We present BPEmb, a collection of pre-trained subword unit embeddings in 275
languages, based on Byte-Pair Encoding (BPE). In an evaluation using
fine-grained entity typing as testbed, BPEmb performs competitively, and for
some languages bet- ter than alternative subword approaches, while requiring
vastly fewer resources and no tokenization. BPEmb is available at
https://github.com/bheinzerling/bpemb