---
layout: publication
title: 'Llava-plus: Learning To Use Tools For Creating Multimodal Agents'
authors: Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan
  Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li
conference: No Venue
year: 2023
bibkey: liu2023llava
additional_links: [{name: Code, url: 'https://huggingface.co/discussions/paper/654db59df5297ada0b9cd405'},
  {name: Paper, url: 'https://arxiv.org/abs/hf2311.05437'}]
tags: ["Has Code", "Tools"]
short_authors: Liu et al.
---
LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in existing capabilities and exhibits new ones. It is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.

https://huggingface.co/discussions/paper/654db59df5297ada0b9cd405