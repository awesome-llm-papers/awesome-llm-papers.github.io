---
layout: publication
title: Pretraining-based Natural Language Generation For Text Summarization
authors: Haoyu Zhang, Jianjun Xu, Ji Wang
conference: Proceedings of the 23rd Conference on Computational Natural Language Learning
  (CoNLL)
year: 2019
bibkey: zhang2019pretraining
citations: 183
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1902.09243'}]
tags: ["Datasets", "Model Architecture"]
short_authors: Haoyu Zhang, Jianjun Xu, Ji Wang
---
In this paper, we propose a novel pretraining-based encoder-decoder
framework, which can generate the output sequence based on the input sequence
in a two-stage manner. For the encoder of our model, we encode the input
sequence into context representations using BERT. For the decoder, there are
two stages in our model, in the first stage, we use a Transformer-based decoder
to generate a draft output sequence. In the second stage, we mask each word of
the draft sequence and feed it to BERT, then by combining the input sequence
and the draft representation generated by BERT, we use a Transformer-based
decoder to predict the refined word for each masked position. To the best of
our knowledge, our approach is the first method which applies the BERT into
text generation tasks. As the first step in this direction, we evaluate our
proposed method on the text summarization task. Experimental results show that
our model achieves new state-of-the-art on both CNN/Daily Mail and New York
Times datasets.