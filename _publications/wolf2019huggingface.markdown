---
layout: publication
title: 'Huggingface''s Transformers: State-of-the-art Natural Language Processing'
authors: "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\
  \ Anthony Moi, Pierric Cistac, Tim Rault, R\xE9mi Louf, Morgan Funtowicz, Joe Davison,\
  \ Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen\
  \ Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M.\
  \ Rush"
conference: Arxiv
year: 2019
bibkey: wolf2019huggingface
citations: 3228
additional_links: [{name: Code, url: 'https://github.com/huggingface/transformers'},
  {name: Paper, url: 'https://arxiv.org/abs/1910.03771'}]
tags: ["Applications", "Model Architecture", "Tools"]
short_authors: Wolf et al.
---
Recent progress in natural language processing has been driven by advances in
both model architecture and model pretraining. Transformer architectures have
facilitated building higher-capacity models and pretraining has made it
possible to effectively utilize this capacity for a wide variety of tasks.
\textit\{Transformers\} is an open-source library with the goal of opening up
these advances to the wider machine learning community. The library consists of
carefully engineered state-of-the art Transformer architectures under a unified
API. Backing this library is a curated collection of pretrained models made by
and available for the community. \textit\{Transformers\} is designed to be
extensible by researchers, simple for practitioners, and fast and robust in
industrial deployments. The library is available at
https://github.com/huggingface/transformers.