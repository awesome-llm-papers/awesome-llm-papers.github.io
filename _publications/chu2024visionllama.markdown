---
layout: publication
title: 'Visionllama: A Unified Llama Interface For Vision Tasks'
authors: Xiangxiang Chu, Jianlin Su, Bo Zhang, Chunhua Shen
conference: No Venue
year: 2024
bibkey: chu2024visionllama
additional_links: [{name: Code, url: 'https://github.com/Meituan-AutoML/VisionLLaMA'},
  {name: Code, url: 'https://huggingface.co/discussions/paper/65e55aec521cc7807343e37b'},
  {name: Paper, url: 'https://arxiv.org/abs/hf2403.00522'}]
tags: ["Model Architecture", "Tools"]
short_authors: Chu et al.
---
Large language models are built on top of a transformer-based architecture to process textual inputs. For example, the LLaMA stands out among many open-source implementations. Can the same transformer be used to process 2D images? In this paper, we answer this question by unveiling a LLaMA-like vision transformer in plain and pyramid forms, termed VisionLLaMA, which is tailored for this purpose. VisionLLaMA is a unified and generic modelling framework for solving most vision tasks. We extensively evaluate its effectiveness using typical pre-training paradigms in a good portion of downstream tasks of image perception and especially image generation. In many cases, VisionLLaMA have exhibited substantial gains over the previous state-of-the-art vision transformers. We believe that VisionLLaMA can serve as a strong new baseline model for vision generation and understanding. Our code will be released at https://github.com/Meituan-AutoML/VisionLLaMA.

https://huggingface.co/discussions/paper/65e55aec521cc7807343e37b