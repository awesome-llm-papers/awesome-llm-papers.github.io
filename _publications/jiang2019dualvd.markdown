---
layout: publication
title: 'Dualvd: An Adaptive Dual Encoding Model For Deep Visual Understanding In Visual
  Dialogue'
authors: Xiaoze Jiang, Jing Yu, Zengchang Qin, Yingying Zhuang, Xingxing Zhang, Yue
  Hu, Qi Wu
conference: Proceedings of the AAAI Conference on Artificial Intelligence
year: 2020
bibkey: jiang2019dualvd
citations: 70
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1911.07251'}]
tags: ["AAAI", "Tools"]
short_authors: Jiang et al.
---
Different from Visual Question Answering task that requires to answer only
one question about an image, Visual Dialogue involves multiple questions which
cover a broad range of visual content that could be related to any objects,
relationships or semantics. The key challenge in Visual Dialogue task is thus
to learn a more comprehensive and semantic-rich image representation which may
have adaptive attentions on the image for variant questions. In this research,
we propose a novel model to depict an image from both visual and semantic
perspectives. Specifically, the visual view helps capture the appearance-level
information, including objects and their relationships, while the semantic view
enables the agent to understand high-level visual semantics from the whole
image to the local regions. Futhermore, on top of such multi-view image
features, we propose a feature selection framework which is able to adaptively
capture question-relevant information hierarchically in fine-grained level. The
proposed method achieved state-of-the-art results on benchmark Visual Dialogue
datasets. More importantly, we can tell which modality (visual or semantic) has
more contribution in answering the current question by visualizing the gate
values. It gives us insights in understanding of human cognition in Visual
Dialogue.