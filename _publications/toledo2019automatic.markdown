---
layout: publication
title: Automatic Argument Quality Assessment -- New Datasets And Methods
authors: Assaf Toledo, Shai Gretz, Edo Cohen-karlik, Roni Friedman, Elad Venezian,
  Dan Lahav, Michal Jacovi, Ranit Aharonov, Noam Slonim
conference: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
  Processing and the 9th International Joint Conference on Natural Language Processing
  (EMNLP-IJCNLP)
year: 2019
bibkey: toledo2019automatic
citations: 69
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1909.01007'}]
tags: ["Datasets", "EMNLP"]
short_authors: Toledo et al.
---
We explore the task of automatic assessment of argument quality. To that end,
we actively collected 6.3k arguments, more than a factor of five compared to
previously examined data. Each argument was explicitly and carefully annotated
for its quality. In addition, 14k pairs of arguments were annotated
independently, identifying the higher quality argument in each pair. In spite
of the inherent subjective nature of the task, both annotation schemes led to
surprisingly consistent results. We release the labeled datasets to the
community. Furthermore, we suggest neural methods based on a recently released
language model, for argument ranking as well as for argument-pair
classification. In the former task, our results are comparable to
state-of-the-art; in the latter task our results significantly outperform
earlier methods.