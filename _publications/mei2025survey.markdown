---
layout: publication
title: A Survey Of Context Engineering For Large Language Models
authors: Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi
  Liu, Mingyu Li, Zhong-zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia,
  Jiafeng Guo, Shenghua Liu
conference: No Venue
year: 2025
bibkey: mei2025survey
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/hf2507.13334'}]
tags: ["Survey Paper"]
short_authors: Mei et al.
---
The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.

https://huggingface.co/discussions/paper/6879aad021b37e676c8e407a