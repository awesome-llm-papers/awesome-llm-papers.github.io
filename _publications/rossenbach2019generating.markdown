---
layout: publication
title: Generating Synthetic Audio Data For Attention-based Speech Recognition Systems
authors: "Nick Rossenbach, Albert Zeyer, Ralf Schl\xFCter, Hermann Ney"
conference: ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)
year: 2020
bibkey: rossenbach2019generating
citations: 66
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1912.09257'}]
tags: ["ICASSP", "Model Architecture"]
short_authors: Rossenbach et al.
---
Recent advances in text-to-speech (TTS) led to the development of flexible
multi-speaker end-to-end TTS systems. We extend state-of-the-art
attention-based automatic speech recognition (ASR) systems with synthetic audio
generated by a TTS system trained only on the ASR corpora itself. ASR and TTS
systems are built separately to show that text-only data can be used to enhance
existing end-to-end ASR systems without the necessity of parameter or
architecture changes. We compare our method with language model integration of
the same text data and with simple data augmentation methods like SpecAugment
and show that performance improvements are mostly independent. We achieve
improvements of up to 33% relative in word-error-rate (WER) over a strong
baseline with data-augmentation in a low-resource environment
(LibriSpeech-100h), closing the gap to a comparable oracle experiment by more
than 50%. We also show improvements of up to 5% relative WER over our most
recent ASR baseline on LibriSpeech-960h.