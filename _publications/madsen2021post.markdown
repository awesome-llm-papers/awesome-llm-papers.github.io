---
layout: publication
title: 'Post-hoc Interpretability For Neural NLP: A Survey'
authors: Andreas Madsen, Siva Reddy, Sarath Chandar
conference: ACM Computing Surveys
year: 2022
bibkey: madsen2021post
citations: 131
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2108.04840'}]
tags: ["Survey Paper"]
short_authors: Andreas Madsen, Siva Reddy, Sarath Chandar
---
Neural networks for NLP are becoming increasingly complex and widespread, and
there is a growing concern if these models are responsible to use. Explaining
models helps to address the safety and ethical concerns and is essential for
accountability. Interpretability serves to provide these explanations in terms
that are understandable to humans. Additionally, post-hoc methods provide
explanations after a model is learned and are generally model-agnostic. This
survey provides a categorization of how recent post-hoc interpretability
methods communicate explanations to humans, it discusses each method in-depth,
and how they are validated, as the latter is often a common concern.