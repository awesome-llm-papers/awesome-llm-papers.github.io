---
layout: publication
title: Multi-task Learning For Argumentation Mining In Low-resource Settings
authors: Claudia Schulz, Steffen Eger, Johannes Daxenberger, Tobias Kahse, Iryna Gurevych
conference: 'Proceedings of the 2018 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies, Volume 2
  (Short Papers)'
year: 2018
bibkey: schulz2018multi
citations: 65
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1804.04083'}]
tags: ["Datasets", "Training Techniques"]
short_authors: Schulz et al.
---
We investigate whether and where multi-task learning (MTL) can improve
performance on NLP problems related to argumentation mining (AM), in particular
argument component identification. Our results show that MTL performs
particularly well (and better than single-task learning) when little training
data is available for the main task, a common scenario in AM. Our findings
challenge previous assumptions that conceptualizations across AM datasets are
divergent and that MTL is difficult for semantic or higher-level tasks.