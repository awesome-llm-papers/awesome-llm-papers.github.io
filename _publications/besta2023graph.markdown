---
layout: publication
title: 'Graph Of Thoughts: Solving Elaborate Problems With Large Language Models'
authors: Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,
  Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk,
  Torsten Hoefler
conference: Proceedings of the AAAI Conference on Artificial Intelligence
year: 2024
bibkey: besta2023graph
citations: 154
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2308.09687'}]
tags: ["AAAI", "Tools"]
short_authors: Besta et al.
---
We introduce Graph of Thoughts (GoT): a framework that advances prompting
capabilities in large language models (LLMs) beyond those offered by paradigms
such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary
advantage of GoT is the ability to model the information generated by an LLM as
an arbitrary graph, where units of information ("LLM thoughts") are vertices,
and edges correspond to dependencies between these vertices. This approach
enables combining arbitrary LLM thoughts into synergistic outcomes, distilling
the essence of whole networks of thoughts, or enhancing thoughts using feedback
loops. We illustrate that GoT offers advantages over state of the art on
different tasks, for example increasing the quality of sorting by 62% over ToT,
while simultaneously reducing costs by >31%. We ensure that GoT is extensible
with new thought transformations and thus can be used to spearhead new
prompting schemes. This work brings the LLM reasoning closer to human thinking
or brain mechanisms such as recurrence, both of which form complex networks.