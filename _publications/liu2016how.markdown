---
layout: publication
title: 'How NOT To Evaluate Your Dialogue System: An Empirical Study Of Unsupervised
  Evaluation Metrics For Dialogue Response Generation'
authors: Chia-wei Liu, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin,
  Joelle Pineau
conference: Proceedings of the 2016 Conference on Empirical Methods in Natural Language
  Processing
year: 2016
bibkey: liu2016how
citations: 954
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1603.08023'}]
tags: ["EMNLP", "Evaluation"]
short_authors: Liu et al.
---
We investigate evaluation metrics for dialogue response generation systems
where supervised labels, such as task completion, are not available. Recent
works in response generation have adopted metrics from machine translation to
compare a model's generated response to a single target response. We show that
these metrics correlate very weakly with human judgements in the non-technical
Twitter domain, and not at all in the technical Ubuntu domain. We provide
quantitative and qualitative results highlighting specific weaknesses in
existing metrics, and provide recommendations for future development of better
automatic evaluation metrics for dialogue systems.