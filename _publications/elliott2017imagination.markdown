---
layout: publication
title: Imagination Improves Multimodal Translation
authors: "Desmond Elliott, \xC1kos K\xE1d\xE1r"
conference: Arxiv
year: 2017
bibkey: elliott2017imagination
citations: 91
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1705.04350'}]
tags: ["Datasets", "Tools"]
short_authors: "Desmond Elliott, \xC1kos K\xE1d\xE1r"
---
We decompose multimodal translation into two sub-tasks: learning to translate
and learning visually grounded representations. In a multitask learning
framework, translations are learned in an attention-based encoder-decoder, and
grounded representations are learned through image representation prediction.
Our approach improves translation performance compared to the state of the art
on the Multi30K dataset. Furthermore, it is equally effective if we train the
image prediction task on the external MS COCO dataset, and we find improvements
if we train the translation model on the external News Commentary parallel
text.