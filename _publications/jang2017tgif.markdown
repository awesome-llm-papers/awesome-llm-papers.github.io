---
layout: publication
title: 'TGIF-QA: Toward Spatio-temporal Reasoning In Visual Question Answering'
authors: Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim
conference: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
bibkey: jang2017tgif
citations: 417
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1704.04497'}]
tags: ["CVPR", "Datasets", "Model Architecture"]
short_authors: Jang et al.
---
Vision and language understanding has emerged as a subject undergoing intense
study in Artificial Intelligence. Among many tasks in this line of research,
visual question answering (VQA) has been one of the most successful ones, where
the goal is to learn a model that understands visual content at region-level
details and finds their associations with pairs of questions and answers in the
natural language form. Despite the rapid progress in the past few years, most
existing work in VQA have focused primarily on images. In this paper, we focus
on extending VQA to the video domain and contribute to the literature in three
important ways. First, we propose three new tasks designed specifically for
video VQA, which require spatio-temporal reasoning from videos to answer
questions correctly. Next, we introduce a new large-scale dataset for video VQA
named TGIF-QA that extends existing VQA work with our new tasks. Finally, we
propose a dual-LSTM based approach with both spatial and temporal attention,
and show its effectiveness over conventional VQA techniques through empirical
evaluations.