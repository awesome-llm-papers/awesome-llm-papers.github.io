---
layout: publication
title: 'Interpreting Recurrent And Attention-based Neural Models: A Case Study On
  Natural Language Inference'
authors: Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli
conference: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
  Processing
year: 2018
bibkey: ghaeini2018interpreting
citations: 90
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1808.03894'}]
tags: ["EMNLP", "Model Architecture"]
short_authors: Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli
---
Deep learning models have achieved remarkable success in natural language
inference (NLI) tasks. While these models are widely explored, they are hard to
interpret and it is often unclear how and why they actually work. In this
paper, we take a step toward explaining such deep learning based models through
a case study on a popular neural model for NLI. In particular, we propose to
interpret the intermediate layers of NLI models by visualizing the saliency of
attention and LSTM gating signals. We present several examples for which our
methods are able to reveal interesting insights and identify the critical
information contributing to the model decisions.