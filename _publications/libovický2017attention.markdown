---
layout: publication
title: Attention Strategies For Multi-source Sequence-to-sequence Learning
authors: "Jind\u0159ich Libovick\xFD, Jind\u0159ich Helcl"
conference: 'Proceedings of the 55th Annual Meeting of the Association for Computational
  Linguistics (Volume 2: Short Papers)'
year: 2017
bibkey: "libovick\xFD2017attention"
citations: 175
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1704.06567'}]
tags: ["Model Architecture"]
short_authors: "Jind\u0159ich Libovick\xFD, Jind\u0159ich Helcl"
---
Modeling attention in neural multi-source sequence-to-sequence learning
remains a relatively unexplored area, despite its usefulness in tasks that
incorporate multiple source languages or modalities. We propose two novel
approaches to combine the outputs of attention mechanisms over each source
sequence, flat and hierarchical. We compare the proposed methods with existing
techniques and present results of systematic evaluation of those methods on the
WMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the
proposed methods achieve competitive results on both tasks.