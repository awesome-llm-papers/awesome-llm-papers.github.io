---
layout: publication
title: Spanish Pre-trained BERT Model And Evaluation Data
authors: "Jos\xE9 Ca\xF1ete, Gabriel Chaperon, Rodrigo Fuentes, Jou-hui Ho, Hojin\
  \ Kang, Jorge P\xE9rez"
conference: Arxiv
year: 2023
bibkey: "ca\xF1ete2023spanish"
citations: 242
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2308.02976'}]
tags: ["Evaluation", "Fine-Tuning", "Has Code", "Model Architecture", "Training Techniques"]
short_authors: "Ca\xF1ete et al."
---
The Spanish language is one of the top 5 spoken languages in the world.
Nevertheless, finding resources to train or evaluate Spanish language models is
not an easy task. In this paper we help bridge this gap by presenting a
BERT-based language model pre-trained exclusively on Spanish data. As a second
contribution, we also compiled several tasks specifically for the Spanish
language in a single repository much in the spirit of the GLUE benchmark. By
fine-tuning our pre-trained Spanish model, we obtain better results compared to
other BERT-based models pre-trained on multilingual corpora for most of the
tasks, even achieving a new state-of-the-art on some of them. We have publicly
released our model, the pre-training data, and the compilation of the Spanish
benchmarks.