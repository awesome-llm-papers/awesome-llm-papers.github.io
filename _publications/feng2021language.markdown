---
layout: publication
title: 'Language Model As An Annotator: Exploring Dialogpt For Dialogue Summarization'
authors: Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu
conference: 'Proceedings of the 59th Annual Meeting of the Association for Computational
  Linguistics and the 11th International Joint Conference on Natural Language Processing
  (Volume 1: Long Papers)'
year: 2021
bibkey: feng2021language
citations: 63
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2105.12544'}]
tags: []
short_authors: Feng et al.
---
Current dialogue summarization systems usually encode the text with a number
of general semantic features (e.g., keywords and topics) to gain more powerful
dialogue modeling capabilities. However, these features are obtained via
open-domain toolkits that are dialog-agnostic or heavily relied on human
annotations. In this paper, we show how DialoGPT, a pre-trained model for
conversational response generation, can be developed as an unsupervised
dialogue annotator, which takes advantage of dialogue background knowledge
encoded in DialoGPT. We apply DialoGPT to label three types of features on two
dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non
pre-trained models as our summarizes. Experimental results show that our
proposed method can obtain remarkable improvements on both datasets and
achieves new state-of-the-art performance on the SAMSum dataset.