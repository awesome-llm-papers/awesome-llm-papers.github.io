---
layout: publication
title: 'Indolem And Indobert: A Benchmark Dataset And Pre-trained Language Model For
  Indonesian NLP'
authors: Fajri Koto, Afshin Rahimi, Jey Han Lau, Timothy Baldwin
conference: Proceedings of the 28th International Conference on Computational Linguistics
year: 2020
bibkey: koto2020indolem
citations: 168
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2011.00677'}]
tags: ["COLING", "Datasets", "Evaluation"]
short_authors: Koto et al.
---
Although the Indonesian language is spoken by almost 200 million people and
the 10th most spoken language in the world, it is under-represented in NLP
research. Previous work on Indonesian has been hampered by a lack of annotated
datasets, a sparsity of language resources, and a lack of resource
standardization. In this work, we release the IndoLEM dataset comprising seven
tasks for the Indonesian language, spanning morpho-syntax, semantics, and
discourse. We additionally release IndoBERT, a new pre-trained language model
for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it
against existing resources. Our experiments show that IndoBERT achieves
state-of-the-art performance over most of the tasks in IndoLEM.