---
layout: publication
title: Neural Natural Language Inference Models Enhanced With External Knowledge
authors: Qian Chen, Xiaodan Zhu, Zhen-hua Ling, Diana Inkpen, Si Wei
conference: 'Proceedings of the 56th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2018
bibkey: chen2017neural
citations: 282
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1711.04289'}]
tags: ["Datasets"]
short_authors: Chen et al.
---
Modeling natural language inference is a very challenging task. With the
availability of large annotated data, it has recently become feasible to train
complex models such as neural-network-based inference models, which have shown
to achieve the state-of-the-art performance. Although there exist relatively
large annotated data, can machines learn all knowledge needed to perform
natural language inference (NLI) from these data? If not, how can
neural-network-based NLI models benefit from external knowledge and how to
build NLI models to leverage it? In this paper, we enrich the state-of-the-art
neural natural language inference models with external knowledge. We
demonstrate that the proposed models improve neural NLI models to achieve the
state-of-the-art performance on the SNLI and MultiNLI datasets.