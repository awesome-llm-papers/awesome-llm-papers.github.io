---
layout: publication
title: Levels Of Explainable Artificial Intelligence For Human-aligned Conversational
  Explanations
authors: Richard Dazeley, Peter Vamplew, Cameron Foale, Charlotte Young, Sunil Aryal,
  Francisco Cruz
conference: Artificial Intelligence
year: 2021
bibkey: dazeley2021levels
citations: 93
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2107.03178'}]
tags: ["Applications", "Survey Paper"]
short_authors: Dazeley et al.
---
Over the last few years there has been rapid research growth into eXplainable
Artificial Intelligence (XAI) and the closely aligned Interpretable Machine
Learning (IML). Drivers for this growth include recent legislative changes and
increased investments by industry and governments, along with increased concern
from the general public. People are affected by autonomous decisions every day
and the public need to understand the decision-making process to accept the
outcomes. However, the vast majority of the applications of XAI/IML are focused
on providing low-level `narrow' explanations of how an individual decision was
reached based on a particular datum. While important, these explanations rarely
provide insights into an agent's: beliefs and motivations; hypotheses of other
(human, animal or AI) agents' intentions; interpretation of external cultural
expectations; or, processes used to generate its own explanation. Yet all of
these factors, we propose, are essential to providing the explanatory depth
that people require to accept and trust the AI's decision-making. This paper
aims to define levels of explanation and describe how they can be integrated to
create a human-aligned conversational explanation system. In so doing, this
paper will survey current approaches and discuss the integration of different
technologies to achieve these levels with Broad eXplainable Artificial
Intelligence (Broad-XAI), and thereby move towards high-level `strong'
explanations.