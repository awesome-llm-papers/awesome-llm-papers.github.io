---
layout: publication
title: 'Mm-eureka: Exploring Visual Aha Moment With Rule-based Large-scale Reinforcement
  Learning'
authors: Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng
  Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng
  Zhang, Wenqi Shao
conference: No Venue
year: 2025
bibkey: meng2025mm
additional_links: [{name: Code, url: 'https://github.com/ModalMinds/MM-EUREKA'}, {
    name: Code, url: 'https://huggingface.co/discussions/paper/67cf9cd137bc7273882147e2'},
  {name: Paper, url: 'https://arxiv.org/abs/hf2503.07365'}]
tags: ["Reinforcement Learning"]
short_authors: Meng et al.
---
We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA

https://huggingface.co/discussions/paper/67cf9cd137bc7273882147e2