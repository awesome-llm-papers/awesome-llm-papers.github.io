---
layout: publication
title: Investigating Meta-learning Algorithms For Low-resource Natural Language Understanding
  Tasks
authors: Zi-yi Dou, Keyi Yu, Antonios Anastasopoulos
conference: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
  Processing and the 9th International Joint Conference on Natural Language Processing
  (EMNLP-IJCNLP)
year: 2019
bibkey: dou2019investigating
citations: 116
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1908.10423'}]
tags: ["EMNLP", "Training Techniques"]
short_authors: Zi-yi Dou, Keyi Yu, Antonios Anastasopoulos
---
Learning general representations of text is a fundamental problem for many
natural language understanding (NLU) tasks. Previously, researchers have
proposed to use language model pre-training and multi-task learning to learn
robust representations. However, these methods can achieve sub-optimal
performance in low-resource scenarios. Inspired by the recent success of
optimization-based meta-learning algorithms, in this paper, we explore the
model-agnostic meta-learning algorithm (MAML) and its variants for low-resource
NLU tasks. We validate our methods on the GLUE benchmark and show that our
proposed models can outperform several strong baselines. We further empirically
demonstrate that the learned representations can be adapted to new tasks
efficiently and effectively.