---
layout: publication
title: 'MAILS -- Meta AI Literacy Scale: Development And Testing Of An AI Literacy
  Questionnaire Based On Well-founded Competency Models And Psychological Change-
  And Meta-competencies'
authors: Astrid Carolus, Martin Koch, Samantha Straka, Marc Erich Latoschik, Carolin
  Wienrich
conference: 'Computers in Human Behavior: Artificial Humans'
year: 2023
bibkey: carolus2023mails
citations: 90
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2302.09319'}]
tags: ["Applications"]
short_authors: Carolus et al.
---
The goal of the present paper is to develop and validate a questionnaire to
assess AI literacy. In particular, the questionnaire should be deeply grounded
in the existing literature on AI literacy, should be modular (i.e., including
different facets that can be used independently of each other) to be flexibly
applicable in professional life depending on the goals and use cases, and
should meet psychological requirements and thus includes further psychological
competencies in addition to the typical facets of AIL. We derived 60 items to
represent different facets of AI Literacy according to Ng and colleagues
conceptualisation of AI literacy and additional 12 items to represent
psychological competencies such as problem solving, learning, and emotion
regulation in regard to AI. For this purpose, data were collected online from
300 German-speaking adults. The items were tested for factorial structure in
confirmatory factor analyses. The result is a measurement instrument that
measures AI literacy with the facets Use & apply AI, Understand AI, Detect AI,
and AI Ethics and the ability to Create AI as a separate construct, and AI
Self-efficacy in learning and problem solving and AI Self-management. This
study contributes to the research on AI literacy by providing a measurement
instrument relying on profound competency models. In addition, higher-order
psychological competencies are included that are particularly important in the
context of pervasive change through AI systems.