---
layout: publication
title: 'Autotrain: No-code Training For State-of-the-art Models'
authors: Abhishek Thakur
conference: No Venue
year: 2024
bibkey: thakur2024autotrain
additional_links: [{name: Code, url: 'https://github.com/huggingface/autotrain-advanced'},
  {name: Code, url: 'https://huggingface.co/discussions/paper/671737e60a87ad788cd4eee3'},
  {name: Paper, url: 'https://arxiv.org/abs/hf2410.15735'}]
tags: ["Applications", "Has Code", "Tools", "Training Techniques"]
short_authors: Abhishek Thakur
---
With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations.

https://huggingface.co/discussions/paper/671737e60a87ad788cd4eee3