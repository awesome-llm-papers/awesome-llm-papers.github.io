---
layout: publication
title: 'CLUE: A Chinese Language Understanding Evaluation Benchmark'
authors: Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu,
  Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming
  Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson,
  Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue,
  Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan
conference: Proceedings of the 28th International Conference on Computational Linguistics
year: 2020
bibkey: xu2020clue
citations: 226
additional_links: [{name: Code, url: 'https://www.CLUEbenchmarks.com'}, {name: Paper,
    url: 'https://arxiv.org/abs/2004.05986'}]
tags: ["Datasets", "Evaluation"]
short_authors: Xu et al.
---
The advent of natural language understanding (NLU) benchmarks for English,
such as GLUE and SuperGLUE allows new NLU models to be evaluated across a
diverse set of tasks. These comprehensive benchmarks have facilitated a broad
range of research and applications in natural language processing (NLP). The
problem, however, is that most such benchmarks are limited to English, which
has made it difficult to replicate many of the successes in English NLU for
other languages. To help remedy this issue, we introduce the first large-scale
Chinese Language Understanding Evaluation (CLUE) benchmark. CLUE is an
open-ended, community-driven project that brings together 9 tasks spanning
several well-established single-sentence/sentence-pair classification tasks, as
well as machine reading comprehension, all on original Chinese text. To
establish results on these tasks, we report scores using an exhaustive set of
current state-of-the-art pre-trained Chinese models (9 in total). We also
introduce a number of supplementary datasets and additional tools to help
facilitate further progress on Chinese NLU. Our benchmark is released at
https://www.CLUEbenchmarks.com