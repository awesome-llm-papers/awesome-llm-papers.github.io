---
layout: publication
title: 'Pretrained Language Models For Text Generation: A Survey'
authors: Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-yun Nie, Ji-rong Wen
conference: Proceedings of the Thirtieth International Joint Conference on Artificial
  Intelligence
year: 2021
bibkey: li2021pretrained
citations: 102
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2201.05273'}]
tags: ["IJCAI", "Survey Paper"]
short_authors: Li et al.
---
Text Generation aims to produce plausible and readable text in a human
language from input data. The resurgence of deep learning has greatly advanced
this field, in particular, with the help of neural generation models based on
pre-trained language models (PLMs). Text generation based on PLMs is viewed as
a promising approach in both academia and industry. In this paper, we provide a
survey on the utilization of PLMs in text generation. We begin with introducing
three key aspects of applying PLMs to text generation: 1) how to encode the
input into representations preserving input semantics which can be fused into
PLMs; 2) how to design an effective PLM to serve as the generation model; and
3) how to effectively optimize PLMs given the reference text and to ensure that
the generated texts satisfy special text properties. Then, we show the major
challenges arisen in these aspects, as well as possible solutions for them. We
also include a summary of various useful resources and typical text generation
applications based on PLMs. Finally, we highlight the future research
directions which will further improve these PLMs for text generation. This
comprehensive survey is intended to help researchers interested in text
generation problems to learn the core concepts, the main techniques and the
latest developments in this area based on PLMs.