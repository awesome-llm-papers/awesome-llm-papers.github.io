---
layout: publication
title: 'Ominicontrol: Minimal And Universal Control For Diffusion Transformer'
authors: Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang
conference: No Venue
year: 2024
bibkey: tan2024ominicontrol
additional_links: [{name: Code, url: 'https://huggingface.co/discussions/paper/6743e910ed93f631f50d72cf'},
  {name: Paper, url: 'https://arxiv.org/abs/hf2411.15098'}]
tags: ["Model Architecture", "Tools"]
short_authors: Tan et al.
---
In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.

https://huggingface.co/discussions/paper/6743e910ed93f631f50d72cf