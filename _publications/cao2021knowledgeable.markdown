---
layout: publication
title: Knowledgeable Or Educated Guess? Revisiting Language Models As Knowledge Bases
authors: Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong
  Xue, Jin Xu
conference: 'Proceedings of the 59th Annual Meeting of the Association for Computational
  Linguistics and the 11th International Joint Conference on Natural Language Processing
  (Volume 1: Long Papers)'
year: 2021
bibkey: cao2021knowledgeable
citations: 66
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2106.09231'}]
tags: ["Model Architecture"]
short_authors: Cao et al.
---
Previous literatures show that pre-trained masked language models (MLMs) such
as BERT can achieve competitive factual knowledge extraction performance on
some datasets, indicating that MLMs can potentially be a reliable knowledge
source. In this paper, we conduct a rigorous study to explore the underlying
predicting mechanisms of MLMs over different extraction paradigms. By
investigating the behaviors of MLMs, we find that previous decent performance
mainly owes to the biased prompts which overfit dataset artifacts. Furthermore,
incorporating illustrative cases and external contexts improve knowledge
prediction mainly due to entity type guidance and golden answer leakage. Our
findings shed light on the underlying predicting mechanisms of MLMs, and
strongly question the previous conclusion that current MLMs can potentially
serve as reliable factual knowledge bases.