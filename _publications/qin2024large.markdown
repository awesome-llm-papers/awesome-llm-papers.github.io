---
layout: publication
title: 'Large Language Models Meet NLP: A Survey'
authors: Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li,
  Min Li, Wanxiang Che, Philip S. Yu
conference: 'Findings of the Association for Computational Linguistics: NAACL 2024'
year: 2024
bibkey: qin2024large
citations: 66
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2405.12819'}]
tags: ["NAACL", "Survey Paper"]
short_authors: Qin et al.
---
While large language models (LLMs) like ChatGPT have shown impressive
capabilities in Natural Language Processing (NLP) tasks, a systematic
investigation of their potential in this field remains largely unexplored. This
study aims to address this gap by exploring the following questions: (1) How
are LLMs currently applied to NLP tasks in the literature? (2) Have traditional
NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for
NLP? To answer these questions, we take the first step to provide a
comprehensive overview of LLMs in NLP. Specifically, we first introduce a
unified taxonomy including (1) parameter-frozen application and (2)
parameter-tuning application to offer a unified perspective for understanding
the current progress of LLMs in NLP. Furthermore, we summarize the new
frontiers and the associated challenges, aiming to inspire further
groundbreaking advancements. We hope this work offers valuable insights into
the \{potential and limitations\} of LLMs in NLP, while also serving as a
practical guide for building effective LLMs in NLP.