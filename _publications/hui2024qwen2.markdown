---
layout: publication
title: Qwen2.5-coder Technical Report
authors: Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu
  Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren,
  Xuancheng Ren, Jingren Zhou, Junyang Lin
conference: No Venue
year: 2024
bibkey: hui2024qwen2
additional_links: [{name: Code, url: 'https://huggingface.co/discussions/paper/66eb93a9772eb73ab0372cf7'},
  {name: Paper, url: 'https://arxiv.org/abs/hf2409.12186'}]
tags: ["Llm For Code", "Model Architecture"]
short_authors: Hui et al.
---
In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.

https://huggingface.co/discussions/paper/66eb93a9772eb73ab0372cf7