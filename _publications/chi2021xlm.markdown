---
layout: publication
title: 'XLM-E: Cross-lingual Language Model Pre-training Via ELECTRA'
authors: Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Bo Zheng, Saksham Singhal,
  Payal Bajaj, Xia Song, Xian-ling Mao, Heyan Huang, Furu Wei
conference: 'Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2022
bibkey: chi2021xlm
citations: 61
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/2106.16138'}]
tags: ["Training Techniques"]
short_authors: Chi et al.
---
In this paper, we introduce ELECTRA-style tasks to cross-lingual language
model pre-training. Specifically, we present two pre-training tasks, namely
multilingual replaced token detection, and translation replaced token
detection. Besides, we pretrain the model, named as XLM-E, on both multilingual
and parallel corpora. Our model outperforms the baseline models on various
cross-lingual understanding tasks with much less computation cost. Moreover,
analysis shows that XLM-E tends to obtain better cross-lingual transferability.