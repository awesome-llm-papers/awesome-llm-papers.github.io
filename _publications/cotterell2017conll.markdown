---
layout: publication
title: 'Conll-sigmorphon 2017 Shared Task: Universal Morphological Reinflection In
  52 Languages'
authors: "Ryan Cotterell, Christo Kirov, John Sylak-glassman, G\xE9raldine Walther,\
  \ Ekaterina Vylomova, Patrick Xia, Manaal Faruqui, Sandra K\xFCbler, David Yarowsky,\
  \ Jason Eisner, Mans Hulden"
conference: 'Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological
  Reinflection'
year: 2017
bibkey: cotterell2017conll
citations: 202
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1706.09031'}]
tags: ["Datasets"]
short_authors: Cotterell et al.
---
The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation
required systems to be trained and tested in each of 52 typologically diverse
languages. In sub-task 1, submitted systems were asked to predict a specific
inflected form of a given lemma. In sub-task 2, systems were given a lemma and
some of its specific inflected forms, and asked to complete the inflectional
paradigm by predicting all of the remaining inflected forms. Both sub-tasks
included high, medium, and low-resource conditions. Sub-task 1 received 24
system submissions, while sub-task 2 received 3 system submissions. Following
the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared
task, all but one of the submissions included a neural component. The results
show that high performance can be achieved with small training datasets, so
long as models have appropriate inductive bias or make use of additional
unlabeled data or synthetic data. However, different biasing and data
augmentation resulted in disjoint sets of inflected forms being predicted
correctly, suggesting that there is room for future improvement.