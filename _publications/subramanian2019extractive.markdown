---
layout: publication
title: On Extractive And Abstractive Neural Document Summarization With Transformer
  Language Models
authors: Sandeep Subramanian, Raymond Li, Jonathan Pilault, Christopher Pal
conference: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
  Processing (EMNLP)
year: 2020
bibkey: subramanian2019extractive
citations: 174
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1909.03186'}]
tags: ["EMNLP", "Model Architecture"]
short_authors: Subramanian et al.
---
We present a method to produce abstractive summaries of long documents that
exceed several thousand words via neural abstractive summarization. We perform
a simple extractive step before generating a summary, which is then used to
condition the transformer language model on relevant information before being
tasked with generating a summary. We show that this extractive step
significantly improves summarization results. We also show that this approach
produces more abstractive summaries compared to prior work that employs a copy
mechanism while still achieving higher rouge scores. Note: The abstract above
was not written by the authors, it was generated by one of the models presented
in this paper.