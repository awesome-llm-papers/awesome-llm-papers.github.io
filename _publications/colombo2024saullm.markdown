---
layout: publication
title: 'Saullm-7b: A Pioneering Large Language Model For Law'
authors: "Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo,\
  \ Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera L\xFAcia Raposo, Sofia\
  \ Morgado, Michael Desa"
conference: No Venue
year: 2024
bibkey: colombo2024saullm
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/hf2403.03883'}]
tags: ["Fine-Tuning", "Model Architecture"]
short_authors: Colombo et al.
---
In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the CC-BY-SA-4.0 License.

https://huggingface.co/discussions/paper/65e953948106a03c253fb759