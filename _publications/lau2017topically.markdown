---
layout: publication
title: Topically Driven Neural Language Model
authors: Jey Han Lau, Timothy Baldwin, Trevor Cohn
conference: 'Proceedings of the 55th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)'
year: 2017
bibkey: lau2017topically
citations: 63
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1704.08012'}]
tags: ["Model Architecture"]
short_authors: Jey Han Lau, Timothy Baldwin, Trevor Cohn
---
Language models are typically applied at the sentence level, without access
to the broader document context. We present a neural language model that
incorporates document context in the form of a topic model-like architecture,
thus providing a succinct representation of the broader document context
outside of the current sentence. Experiments over a range of datasets
demonstrate that our model outperforms a pure sentence-based model in terms of
language model perplexity, and leads to topics that are potentially more
coherent than those produced by a standard LDA topic model. Our model also has
the ability to generate related sentences for a topic, providing another way to
interpret topics.