---
layout: publication
title: Mapping Instructions To Actions In 3D Environments With Visual Goal Prediction
authors: Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin,
  Yoav Artzi
conference: Proceedings of the 2018 Conference on Empirical Methods in Natural Language
  Processing
year: 2018
bibkey: misra2018mapping
citations: 142
additional_links: [{name: Paper, url: 'https://arxiv.org/abs/1809.00786'}]
tags: ["EMNLP", "Instruction Following"]
short_authors: Misra et al.
---
We propose to decompose instruction execution to goal prediction and action
generation. We design a model that maps raw visual observations to goals using
LINGUNET, a language-conditioned image generation network, and then generates
the actions required to complete them. Our model is trained from demonstration
only without external resources. To evaluate our approach, we introduce two
benchmarks for instruction following: LANI, a navigation task; and CHAI, where
an agent executes household instructions. Our evaluation demonstrates the
advantages of our model decomposition, and illustrates the challenges posed by
our new benchmarks.