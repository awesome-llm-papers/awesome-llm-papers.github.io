[
{"key": "2022no", "citations": "297", "year": "2022", "title":"No Language Left Behind: Scaling Human-centered Machine Translation", "abstract": "<p>Driven by the goal of eradicating language barriers on a global scale,\nmachine translation has solidified itself as a key focus of artificial\nintelligence research today. However, such efforts have coalesced around a\nsmall subset of languages, leaving behind the vast majority of mostly\nlow-resource languages. What does it take to break the 200 language barrier\nwhile ensuring safe, high quality results, all while keeping ethical\nconsiderations in mind? In No Language Left Behind, we took on this challenge\nby first contextualizing the need for low-resource language translation support\nthrough exploratory interviews with native speakers. Then, we created datasets\nand models aimed at narrowing the performance gap between low and high-resource\nlanguages. More specifically, we developed a conditional compute model based on\nSparsely Gated Mixture of Experts that is trained on data obtained with novel\nand effective data mining techniques tailored for low-resource languages. We\npropose multiple architectural and training improvements to counteract\noverfitting while training on thousands of tasks. Critically, we evaluated the\nperformance of over 40,000 different translation directions using a\nhuman-translated benchmark, Flores-200, and combined human evaluation with a\nnovel toxicity benchmark covering all languages in Flores-200 to assess\ntranslation safety. Our model achieves an improvement of 44% BLEU relative to\nthe previous state-of-the-art, laying important groundwork towards realizing a\nuniversal translation system. Finally, we open source all contributions\ndescribed in this work, accessible at\nhttps://github.com/facebookresearch/fairseq/tree/nllb.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Training Techniques"] },
{"key": "2023gpt", "citations": "1459", "year": "2023", "title":"GPT-4 Technical Report", "abstract": "<p>We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4’s performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "aakanksha2022palm", "citations": "1913", "year": "2022", "title":"Palm: Scaling Language Modeling With Pathways", "abstract": "<p>Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Few-Shot","Llm For Code","Model Architecture","Training Techniques"] },
{"key": "abbasian2023foundation", "citations": "76", "year": "2024", "title":"Foundation Metrics For Evaluating Effectiveness Of Healthcare Conversations Powered By Generative AI", "abstract": "<p>Generative Artificial Intelligence is set to revolutionize healthcare\ndelivery by transforming traditional patient care into a more personalized,\nefficient, and proactive process. Chatbots, serving as interactive\nconversational models, will probably drive this patient-centered transformation\nin healthcare. Through the provision of various services, including diagnosis,\npersonalized lifestyle recommendations, and mental health support, the\nobjective is to substantially augment patient health outcomes, all the while\nmitigating the workload burden on healthcare providers. The life-critical\nnature of healthcare applications necessitates establishing a unified and\ncomprehensive set of evaluation metrics for conversational models. Existing\nevaluation metrics proposed for various generic large language models (LLMs)\ndemonstrate a lack of comprehension regarding medical and health concepts and\ntheir significance in promoting patients’ well-being. Moreover, these metrics\nneglect pivotal user-centered aspects, including trust-building, ethics,\npersonalization, empathy, user comprehension, and emotional support. The\npurpose of this paper is to explore state-of-the-art LLM-based evaluation\nmetrics that are specifically applicable to the assessment of interactive\nconversational models in healthcare. Subsequently, we present an comprehensive\nset of evaluation metrics designed to thoroughly assess the performance of\nhealthcare chatbots from an end-user perspective. These metrics encompass an\nevaluation of language processing abilities, impact on real-world clinical\ntasks, and effectiveness in user-interactive conversations. Finally, we engage\nin a discussion concerning the challenges associated with defining and\nimplementing these metrics, with particular emphasis on confounding factors\nsuch as the target audience, evaluation methods, and prompt techniques involved\nin the evaluation process.</p>\n", "tags": ["Applications","Ethics & Fairness","Evaluation","Prompting"] },
{"key": "abbasiantaeb2020text", "citations": "60", "year": "2021", "title":"Text-based Question Answering From Information Retrieval And Deep Neural Network Perspectives: A Survey", "abstract": "<p>Text-based Question Answering (QA) is a challenging task which aims at\nfinding short concrete answers for users’ questions. This line of research has\nbeen widely studied with information retrieval techniques and has received\nincreasing attention in recent years by considering deep neural network\napproaches. Deep learning approaches, which are the main focus of this paper,\nprovide a powerful technique to learn multiple layers of representations and\ninteraction between questions and texts. In this paper, we provide a\ncomprehensive overview of different models proposed for the QA task, including\nboth traditional information retrieval perspective, and more recent deep neural\nnetwork perspective. We also introduce well-known datasets for the task and\npresent available results from the literature to have a comparison between\ndifferent techniques.</p>\n", "tags": ["Datasets","Model Architecture","Survey Paper"] },
{"key": "abdellatif2020comparison", "citations": "79", "year": "2021", "title":"A Comparison Of Natural Language Understanding Platforms For Chatbots In Software Engineering", "abstract": "<p>Chatbots are envisioned to dramatically change the future of Software\nEngineering, allowing practitioners to chat and inquire about their software\nprojects and interact with different services using natural language. At the\nheart of every chatbot is a Natural Language Understanding (NLU) component that\nenables the chatbot to understand natural language input. Recently, many NLU\nplatforms were provided to serve as an off-the-shelf NLU component for\nchatbots, however, selecting the best NLU for Software Engineering chatbots\nremains an open challenge.\n  Therefore, in this paper, we evaluate four of the most commonly used NLUs,\nnamely IBM Watson, Google Dialogflow, Rasa, and Microsoft LUIS to shed light on\nwhich NLU should be used in Software Engineering based chatbots. Specifically,\nwe examine the NLUs’ performance in classifying intents, confidence scores\nstability, and extracting entities. To evaluate the NLUs, we use two datasets\nthat reflect two common tasks performed by Software Engineering practitioners,\n1) the task of chatting with the chatbot to ask questions about software\nrepositories 2) the task of asking development questions on Q&amp;A forums (e.g.,\nStack Overflow). According to our findings, IBM Watson is the best performing\nNLU when considering the three aspects (intents classification, confidence\nscores, and entity extraction). However, the results from each individual\naspect show that, in intents classification, IBM Watson performs the best with\nan F1-measure &gt; 84%, but in confidence scores, Rasa comes on top with a median\nconfidence score higher than 0.91. Our results also show that all NLUs, except\nfor Dialogflow, generally provide trustable confidence scores. For entity\nextraction, Microsoft LUIS and IBM Watson outperform other NLUs in the two SE\ntasks. Our results provide guidance to software engineering practitioners when\ndeciding which NLU to use in their chatbots.</p>\n", "tags": ["Datasets","Llm For Code"] },
{"key": "abdin2024phi", "citations": "60", "year": "2024", "title":"Phi-4 Technical Report", "abstract": "<p>We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size – especially on reasoning-focused benchmarks\n– due to improved data, training curriculum, and innovations in the\npost-training scheme.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "abdulmageed2020arbert", "citations": "133", "year": "2020", "title":"ARBERT & MARBERT: Deep Bidirectional Transformers For Arabic", "abstract": "<p>Pre-trained language models (LMs) are currently integral to many natural\nlanguage processing systems. Although multilingual LMs were also introduced to\nserve many languages, these have limitations such as being costly at inference\ntime and the size and diversity of non-English data involved in their\npre-training. We remedy these issues for a collection of diverse Arabic\nvarieties by introducing two powerful deep bidirectional transformer-based\nmodels, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a\nnew benchmark for multi-dialectal Arabic language understanding evaluation.\nARLUE is built using 42 datasets targeting six different task clusters,\nallowing us to offer a series of standardized experiments under rich\nconditions. When fine-tuned on ARLUE, our models collectively achieve new\nstate-of-the-art results across the majority of tasks (37 out of 48\nclassification tasks, on the 42 datasets). Our best model acquires the highest\nARLUE score (77.40) across all six task clusters, outperforming all other\nmodels including XLM-R Large (~ 3.4 x larger size). Our models are publicly\navailable at https://github.com/UBC-NLP/marbert and ARLUE will be released\nthrough the same repository.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "abid2021persistent", "citations": "325", "year": "2021", "title":"Persistent Anti-muslim Bias In Large Language Models", "abstract": "<p>It has been observed that large-scale language models capture undesirable\nsocietal biases, e.g. relating to race and gender; yet religious bias has been\nrelatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual\nlanguage model, captures persistent Muslim-violence bias. We probe GPT-3 in\nvarious ways, including prompt completion, analogical reasoning, and story\ngeneration, to understand this anti-Muslim bias, demonstrating that it appears\nconsistently and creatively in different uses of the model and that it is\nsevere even compared to biases about other religious groups. For instance,\n“Muslim” is analogized to “terrorist” in 23% of test cases, while “Jewish” is\nmapped to “money” in 5% of test cases. We quantify the positive distraction\nneeded to overcome this bias with adversarial text prompts, and find that use\nof the most positive 6 adjectives reduces violent completions for “Muslims”\nfrom 66% to 20%, but which is still higher than for other religious groups.</p>\n", "tags": ["AAAI","Ethics & Fairness"] },
{"key": "achlioptas2021artemis", "citations": "98", "year": "2021", "title":"Artemis: Affective Language For Visual Art", "abstract": "<p>We present a novel large-scale dataset and accompanying machine learning\nmodels aimed at providing a detailed understanding of the interplay between\nvisual content, its emotional effect, and explanations for the latter in\nlanguage. In contrast to most existing annotation datasets in computer vision,\nwe focus on the affective experience triggered by visual artworks and ask the\nannotators to indicate the dominant emotion they feel for a given image and,\ncrucially, to also provide a grounded verbal explanation for their emotion\nchoice. As we demonstrate below, this leads to a rich set of signals for both\nthe objective content and the affective impact of an image, creating\nassociations with abstract concepts (e.g., “freedom” or “love”), or references\nthat go beyond what is directly visible, including visual similes and\nmetaphors, or subjective references to personal experiences. We focus on visual\nart (e.g., paintings, artistic photographs) as it is a prime example of imagery\ncreated to elicit emotional responses from its viewers. Our dataset, termed\nArtEmis, contains 439K emotion attributions and explanations from humans, on\n81K artworks from WikiArt. Building on this data, we train and demonstrate a\nseries of captioning systems capable of expressing and explaining emotions from\nvisual stimuli. Remarkably, the captions produced by these systems often\nsucceed in reflecting the semantic and abstract content of the image, going\nwell beyond systems trained on existing datasets. The collected dataset and\ndeveloped methods are available at https://artemisdataset.org.</p>\n", "tags": ["CVPR","Has Code"] },
{"key": "adam2020how", "citations": "544", "year": "2020", "title":"How Much Knowledge Can You Pack Into The Parameters Of A Language Model?", "abstract": "<p>It has recently been observed that neural language models trained on\nunstructured text can implicitly store and retrieve knowledge using natural\nlanguage queries. In this short paper, we measure the practical utility of this\napproach by fine-tuning pre-trained models to answer questions without access\nto any external context or knowledge. We show that this approach scales with\nmodel size and performs competitively with open-domain systems that explicitly\nretrieve answers from an external knowledge source when answering questions. To\nfacilitate reproducibility and future work, we release our code and trained\nmodels at https://goo.gle/t5-cbqa.</p>\n", "tags": ["EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "adams2019massively", "citations": "76", "year": "2019", "title":"Massively Multilingual Adversarial Speech Recognition", "abstract": "<p>We report on adaptation of multilingual end-to-end speech recognition models\ntrained on as many as 100 languages. Our findings shed light on the relative\nimportance of similarity between the target and pretraining languages along the\ndimensions of phonetics, phonology, language family, geographical location, and\northography. In this context, experiments demonstrate the effectiveness of two\nadditional pretraining objectives in encouraging language-independent encoder\nrepresentations: a context-independent phoneme objective paired with a\nlanguage-adversarial classification objective.</p>\n", "tags": ["Security"] },
{"key": "adebayo2020debugging", "citations": "68", "year": "2020", "title":"Debugging Tests For Model Explanations", "abstract": "<p>We investigate whether post-hoc model explanations are effective for\ndiagnosing model errors–model debugging. In response to the challenge of\nexplaining a model’s prediction, a vast array of explanation methods have been\nproposed. Despite increasing use, it is unclear if they are effective. To\nstart, we categorize \\textit{bugs}, based on their source, into:~\\textit{data,\nmodel, and test-time} contamination bugs. For several explanation methods, we\nassess their ability to: detect spurious correlation artifacts (data\ncontamination), diagnose mislabeled training examples (data contamination),\ndifferentiate between a (partially) re-initialized model and a trained one\n(model contamination), and detect out-of-distribution inputs (test-time\ncontamination). We find that the methods tested are able to diagnose a spurious\nbackground bug, but not conclusively identify mislabeled training examples. In\naddition, a class of methods, that modify the back-propagation algorithm are\ninvariant to the higher layer parameters of a deep network; hence, ineffective\nfor diagnosing model contamination. We complement our analysis with a human\nsubject study, and find that subjects fail to identify defective models using\nattributions, but instead rely, primarily, on model predictions. Taken\ntogether, our results provide guidance for practitioners and researchers\nturning to explanations as tools for model debugging.</p>\n", "tags": ["Tools"] },
{"key": "adelani2019generating", "citations": "87", "year": "2020", "title":"Generating Sentiment-preserving Fake Online Reviews Using Neural Language Models And Their Human- And Machine-based Detection", "abstract": "<p>Advanced neural language models (NLMs) are widely used in sequence generation\ntasks because they are able to produce fluent and meaningful sentences. They\ncan also be used to generate fake reviews, which can then be used to attack\nonline review systems and influence the buying decisions of online shoppers. To\nperform such attacks, it is necessary for experts to train a tailored LM for a\nspecific topic. In this work, we show that a low-skilled threat model can be\nbuilt just by combining publicly available LMs and show that the produced fake\nreviews can fool both humans and machines. In particular, we use the GPT-2 NLM\nto generate a large number of high-quality reviews based on a review with the\ndesired sentiment and then using a BERT based text classifier (with accuracy of\n96%) to filter out reviews with undesired sentiments. Because none of the words\nin the review are modified, fluent samples like the training data can be\ngenerated from the learned distribution. A subjective evaluation with 80\nparticipants demonstrated that this simple method can produce reviews that are\nas fluent as those written by people. It also showed that the participants\ntended to distinguish fake reviews randomly. Three countermeasures, Grover,\nGLTR, and OpenAI GPT-2 detector, were found to be difficult to accurately\ndetect fake review.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "adiwardana2020towards", "citations": "444", "year": "2020", "title":"Towards A Human-like Open-domain Chatbot", "abstract": "<p>We present Meena, a multi-turn open-domain chatbot trained end-to-end on data\nmined and filtered from public domain social media conversations. This 2.6B\nparameter neural network is simply trained to minimize perplexity of the next\ntoken. We also propose a human evaluation metric called Sensibleness and\nSpecificity Average (SSA), which captures key elements of a human-like\nmulti-turn conversation. Our experiments show strong correlation between\nperplexity and SSA. The fact that the best perplexity end-to-end trained Meena\nscores high on SSA (72% on multi-turn evaluation) suggests that a human-level\nSSA of 86% is potentially within reach if we can better optimize perplexity.\nAdditionally, the full version of Meena (with a filtering mechanism and tuned\ndecoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots\nwe evaluated.</p>\n", "tags": ["Evaluation"] },
{"key": "agarwal2017deep", "citations": "116", "year": "2018", "title":"A Deep Network Model For Paraphrase Detection In Short Text Messages", "abstract": "<p>This paper is concerned with paraphrase detection. The ability to detect\nsimilar sentences written in natural language is crucial for several\napplications, such as text mining, text summarization, plagiarism detection,\nauthorship authentication and question answering. Given two sentences, the\nobjective is to detect whether they are semantically identical. An important\ninsight from this work is that existing paraphrase systems perform well when\napplied on clean texts, but they do not necessarily deliver good performance\nagainst noisy texts. Challenges with paraphrase detection on user generated\nshort texts, such as Twitter, include language irregularity and noise. To cope\nwith these challenges, we propose a novel deep neural network-based approach\nthat relies on coarse-grained sentence modeling using a convolutional neural\nnetwork and a long short-term memory model, combined with a specific\nfine-grained word-level similarity matching model. Our experimental results\nshow that the proposed approach outperforms existing state-of-the-art\napproaches on user-generated noisy social media data, such as Twitter texts,\nand achieves highly competitive performance on a cleaner corpus.</p>\n", "tags": ["Applications","Datasets"] },
{"key": "agarwal2019towards", "citations": "132", "year": "2020", "title":"Towards Causal VQA: Revealing And Reducing Spurious Correlations By Invariant And Covariant Semantic Editing", "abstract": "<p>Despite significant success in Visual Question Answering (VQA), VQA models\nhave been shown to be notoriously brittle to linguistic variations in the\nquestions. Due to deficiencies in models and datasets, today’s models often\nrely on correlations rather than predictions that are causal w.r.t. data. In\nthis paper, we propose a novel way to analyze and measure the robustness of the\nstate of the art models w.r.t semantic visual variations as well as propose\nways to make models more robust against spurious correlations. Our method\nperforms automated semantic image manipulations and tests for consistency in\nmodel predictions to quantify the model robustness as well as generate\nsynthetic data to counter these problems. We perform our analysis on three\ndiverse, state of the art VQA models and diverse question types with a\nparticular focus on challenging counting questions. In addition, we show that\nmodels can be made significantly more robust against inconsistent predictions\nusing our edited data. Finally, we show that results also translate to\nreal-world error cases of state of the art models, which results in improved\noverall performance.</p>\n", "tags": ["CVPR","Datasets","Security"] },
{"key": "aghajanyan2020better", "citations": "103", "year": "2020", "title":"Better Fine-tuning By Reducing Representational Collapse", "abstract": "<p>Although widely adopted, existing approaches for fine-tuning pre-trained\nlanguage models have been shown to be unstable across hyper-parameter settings,\nmotivating recent work on trust region methods. In this paper, we present a\nsimplified and efficient method rooted in trust region theory that replaces\npreviously used adversarial objectives with parametric noise (sampling from\neither a normal or uniform distribution), thereby discouraging representation\nchange during fine-tuning when possible without hurting performance. We also\nintroduce a new analysis to motivate the use of trust region methods more\ngenerally, by studying representational collapse; the degradation of\ngeneralizable representations from pre-trained models as they are fine-tuned\nfor a specific end task. Extensive experiments show that our fine-tuning method\nmatches or exceeds the performance of previous trust region methods on a range\nof understanding and generation tasks (including DailyMail/CNN, Gigaword,\nReddit TIFU, and the GLUE benchmark), while also being much faster. We also\nshow that it is less prone to representation collapse; the pre-trained models\nmaintain more generalizable representations every time they are fine-tuned.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "aghajanyan2020intrinsic", "citations": "171", "year": "2021", "title":"Intrinsic Dimensionality Explains The Effectiveness Of Language Model Fine-tuning", "abstract": "<p>Although pretrained language models can be fine-tuned to produce\nstate-of-the-art results for a very wide range of language understanding tasks,\nthe dynamics of this process are not well understood, especially in the low\ndata regime. Why can we use relatively vanilla gradient descent algorithms\n(e.g., without strong regularization) to tune a model with hundreds of millions\nof parameters on datasets with only hundreds or thousands of labeled examples?\nIn this paper, we argue that analyzing fine-tuning through the lens of\nintrinsic dimension provides us with empirical and theoretical intuitions to\nexplain this remarkable phenomenon. We empirically show that common pre-trained\nmodels have a very low intrinsic dimension; in other words, there exists a low\ndimension reparameterization that is as effective for fine-tuning as the full\nparameter space. For example, by optimizing only 200 trainable parameters\nrandomly projected back into the full space, we can tune a RoBERTa model to\nachieve 90% of the full parameter performance levels on MRPC. Furthermore, we\nempirically show that pre-training implicitly minimizes intrinsic dimension\nand, perhaps surprisingly, larger models tend to have lower intrinsic dimension\nafter a fixed number of pre-training updates, at least in part explaining their\nextreme effectiveness. Lastly, we connect intrinsic dimensionality with low\ndimensional task representations and compression based generalization bounds to\nprovide intrinsic-dimension-based generalization bounds that are independent of\nthe full parameter count.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "aghajanyan2021muppet", "citations": "165", "year": "2021", "title":"Muppet: Massive Multi-task Representations With Pre-finetuning", "abstract": "<p>We propose pre-finetuning, an additional large-scale learning stage between\nlanguage model pre-training and fine-tuning. Pre-finetuning is massively\nmulti-task learning (around 50 datasets, over 4.8 million total labeled\nexamples), and is designed to encourage learning of representations that\ngeneralize better to many different tasks. We show that pre-finetuning\nconsistently improves performance for pretrained discriminators (e.g.~RoBERTa)\nand generation models (e.g.~BART) on a wide range of tasks (sentence\nprediction, commonsense reasoning, MRC, etc.), while also significantly\nimproving sample efficiency during fine-tuning. We also show that large-scale\nmulti-tasking is crucial; pre-finetuning can hurt performance when few tasks\nare used up until a critical point (usually above 15) after which performance\nimproves linearly in the number of tasks.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "agostinelli2023musiclm", "citations": "146", "year": "2023", "title":"Musiclm: Generating Music From Text", "abstract": "<p>We introduce MusicLM, a model generating high-fidelity music from text\ndescriptions such as “a calming violin melody backed by a distorted guitar\nriff”. MusicLM casts the process of conditional music generation as a\nhierarchical sequence-to-sequence modeling task, and it generates music at 24\nkHz that remains consistent over several minutes. Our experiments show that\nMusicLM outperforms previous systems both in audio quality and adherence to the\ntext description. Moreover, we demonstrate that MusicLM can be conditioned on\nboth text and a melody in that it can transform whistled and hummed melodies\naccording to the style described in a text caption. To support future research,\nwe publicly release MusicCaps, a dataset composed of 5.5k music-text pairs,\nwith rich text descriptions provided by human experts.</p>\n", "tags": ["Datasets","Time Series"] },
{"key": "agrawal2016analyzing", "citations": "294", "year": "2016", "title":"Analyzing The Behavior Of Visual Question Answering Models", "abstract": "<p>Recently, a number of deep-learning based models have been proposed for the\ntask of Visual Question Answering (VQA). The performance of most models is\nclustered around 60-70%. In this paper we propose systematic methods to analyze\nthe behavior of these models as a first step towards recognizing their\nstrengths and weaknesses, and identifying the most fruitful directions for\nprogress. We analyze two models, one each from two major classes of VQA models\n– with-attention and without-attention and show the similarities and\ndifferences in the behavior of these models. We also analyze the winning entry\nof the VQA Challenge 2016.\n  Our behavior analysis reveals that despite recent progress, today’s VQA\nmodels are “myopic” (tend to fail on sufficiently novel instances), often “jump\nto conclusions” (converge on a predicted answer after ‘listening’ to just half\nthe question), and are “stubborn” (do not change their answers across images).</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "agrawal2017don", "citations": "548", "year": "2018", "title":"Don't Just Assume; Look And Answer: Overcoming Priors For Visual Question Answering", "abstract": "<p>A number of studies have found that today’s Visual Question Answering (VQA)\nmodels are heavily driven by superficial correlations in the training data and\nlack sufficient image grounding. To encourage development of models geared\ntowards the latter, we propose a new setting for VQA where for every question\ntype, train and test sets have different prior distributions of answers.\nSpecifically, we present new splits of the VQA v1 and VQA v2 datasets, which we\ncall Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2\nrespectively). First, we evaluate several existing VQA models under this new\nsetting and show that their performance degrades significantly compared to the\noriginal VQA setting. Second, we propose a novel Grounded Visual Question\nAnswering model (GVQA) that contains inductive biases and restrictions in the\narchitecture specifically designed to prevent the model from ‘cheating’ by\nprimarily relying on priors in the training data. Specifically, GVQA explicitly\ndisentangles the recognition of visual concepts present in the image from the\nidentification of plausible answer space for a given question, enabling the\nmodel to more robustly generalize across different distributions of answers.\nGVQA is built off an existing VQA model – Stacked Attention Networks (SAN).\nOur experiments demonstrate that GVQA significantly outperforms SAN on both\nVQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more\npowerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in\nseveral cases. GVQA offers strengths complementary to SAN when trained and\nevaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more\ntransparent and interpretable than existing VQA models.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "agrawal2022large", "citations": "191", "year": "2022", "title":"Large Language Models Are Few-shot Clinical Information Extractors", "abstract": "<p>A long-running goal of the clinical NLP community is the extraction of\nimportant variables trapped in clinical notes. However, roadblocks have\nincluded dataset shift from the general domain and a lack of public clinical\ncorpora and annotations. In this work, we show that large language models, such\nas InstructGPT, perform well at zero- and few-shot information extraction from\nclinical text despite not being trained specifically for the clinical domain.\nWhereas text classification and generation performance have already been\nstudied extensively in such models, here we additionally demonstrate how to\nleverage them to tackle a diverse set of NLP tasks which require more\nstructured outputs, including span identification, token-level sequence\nclassification, and relation extraction. Further, due to the dearth of\navailable data to evaluate these systems, we introduce new datasets for\nbenchmarking few-shot clinical information extraction based on a manual\nre-annotation of the CASI dataset for new tasks. On the clinical extraction\ntasks we studied, the GPT-3 systems significantly outperform existing zero- and\nfew-shot baselines.</p>\n", "tags": ["Datasets","EMNLP","Few-Shot","Model Architecture"] },
{"key": "aharoni2017towards", "citations": "149", "year": "2017", "title":"Towards String-to-tree Neural Machine Translation", "abstract": "<p>We present a simple method to incorporate syntactic information about the\ntarget language in a neural machine translation system by translating into\nlinearized, lexicalized constituency trees. An experiment on the WMT16\nGerman-English news translation task resulted in an improved BLEU score when\ncompared to a syntax-agnostic NMT baseline trained on the same dataset. An\nanalysis of the translations from the syntax-aware system shows that it\nperforms more reordering during translation in comparison to the baseline. A\nsmall-scale human evaluation also showed an advantage to the syntax-aware\nsystem.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "aharoni2019massively", "citations": "412", "year": "2019", "title":"Massively Multilingual Neural Machine Translation", "abstract": "<p>Multilingual neural machine translation (NMT) enables training a single model\nthat supports translation from multiple source languages into multiple target\nlanguages. In this paper, we push the limits of multilingual NMT in terms of\nnumber of languages being used. We perform extensive experiments in training\nmassively multilingual NMT models, translating up to 102 languages to and from\nEnglish within a single model. We explore different setups for training such\nmodels and analyze the trade-offs between translation quality and various\nmodeling decisions. We report results on the publicly available TED talks\nmultilingual corpus where we show that massively multilingual many-to-many\nmodels are effective in low resource settings, outperforming the previous\nstate-of-the-art while supporting up to 59 languages. Our experiments on a\nlarge-scale dataset with 102 languages to and from English and up to one\nmillion examples per direction also show promising results, surpassing strong\nbilingual baselines and encouraging future work on massively multilingual NMT.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "aharoni2020unsupervised", "citations": "174", "year": "2020", "title":"Unsupervised Domain Clusters In Pretrained Language Models", "abstract": "<p>The notion of “in-domain data” in NLP is often over-simplistic and vague, as\ntextual data varies in many nuanced linguistic aspects such as topic, style or\nlevel of formality. In addition, domain labels are many times unavailable,\nmaking it challenging to build domain-specific systems. We show that massive\npre-trained language models implicitly learn sentence representations that\ncluster by domains without supervision – suggesting a simple data-driven\ndefinition of domains in textual data. We harness this property and propose\ndomain data selection methods based on such models, which require only a small\nset of in-domain monolingual data. We evaluate our data selection methods for\nneural machine translation across five diverse domains, where they outperform\nan established approach as measured by both BLEU and by precision and recall of\nsentence selection with respect to an oracle.</p>\n", "tags": [] },
{"key": "aher2022using", "citations": "71", "year": "2022", "title":"Using Large Language Models To Simulate Multiple Humans And Replicate Human Subject Studies", "abstract": "<p>We introduce a new type of test, called a Turing Experiment (TE), for\nevaluating to what extent a given language model, such as GPT models, can\nsimulate different aspects of human behavior. A TE can also reveal consistent\ndistortions in a language model’s simulation of a specific human behavior.\nUnlike the Turing Test, which involves simulating a single arbitrary\nindividual, a TE requires simulating a representative sample of participants in\nhuman subject research. We carry out TEs that attempt to replicate\nwell-established findings from prior studies. We design a methodology for\nsimulating TEs and illustrate its use to compare how well different language\nmodels are able to reproduce classic economic, psycholinguistic, and social\npsychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock\nExperiment, and Wisdom of Crowds. In the first three TEs, the existing findings\nwere replicated using recent models, while the last TE reveals a\n“hyper-accuracy distortion” present in some language models (including ChatGPT\nand GPT-4), which could affect downstream applications in education and the\narts.</p>\n", "tags": ["Applications","Model Architecture"] },
{"key": "ahmad2020transformer", "citations": "317", "year": "2020", "title":"A Transformer-based Approach For Source Code Summarization", "abstract": "<p>Generating a readable summary that describes the functionality of a program\nis known as source code summarization. In this task, learning code\nrepresentation by modeling the pairwise relationship between code tokens to\ncapture their long-range dependencies is crucial. To learn code representation\nfor summarization, we explore the Transformer model that uses a self-attention\nmechanism and has shown to be effective in capturing long-range dependencies.\nIn this work, we show that despite the approach is simple, it outperforms the\nstate-of-the-art techniques by a significant margin. We perform extensive\nanalysis and ablation studies that reveal several important findings, e.g., the\nabsolute encoding of source code tokens’ position hinders, while relative\nencoding significantly improves the summarization performance. We have made our\ncode publicly available to facilitate future research.</p>\n", "tags": ["Llm For Code","Model Architecture"] },
{"key": "ahmad2021unified", "citations": "431", "year": "2021", "title":"Unified Pre-training For Program Understanding And Generation", "abstract": "<p>Code summarization and generation empower conversion between programming\nlanguage (PL) and natural language (NL), while code translation avails the\nmigration of legacy code from one PL to another. This paper introduces PLBART,\na sequence-to-sequence model capable of performing a broad spectrum of program\nand language understanding and generation tasks. PLBART is pre-trained on an\nextensive collection of Java and Python functions and associated NL text via\ndenoising autoencoding. Experiments on code summarization in the English\nlanguage, code generation, and code translation in seven programming languages\nshow that PLBART outperforms or rivals state-of-the-art models. Moreover,\nexperiments on discriminative tasks, e.g., program repair, clone detection, and\nvulnerable code detection, demonstrate PLBART’s effectiveness in program\nunderstanding. Furthermore, analysis reveals that PLBART learns program syntax,\nstyle (e.g., identifier naming convention), logical flow (e.g., if block inside\nan else block is equivalent to else if block) that are crucial to program\nsemantics and thus excels even with limited annotations.</p>\n", "tags": ["Llm For Code","NAACL","Training Techniques"] },
{"key": "ahmad2022chemberta", "citations": "82", "year": "2022", "title":"Chemberta-2: Towards Chemical Foundation Models", "abstract": "<p>Large pretrained models such as GPT-3 have had tremendous impact on modern\nnatural language processing by leveraging self-supervised learning to learn\nsalient representations that can be used to readily finetune on a wide variety\nof downstream tasks. We investigate the possibility of transferring such\nadvances to molecular machine learning by building a chemical foundation model,\nChemBERTa-2, using the language of SMILES. While labeled data for molecular\nprediction tasks is typically scarce, libraries of SMILES strings are readily\navailable. In this work, we build upon ChemBERTa by optimizing the pretraining\nprocess. We compare multi-task and self-supervised pretraining by varying\nhyperparameters and pretraining dataset size, up to 77M compounds from PubChem.\nTo our knowledge, the 77M set constitutes one of the largest datasets used for\nmolecular pretraining to date. We find that with these pretraining\nimprovements, we are competitive with existing state-of-the-art architectures\non the MoleculeNet benchmark suite. We analyze the degree to which improvements\nin pretraining translate to improvement on downstream tasks.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "ahmad2023towards", "citations": "106", "year": "2023", "title":"Towards Human-bot Collaborative Software Architecting With Chatgpt", "abstract": "<p>Architecting software-intensive systems can be a complex process. It deals\nwith the daunting tasks of unifying stakeholders’ perspectives, designers’\nintellect, tool-based automation, pattern-driven reuse, and so on, to sketch a\nblueprint that guides software implementation and evaluation. Despite its\nbenefits, architecture-centric software engineering (ACSE) inherits a multitude\nof challenges. ACSE challenges could stem from a lack of standardized\nprocesses, socio-technical limitations, and scarcity of human expertise etc.\nthat can impede the development of existing and emergent classes of software\n(e.g., IoTs, blockchain, quantum systems). Software Development Bots (DevBots)\ntrained on large language models can help synergise architects’ knowledge with\nartificially intelligent decision support to enable rapid architecting in a\nhuman-bot collaborative ACSE. An emerging solution to enable this collaboration\nis ChatGPT, a disruptive technology not primarily introduced for software\nengineering, but is capable of articulating and refining architectural\nartifacts based on natural language processing. We detail a case study that\ninvolves collaboration between a novice software architect and ChatGPT for\narchitectural analysis, synthesis, and evaluation of a services-driven software\napplication. Preliminary results indicate that ChatGPT can mimic an architect’s\nrole to support and often lead ACSE, however; it requires human oversight and\ndecision support for collaborative architecting. Future research focuses on\nharnessing empirical evidence about architects’ productivity and exploring\nsocio-technical aspects of architecting with ChatGPT to tackle emerging and\nfuturistic challenges of ACSE.</p>\n", "tags": ["Evaluation","Llm For Code","Model Architecture"] },
{"key": "ahmed2017weighted", "citations": "138", "year": "2017", "title":"Weighted Transformer Network For Machine Translation", "abstract": "<p>State-of-the-art results on neural machine translation often use attentional\nsequence-to-sequence models with some form of convolution or recursion. Vaswani\net al. (2017) propose a new architecture that avoids recurrence and convolution\ncompletely. Instead, it uses only self-attention and feed-forward layers. While\nthe proposed architecture achieves state-of-the-art results on several machine\ntranslation tasks, it requires a large number of parameters and training\niterations to converge. We propose Weighted Transformer, a Transformer with\nmodified attention layers, that not only outperforms the baseline network in\nBLEU score but also converges 15-40% faster. Specifically, we replace the\nmulti-head attention by multiple self-attention branches that the model learns\nto combine during the training process. Our model improves the state-of-the-art\nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation\ntask and by 0.4 on the English-to-French translation task.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "ahmed2022few", "citations": "119", "year": "2022", "title":"Few-shot Training Llms For Project-specific Code-summarization", "abstract": "<p>Very large language models (LLMs), such as GPT-3 and Codex have achieved\nstate-of-the-art performance on several natural-language tasks, and show great\npromise also for code. A particularly exciting aspect of LLMs is their knack\nfor few-shot and zero-shot learning: they can learn to perform a task with very\nfew examples. Few-shotting has particular synergies in software engineering,\nwhere there are a lot of phenomena (identifier names, APIs, terminology, coding\npatterns) that are known to be highly project-specific. However,\nproject-specific data can be quite limited, especially early in the history of\na project; thus the few-shot learning capacity of LLMs might be very relevant.\nIn this paper, we investigate the use few-shot training with the very large GPT\n(Generative Pre-trained Transformer) Codex model, and find evidence suggesting\nthat one can significantly surpass state-of-the-art models for\ncode-summarization, leveraging project-specific training.</p>\n", "tags": ["Few-Shot","Llm For Code","Model Architecture","Training Techniques"] },
{"key": "ahn2017text2action", "citations": "112", "year": "2018", "title":"Text2action: Generative Adversarial Synthesis From Language To Action", "abstract": "<p>In this paper, we propose a generative model which learns the relationship\nbetween language and human action in order to generate a human action sequence\ngiven a sentence describing human behavior. The proposed generative model is a\ngenerative adversarial network (GAN), which is based on the sequence to\nsequence (SEQ2SEQ) model. Using the proposed generative network, we can\nsynthesize various actions for a robot or a virtual agent using a text encoder\nrecurrent neural network (RNN) and an action decoder RNN. The proposed\ngenerative network is trained from 29,770 pairs of actions and sentence\nannotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video\ndataset. We demonstrate that the network can generate human-like actions which\ncan be transferred to a Baxter robot, such that the robot performs an action\nbased on a provided sentence. Results show that the proposed generative network\ncorrectly models the relationship between language and action and can generate\na diverse set of actions from the same sentence.</p>\n", "tags": ["Agentic","Datasets","ICRA"] },
{"key": "ahuja2019language2pose", "citations": "181", "year": "2019", "title":"Language2pose: Natural Language Grounded Pose Forecasting", "abstract": "<p>Generating animations from natural language sentences finds its applications\nin a a number of domains such as movie script visualization, virtual human\nanimation and, robot motion planning. These sentences can describe different\nkinds of actions, speeds and direction of these actions, and possibly a target\ndestination. The core modeling challenge in this language-to-pose application\nis how to map linguistic concepts to motion animations.\n  In this paper, we address this multimodal problem by introducing a neural\narchitecture called Joint Language to Pose (or JL2P), which learns a joint\nembedding of language and pose. This joint embedding space is learned\nend-to-end using a curriculum learning approach which emphasizes shorter and\neasier sequences first before moving to longer and harder ones. We evaluate our\nproposed model on a publicly available corpus of 3D pose data and\nhuman-annotated sentences. Both objective metrics and human judgment evaluation\nconfirm that our proposed approach is able to generate more accurate animations\nand are deemed visually more representative by humans than other data driven\napproaches.</p>\n", "tags": ["Applications","Datasets","Evaluation","Model Architecture","Time Series"] },
{"key": "ainslie2020etc", "citations": "246", "year": "2020", "title":"ETC: Encoding Long And Structured Inputs In Transformers", "abstract": "<p>Transformer models have advanced the state of the art in many Natural\nLanguage Processing (NLP) tasks. In this paper, we present a new Transformer\narchitecture, Extended Transformer Construction (ETC), that addresses two key\nchallenges of standard Transformer architectures, namely scaling input length\nand encoding structured inputs. To scale attention to longer inputs, we\nintroduce a novel global-local attention mechanism between global tokens and\nregular input tokens. We also show that combining global-local attention with\nrelative position encodings and a Contrastive Predictive Coding (CPC)\npre-training objective allows ETC to encode structured inputs. We achieve\nstate-of-the-art results on four natural language datasets requiring long\nand/or structured inputs.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "ainslie2023gqa", "citations": "105", "year": "2023", "title":"GQA: Training Generalized Multi-query Transformer Models From Multi-head Checkpoints", "abstract": "<p>Multi-query attention (MQA), which only uses a single key-value head,\ndrastically speeds up decoder inference. However, MQA can lead to quality\ndegradation, and moreover it may not be desirable to train a separate model\njust for faster inference. We (1) propose a recipe for uptraining existing\nmulti-head language model checkpoints into models with MQA using 5% of original\npre-training compute, and (2) introduce grouped-query attention (GQA), a\ngeneralization of multi-query attention which uses an intermediate (more than\none, less than number of query heads) number of key-value heads. We show that\nuptrained GQA achieves quality close to multi-head attention with comparable\nspeed to MQA.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "aitor2022solving", "citations": "236", "year": "2022", "title":"Solving Quantitative Reasoning Problems With Language Models", "abstract": "<p>Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.</p>\n", "tags": ["Tools"] },
{"key": "akbari2018multi", "citations": "68", "year": "2019", "title":"Multi-level Multimodal Common Semantic Space For Image-phrase Grounding", "abstract": "<p>We address the problem of phrase grounding by lear ing a multi-level common\nsemantic space shared by the textual and visual modalities. We exploit multiple\nlevels of feature maps of a Deep Convolutional Neural Network, as well as\ncontextualized word and sentence embeddings extracted from a character-based\nlanguage model. Following dedicated non-linear mappings for visual features at\neach level, word, and sentence embeddings, we obtain multiple instantiations of\nour common semantic space in which comparisons between any target text and the\nvisual content is performed with cosine similarity. We guide the model by a\nmulti-level multimodal attention mechanism which outputs attended visual\nfeatures at each level. The best level is chosen to be compared with text\ncontent for maximizing the pertinence scores of image-sentence pairs of the\nground truth. Experiments conducted on three publicly available datasets show\nsignificant performance gains (20%-60% relative) over the state-of-the-art in\nphrase localization and set a new performance record on those datasets. We\nprovide a detailed ablation study to show the contribution of each element of\nour approach and release our code on GitHub.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "akoury2020storium", "citations": "74", "year": "2020", "title":"STORIUM: A Dataset And Evaluation Platform For Machine-in-the-loop Story Generation", "abstract": "<p>Systems for story generation are asked to produce plausible and enjoyable\nstories given an input context. This task is underspecified, as a vast number\nof diverse stories can originate from a single input. The large output space\nmakes it difficult to build and evaluate story generation models, as (1)\nexisting datasets lack rich enough contexts to meaningfully guide models, and\n(2) existing evaluations (both crowdsourced and automatic) are unreliable for\nassessing long-form creative text. To address these issues, we introduce a\ndataset and evaluation platform built from STORIUM, an online collaborative\nstorytelling community. Our author-generated dataset contains 6K lengthy\nstories (125M tokens) with fine-grained natural language annotations (e.g.,\ncharacter goals and attributes) interspersed throughout each narrative, forming\na robust source for guiding models. We evaluate language models fine-tuned on\nour dataset by integrating them onto STORIUM, where real authors can query a\nmodel for suggested story continuations and then edit them. Automatic metrics\ncomputed over these edits correlate well with both user ratings of generated\nstories and qualitative feedback from semi-structured user interviews. We\nrelease both the STORIUM dataset and evaluation platform to spur more\nprincipled research into story generation.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Tools"] },
{"key": "alamri2019audio", "citations": "151", "year": "2019", "title":"Audio-visual Scene-aware Dialog", "abstract": "<p>We introduce the task of scene-aware dialog. Our goal is to generate a\ncomplete and natural response to a question about a scene, given video and\naudio of the scene and the history of previous turns in the dialog. To answer\nsuccessfully, agents must ground concepts from the question in the video while\nleveraging contextual cues from the dialog history. To benchmark this task, we\nintroduce the Audio Visual Scene-Aware Dialog (AVSD) Dataset. For each of more\nthan 11,000 videos of human actions from the Charades dataset, our dataset\ncontains a dialog about the video, plus a final summary of the video by one of\nthe dialog participants. We train several baseline systems for this task and\nevaluate the performance of the trained models using both qualitative and\nquantitative metrics. Our results indicate that models must utilize all the\navailable inputs (video, audio, question, and dialog history) to perform best\non this dataset.</p>\n", "tags": ["CVPR","Datasets","Evaluation"] },
{"key": "alayrac2022flamingo", "citations": "791", "year": "2022", "title":"Flamingo: A Visual Language Model For Few-shot Learning", "abstract": "<p>Building models that can be rapidly adapted to novel tasks using only a\nhandful of annotated examples is an open challenge for multimodal machine\nlearning research. We introduce Flamingo, a family of Visual Language Models\n(VLM) with this ability. We propose key architectural innovations to: (i)\nbridge powerful pretrained vision-only and language-only models, (ii) handle\nsequences of arbitrarily interleaved visual and textual data, and (iii)\nseamlessly ingest images or videos as inputs. Thanks to their flexibility,\nFlamingo models can be trained on large-scale multimodal web corpora containing\narbitrarily interleaved text and images, which is key to endow them with\nin-context few-shot learning capabilities. We perform a thorough evaluation of\nour models, exploring and measuring their ability to rapidly adapt to a variety\nof image and video tasks. These include open-ended tasks such as visual\nquestion-answering, where the model is prompted with a question which it has to\nanswer; captioning tasks, which evaluate the ability to describe a scene or an\nevent; and close-ended tasks such as multiple-choice visual question-answering.\nFor tasks lying anywhere on this spectrum, a single Flamingo model can achieve\na new state of the art with few-shot learning, simply by prompting the model\nwith task-specific examples. On numerous benchmarks, Flamingo outperforms\nmodels fine-tuned on thousands of times more task-specific data.</p>\n", "tags": ["Evaluation","Few-Shot","Prompting"] },
{"key": "albert2023mamba", "citations": "518", "year": "2023", "title":"Mamba: Linear-time Sequence Modeling With Selective State Spaces", "abstract": "<p>Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers’ computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\\(\\times\\)\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.</p>\n", "tags": ["Applications","Evaluation","Model Architecture","Time Series"] },
{"key": "albert2023mistral", "citations": "149", "year": "2023", "title":"Mistral 7B", "abstract": "<p>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses\nthe Llama 2 13B – Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.</p>\n", "tags": ["Efficiency","Instruction Following","Llm For Code","Memory & Context","Model Architecture"] },
{"key": "alberti2019bert", "citations": "111", "year": "2019", "title":"A BERT Baseline For The Natural Questions", "abstract": "<p>This technical note describes a new baseline for the Natural Questions. Our\nmodel is based on BERT and reduces the gap between the model F1 scores reported\nin the original dataset paper and the human upper bound by 30% and 50% relative\nfor the long and short answer tasks respectively. This baseline has been\nsubmitted to the official NQ leaderboard at\nai.google.com/research/NaturalQuestions. Code, preprocessed data and pretrained\nmodel are available at\nhttps://github.com/google-research/language/tree/master/language/question_answering/bert_joint.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "alberti2019fusion", "citations": "171", "year": "2019", "title":"Fusion Of Detected Objects In Text For Visual Question Answering", "abstract": "<p>To advance models of multimodal context, we introduce a simple yet powerful\nneural architecture for data that combines vision and natural language. The\n“Bounding Boxes in Text Transformer” (B2T2) also leverages referential\ninformation binding words to portions of the image in a single unified\narchitecture. B2T2 is highly effective on the Visual Commonsense Reasoning\nbenchmark (https://visualcommonsense.com), achieving a new state-of-the-art\nwith a 25% relative reduction in error rate compared to published baselines and\nobtaining the best performance to date on the public leaderboard (as of May 22,\n2019). A detailed ablation analysis shows that the early integration of the\nvisual features into the text analysis is key to the effectiveness of the new\narchitecture. A reference implementation of our models is provided\n(https://github.com/google-research/language/tree/master/language/question_answering/b2t2).</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code","Model Architecture"] },
{"key": "alberti2019synthetic", "citations": "217", "year": "2019", "title":"Synthetic QA Corpora Generation With Roundtrip Consistency", "abstract": "<p>We introduce a novel method of generating synthetic question answering\ncorpora by combining models of question generation and answer extraction, and\nby filtering the results to ensure roundtrip consistency. By pretraining on the\nresulting corpora we obtain significant improvements on SQuAD2 and NQ,\nestablishing a new state-of-the-art on the latter. Our synthetic data\ngeneration models, for both question generation and answer extraction, can be\nfully reproduced by finetuning a publicly available BERT model on the\nextractive subsets of SQuAD2 and NQ. We also describe a more powerful variant\nthat does full sequence-to-sequence pretraining for question generation,\nobtaining exact match and F1 at less than 0.1% and 0.4% from human performance\non SQuAD2.</p>\n", "tags": ["Model Architecture"] },
{"key": "alexander2016strategic", "citations": "88", "year": "2016", "title":"Strategic Attentive Writer For Learning Macro-actions", "abstract": "<p>We present a novel deep recurrent neural network architecture that learns to\nbuild implicit plans in an end-to-end manner by purely interacting with an\nenvironment in reinforcement learning setting. The network builds an internal\nplan, which is continuously updated upon observation of the next input from the\nenvironment. It can also partition this internal representation into contiguous\nsub- sequences by learning for how long the plan can be committed to - i.e.\nfollowed without re-planing. Combining these properties, the proposed model,\ndubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally\nabstracted macro- actions of varying lengths that are solely learnt from data\nwithout any prior information. These macro-actions enable both structured\nexploration and economic computation. We experimentally demonstrate that STRAW\ndelivers strong improvements on several ATARI games by employing temporally\nextended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same\ntime a general algorithm that can be applied on any sequence data. To that end,\nwe also show that when trained on text prediction task, STRAW naturally\npredicts frequent n-grams (instead of macro-actions), demonstrating the\ngenerality of the approach.</p>\n", "tags": ["Agentic","Model Architecture","Reinforcement Learning"] },
{"key": "aliannejadi2019asking", "citations": "156", "year": "2019", "title":"Asking Clarifying Questions In Open-domain Information-seeking Conversations", "abstract": "<p>Users often fail to formulate their complex information needs in a single\nquery. As a consequence, they may need to scan multiple result pages or\nreformulate their queries, which may be a frustrating experience.\nAlternatively, systems can improve user satisfaction by proactively asking\nquestions of the users to clarify their information needs. Asking clarifying\nquestions is especially important in conversational systems since they can only\nreturn a limited number of (often only one) result(s). In this paper, we\nformulate the task of asking clarifying questions in open-domain\ninformation-seeking conversational systems. To this end, we propose an offline\nevaluation methodology for the task and collect a dataset, called Qulac,\nthrough crowdsourcing. Our dataset is built on top of the TREC Web Track\n2009-2012 data and consists of over 10K question-answer pairs for 198 TREC\ntopics with 762 facets. Our experiments on an oracle model demonstrate that\nasking only one good question leads to over 170% retrieval performance\nimprovement in terms of P@1, which clearly demonstrates the potential impact of\nthe task. We further propose a retrieval framework consisting of three\ncomponents: question retrieval, question selection, and document retrieval. In\nparticular, our question selection model takes into account the original query\nand previous question-answer interactions while selecting the next question.\nOur model significantly outperforms competitive baselines. To foster research\nin this area, we have made Qulac publicly available.</p>\n", "tags": ["Datasets","Evaluation","SIGIR","Tools"] },
{"key": "almazrouei2023falcon", "citations": "80", "year": "2023", "title":"The Falcon Series Of Open Language Models", "abstract": "<p>We introduce the Falcon series: 7B, 40B, and 180B parameters causal\ndecoder-only models trained on a diverse high-quality corpora predominantly\nassembled from web data. The largest model, Falcon-180B, has been trained on\nover 3.5 trillion tokens of text–the largest openly documented pretraining\nrun. Falcon-180B significantly outperforms models such as PaLM or Chinchilla,\nand improves upon concurrently developed models such as LLaMA 2 or\nInflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining\nand inference cost, making it, to our knowledge, one of the three best language\nmodels in the world along with GPT-4 and PaLM-2-Large. We report detailed\nevaluations, as well as a deep dive into the methods and custom tooling\nemployed to pretrain Falcon. Notably, we report on our custom distributed\ntraining codebase, allowing us to efficiently pretrain these models on up to\n4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a\n600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models\nunder a permissive license to foster open-science and accelerate the\ndevelopment of an open ecosystem of large language models.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "alon2018code2seq", "citations": "400", "year": "2018", "title":"Code2seq: Generating Sequences From Structured Representations Of Code", "abstract": "<p>The ability to generate natural language sequences from source code snippets\nhas a variety of applications such as code summarization, documentation, and\nretrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine\ntranslation (NMT), have achieved state-of-the-art performance on these tasks by\ntreating source code as a sequence of tokens. We present \\({\\rm {\\scriptsize\nCODE2SEQ}}\\): an alternative approach that leverages the syntactic structure of\nprogramming languages to better encode source code. Our model represents a code\nsnippet as the set of compositional paths in its abstract syntax tree (AST) and\nuses attention to select the relevant paths while decoding. We demonstrate the\neffectiveness of our approach for two tasks, two programming languages, and\nfour datasets of up to \\(16\\)M examples. Our model significantly outperforms\nprevious models that were specifically designed for programming languages, as\nwell as state-of-the-art NMT models. An interactive online demo of our model is\navailable at http://code2seq.org. Our code, data and trained models are\navailable at http://github.com/tech-srl/code2seq.</p>\n", "tags": ["Applications","Has Code","Llm For Code","Model Architecture"] },
{"key": "alrfou2018character", "citations": "298", "year": "2019", "title":"Character-level Language Modeling With Deeper Self-attention", "abstract": "<p>LSTMs and other RNN variants have shown strong performance on character-level\nlanguage modeling. These models are typically trained using truncated\nbackpropagation through time, and it is common to assume that their success\nstems from their ability to remember long-term contexts. In this paper, we show\nthat a deep (64-layer) transformer model with fixed context outperforms RNN\nvariants by a large margin, achieving state of the art on two popular\nbenchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good\nresults at this depth, we show that it is important to add auxiliary losses,\nboth at intermediate network layers and intermediate sequence positions.</p>\n", "tags": ["AAAI","Model Architecture"] },
{"key": "alsentzer2019publicly", "citations": "664", "year": "2019", "title":"Publicly Available Clinical BERT Embeddings", "abstract": "<p>Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2018) have dramatically improved performance for many natural\nlanguage processing (NLP) tasks in recent months. However, these models have\nbeen minimally explored on specialty corpora, such as clinical text; moreover,\nin the clinical domain, no publicly-available pre-trained BERT models yet\nexist. In this work, we address this need by exploring and releasing BERT\nmodels for clinical text: one for generic clinical text and another for\ndischarge summaries specifically. We demonstrate that using a domain-specific\nmodel yields performance improvements on three common clinical NLP tasks as\ncompared to nonspecific embeddings. These domain-specific models are not as\nperformant on two clinical de-identification tasks, and argue that this is a\nnatural consequence of the differences between de-identified source text and\nsynthetically non de-identified task text.</p>\n", "tags": ["Model Architecture"] },
{"key": "alshedivat2019consistency", "citations": "64", "year": "2019", "title":"Consistency By Agreement In Zero-shot Neural Machine Translation", "abstract": "<p>Generalization and reliability of multilingual translation often highly\ndepend on the amount of available parallel data for each language pair of\ninterest. In this paper, we focus on zero-shot generalization—a challenging\nsetup that tests models on translation directions they have not been optimized\nfor at training time. To solve the problem, we (i) reformulate multilingual\ntranslation as probabilistic inference, (ii) define the notion of zero-shot\nconsistency and show why standard training often results in models unsuitable\nfor zero-shot tasks, and (iii) introduce a consistent agreement-based training\nmethod that encourages the model to produce equivalent translations of parallel\nsentences in auxiliary languages. We test our multilingual NMT models on\nmultiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl)\nand show that agreement-based learning often results in 2-3 BLEU zero-shot\nimprovement over strong baselines without any loss in performance on supervised\ntranslation directions.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "alt2019fine", "citations": "113", "year": "2019", "title":"Fine-tuning Pre-trained Transformer Language Models To Distantly Supervised Relation Extraction", "abstract": "<p>Distantly supervised relation extraction is widely used to extract relational\nfacts from text, but suffers from noisy labels. Current relation extraction\nmethods try to alleviate the noise by multi-instance learning and by providing\nsupporting linguistic and contextual information to more efficiently guide the\nrelation classification. While achieving state-of-the-art results, we observed\nthese models to be biased towards recognizing a limited set of relations with\nhigh precision, while ignoring those in the long tail. To address this gap, we\nutilize a pre-trained language model, the OpenAI Generative Pre-trained\nTransformer (GPT) [Radford et al., 2018]. The GPT and similar models have been\nshown to capture semantic and syntactic features, and also a notable amount of\n“common-sense” knowledge, which we hypothesize are important features for\nrecognizing a more diverse set of relations. By extending the GPT to the\ndistantly supervised setting, and fine-tuning it on the NYT10 dataset, we show\nthat it predicts a larger set of distinct relation types with high confidence.\nManual and automated evaluation of our model shows that it achieves a\nstate-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs\nespecially well at higher recall levels.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "alvarezmelis2017causal", "citations": "176", "year": "2017", "title":"A Causal Framework For Explaining The Predictions Of Black-box Sequence-to-sequence Models", "abstract": "<p>We interpret the predictions of any black-box structured input-structured\noutput model around a specific input-output pair. Our method returns an\n“explanation” consisting of groups of input-output tokens that are causally\nrelated. These dependencies are inferred by querying the black-box model with\nperturbed inputs, generating a graph over tokens from the responses, and\nsolving a partitioning problem to select the most relevant components. We focus\nthe general approach on sequence-to-sequence problems, adopting a variational\nautoencoder to yield meaningful input perturbations. We test our method across\nseveral NLP sequence generation tasks.</p>\n", "tags": ["EMNLP"] },
{"key": "alvarezmelis2018towards", "citations": "352", "year": "2018", "title":"Towards Robust Interpretability With Self-explaining Neural Networks", "abstract": "<p>Most recent work on interpretability of complex machine learning models has\nfocused on estimating \\(\\textit{a posteriori}\\) explanations for previously\ntrained models around specific predictions. \\(\\textit{Self-explaining}\\) models\nwhere interpretability plays a key role already during learning have received\nmuch less attention. We propose three desiderata for explanations in general –\nexplicitness, faithfulness, and stability – and show that existing methods do\nnot satisfy them. In response, we design self-explaining models in stages,\nprogressively generalizing linear classifiers to complex yet architecturally\nexplicit models. Faithfulness and stability are enforced via regularization\nspecifically tailored to such models. Experimental results across various\nbenchmark datasets show that our framework offers a promising direction for\nreconciling model complexity and interpretability.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "aman2022language", "citations": "63", "year": "2022", "title":"Language Models Of Code Are Few-shot Commonsense Learners", "abstract": "<p>We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event – or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize’’ the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.</p>\n", "tags": ["EMNLP","Few-Shot","Llm For Code","Model Architecture"] },
{"key": "amanda2021general", "citations": "88", "year": "2021", "title":"A General Language Assistant As A Laboratory For Alignment", "abstract": "<p>Given the broad capabilities of large language models, it should be possible\nto work towards a general-purpose, text-based assistant that is aligned with\nhuman values, meaning that it is helpful, honest, and harmless. As an initial\nforay in this direction we study simple baseline techniques and evaluations,\nsuch as prompting. We find that the benefits from modest interventions increase\nwith model size, generalize to a variety of alignment evaluations, and do not\ncompromise the performance of large models. Next we investigate scaling trends\nfor several training objectives relevant to alignment, comparing imitation\nlearning, binary discrimination, and ranked preference modeling. We find that\nranked preference modeling performs much better than imitation learning, and\noften scales more favorably with model size. In contrast, binary discrimination\ntypically performs and scales very similarly to imitation learning. Finally we\nstudy a `preference model pre-training’ stage of training, with the goal of\nimproving sample efficiency when finetuning on human preferences.</p>\n", "tags": ["Efficiency","Prompting","Training Techniques"] },
{"key": "amelia2022improving", "citations": "111", "year": "2022", "title":"Improving Alignment Of Dialogue Agents Via Targeted Human Judgements", "abstract": "<p>We present Sparrow, an information-seeking dialogue agent trained to be more\nhelpful, correct, and harmless compared to prompted language model baselines.\nWe use reinforcement learning from human feedback to train our models with two\nnew additions to help human raters judge agent behaviour. First, to make our\nagent more helpful and harmless, we break down the requirements for good\ndialogue into natural language rules the agent should follow, and ask raters\nabout each rule separately. We demonstrate that this breakdown enables us to\ncollect more targeted human judgements of agent behaviour and allows for more\nefficient rule-conditional reward models. Second, our agent provides evidence\nfrom sources supporting factual claims when collecting preference judgements\nover model statements. For factual questions, evidence provided by Sparrow\nsupports the sampled response 78% of the time. Sparrow is preferred more often\nthan baselines while being more resilient to adversarial probing by humans,\nviolating our rules only 8% of the time when probed. Finally, we conduct\nextensive analyses showing that though our model learns to follow our rules it\ncan exhibit distributional biases.</p>\n", "tags": ["Agentic","Evaluation","Reinforcement Learning"] },
{"key": "ammanabrolu2019story", "citations": "62", "year": "2020", "title":"Story Realization: Expanding Plot Events Into Sentences", "abstract": "<p>Neural network based approaches to automated story plot generation attempt to\nlearn how to generate novel plots from a corpus of natural language plot\nsummaries. Prior work has shown that a semantic abstraction of sentences called\nevents improves neural plot generation and and allows one to decompose the\nproblem into: (1) the generation of a sequence of events (event-to-event) and\n(2) the transformation of these events into natural language sentences\n(event-to-sentence). However, typical neural language generation approaches to\nevent-to-sentence can ignore the event details and produce\ngrammatically-correct but semantically-unrelated sentences. We present an\nensemble-based model that generates natural language guided by events.We\nprovide results—including a human subjects study—for a full end-to-end\nautomated story generation system showing that our method generates more\ncoherent and plausible stories than baseline approaches.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "anabytavor2019not", "citations": "278", "year": "2020", "title":"Not Enough Data? Deep Learning To The Rescue!", "abstract": "<p>Based on recent advances in natural language modeling and those in text\ngeneration capabilities, we propose a novel data augmentation method for text\nclassification tasks. We use a powerful pre-trained neural network model to\nartificially synthesize new labeled data for supervised learning. We mainly\nfocus on cases with scarce labeled data. Our method, referred to as\nlanguage-model-based data augmentation (LAMBADA), involves fine-tuning a\nstate-of-the-art language generator to a specific task through an initial\ntraining phase on the existing (usually small) labeled data. Using the\nfine-tuned model and given a class label, new sentences for the class are\ngenerated. Our process then filters these new sentences by using a classifier\ntrained on the original data. In a series of experiments, we show that LAMBADA\nimproves classifiers’ performance on a variety of datasets. Moreover, LAMBADA\nsignificantly improves upon the state-of-the-art techniques for data\naugmentation, specifically those applicable to text classification tasks with\nlittle data.</p>\n", "tags": ["AAAI","Datasets","Fine-Tuning","Training Techniques"] },
{"key": "anantha2020open", "citations": "92", "year": "2021", "title":"Open-domain Question Answering Goes Conversational Via Question Rewriting", "abstract": "<p>We introduce a new dataset for Question Rewriting in Conversational Context\n(QReCC), which contains 14K conversations with 80K question-answer pairs. The\ntask in QReCC is to find answers to conversational questions within a\ncollection of 10M web pages (split into 54M passages). Answers to questions in\nthe same conversation may be distributed across several web pages. QReCC\nprovides annotations that allow us to train and evaluate individual subtasks of\nquestion rewriting, passage retrieval and reading comprehension required for\nthe end-to-end conversational question answering (QA) task. We report the\neffectiveness of a strong baseline approach that combines the state-of-the-art\nmodel for question rewriting, and competitive models for open-domain QA. Our\nresults set the first baseline for the QReCC dataset with F1 of 19.10, compared\nto the human upper bound of 75.45, indicating the difficulty of the setup and a\nlarge room for improvement.</p>\n", "tags": ["Datasets","NAACL"] },
{"key": "anastasopoulos2018tied", "citations": "176", "year": "2018", "title":"Tied Multitask Learning For Neural Speech Translation", "abstract": "<p>We explore multitask models for neural translation of speech, augmenting them\nin order to reflect two intuitive notions. First, we introduce a model where\nthe second task decoder receives information from the decoder of the first\ntask, since higher-level intermediate representations should provide useful\ninformation. Second, we apply regularization that encourages transitivity and\ninvertibility. We show that the application of these notions on jointly trained\nmodels improves performance on the tasks of low-resource speech transcription\nand translation. It also leads to better performance when using attention\ninformation for word discovery over unsegmented input.</p>\n", "tags": ["Model Architecture","NAACL"] },
{"key": "anastasopoulos2019pushing", "citations": "78", "year": "2019", "title":"Pushing The Limits Of Low-resource Morphological Inflection", "abstract": "<p>Recent years have seen exceptional strides in the task of automatic\nmorphological inflection generation. However, for a long tail of languages the\nnecessary resources are hard to come by, and state-of-the-art neural methods\nthat work well under higher resource settings perform poorly in the face of a\npaucity of data. In response, we propose a battery of improvements that greatly\nimprove performance under such low-resource conditions. First, we present a\nnovel two-step attention architecture for the inflection decoder. In addition,\nwe investigate the effects of cross-lingual transfer from single and multiple\nlanguages, as well as monolingual data hallucination. The macro-averaged\naccuracy of our models outperforms the state-of-the-art by 15 percentage\npoints. Also, we identify the crucial factors for success with cross-lingual\ntransfer for morphological inflection: typological similarity and a common\nrepresentation across languages.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "anderson2016spice", "citations": "1724", "year": "2016", "title":"SPICE: Semantic Propositional Image Caption Evaluation", "abstract": "<p>There is considerable interest in the task of automatically generating image\ncaptions. However, evaluation is challenging. Existing automatic evaluation\nmetrics are primarily sensitive to n-gram overlap, which is neither necessary\nnor sufficient for the task of simulating human judgment. We hypothesize that\nsemantic propositional content is an important component of human caption\nevaluation, and propose a new automated caption evaluation metric defined over\nscene graphs coined SPICE. Extensive evaluations across a range of models and\ndatasets indicate that SPICE captures human judgments over model-generated\ncaptions better than other automatic metrics (e.g., system-level correlation of\n0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and\n0.53 for METEOR). Furthermore, SPICE can answer questions such as <code class=\"language-plaintext highlighter-rouge\">which\ncaption-generator best understands colors?' and </code>can caption-generators count?’</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "anderson2017bottom", "citations": "4664", "year": "2018", "title":"Bottom-up And Top-down Attention For Image Captioning And Visual Question Answering", "abstract": "<p>Top-down visual attention mechanisms have been used extensively in image\ncaptioning and visual question answering (VQA) to enable deeper image\nunderstanding through fine-grained analysis and even multiple steps of\nreasoning. In this work, we propose a combined bottom-up and top-down attention\nmechanism that enables attention to be calculated at the level of objects and\nother salient image regions. This is the natural basis for attention to be\nconsidered. Within our approach, the bottom-up mechanism (based on Faster\nR-CNN) proposes image regions, each with an associated feature vector, while\nthe top-down mechanism determines feature weightings. Applying this approach to\nimage captioning, our results on the MSCOCO test server establish a new\nstate-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of\n117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of\nthe method, applying the same approach to VQA we obtain first place in the 2017\nVQA Challenge.</p>\n", "tags": ["CVPR","Model Architecture"] },
{"key": "anderson2017vision", "citations": "960", "year": "2018", "title":"Vision-and-language Navigation: Interpreting Visually-grounded Navigation Instructions In Real Environments", "abstract": "<p>A robot that can carry out a natural-language instruction has been a dream\nsince before the Jetsons cartoon series imagined a life of leisure mediated by\na fleet of attentive robot helpers. It is a dream that remains stubbornly\ndistant. However, recent advances in vision and language methods have made\nincredible progress in closely related areas. This is significant because a\nrobot interpreting a natural-language navigation instruction on the basis of\nwhat it sees is carrying out a vision and language process that is similar to\nVisual Question Answering. Both tasks can be interpreted as visually grounded\nsequence-to-sequence translation problems, and many of the same methods are\napplicable. To enable and encourage the application of vision and language\nmethods to the problem of interpreting visually-grounded navigation\ninstructions, we present the Matterport3D Simulator – a large-scale\nreinforcement learning environment based on real imagery. Using this simulator,\nwhich can in future support a range of embodied vision and language tasks, we\nprovide the first benchmark dataset for visually-grounded natural language\nnavigation in real buildings – the Room-to-Room (R2R) dataset.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Reinforcement Learning"] },
{"key": "andor2019giving", "citations": "83", "year": "2019", "title":"Giving BERT A Calculator: Finding Operations And Arguments With Reading Comprehension", "abstract": "<p>Reading comprehension models have been successfully applied to extractive\ntext answers, but it is unclear how best to generalize these models to\nabstractive numerical answers. We enable a BERT-based reading comprehension\nmodel to perform lightweight numerical reasoning. We augment the model with a\npredefined set of executable ‘programs’ which encompass simple arithmetic as\nwell as extraction. Rather than having to learn to manipulate numbers directly,\nthe model can pick a program and execute it. On the recent Discrete Reasoning\nOver Passages (DROP) dataset, designed to challenge reading comprehension\nmodels, we show a 33% absolute improvement by adding shallow programs. The\nmodel can learn to predict new operations when appropriate in a math word\nproblem setting (Roy and Roth, 2015) with very few training examples.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "andreas2016learning", "citations": "489", "year": "2016", "title":"Learning To Compose Neural Networks For Question Answering", "abstract": "<p>We describe a question answering model that applies to both images and\nstructured knowledge bases. The model uses natural language strings to\nautomatically assemble neural networks from a collection of composable modules.\nParameters for these modules are learned jointly with network-assembly\nparameters via reinforcement learning, with only (world, question, answer)\ntriples as supervision. Our approach, which we term a dynamic neural model\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\nand structured domains.</p>\n", "tags": ["Datasets","Evaluation","NAACL","Reinforcement Learning"] },
{"key": "andreas2016reasoning", "citations": "122", "year": "2016", "title":"Reasoning About Pragmatics With Neural Listeners And Speakers", "abstract": "<p>We present a model for pragmatically describing scenes, in which contrastive\nbehavior results from a combination of inference-driven pragmatics and learned\nsemantics. Like previous learned approaches to language generation, our model\nuses a simple feature-driven architecture (here a pair of neural “listener” and\n“speaker” models) to ground language in the world. Like inference-driven\napproaches to pragmatics, our model actively reasons about listener behavior\nwhen selecting utterances. For training, our approach requires only ordinary\ncaptions, annotated <em>without</em> demonstration of the pragmatic behavior the model\nultimately exhibits. In human evaluations on a referring expression game, our\napproach succeeds 81% of the time, compared to a 69% success rate using\nexisting techniques.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "andreas2017learning", "citations": "82", "year": "2018", "title":"Learning With Latent Language", "abstract": "<p>The named concepts and compositional operators present in natural language\nprovide a rich source of information about the kinds of abstractions humans use\nto navigate the world. Can this linguistic background knowledge improve the\ngenerality and efficiency of learned classifiers and control policies? This\npaper aims to show that using the space of natural language strings as a\nparameter space is an effective way to capture natural task structure. In a\npretraining phase, we learn a language interpretation model that transforms\ninputs (e.g. images) into outputs (e.g. labels) given natural language\ndescriptions. To learn a new concept (e.g. a classifier), we search directly in\nthe space of descriptions to minimize the interpreter’s loss on training\nexamples. Crucially, our models do not require language data to learn these\nconcepts: language is used only in pretraining to impose structure on\nsubsequent learning. Results on image classification, text editing, and\nreinforcement learning show that, in all settings, models with a linguistic\nparameterization outperform those without.</p>\n", "tags": ["NAACL","Reinforcement Learning","Training Techniques"] },
{"key": "andreas2019good", "citations": "195", "year": "2020", "title":"Good-enough Compositional Data Augmentation", "abstract": "<p>We propose a simple data augmentation protocol aimed at providing a\ncompositional inductive bias in conditional and unconditional sequence models.\nUnder this protocol, synthetic training examples are constructed by taking real\ntraining examples and replacing (possibly discontinuous) fragments with other\nfragments that appear in at least one similar environment. The protocol is\nmodel-agnostic and useful for a variety of tasks. Applied to neural\nsequence-to-sequence models, it reduces error rate by as much as 87% on\ndiagnostic tasks from the SCAN dataset and 16% on a semantic parsing task.\nApplied to n-gram language models, it reduces perplexity by roughly 1% on small\ncorpora in several languages.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "andres2023chemcrow", "citations": "106", "year": "2023", "title":"Chemcrow: Augmenting Large-language Models With Chemistry Tools", "abstract": "<p>Over the last decades, excellent computational chemistry tools have been\ndeveloped. Integrating them into a single platform with enhanced accessibility\ncould help reaching their full potential by overcoming steep learning curves.\nRecently, large-language models (LLMs) have shown strong performance in tasks\nacross domains, but struggle with chemistry-related problems. Moreover, these\nmodels lack access to external knowledge sources, limiting their usefulness in\nscientific applications. In this study, we introduce ChemCrow, an LLM chemistry\nagent designed to accomplish tasks across organic synthesis, drug discovery,\nand materials design. By integrating 18 expert-designed tools, ChemCrow\naugments the LLM performance in chemistry, and new capabilities emerge. Our\nagent autonomously planned and executed the syntheses of an insect repellent,\nthree organocatalysts, and guided the discovery of a novel chromophore. Our\nevaluation, including both LLM and expert assessments, demonstrates ChemCrow’s\neffectiveness in automating a diverse set of chemical tasks. Surprisingly, we\nfind that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4\ncompletions and Chemcrow’s performance. Our work not only aids expert chemists\nand lowers barriers for non-experts, but also fosters scientific advancement by\nbridging the gap between experimental and computational chemistry.</p>\n", "tags": ["Agentic","Applications","Evaluation","Model Architecture","Tools"] },
{"key": "andrew2022can", "citations": "91", "year": "2022", "title":"Can Language Models Learn From Explanations In Context?", "abstract": "<p>Language Models (LMs) can perform new tasks by adapting to a few in-context\nexamples. For humans, explanations that connect examples to task principles can\nimprove learning. We therefore investigate whether explanations of few-shot\nexamples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how\ndifferent types of explanations, instructions, and controls affect zero- and\nfew-shot performance. We analyze these results using statistical multilevel\nmodeling techniques that account for the nested dependencies among conditions,\ntasks, prompts, and models. We find that explanations can improve performance\n– even without tuning. Furthermore, explanations hand-tuned for performance on\na small validation set offer substantially larger benefits, and building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Finally, even untuned explanations\noutperform carefully matched controls, suggesting that the benefits are due to\nthe link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit. In summary, explanations can\nsupport the in-context learning of large LMs on challenging tasks.</p>\n", "tags": ["EMNLP","Few-Shot","In Context Learning","Prompting"] },
{"key": "andy2022socratic", "citations": "143", "year": "2022", "title":"Socratic Models: Composing Zero-shot Multimodal Reasoning With Language", "abstract": "<p>Large pretrained (e.g., “foundation”) models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.</p>\n", "tags": ["Applications","Prompting","Tools"] },
{"key": "aneja2017convolutional", "citations": "390", "year": "2018", "title":"Convolutional Image Captioning", "abstract": "<p>Image captioning is an important but challenging task, applicable to virtual\nassistants, editing tools, image indexing, and support of the disabled. Its\nchallenges are due to the variability and ambiguity of possible image\ndescriptions. In recent years significant progress has been made in image\ncaptioning, using Recurrent Neural Networks powered by long-short-term-memory\n(LSTM) units. Despite mitigating the vanishing gradient problem, and despite\ntheir compelling ability to memorize dependencies, LSTM units are complex and\ninherently sequential across time. To address this issue, recent work has shown\nbenefits of convolutional networks for machine translation and conditional\nimage generation. Inspired by their success, in this paper, we develop a\nconvolutional image captioning technique. We demonstrate its efficacy on the\nchallenging MSCOCO dataset and demonstrate performance on par with the\nbaseline, while having a faster training time per number of parameters. We also\nperform a detailed analysis, providing compelling reasons in favor of\nconvolutional language generation approaches.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "aneja2019sequential", "citations": "60", "year": "2019", "title":"Sequential Latent Spaces For Modeling The Intention During Diverse Image Captioning", "abstract": "<p>Diverse and accurate vision+language modeling is an important goal to retain\ncreative freedom and maintain user engagement. However, adequately capturing\nthe intricacies of diversity in language models is challenging. Recent works\ncommonly resort to latent variable models augmented with more or less\nsupervision from object detectors or part-of-speech tags. Common to all those\nmethods is the fact that the latent variable either only initializes the\nsentence generation process or is identical across the steps of generation.\nBoth methods offer no fine-grained control. To address this concern, we propose\nSeq-CVAE which learns a latent space for every word position. We encourage this\ntemporal latent space to capture the ‘intention’ about how to complete the\nsentence by mimicking a representation which summarizes the future. We\nillustrate the efficacy of the proposed approach to anticipate the sentence\ncontinuation on the challenging MSCOCO dataset, significantly improving\ndiversity metrics compared to baselines while performing on par w.r.t sentence\nquality.</p>\n", "tags": ["Datasets","Evaluation","ICCV"] },
{"key": "anil2023palm", "citations": "135", "year": "2023", "title":"Palm 2 Technical Report", "abstract": "<p>We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n  When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.</p>\n", "tags": ["Efficiency","Ethics & Fairness","Evaluation Frameworks","Model Architecture"] },
{"key": "antonia2022selection", "citations": "95", "year": "2022", "title":"Selection-inference: Exploiting Large Language Models For Interpretable Logical Reasoning", "abstract": "<p>Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.</p>\n", "tags": ["Evaluation","Few-Shot","Fine-Tuning","Tools","Training Techniques"] },
{"key": "antoun2020arabert", "citations": "518", "year": "2020", "title":"Arabert: Transformer-based Model For Arabic Language Understanding", "abstract": "<p>The Arabic language is a morphologically rich language with relatively few\nresources and a less explored syntax compared to English. Given these\nlimitations, Arabic Natural Language Processing (NLP) tasks like Sentiment\nAnalysis (SA), Named Entity Recognition (NER), and Question Answering (QA),\nhave proven to be very challenging to tackle. Recently, with the surge of\ntransformers based models, language-specific BERT based models have proven to\nbe very efficient at language understanding, provided they are pre-trained on a\nvery large corpus. Such models were able to set new standards and achieve\nstate-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT\nspecifically for the Arabic language in the pursuit of achieving the same\nsuccess that BERT did for the English language. The performance of AraBERT is\ncompared to multilingual BERT from Google and other state-of-the-art\napproaches. The results showed that the newly developed AraBERT achieved\nstate-of-the-art performance on most tested Arabic NLP tasks. The pretrained\naraBERT models are publicly available on https://github.com/aub-mind/arabert\nhoping to encourage research and applications for Arabic NLP.</p>\n", "tags": ["Applications","Datasets","Has Code","Model Architecture"] },
{"key": "antoun2020araelectra", "citations": "63", "year": "2020", "title":"Araelectra: Pre-training Text Discriminators For Arabic Language Understanding", "abstract": "<p>Advances in English language representation enabled a more sample-efficient\npre-training task by Efficiently Learning an Encoder that Classifies Token\nReplacements Accurately (ELECTRA). Which, instead of training a model to\nrecover masked tokens, it trains a discriminator model to distinguish true\ninput tokens from corrupted tokens that were replaced by a generator network.\nOn the other hand, current Arabic language representation approaches rely only\non pretraining via masked language modeling. In this paper, we develop an\nArabic language representation model, which we name AraELECTRA. Our model is\npretrained using the replaced token detection objective on large Arabic text\ncorpora. We evaluate our model on multiple Arabic NLP tasks, including reading\ncomprehension, sentiment analysis, and named-entity recognition and we show\nthat AraELECTRA outperforms current state-of-the-art Arabic language\nrepresentation models, given the same pretraining data and with even a smaller\nmodel size.</p>\n", "tags": ["Training Techniques"] },
{"key": "aohan2022glm", "citations": "251", "year": "2022", "title":"GLM-130B: An Open Bilingual Pre-trained Model", "abstract": "<p>We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 (davinci) and unveil how models of such a scale\ncan be successfully pre-trained. Over the course of this effort, we face\nnumerous unexpected technical and engineering challenges, particularly on loss\nspikes and divergence. In this paper, we introduce the training process of\nGLM-130B including its design choices, training strategies for both efficiency\nand stability, and engineering efforts. The resultant GLM-130B model offers\nsignificant outperformance over GPT-3 175B (davinci) on a wide range of popular\nEnglish benchmarks while the performance advantage is not observed in OPT-175B\nand BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN\n3.0 260B – the largest Chinese language model – across related benchmarks.\nFinally, we leverage a unique scaling property of GLM-130B to reach INT4\nquantization without post training, with almost no performance loss, making it\nthe first among 100B-scale models and more importantly, allowing its effective\ninference on 4\\(\\times\\)RTX 3090 (24G) or 8\\(\\times\\)RTX 2080 Ti (11G) GPUs, the\nmost affordable GPUs required for using 100B-scale models. The GLM-130B model\nweights are publicly accessible and its code, training logs, related toolkit,\nand lessons learned are open-sourced at\nhttps://github.com/THUDM/GLM-130B/.</p>\n", "tags": ["Efficiency","Has Code","Model Architecture","Training Techniques"] },
{"key": "appalaraju2021docformer", "citations": "191", "year": "2021", "title":"Docformer: End-to-end Transformer For Document Understanding", "abstract": "<p>We present DocFormer – a multi-modal transformer based architecture for the\ntask of Visual Document Understanding (VDU). VDU is a challenging problem which\naims to understand documents in their varied formats (forms, receipts etc.) and\nlayouts. In addition, DocFormer is pre-trained in an unsupervised fashion using\ncarefully designed tasks which encourage multi-modal interaction. DocFormer\nuses text, vision and spatial features and combines them using a novel\nmulti-modal self-attention layer. DocFormer also shares learned spatial\nembeddings across modalities which makes it easy for the model to correlate\ntext to visual tokens and vice versa. DocFormer is evaluated on 4 different\ndatasets each with strong baselines. DocFormer achieves state-of-the-art\nresults on all of them, sometimes beating models 4x its size (in no. of\nparameters).</p>\n", "tags": ["Datasets","ICCV","Model Architecture"] },
{"key": "araabi2020optimizing", "citations": "60", "year": "2020", "title":"Optimizing Transformer For Low-resource Neural Machine Translation", "abstract": "<p>Language pairs with limited amounts of parallel data, also known as\nlow-resource languages, remain a challenge for neural machine translation.\nWhile the Transformer model has achieved significant improvements for many\nlanguage pairs and has become the de facto mainstream architecture, its\ncapability under low-resource conditions has not been fully investigated yet.\nOur experiments on different subsets of the IWSLT14 training data show that the\neffectiveness of Transformer under low-resource conditions is highly dependent\non the hyper-parameter settings. Our experiments show that using an optimized\nTransformer for low-resource conditions improves the translation quality up to\n7.3 BLEU points compared to using the Transformer default settings.</p>\n", "tags": ["COLING","Model Architecture","Training Techniques"] },
{"key": "aribandi2021ext5", "citations": "73", "year": "2021", "title":"Ext5: Towards Extreme Multi-task Scaling For Transfer Learning", "abstract": "<p>Despite the recent success of multi-task learning and transfer learning for\nnatural language processing (NLP), few works have systematically studied the\neffect of scaling up the number of tasks during pre-training. Towards this\ngoal, this paper introduces ExMix (Extreme Mixture): a massive collection of\n107 supervised NLP tasks across diverse domains and task-families. Using ExMix,\nwe study the effect of multi-task pre-training at the largest scale to date,\nand analyze co-training transfer amongst common families of tasks. Through this\nanalysis, we show that manually curating an ideal set of tasks for multi-task\npre-training is not straightforward, and that multi-task scaling can vastly\nimprove models on its own. Finally, we propose ExT5: a model pre-trained using\na multi-task objective of self-supervised span denoising and supervised ExMix.\nVia extensive experiments, we show that ExT5 outperforms strong T5 baselines on\nSuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of\nExMix. ExT5 also significantly improves sample efficiency while pre-training.</p>\n", "tags": ["Efficiency","Fine-Tuning","Training Techniques"] },
{"key": "arivazhagan2019massively", "citations": "320", "year": "2019", "title":"Massively Multilingual Neural Machine Translation In The Wild: Findings And Challenges", "abstract": "<p>We introduce our efforts towards building a universal neural machine\ntranslation (NMT) system capable of translating between any language pair. We\nset a milestone towards this goal by building a single massively multilingual\nNMT model handling 103 languages trained on over 25 billion examples. Our\nsystem demonstrates effective transfer learning ability, significantly\nimproving translation quality of low-resource languages, while keeping\nhigh-resource language translation quality on-par with competitive bilingual\nbaselines. We provide in-depth analysis of various aspects of model building\nthat are crucial to achieving quality and practicality in universal NMT. While\nwe prototype a high-quality universal translation system, our extensive\nempirical analysis exposes issues that need to be further addressed, and we\nsuggest directions for future research.</p>\n", "tags": ["Fine-Tuning"] },
{"key": "arivazhagan2019missing", "citations": "98", "year": "2019", "title":"The Missing Ingredient In Zero-shot Neural Machine Translation", "abstract": "<p>Multilingual Neural Machine Translation (NMT) models are capable of\ntranslating between multiple source and target languages. Despite various\napproaches to train such models, they have difficulty with zero-shot\ntranslation: translating between language pairs that were not together seen\nduring training. In this paper we first diagnose why state-of-the-art\nmultilingual NMT models that rely purely on parameter sharing, fail to\ngeneralize to unseen language pairs. We then propose auxiliary losses on the\nNMT encoder that impose representational invariance across languages. Our\nsimple approach vastly improves zero-shot translation quality without\nregressing on supervised directions. For the first time, on WMT14\nEnglish-FrenchGerman, we achieve zero-shot performance that is on par with\npivoting. We also demonstrate the easy scalability of our approach to multiple\nlanguages on the IWSLT 2017 shared task.</p>\n", "tags": ["Training Techniques"] },
{"key": "arivazhagan2019monotonic", "citations": "176", "year": "2019", "title":"Monotonic Infinite Lookback Attention For Simultaneous Machine Translation", "abstract": "<p>Simultaneous machine translation begins to translate each source sentence\nbefore the source speaker is finished speaking, with applications to live and\nstreaming scenarios. Simultaneous systems must carefully schedule their reading\nof the source sentence to balance quality against latency. We present the first\nsimultaneous translation system to learn an adaptive schedule jointly with a\nneural machine translation (NMT) model that attends over all source tokens read\nthus far. We do so by introducing Monotonic Infinite Lookback (MILk) attention,\nwhich maintains both a hard, monotonic attention head to schedule the reading\nof the source sentence, and a soft attention head that extends from the\nmonotonic head back to the beginning of the source. We show that MILk’s\nadaptive schedule allows it to arrive at latency-quality trade-offs that are\nfavorable to those of a recently proposed wait-k strategy for many latency\nvalues.</p>\n", "tags": ["Applications","Model Architecture"] },
{"key": "arivazhagan2020re", "citations": "61", "year": "2020", "title":"Re-translation Versus Streaming For Simultaneous Translation", "abstract": "<p>There has been great progress in improving streaming machine translation, a\nsimultaneous paradigm where the system appends to a growing hypothesis as more\nsource content becomes available. We study a related problem in which revisions\nto the hypothesis beyond strictly appending words are permitted. This is\nsuitable for applications such as live captioning an audio feed. In this\nsetting, we compare custom streaming approaches to re-translation, a\nstraightforward strategy where each new source token triggers a distinct\ntranslation from scratch. We find re-translation to be as good or better than\nstate-of-the-art streaming systems, even when operating under constraints that\nallow very few revisions. We attribute much of this success to a previously\nproposed data-augmentation technique that adds prefix-pairs to the training\ndata, which alongside wait-k inference forms a strong baseline for streaming\ntranslation. We also highlight re-translation’s ability to wrap arbitrarily\npowerful MT systems with an experiment showing large improvements from an\nupgrade to its base model.</p>\n", "tags": ["Applications","Training Techniques"] },
{"key": "arora2020inltk", "citations": "65", "year": "2020", "title":"Inltk: Natural Language Toolkit For Indic Languages", "abstract": "<p>We present iNLTK, an open-source NLP library consisting of pre-trained\nlanguage models and out-of-the-box support for Data Augmentation, Textual\nSimilarity, Sentence Embeddings, Word Embeddings, Tokenization and Text\nGeneration in 13 Indic Languages. By using pre-trained models from iNLTK for\ntext classification on publicly available datasets, we significantly outperform\npreviously reported results. On these datasets, we also show that by using\npre-trained models and data augmentation from iNLTK, we can achieve more than\n95% of the previous best performance by using less than 10% of the training\ndata. iNLTK is already being widely used by the community and has 40,000+\ndownloads, 600+ stars and 100+ forks on GitHub. The library is available at\nhttps://github.com/goru001/inltk.</p>\n", "tags": ["Datasets","Has Code","Tools","Training Techniques"] },
{"key": "arras2016explaining", "citations": "100", "year": "2016", "title":"Explaining Predictions Of Non-linear Classifiers In NLP", "abstract": "<p>Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a “word deleting” perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies.</p>\n", "tags": ["EMNLP","NAACL"] },
{"key": "arras2019evaluating", "citations": "69", "year": "2019", "title":"Evaluating Recurrent Neural Network Explanations", "abstract": "<p>Recently, several methods have been proposed to explain the predictions of\nrecurrent neural networks (RNNs), in particular of LSTMs. The goal of these\nmethods is to understand the network’s decisions by assigning to each input\nvariable, e.g., a word, a relevance indicating to which extent it contributed\nto a particular prediction. In previous works, some of these methods were not\nyet compared to one another, or were evaluated only qualitatively. We close\nthis gap by systematically and quantitatively comparing these methods in\ndifferent settings, namely (1) a toy arithmetic task which we use as a sanity\ncheck, (2) a five-class sentiment prediction of movie reviews, and besides (3)\nwe explore the usefulness of word relevances to build sentence-level\nrepresentations. Lastly, using the method that performed best in our\nexperiments, we show how specific linguistic phenomena such as the negation in\nsentiment analysis reflect in terms of relevance patterns, and how the\nrelevance visualization can help to understand the misclassification of\nindividual samples.</p>\n", "tags": ["Evaluation"] },
{"key": "artetxe2017unsupervised", "citations": "632", "year": "2017", "title":"Unsupervised Neural Machine Translation", "abstract": "<p>In spite of the recent success of neural machine translation (NMT) in\nstandard benchmarks, the lack of large parallel corpora poses a major practical\nproblem for many language pairs. There have been several proposals to alleviate\nthis issue with, for instance, triangulation and semi-supervised learning\ntechniques, but they still require a strong cross-lingual signal. In this work,\nwe completely remove the need of parallel data and propose a novel method to\ntrain an NMT system in a completely unsupervised manner, relying on nothing but\nmonolingual corpora. Our model builds upon the recent work on unsupervised\nembedding mappings, and consists of a slightly modified attentional\nencoder-decoder model that can be trained on monolingual corpora alone using a\ncombination of denoising and backtranslation. Despite the simplicity of the\napproach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014\nFrench-to-English and German-to-English translation. The model can also profit\nfrom small parallel corpora, and attains 21.81 and 15.24 points when combined\nwith 100,000 parallel sentences, respectively. Our implementation is released\nas an open source project.</p>\n", "tags": ["Training Techniques"] },
{"key": "artetxe2019cross", "citations": "566", "year": "2020", "title":"On The Cross-lingual Transferability Of Monolingual Representations", "abstract": "<p>State-of-the-art unsupervised multilingual models (e.g., multilingual BERT)\nhave been shown to generalize in a zero-shot cross-lingual setting. This\ngeneralization ability has been attributed to the use of a shared subword\nvocabulary and joint training across multiple languages giving rise to deep\nmultilingual abstractions. We evaluate this hypothesis by designing an\nalternative approach that transfers a monolingual model to new languages at the\nlexical level. More concretely, we first train a transformer-based masked\nlanguage model on one language, and transfer it to a new language by learning a\nnew embedding matrix with the same masked language modeling objective, freezing\nparameters of all other layers. This approach does not rely on a shared\nvocabulary or joint training. However, we show that it is competitive with\nmultilingual BERT on standard cross-lingual classification benchmarks and on a\nnew Cross-lingual Question Answering Dataset (XQuAD). Our results contradict\ncommon beliefs of the basis of the generalization ability of multilingual\nmodels and suggest that deep monolingual models learn some abstractions that\ngeneralize across languages. We also release XQuAD as a more comprehensive\ncross-lingual benchmark, which comprises 240 paragraphs and 1190\nquestion-answer pairs from SQuAD v1.1 translated into ten languages by\nprofessional translators.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "artetxe2020translation", "citations": "68", "year": "2020", "title":"Translation Artifacts In Cross-lingual Transfer Learning", "abstract": "<p>Both human and machine translation play a central role in cross-lingual\ntransfer learning: many multilingual datasets have been created through\nprofessional translation services, and using machine translation to translate\neither the test set or the training set is a widely used transfer technique. In\nthis paper, we show that such translation process can introduce subtle\nartifacts that have a notable impact in existing cross-lingual models. For\ninstance, in natural language inference, translating the premise and the\nhypothesis independently can reduce the lexical overlap between them, which\ncurrent models are highly sensitive to. We show that some previous findings in\ncross-lingual transfer learning need to be reconsidered in the light of this\nphenomenon. Based on the gained insights, we also improve the state-of-the-art\nin XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points,\nrespectively.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "asai2018multilingual", "citations": "66", "year": "2018", "title":"Multilingual Extractive Reading Comprehension By Runtime Machine Translation", "abstract": "<p>Despite recent work in Reading Comprehension (RC), progress has been mostly\nlimited to English due to the lack of large-scale datasets in other languages.\nIn this work, we introduce the first RC system for languages without RC\ntraining data. Given a target language without RC training data and a pivot\nlanguage with RC training data (e.g. English), our method leverages existing RC\nresources in the pivot language by combining a competitive RC model in the\npivot language with an attentive Neural Machine Translation (NMT) model. We\nfirst translate the data from the target to the pivot language, and then obtain\nan answer using the RC model in the pivot language. Finally, we recover the\ncorresponding answer in the original language using soft-alignment attention\nscores from the NMT model. We create evaluation sets of RC data in two\nnon-English languages, namely Japanese and French, to evaluate our method.\nExperimental results on these datasets show that our method significantly\noutperforms a back-translation baseline of a state-of-the-art product-level\nmachine translation system.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "asai2019learning", "citations": "159", "year": "2019", "title":"Learning To Retrieve Reasoning Paths Over Wikipedia Graph For Question Answering", "abstract": "<p>Answering questions that require multi-hop reasoning at web-scale\nnecessitates retrieving multiple evidence documents, one of which often has\nlittle lexical or semantic relationship to the question. This paper introduces\na new graph-based recurrent retrieval approach that learns to retrieve\nreasoning paths over the Wikipedia graph to answer multi-hop open-domain\nquestions. Our retriever model trains a recurrent neural network that learns to\nsequentially retrieve evidence paragraphs in the reasoning path by conditioning\non the previously retrieved documents. Our reader model ranks the reasoning\npaths and extracts the answer span included in the best reasoning path.\nExperimental results show state-of-the-art results in three open-domain QA\ndatasets, showcasing the effectiveness and robustness of our method. Notably,\nour method achieves significant improvement in HotpotQA, outperforming the\nprevious best model by more than 14 points.</p>\n", "tags": ["Datasets","Retrieval Systems"] },
{"key": "asai2020logic", "citations": "63", "year": "2020", "title":"Logic-guided Data Augmentation And Regularization For Consistent Question Answering", "abstract": "<p>Many natural language questions require qualitative, quantitative or logical\ncomparisons between two entities or events. This paper addresses the problem of\nimproving the accuracy and consistency of responses to comparison questions by\nintegrating logic rules and neural models. Our method leverages logical and\nlinguistic knowledge to augment labeled training data and then uses a\nconsistency-based regularizer to train the model. Improving the global\nconsistency of predictions, our approach achieves large improvements over\nprevious methods in a variety of question answering (QA) tasks including\nmultiple-choice qualitative reasoning, cause-effect reasoning, and extractive\nmachine reading comprehension. In particular, our method significantly improves\nthe performance of RoBERTa-based models by 1-5% across datasets. We advance the\nstate of the art by around 5-8% on WIQA and QuaRel and reduce consistency\nviolations by 58% on HotpotQA. We further demonstrate that our approach can\nlearn effectively from limited data.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "asai2020xor", "citations": "85", "year": "2021", "title":"XOR QA: Cross-lingual Open-retrieval Question Answering", "abstract": "<p>Multilingual question answering tasks typically assume answers exist in the\nsame language as the question. Yet in practice, many languages face both\ninformation scarcity – where languages have few reference articles – and\ninformation asymmetry – where questions reference concepts from other\ncultures. This work extends open-retrieval question answering to a\ncross-lingual setting enabling questions from one language to be answered via\nanswer content from another language. We construct a large-scale dataset built\non questions from TyDi QA lacking same-language answers. Our task formulation,\ncalled Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k\ninformation-seeking questions from across 7 diverse non-English languages.\nBased on this dataset, we introduce three new tasks that involve cross-lingual\ndocument retrieval using multi-lingual and English resources. We establish\nbaselines with state-of-the-art machine translation systems and cross-lingual\npretrained models. Experimental results suggest that XOR QA is a challenging\ntask that will facilitate the development of novel techniques for multilingual\nquestion answering. Our data and code are available at\nhttps://nlp.cs.washington.edu/xorqa.</p>\n", "tags": ["Datasets","NAACL"] },
{"key": "asghar2017affective", "citations": "152", "year": "2018", "title":"Affective Neural Response Generation", "abstract": "<p>Existing neural conversational models process natural language primarily on a\nlexico-syntactic level, thereby ignoring one of the most crucial components of\nhuman-to-human dialogue: its affective content. We take a step in this\ndirection by proposing three novel ways to incorporate affective/emotional\naspects into long short term memory (LSTM) encoder-decoder neural conversation\nmodels: (1) affective word embeddings, which are cognitively engineered, (2)\naffect-based objective functions that augment the standard cross-entropy loss,\nand (3) affectively diverse beam search for decoding. Experiments show that\nthese techniques improve the open-domain conversational prowess of\nencoder-decoder networks by enabling them to produce emotionally rich responses\nthat are more interesting and natural.</p>\n", "tags": ["Model Architecture"] },
{"key": "ashish2017attention", "citations": "59469", "year": "2017", "title":"Attention Is All You Need", "abstract": "<p>The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "asri2016sequence", "citations": "91", "year": "2016", "title":"A Sequence-to-sequence Model For User Simulation In Spoken Dialogue Systems", "abstract": "<p>User simulation is essential for generating enough data to train a\nstatistical spoken dialogue system. Previous models for user simulation suffer\nfrom several drawbacks, such as the inability to take dialogue history into\naccount, the need of rigid structure to ensure coherent user behaviour, heavy\ndependence on a specific domain, the inability to output several user\nintentions during one dialogue turn, or the requirement of a summarized action\nspace for tractability. This paper introduces a data-driven user simulator\nbased on an encoder-decoder recurrent neural network. The model takes as input\na sequence of dialogue contexts and outputs a sequence of dialogue acts\ncorresponding to user intentions. The dialogue contexts include information\nabout the machine acts and the status of the user goal. We show on the Dialogue\nState Tracking Challenge 2 (DSTC2) dataset that the sequence-to-sequence model\noutperforms an agenda-based simulator and an n-gram simulator, according to\nF-score. Furthermore, we show how this model can be used on the original action\nspace and thereby models user behaviour with finer granularity.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","INTERSPEECH"] },
{"key": "asri2017frames", "citations": "230", "year": "2017", "title":"Frames: A Corpus For Adding Memory To Goal-oriented Dialogue Systems", "abstract": "<p>This paper presents the Frames dataset (Frames is available at\nhttp://datasets.maluuba.com/Frames), a corpus of 1369 human-human dialogues\nwith an average of 15 turns per dialogue. We developed this dataset to study\nthe role of memory in goal-oriented dialogue systems. Based on Frames, we\nintroduce a task called frame tracking, which extends state tracking to a\nsetting where several states are tracked simultaneously. We propose a baseline\nmodel for this task. We show that Frames can also be used to study memory in\ndialogue management and information presentation through natural language\ngeneration.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn"] },
{"key": "atzmon2016learning", "citations": "61", "year": "2016", "title":"Learning To Generalize To New Compositions In Image Understanding", "abstract": "<p>Recurrent neural networks have recently been used for learning to describe\nimages using natural language. However, it has been observed that these models\ngeneralize poorly to scenes that were not observed during training, possibly\ndepending too strongly on the statistics of the text in the training data. Here\nwe propose to describe images using short structured representations, aiming to\ncapture the crux of a description. These structured representations allow us to\ntease-out and evaluate separately two types of generalization: standard\ngeneralization to new images with similar scenes, and generalization to new\ncombinations of known entities. We compare two learning approaches on the\nMS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,\nAttend and Tell), and a simple structured prediction model on top of a deep\nnetwork. We find that the structured model generalizes to new compositions\nsubstantially better than the LSTM, ~7 times the accuracy of predicting\nstructured representations. By providing a concrete method to quantify\ngeneralization for unseen combinations, we argue that structured\nrepresentations and compositional splits are a useful benchmark for image\ncaptioning, and advocate compositional models that capture linguistic and\nvisual structure.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "austin2021program", "citations": "218", "year": "2021", "title":"Program Synthesis With Large Language Models", "abstract": "<p>This paper explores the limits of the current generation of large language\nmodels for program synthesis in general purpose programming languages. We\nevaluate a collection of such models (with between 244M and 137B parameters) on\ntwo new benchmarks, MBPP and MathQA-Python, in both the few-shot and\nfine-tuning regimes. Our benchmarks are designed to measure the ability of\nthese models to synthesize short Python programs from natural language\ndescriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974\nprogramming tasks, designed to be solvable by entry-level programmers. The\nMathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914\nproblems that evaluate the ability of the models to synthesize code from more\ncomplex text. On both datasets, we find that synthesis performance scales\nlog-linearly with model size. Our largest models, even without finetuning on a\ncode dataset, can synthesize solutions to 59.6 percent of the problems from\nMBPP using few-shot learning with a well-designed prompt. Fine-tuning on a\nheld-out portion of the dataset improves performance by about 10 percentage\npoints across most model sizes. On the MathQA-Python dataset, the largest\nfine-tuned model achieves 83.8 percent accuracy. Going further, we study the\nmodel’s ability to engage in dialog about code, incorporating human feedback to\nimprove its solutions. We find that natural language feedback from a human\nhalves the error rate compared to the model’s initial prediction. Additionally,\nwe conduct an error analysis to shed light on where these models fall short and\nwhat types of programs are most difficult to generate. Finally, we explore the\nsemantic grounding of these models by fine-tuning them to predict the results\nof program execution. We find that even our best models are generally unable to\npredict the output of a program given a specific input.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Prompting","Training Techniques"] },
{"key": "avrahami2021blended", "citations": "436", "year": "2022", "title":"Blended Diffusion For Text-driven Editing Of Natural Images", "abstract": "<p>Natural language offers a highly intuitive interface for image editing. In\nthis paper, we introduce the first solution for performing local (region-based)\nedits in generic natural images, based on a natural language description along\nwith an ROI mask. We achieve our goal by leveraging and combining a pretrained\nlanguage-image model (CLIP), to steer the edit towards a user-provided text\nprompt, with a denoising diffusion probabilistic model (DDPM) to generate\nnatural-looking results. To seamlessly fuse the edited region with the\nunchanged parts of the image, we spatially blend noised versions of the input\nimage with the local text-guided diffusion latent at a progression of noise\nlevels. In addition, we show that adding augmentations to the diffusion process\nmitigates adversarial results. We compare against several baselines and related\nmethods, both qualitatively and quantitatively, and show that our method\noutperforms these solutions in terms of overall realism, ability to preserve\nthe background and matching the text. Finally, we show several text-driven\nediting applications, including adding a new object to an image,\nremoving/replacing/altering existing objects, background replacement, and image\nextrapolation. Code is available at:\nhttps://omriavrahami.com/blended-diffusion-page/</p>\n", "tags": ["Applications","CVPR","Prompting"] },
{"key": "avrahami2022spatext", "citations": "88", "year": "2023", "title":"Spatext: Spatio-textual Representation For Controllable Image Generation", "abstract": "<p>Recent text-to-image diffusion models are able to generate convincing results\nof unprecedented quality. However, it is nearly impossible to control the\nshapes of different regions/objects or their layout in a fine-grained fashion.\nPrevious attempts to provide such controls were hindered by their reliance on a\nfixed set of labels. To this end, we present SpaText - a new method for\ntext-to-image generation using open-vocabulary scene control. In addition to a\nglobal text prompt that describes the entire scene, the user provides a\nsegmentation map where each region of interest is annotated by a free-form\nnatural language description. Due to lack of large-scale datasets that have a\ndetailed textual description for each region in the image, we choose to\nleverage the current large-scale text-to-image datasets and base our approach\non a novel CLIP-based spatio-textual representation, and show its effectiveness\non two state-of-the-art diffusion models: pixel-based and latent-based. In\naddition, we show how to extend the classifier-free guidance method in\ndiffusion models to the multi-conditional case and present an alternative\naccelerated inference algorithm. Finally, we offer several automatic evaluation\nmetrics and use them, in addition to FID scores and a user study, to evaluate\nour method and show that it achieves state-of-the-art results on image\ngeneration with free-form textual scene control.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Prompting"] },
{"key": "azuma2021scanqa", "citations": "61", "year": "2022", "title":"Scanqa: 3D Question Answering For Spatial Scene Understanding", "abstract": "<p>We propose a new 3D spatial understanding task of 3D Question Answering\n(3D-QA). In the 3D-QA task, models receive visual information from the entire\n3D scene of the rich RGB-D indoor scan and answer the given textual questions\nabout the 3D scene. Unlike the 2D-question answering of VQA, the conventional\n2D-QA models suffer from problems with spatial understanding of object\nalignment and directions and fail the object identification from the textual\nquestions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model,\nwhere the model learns a fused descriptor from 3D object proposals and encoded\nsentence embeddings. This learned descriptor correlates the language\nexpressions with the underlying geometric features of the 3D scan and\nfacilitates the regression of 3D bounding boxes to determine described objects\nin textual questions and outputs correct answers. We collected human-edited\nquestion-answer pairs with free-form answers that are grounded to 3D objects in\neach 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs\nfrom the 800 indoor scenes drawn from the ScanNet dataset. To the best of our\nknowledge, the proposed 3D-QA task is the first large-scale effort to perform\nobject-grounded question-answering in 3D environments.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "babu2021xls", "citations": "291", "year": "2022", "title":"XLS-R: Self-supervised Cross-lingual Speech Representation Learning At Scale", "abstract": "<p>This paper presents XLS-R, a large-scale model for cross-lingual speech\nrepresentation learning based on wav2vec 2.0. We train models with up to 2B\nparameters on nearly half a million hours of publicly available speech audio in\n128 languages, an order of magnitude more public data than the largest known\nprior work. Our evaluation covers a wide range of tasks, domains, data regimes\nand languages, both high and low-resource. On the CoVoST-2 speech translation\nbenchmark, we improve the previous state of the art by an average of 7.4 BLEU\nover 21 translation directions into English. For speech recognition, XLS-R\nimproves over the best known prior work on BABEL, MLS, CommonVoice as well as\nVoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets\na new state of the art on VoxLingua107 language identification. Moreover, we\nshow that with sufficient model size, cross-lingual pretraining can outperform\nEnglish-only pretraining when translating English speech into other languages,\na setting which favors monolingual pretraining. We hope XLS-R can help to\nimprove speech processing tasks for many more languages of the world.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH","Training Techniques"] },
{"key": "bach2022promptsource", "citations": "133", "year": "2022", "title":"Promptsource: An Integrated Development Environment And Repository For Natural Language Prompts", "abstract": "<p>PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.</p>\n", "tags": ["Datasets","Has Code","Prompting","Tools"] },
{"key": "bae2019summary", "citations": "63", "year": "2019", "title":"Summary Level Training Of Sentence Rewriting For Abstractive Summarization", "abstract": "<p>As an attempt to combine extractive and abstractive summarization, Sentence\nRewriting models adopt the strategy of extracting salient sentences from a\ndocument first and then paraphrasing the selected ones to generate a summary.\nHowever, the existing models in this framework mostly rely on sentence-level\nrewards or suboptimal labels, causing a mismatch between a training objective\nand evaluation metric. In this paper, we present a novel training signal that\ndirectly maximizes summary-level ROUGE scores through reinforcement learning.\nIn addition, we incorporate BERT into our model, making good use of its ability\non natural language understanding. In extensive experiments, we show that a\ncombination of our proposed model and training procedure obtains new\nstate-of-the-art performance on both CNN/Daily Mail and New York Times\ndatasets. We also demonstrate that it generalizes better on DUC-2002 test set.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "baevski2018adaptive", "citations": "266", "year": "2018", "title":"Adaptive Input Representations For Neural Language Modeling", "abstract": "<p>We introduce adaptive input representations for neural language modeling\nwhich extend the adaptive softmax of Grave et al. (2017) to input\nrepresentations of variable capacity. There are several choices on how to\nfactorize the input and output layers, and whether to model words, characters\nor sub-word units. We perform a systematic comparison of popular choices for a\nself-attentional architecture. Our experiments show that models equipped with\nadaptive embeddings are more than twice as fast to train than the popular\ncharacter input CNN while having a lower number of parameters. On the\nWikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5\nperplexity compared to the previously best published result and on the Billion\nWord benchmark, we achieve 23.02 perplexity.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "baevski2019cloze", "citations": "203", "year": "2019", "title":"Cloze-driven Pretraining Of Self-attention Networks", "abstract": "<p>We present a new approach for pretraining a bi-directional transformer model\nthat provides significant performance gains across a variety of language\nunderstanding problems. Our model solves a cloze-style word reconstruction\ntask, where each word is ablated and must be predicted given the rest of the\ntext. Experiments demonstrate large performance gains on GLUE and new state of\nthe art results on NER as well as constituency parsing benchmarks, consistent\nwith the concurrently introduced BERT model. We also present a detailed\nanalysis of a number of factors that contribute to effective pretraining,\nincluding data domain and size, model capacity, and variations on the cloze\nobjective.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "bahar2019comparative", "citations": "80", "year": "2019", "title":"A Comparative Study On End-to-end Speech To Text Translation", "abstract": "<p>Recent advances in deep learning show that end-to-end speech to text\ntranslation model is a promising approach to direct the speech translation\nfield. In this work, we provide an overview of different end-to-end\narchitectures, as well as the usage of an auxiliary connectionist temporal\nclassification (CTC) loss for better convergence. We also investigate on\npre-training variants such as initializing different components of a model\nusing pre-trained models, and their impact on the final performance, which\ngives boosts up to 4% in BLEU and 5% in TER. Our experiments are performed on\n270h IWSLT TED-talks En-&gt;De, and 100h LibriSpeech Audiobooks En-&gt;Fr. We also\nshow improvements over the current end-to-end state-of-the-art systems on both\ntasks.</p>\n", "tags": ["ASRU","Training Techniques"] },
{"key": "bahdanau2016actor", "citations": "326", "year": "2016", "title":"An Actor-critic Algorithm For Sequence Prediction", "abstract": "<p>We present an approach to training neural networks to generate sequences\nusing actor-critic methods from reinforcement learning (RL). Current\nlog-likelihood training methods are limited by the discrepancy between their\ntraining and testing modes, as models must generate tokens conditioned on their\nprevious guesses rather than the ground-truth tokens. We address this problem\nby introducing a \\textit{critic} network that is trained to predict the value\nof an output token, given the policy of an \\textit{actor} network. This results\nin a training procedure that is much closer to the test phase, and allows us to\ndirectly optimize for a task-specific score such as BLEU. Crucially, since we\nleverage these techniques in the supervised learning setting rather than the\ntraditional RL setting, we condition the critic network on the ground-truth\noutput. We show that our method leads to improved performance on both a\nsynthetic task, and for German-English machine translation. Our analysis paves\nthe way for such methods to be applied in natural language generation tasks,\nsuch as machine translation, caption generation, and dialogue modelling.</p>\n", "tags": ["Agentic","Reinforcement Learning","Training Techniques"] },
{"key": "bahdanau2017learning", "citations": "77", "year": "2017", "title":"Learning To Compute Word Embeddings On The Fly", "abstract": "<p>Words in natural language follow a Zipfian distribution whereby some words\nare frequent but most are rare. Learning representations for words in the “long\ntail” of this distribution requires enormous amounts of data. Representations\nof rare words trained directly on end tasks are usually poor, requiring us to\npre-train embeddings on external data, or treat all rare words as\nout-of-vocabulary words with a unique representation. We provide a method for\npredicting embeddings of rare words on the fly from small amounts of auxiliary\ndata with a network trained end-to-end for the downstream task. We show that\nthis improves results against baselines where embeddings are trained on the end\ntask for reading comprehension, recognizing textual entailment and language\nmodeling.</p>\n", "tags": [] },
{"key": "bahdanau2018learning", "citations": "64", "year": "2018", "title":"Learning To Understand Goal Specifications By Modelling Reward", "abstract": "<p>Recent work has shown that deep reinforcement-learning agents can learn to\nfollow language-like instructions from infrequent environment rewards. However,\nthis places on environment designers the onus of designing language-conditional\nreward functions which may not be easily or tractably implemented as the\ncomplexity of the environment and the language scales. To overcome this\nlimitation, we present a framework within which instruction-conditional RL\nagents are trained using rewards obtained not from the environment, but from\nreward models which are jointly trained from expert examples. As reward models\nimprove, they learn to accurately reward agents for completing tasks for\nenvironment configurations—and for instructions—not present amongst the\nexpert data. This framework effectively separates the representation of what\ninstructions require from how they can be executed. In a simple grid world, it\nenables an agent to learn a range of commands requiring interaction with blocks\nand understanding of spatial relations and underspecified abstract\narrangements. We further show the method allows our agent to adapt to changes\nin the environment without requiring new expert examples.</p>\n", "tags": ["Agentic","Reinforcement Learning","Tools"] },
{"key": "bahdanau2018systematic", "citations": "78", "year": "2018", "title":"Systematic Generalization: What Is Required And Can It Be Learned?", "abstract": "<p>Numerous models for grounded language understanding have been recently\nproposed, including (i) generic models that can be easily adapted to any given\ntask and (ii) intuitively appealing modular models that require background\nknowledge to be instantiated. We compare both types of models in how much they\nlend themselves to a particular form of systematic generalization. Using a\nsynthetic VQA test, we evaluate which models are capable of reasoning about all\npossible object pairs after training on only a small subset of them. Our\nfindings show that the generalization of modular models is much more systematic\nand that it is highly sensitive to the module layout, i.e. to how exactly the\nmodules are connected. We furthermore investigate if modular models that\ngeneralize well could be made more end-to-end by learning their layout and\nparametrization. We find that end-to-end methods from prior work often learn\ninappropriate layouts or parametrizations that do not facilitate systematic\ngeneralization. Our results suggest that, in addition to modularity, systematic\ngeneralization in language understanding may require explicit regularizers or\npriors.</p>\n", "tags": ["Training Techniques"] },
{"key": "baheti2018generating", "citations": "99", "year": "2018", "title":"Generating More Interesting Responses In Neural Conversation Models With Distributional Constraints", "abstract": "<p>Neural conversation models tend to generate safe, generic responses for most\ninputs. This is due to the limitations of likelihood-based decoding objectives\nin generation tasks with diverse outputs, such as conversation. To address this\nchallenge, we propose a simple yet effective approach for incorporating side\ninformation in the form of distributional constraints over the generated\nresponses. We propose two constraints that help generate more content rich\nresponses that are based on a model of syntax and topics (Griffiths et al.,\n2005) and semantic similarity (Arora et al., 2016). We evaluate our approach\nagainst a variety of competitive baselines, using both automatic metrics and\nhuman judgments, showing that our proposed approach generates responses that\nare much less generic without sacrificing plausibility. A working demo of our\ncode can be found at https://github.com/abaheti95/DC-NeuralConversation.</p>\n", "tags": ["EMNLP","Evaluation","Has Code"] },
{"key": "bahng2022exploring", "citations": "97", "year": "2022", "title":"Exploring Visual Prompts For Adapting Large-scale Models", "abstract": "<p>We investigate the efficacy of visual prompting to adapt large-scale models\nin vision. Following the recent approach from prompt tuning and adversarial\nreprogramming, we learn a single image perturbation such that a frozen model\nprompted with this perturbation performs a new task. Through comprehensive\nexperiments, we demonstrate that visual prompting is particularly effective for\nCLIP and robust to distribution shift, achieving performance competitive with\nstandard linear probes. We further analyze properties of the downstream\ndataset, prompt design, and output transformation in regard to adaptation\nperformance. The surprising effectiveness of visual prompting provides a new\nperspective on adapting pre-trained models in vision. Code is available at\nhttp://hjbahng.github.io/visual_prompting .</p>\n", "tags": ["Datasets","Has Code","Prompting"] },
{"key": "bai2018empirical", "citations": "3968", "year": "2018", "title":"An Empirical Evaluation Of Generic Convolutional And Recurrent Networks For Sequence Modeling", "abstract": "<p>For most deep learning practitioners, sequence modeling is synonymous with\nrecurrent networks. Yet recent results indicate that convolutional\narchitectures can outperform recurrent networks on tasks such as audio\nsynthesis and machine translation. Given a new sequence modeling task or\ndataset, which architecture should one use? We conduct a systematic evaluation\nof generic convolutional and recurrent architectures for sequence modeling. The\nmodels are evaluated across a broad range of standard tasks that are commonly\nused to benchmark recurrent networks. Our results indicate that a simple\nconvolutional architecture outperforms canonical recurrent networks such as\nLSTMs across a diverse range of tasks and datasets, while demonstrating longer\neffective memory. We conclude that the common association between sequence\nmodeling and recurrent networks should be reconsidered, and convolutional\nnetworks should be regarded as a natural starting point for sequence modeling\ntasks. To assist related work, we have made code available at\nhttp://github.com/locuslab/TCN .</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Time Series"] },
{"key": "bai2018trellis", "citations": "70", "year": "2018", "title":"Trellis Networks For Sequence Modeling", "abstract": "<p>We present trellis networks, a new architecture for sequence modeling. On the\none hand, a trellis network is a temporal convolutional network with special\nstructure, characterized by weight tying across depth and direct injection of\nthe input into deep layers. On the other hand, we show that truncated recurrent\nnetworks are equivalent to trellis networks with special sparsity structure in\ntheir weight matrices. Thus trellis networks with general weight matrices\ngeneralize truncated recurrent networks. We leverage these connections to\ndesign high-performing trellis networks that absorb structural and algorithmic\nelements from both recurrent and convolutional models. Experiments demonstrate\nthat trellis networks outperform the current state of the art methods on a\nvariety of challenging benchmarks, including word-level language modeling and\ncharacter-level language modeling tasks, and stress tests designed to evaluate\nlong-term memory retention. The code is available at\nhttps://github.com/locuslab/trellisnet .</p>\n", "tags": ["Has Code","Model Architecture","Time Series"] },
{"key": "bai2020binarybert", "citations": "69", "year": "2021", "title":"Binarybert: Pushing The Limit Of BERT Quantization", "abstract": "<p>The rapid development of large pre-trained language models has greatly\nincreased the demand for model compression techniques, among which quantization\nis a popular solution. In this paper, we propose BinaryBERT, which pushes BERT\nquantization to the limit by weight binarization. We find that a binary BERT is\nhard to be trained directly than a ternary counterpart due to its complex and\nirregular loss landscape. Therefore, we propose ternary weight splitting, which\ninitializes BinaryBERT by equivalently splitting from a half-sized ternary\nnetwork. The binary model thus inherits the good performance of the ternary\none, and can be further enhanced by fine-tuning the new architecture after\nsplitting. Empirical results show that our BinaryBERT has only a slight\nperformance drop compared with the full-precision model while being 24x\nsmaller, achieving the state-of-the-art compression results on the GLUE and\nSQuAD benchmarks.</p>\n", "tags": ["Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "bai2021are", "citations": "87", "year": "2021", "title":"Are Transformers More Robust Than Cnns?", "abstract": "<p>Transformer emerges as a powerful tool for visual recognition. In addition to\ndemonstrating competitive performance on a broad range of visual benchmarks,\nrecent works also argue that Transformers are much more robust than\nConvolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these\nconclusions are drawn from unfair experimental settings, where Transformers and\nCNNs are compared at different scales and are applied with distinct training\nframeworks. In this paper, we aim to provide the first fair &amp; in-depth\ncomparisons between Transformers and CNNs, focusing on robustness evaluations.\n  With our unified training setup, we first challenge the previous belief that\nTransformers outshine CNNs when measuring adversarial robustness. More\nsurprisingly, we find CNNs can easily be as robust as Transformers on defending\nagainst adversarial attacks, if they properly adopt Transformers’ training\nrecipes. While regarding generalization on out-of-distribution samples, we show\npre-training on (external) large-scale datasets is not a fundamental request\nfor enabling Transformers to achieve better performance than CNNs. Moreover,\nour ablations suggest such stronger generalization is largely benefited by the\nTransformer’s self-attention-like architectures per se, rather than by other\ntraining setups. We hope this work can help the community better understand and\nbenchmark the robustness of Transformers and CNNs. The code and models are\npublicly available at https://github.com/ytongbai/ViTs-vs-CNNs.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Security","Training Techniques"] },
{"key": "bai2022training", "citations": "251", "year": "2022", "title":"Training A Helpful And Harmless Assistant With Reinforcement Learning From Human Feedback", "abstract": "<p>We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization. We explore an iterated online mode of\ntraining, where preference models and RL policies are updated on a weekly\ncadence with fresh human feedback data, efficiently improving our datasets and\nmodels. Finally, we investigate the robustness of RLHF training, and identify a\nroughly linear relation between the RL reward and the square root of the KL\ndivergence between the policy and its initialization. Alongside our main\nresults, we perform peripheral analyses on calibration, competing objectives,\nand the use of OOD detection, compare our models with human writers, and\nprovide samples from our models using prompts appearing in recent related work.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning","Security","Training Techniques"] },
{"key": "bajaj2016ms", "citations": "1353", "year": "2016", "title":"MS MARCO: A Human Generated Machine Reading Comprehension Dataset", "abstract": "<p>We introduce a large scale MAchine Reading COmprehension dataset, which we\nname MS MARCO. The dataset comprises of 1,010,916 anonymized\nquestions—sampled from Bing’s search query logs—each with a human generated\nanswer and 182,669 completely human rewritten generated answers. In addition,\nthe dataset contains 8,841,823 passages—extracted from 3,563,535 web\ndocuments retrieved by Bing—that provide the information necessary for\ncurating the natural language answers. A question in the MS MARCO dataset may\nhave multiple answers or no answers at all. Using this dataset, we propose\nthree different tasks with varying levels of difficulty: (i) predict if a\nquestion is answerable given a set of context passages, and extract and\nsynthesize the answer as a human would (ii) generate a well-formed answer (if\npossible) based on the context passages that can be understood with the\nquestion and passage context, and finally (iii) rank a set of retrieved\npassages given a question. The size of the dataset and the fact that the\nquestions are derived from real user search queries distinguishes MS MARCO from\nother well-known publicly available datasets for machine reading comprehension\nand question-answering. We believe that the scale and the real-world nature of\nthis dataset makes it attractive for benchmarking machine reading comprehension\nand question-answering models.</p>\n", "tags": ["Datasets"] },
{"key": "bajgar2016embracing", "citations": "63", "year": "2016", "title":"Embracing Data Abundance: Booktest Dataset For Reading Comprehension", "abstract": "<p>There is a practically unlimited amount of natural language data available.\nStill, recent work in text comprehension has focused on datasets which are\nsmall relative to current computing possibilities. This article is making a\ncase for the community to move to larger data and as a step in that direction\nit is proposing the BookTest, a new dataset similar to the popular Children’s\nBook Test (CBT), however more than 60 times larger. We show that training on\nthe new data improves the accuracy of our Attention-Sum Reader model on the\noriginal CBT test data by a much larger margin than many recent attempts to\nimprove the model architecture. On one version of the dataset our ensemble even\nexceeds the human baseline provided by Facebook. We then show in our own human\nstudy that there is still space for further improvement.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "bakker2022fine", "citations": "77", "year": "2022", "title":"Fine-tuning Language Models To Find Agreement Among Humans With Diverse Preferences", "abstract": "<p>Recent work in large language modeling (LLMs) has used fine-tuning to align\noutputs with the preferences of a prototypical user. This work assumes that\nhuman preferences are static and homogeneous across individuals, so that\naligning to a a single “generic” user will confer more general alignment. Here,\nwe embrace the heterogeneity of human preferences to consider a different\nchallenge: how might a machine help people with diverse views find agreement?\nWe fine-tune a 70 billion parameter LLM to generate statements that maximize\nthe expected approval for a group of people with potentially diverse opinions.\nHuman participants provide written opinions on thousands of questions touching\non moral and political issues (e.g., “should we raise taxes on the rich?”), and\nrate the LLM’s generated candidate consensus statements for agreement and\nquality. A reward model is then trained to predict individual preferences,\nenabling it to quantify and rank consensus statements in terms of their appeal\nto the overall group, defined according to different aggregation (social\nwelfare) functions. The model produces consensus statements that are preferred\nby human users over those from prompted LLMs (&gt;70%) and significantly\noutperforms a tight fine-tuned baseline that lacks the final ranking step.\nFurther, our best model’s consensus statements are preferred over the best\nhuman-generated opinions (&gt;65%). We find that when we silently constructed\nconsensus statements from only a subset of group members, those who were\nexcluded were more likely to dissent, revealing the sensitivity of the\nconsensus to individual contributions. These results highlight the potential to\nuse LLMs to help groups of humans align their values with one another.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "balaji2022ediff", "citations": "200", "year": "2022", "title":"Ediff-i: Text-to-image Diffusion Models With An Ensemble Of Expert Denoisers", "abstract": "<p>Large-scale diffusion-based generative models have led to breakthroughs in\ntext-conditioned high-resolution image synthesis. Starting from random noise,\nsuch text-to-image diffusion models gradually synthesize images in an iterative\nfashion while conditioning on text prompts. We find that their synthesis\nbehavior qualitatively changes throughout this process: Early in sampling,\ngeneration strongly relies on the text prompt to generate text-aligned content,\nwhile later, the text conditioning is almost entirely ignored. This suggests\nthat sharing model parameters throughout the entire generation process may not\nbe ideal. Therefore, in contrast to existing works, we propose to train an\nensemble of text-to-image diffusion models specialized for different synthesis\nstages. To maintain training efficiency, we initially train a single model,\nwhich is then split into specialized models that are trained for the specific\nstages of the iterative generation process. Our ensemble of diffusion models,\ncalled eDiff-I, results in improved text alignment while maintaining the same\ninference computation cost and preserving high visual quality, outperforming\nprevious large-scale text-to-image diffusion models on the standard benchmark.\nIn addition, we train our model to exploit a variety of embeddings for\nconditioning, including the T5 text, CLIP text, and CLIP image embeddings. We\nshow that these different embeddings lead to different behaviors. Notably, the\nCLIP image embedding allows an intuitive way of transferring the style of a\nreference image to the target text-to-image output. Lastly, we show a technique\nthat enables eDiff-I’s “paint-with-words” capability. A user can select the\nword in the input text and paint it in a canvas to control the output, which is\nvery handy for crafting the desired image in mind. The project page is\navailable at https://deepimagination.cc/eDiff-I/</p>\n", "tags": ["Datasets","Evaluation","Has Code","Prompting","Training Techniques"] },
{"key": "balakrishnan2019constrained", "citations": "77", "year": "2019", "title":"Constrained Decoding For Neural NLG From Compositional Representations In Task-oriented Dialogue", "abstract": "<p>Generating fluent natural language responses from structured semantic\nrepresentations is a critical step in task-oriented conversational systems.\nAvenues like the E2E NLG Challenge have encouraged the development of neural\napproaches, particularly sequence-to-sequence (Seq2Seq) models for this\nproblem. The semantic representations used, however, are often underspecified,\nwhich places a higher burden on the generation model for sentence planning, and\nalso limits the extent to which generated responses can be controlled in a live\nsystem. In this paper, we (1) propose using tree-structured semantic\nrepresentations, like those used in traditional rule-based NLG systems, for\nbetter discourse-level structuring and sentence-level planning; (2) introduce a\nchallenging dataset using this representation for the weather domain; (3)\nintroduce a constrained decoding approach for Seq2Seq models that leverages\nthis representation to improve semantic correctness; and (4) demonstrate\npromising results on our dataset and the E2E dataset.</p>\n", "tags": ["Datasets"] },
{"key": "ballesteros2016training", "citations": "66", "year": "2016", "title":"Training With Exploration Improves A Greedy Stack-lstm Parser", "abstract": "<p>We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to\nsupport a training-with-exploration procedure using dynamic oracles(Goldberg\nand Nivre, 2013) instead of cross-entropy minimization. This form of training,\nwhich accounts for model predictions at training time rather than assuming an\nerror-free action history, improves parsing accuracies for both English and\nChinese, obtaining very strong results for both languages. We discuss some\nmodifications needed in order to get training with exploration to work well for\na probabilistic neural-network.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "ballesteros2017amr", "citations": "74", "year": "2017", "title":"AMR Parsing Using Stack-lstms", "abstract": "<p>We present a transition-based AMR parser that directly generates AMR parses\nfrom plain text. We use Stack-LSTMs to represent our parser state and make\ndecisions greedily. In our experiments, we show that our parser achieves very\ncompetitive scores on English using only AMR training data. Adding additional\ninformation, such as POS tags and dependency trees, improves the results\nfurther.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "bansal2018pre", "citations": "176", "year": "2019", "title":"Pre-training On High-resource Speech Recognition Improves Low-resource Speech-to-text Translation", "abstract": "<p>We present a simple approach to improve direct speech-to-text translation\n(ST) when the source language is low-resource: we pre-train the model on a\nhigh-resource automatic speech recognition (ASR) task, and then fine-tune its\nparameters for ST. We demonstrate that our approach is effective by\npre-training on 300 hours of English ASR data to improve Spanish-English ST\nfrom 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data\nare available. Through an ablation study, we find that the pre-trained encoder\n(acoustic model) accounts for most of the improvement, despite the fact that\nthe shared language in these tasks is the target language text, not the source\nlanguage audio. Applying this insight, we show that pre-training on ASR helps\nST even when the ASR language differs from both source and target ST languages:\npre-training on French ASR also improves Spanish-English ST. Finally, we show\nthat the approach improves performance on a true low-resource task:\npre-training on a combination of English ASR and French ASR improves\nMboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1\nBLEU.</p>\n", "tags": ["Training Techniques"] },
{"key": "bansal2019learning", "citations": "93", "year": "2020", "title":"Learning To Few-shot Learn Across Diverse Natural Language Classification Tasks", "abstract": "<p>Self-supervised pre-training of transformer models has shown enormous success\nin improving performance on a number of downstream tasks. However, fine-tuning\non a new task still requires large amounts of task-specific labelled data to\nachieve good performance. We consider this problem of learning to generalize to\nnew tasks with few examples as a meta-learning problem. While meta-learning has\nshown tremendous progress in recent years, its application is still limited to\nsimulated problems or problems with limited diversity across tasks. We develop\na novel method, LEOPARD, which enables optimization-based meta-learning across\ntasks with different number of classes, and evaluate different methods on\ngeneralization to diverse NLP classification tasks. LEOPARD is trained with the\nstate-of-the-art transformer architecture and shows better generalization to\ntasks not seen at all during training, with as few as 4 examples per label.\nAcross 17 NLP tasks, including diverse domains of entity typing, natural\nlanguage inference, sentiment analysis, and several other text classification\ntasks, we show that LEOPARD learns better initial parameters for few-shot\nlearning than self-supervised pre-training or multi-task training,\noutperforming many strong baselines, for example, yielding 14.5% average\nrelative gain in accuracy on unseen tasks with only 4 examples per label.</p>\n", "tags": ["COLING","Efficiency","Few-Shot","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "bansal2020does", "citations": "414", "year": "2021", "title":"Does The Whole Exceed Its Parts? The Effect Of AI Explanations On Complementary Team Performance", "abstract": "<p>Many researchers motivate explainable AI with studies showing that human-AI\nteam performance on decision-making tasks improves when the AI explains its\nrecommendations. However, prior studies observed improvements from explanations\nonly when the AI, alone, outperformed both the human and the best team. Can\nexplanations help lead to complementary performance, where team accuracy is\nhigher than either the human or the AI working solo? We conduct mixed-method\nuser studies on three datasets, where an AI with accuracy comparable to humans\nhelps participants solve a task (explaining itself in some conditions). While\nwe observed complementary improvements from AI augmentation, they were not\nincreased by explanations. Rather, explanations increased the chance that\nhumans will accept the AI’s recommendation, regardless of its correctness. Our\nresult poses new challenges for human-centered AI: Can we develop explanatory\napproaches that encourage appropriate trust in AI, and therefore help generate\n(or improve) complementary performance?</p>\n", "tags": [] },
{"key": "bansal2020self", "citations": "68", "year": "2020", "title":"Self-supervised Meta-learning For Few-shot Natural Language Classification Tasks", "abstract": "<p>Self-supervised pre-training of transformer models has revolutionized NLP\napplications. Such pre-training with language modeling objectives provides a\nuseful initial point for parameters that generalize well to new tasks with\nfine-tuning. However, fine-tuning is still data inefficient – when there are\nfew labeled examples, accuracy can be low. Data efficiency can be improved by\noptimizing pre-training directly for future fine-tuning with few examples; this\ncan be treated as a meta-learning problem. However, standard meta-learning\ntechniques require many training tasks in order to generalize; unfortunately,\nfinding a diverse set of such supervised tasks is usually difficult. This paper\nproposes a self-supervised approach to generate a large, rich, meta-learning\ntask distribution from unlabeled text. This is achieved using a cloze-style\nobjective, but creating separate multi-class classification tasks by gathering\ntokens-to-be blanked from among only a handful of vocabulary terms. This yields\nas many unique meta-training tasks as the number of subsets of vocabulary\nterms. We meta-train a transformer model on this distribution of tasks using a\nrecent meta-learning framework. On 17 NLP tasks, we show that this\nmeta-training leads to better few-shot generalization than language-model\npre-training followed by finetuning. Furthermore, we show how the\nself-supervised tasks can be combined with supervised tasks for meta-learning,\nproviding substantial accuracy gains over previous supervised meta-learning.</p>\n", "tags": ["Applications","EMNLP","Few-Shot","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "bao2018deriving", "citations": "92", "year": "2018", "title":"Deriving Machine Attention From Human Rationales", "abstract": "<p>Attention-based models are successful when trained on large amounts of data.\nIn this paper, we demonstrate that even in the low-resource scenario, attention\ncan be learned effectively. To this end, we start with discrete human-annotated\nrationales and map them into continuous attention. Our central hypothesis is\nthat this mapping is general across domains, and thus can be transferred from\nresource-rich domains to low-resource ones. Our model jointly learns a\ndomain-invariant representation and induces the desired mapping between\nrationales and attention. Our empirical results validate this hypothesis and\nshow that our approach delivers significant gains over state-of-the-art\nbaselines, yielding over 15% average error reduction on benchmark datasets.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "bao2019plato", "citations": "217", "year": "2020", "title":"PLATO: Pre-trained Dialogue Generation Model With Discrete Latent Variable", "abstract": "<p>Pre-training models have been proved effective for a wide range of natural\nlanguage processing tasks. Inspired by this, we propose a novel dialogue\ngeneration pre-training framework to support various kinds of conversations,\nincluding chit-chat, knowledge grounded dialogues, and conversational question\nanswering. In this framework, we adopt flexible attention mechanisms to fully\nleverage the bi-directional context and the uni-directional characteristic of\nlanguage generation. We also introduce discrete latent variables to tackle the\ninherent one-to-many mapping problem in response generation. Two reciprocal\ntasks of response generation and latent act recognition are designed and\ncarried out simultaneously within a shared network. Comprehensive experiments\non three publicly available datasets verify the effectiveness and superiority\nof the proposed framework.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture","Tools","Training Techniques"] },
{"key": "bao2020plato", "citations": "64", "year": "2021", "title":"PLATO-2: Towards Building An Open-domain Chatbot Via Curriculum Learning", "abstract": "<p>To build a high-quality open-domain chatbot, we introduce the effective\ntraining process of PLATO-2 via curriculum learning. There are two stages\ninvolved in the learning process. In the first stage, a coarse-grained\ngeneration model is trained to learn response generation under the simplified\nframework of one-to-one mapping. In the second stage, a fine-grained generative\nmodel augmented with latent variables and an evaluation model are further\ntrained to generate diverse responses and to select the best response,\nrespectively. PLATO-2 was trained on both Chinese and English data, whose\neffectiveness and superiority are verified through comprehensive evaluations,\nachieving new state-of-the-art results.</p>\n", "tags": ["Evaluation","Tools","Training Techniques"] },
{"key": "bao2020unilmv2", "citations": "172", "year": "2020", "title":"Unilmv2: Pseudo-masked Language Models For Unified Language Model Pre-training", "abstract": "<p>We propose to pre-train a unified language model for both autoencoding and\npartially autoregressive language modeling tasks using a novel training\nprocedure, referred to as a pseudo-masked language model (PMLM). Given an input\ntext with masked tokens, we rely on conventional masks to learn inter-relations\nbetween corrupted tokens and context via autoencoding, and pseudo masks to\nlearn intra-relations between masked spans via partially autoregressive\nmodeling. With well-designed position embeddings and self-attention masks, the\ncontext encodings are reused to avoid redundant computation. Moreover,\nconventional masks used for autoencoding provide global masking information, so\nthat all the position embeddings are accessible in partially autoregressive\nlanguage modeling. In addition, the two tasks pre-train a unified language\nmodel as a bidirectional encoder and a sequence-to-sequence decoder,\nrespectively. Our experiments show that the unified language models pre-trained\nusing PMLM achieve new state-of-the-art results on a wide range of natural\nlanguage understanding and generation tasks across several widely used\nbenchmarks.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "bao2021vlmo", "citations": "185", "year": "2021", "title":"Vlmo: Unified Vision-language Pre-training With Mixture-of-modality-experts", "abstract": "<p>We present a unified Vision-Language pretrained Model (VLMo) that jointly\nlearns a dual encoder and a fusion encoder with a modular Transformer network.\nSpecifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,\nwhere each block contains a pool of modality-specific experts and a shared\nself-attention layer. Because of the modeling flexibility of MoME, pretrained\nVLMo can be fine-tuned as a fusion encoder for vision-language classification\ntasks, or used as a dual encoder for efficient image-text retrieval. Moreover,\nwe propose a stagewise pre-training strategy, which effectively leverages\nlarge-scale image-only and text-only data besides image-text pairs.\nExperimental results show that VLMo achieves state-of-the-art results on\nvarious vision-language tasks, including VQA, NLVR2 and image-text retrieval.\nThe code and pretrained models are available at https://aka.ms/vlmo.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "bao2023tallrec", "citations": "150", "year": "2023", "title":"Tallrec: An Effective And Efficient Tuning Framework To Align Large Language Model With Recommendation", "abstract": "<p>Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains, thereby prompting researchers to explore their potential for\nuse in recommendation systems. Initial attempts have leveraged the exceptional\ncapabilities of LLMs, such as rich knowledge and strong generalization through\nIn-context Learning, which involves phrasing the recommendation task as\nprompts. Nevertheless, the performance of LLMs in recommendation tasks remains\nsuboptimal due to a substantial disparity between the training tasks for LLMs\nand recommendation tasks, as well as inadequate recommendation data during\npre-training. To bridge the gap, we consider building a Large Recommendation\nLanguage Model by tunning LLMs with recommendation data. To this end, we\npropose an efficient and effective Tuning framework for Aligning LLMs with\nRecommendation, namely TALLRec. We have demonstrated that the proposed TALLRec\nframework can significantly enhance the recommendation capabilities of LLMs in\nthe movie and book domains, even with a limited dataset of fewer than 100\nsamples. Additionally, the proposed framework is highly efficient and can be\nexecuted on a single RTX 3090 with LLaMA-7B. Furthermore, the fine-tuned LLM\nexhibits robust cross-domain generalization. Our code and data are available at\nhttps://github.com/SAI990323/TALLRec.</p>\n", "tags": ["Datasets","Has Code","In Context Learning","Prompting","Tools","Training Techniques"] },
{"key": "baolin2023check", "citations": "121", "year": "2023", "title":"Check Your Facts And Try Again: Improving Large Language Models With External Knowledge And Automated Feedback", "abstract": "<p>Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT’s hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.</p>\n", "tags": ["Applications"] },
{"key": "bapna2017towards", "citations": "134", "year": "2017", "title":"Towards Zero-shot Frame Semantic Parsing For Domain Scaling", "abstract": "<p>State-of-the-art slot filling models for goal-oriented human/machine\nconversational language understanding systems rely on deep learning methods.\nWhile multi-task training of such models alleviates the need for large\nin-domain annotated datasets, bootstrapping a semantic parsing model for a new\ndomain using only the semantic frame, such as the back-end API or knowledge\ngraph schema, is still one of the holy grail tasks of language understanding\nfor dialogue systems. This paper proposes a deep learning based approach that\ncan utilize only the slot description in context without the need for any\nlabeled or unlabeled in-domain examples, to quickly bootstrap a new domain. The\nmain idea of this paper is to leverage the encoding of the slot names and\ndescriptions within a multi-task deep learned slot filling model, to implicitly\nalign slots across domains. The proposed approach is promising for solving the\ndomain scaling problem and eliminating the need for any manually annotated data\nor explicit schema alignment. Furthermore, our experiments on multiple domains\nshow that this approach results in significantly better slot-filling\nperformance when compared to using only in-domain data, especially in the low\ndata regime.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","INTERSPEECH","Tools","Training Techniques"] },
{"key": "bapna2018training", "citations": "145", "year": "2018", "title":"Training Deeper Neural Machine Translation Models With Transparent Attention", "abstract": "<p>While current state-of-the-art NMT models, such as RNN seq2seq and\nTransformers, possess a large number of parameters, they are still shallow in\ncomparison to convolutional models used for both text and vision applications.\nIn this work we attempt to train significantly (2-3x) deeper Transformer and\nBi-RNN encoders for machine translation. We propose a simple modification to\nthe attention mechanism that eases the optimization of deeper models, and\nresults in consistent gains of 0.7-1.1 BLEU on the benchmark WMT’14\nEnglish-German and WMT’15 Czech-English tasks for both architectures.</p>\n", "tags": ["Applications","Datasets","EMNLP","Evaluation","Model Architecture","Training Techniques"] },
{"key": "bapna2019non", "citations": "76", "year": "2019", "title":"Non-parametric Adaptation For Neural Machine Translation", "abstract": "<p>Neural Networks trained with gradient descent are known to be susceptible to\ncatastrophic forgetting caused by parameter shift during the training process.\nIn the context of Neural Machine Translation (NMT) this results in poor\nperformance on heterogeneous datasets and on sub-tasks like rare phrase\ntranslation. On the other hand, non-parametric approaches are immune to\nforgetting, perfectly complementing the generalization ability of NMT. However,\nattempts to combine non-parametric or retrieval based approaches with NMT have\nonly been successful on narrow domains, possibly due to over-reliance on\nsentence level retrieval. We propose a novel n-gram level retrieval approach\nthat relies on local phrase level similarities, allowing us to retrieve\nneighbors that are useful for translation even when overall sentence similarity\nis low. We complement this with an expressive neural network, allowing our\nmodel to extract information from the noisy retrieved context. We evaluate our\nsemi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT,\nJRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets.\nThe semi-parametric nature of our approach opens the door for non-parametric\ndomain adaptation, demonstrating strong inference-time adaptation performance\non new domains without the need for any parameter updates.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "barbieri2017are", "citations": "130", "year": "2017", "title":"Are Emojis Predictable?", "abstract": "<p>Emojis are ideograms which are naturally combined with plain text to visually\ncomplement or condense the meaning of a message. Despite being widely used in\nsocial media, their underlying semantics have received little attention from a\nNatural Language Processing standpoint. In this paper, we investigate the\nrelation between words and emojis, studying the novel task of predicting which\nemojis are evoked by text-based tweet messages. We train several models based\non Long Short-Term Memory networks (LSTMs) in this task. Our experimental\nresults show that our neural model outperforms two baselines as well as humans\nsolving the same task, suggesting that computational models are able to better\ncapture the underlying semantics of emojis.</p>\n", "tags": ["Model Architecture","NAACL"] },
{"key": "barbieri2021xlm", "citations": "102", "year": "2021", "title":"XLM-T: Multilingual Language Models In Twitter For Sentiment Analysis And Beyond", "abstract": "<p>Language models are ubiquitous in current NLP, and their multilingual\ncapacity has recently attracted considerable attention. However, current\nanalyses have almost exclusively focused on (multilingual variants of) standard\nbenchmarks, and have relied on clean pre-training and task-specific corpora as\nmultilingual signals. In this paper, we introduce XLM-T, a model to train and\nevaluate multilingual language models in Twitter. In this paper we provide: (1)\na new strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020)\nmodel pre-trained on millions of tweets in over thirty languages, alongside\nstarter code to subsequently fine-tune on a target task; and (2) a set of\nunified sentiment analysis Twitter datasets in eight different languages and a\nXLM-T model fine-tuned on them.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "barikeri2021redditbias", "citations": "82", "year": "2021", "title":"Redditbias: A Real-world Resource For Bias Evaluation And Debiasing Of Conversational Language Models", "abstract": "<p>Text representation models are prone to exhibit a range of societal biases,\nreflecting the non-controlled and biased nature of the underlying pretraining\ndata, which consequently leads to severe ethical issues and even bias\namplification. Recent work has predominantly focused on measuring and\nmitigating bias in pretrained language models. Surprisingly, the landscape of\nbias measurements and mitigation resources and methods for conversational\nlanguage models is still very scarce: it is limited to only a few types of\nbias, artificially constructed resources, and completely ignores the impact\nthat debiasing methods may have on the final performance in dialog tasks, e.g.,\nconversational response generation. In this work, we present RedditBias, the\nfirst conversational data set grounded in the actual human conversations from\nReddit, allowing for bias measurement and mitigation across four important bias\ndimensions: gender, race, religion, and queerness. Further, we develop an\nevaluation framework which simultaneously 1) measures bias on the developed\nRedditBias resource, and 2) evaluates model capability in dialog tasks after\nmodel debiasing. We use the evaluation framework to benchmark the widely used\nconversational DialoGPT model along with the adaptations of four debiasing\nmethods. Our results indicate that DialoGPT is biased with respect to religious\ngroups and that some debiasing techniques can remove this bias while preserving\ndownstream task performance.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Tools"] },
{"key": "barke2022grounded", "citations": "200", "year": "2023", "title":"Grounded Copilot: How Programmers Interact With Code-generating Models", "abstract": "<p>Powered by recent advances in code-generating models, AI assistants like\nGithub Copilot promise to change the face of programming forever. But what is\nthis new face of programming? We present the first grounded theory analysis of\nhow programmers interact with Copilot, based on observing 20 participants–with\na range of prior experience using the assistant–as they solve diverse\nprogramming tasks across four languages. Our main finding is that interactions\nwith programming assistants are bimodal: in acceleration mode, the programmer\nknows what to do next and uses Copilot to get there faster; in exploration\nmode, the programmer is unsure how to proceed and uses Copilot to explore their\noptions. Based on our theory, we provide recommendations for improving the\nusability of future AI programming assistants.</p>\n", "tags": ["Has Code","Llm For Code"] },
{"key": "barone2017deep", "citations": "107", "year": "2017", "title":"Deep Architectures For Neural Machine Translation", "abstract": "<p>It has been shown that increasing model depth improves the quality of neural\nmachine translation. However, different architectural variants to increase\nmodel depth have been proposed, and so far, there has been no thorough\ncomparative study.\n  In this work, we describe and evaluate several existing approaches to\nintroduce depth in neural machine translation. Additionally, we explore novel\narchitectural variants, including deep transition RNNs, and we vary how\nattention is used in the deep decoder. We introduce a novel “BiDeep” RNN\narchitecture that combines deep transition RNNs and stacked RNNs.\n  Our evaluation is carried out on the English to German WMT news translation\ndataset, using a single-GPU machine for both training and inference. We find\nthat several of our proposed architectures improve upon existing approaches in\nterms of speed and translation quality. We obtain best improvements with a\nBiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU\nover a strong shallow baseline.\n  We release our code for ease of adoption.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "barone2017parallel", "citations": "68", "year": "2017", "title":"A Parallel Corpus Of Python Functions And Documentation Strings For Automated Code Documentation And Code Generation", "abstract": "<p>Automated documentation of programming source code and automated code\ngeneration from natural language are challenging tasks of both practical and\nscientific interest. Progress in these areas has been limited by the low\navailability of parallel corpora of code and natural language descriptions,\nwhich tend to be small and constrained to specific domains.\n  In this work we introduce a large and diverse parallel corpus of a hundred\nthousands Python functions with their documentation strings (“docstrings”)\ngenerated by scraping open source repositories on GitHub. We describe baseline\nresults for the code documentation and code generation tasks obtained by neural\nmachine translation. We also experiment with data augmentation techniques to\nfurther increase the amount of training data.\n  We release our datasets and processing scripts in order to stimulate research\nin these areas.</p>\n", "tags": ["Datasets","Has Code","Llm For Code","Training Techniques"] },
{"key": "barone2017regularization", "citations": "97", "year": "2017", "title":"Regularization Techniques For Fine-tuning In Neural Machine Translation", "abstract": "<p>We investigate techniques for supervised domain adaptation for neural machine\ntranslation where an existing model trained on a large out-of-domain dataset is\nadapted to a small in-domain dataset. In this scenario, overfitting is a major\nchallenge. We investigate a number of techniques to reduce overfitting and\nimprove transfer learning, including regularization techniques such as dropout\nand L2-regularization towards an out-of-domain prior. In addition, we introduce\ntuneout, a novel regularization technique inspired by dropout. We apply these\ntechniques, alone and in combination, to neural machine translation, obtaining\nimprovements on IWSLT datasets for English-&gt;German and English-&gt;Russian. We\nalso investigate the amounts of in-domain training data needed for domain\nadaptation in NMT, and find a logarithmic relationship between the amount of\ntraining data and gain in BLEU score.</p>\n", "tags": ["EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "barrett2018measuring", "citations": "133", "year": "2018", "title":"Measuring Abstract Reasoning In Neural Networks", "abstract": "<p>Whether neural networks can learn abstract reasoning or whether they merely\nrely on superficial statistics is a topic of recent debate. Here, we propose a\ndataset and challenge designed to probe abstract reasoning, inspired by a\nwell-known human IQ test. To succeed at this challenge, models must cope with\nvarious generalisation `regimes’ in which the training and test data differ in\nclearly-defined ways. We show that popular models such as ResNets perform\npoorly, even when the training and test sets differ only minimally, and we\npresent a novel architecture, with a structure designed to encourage reasoning,\nthat does significantly better. When we vary the way in which the test\nquestions and training data differ, we find that our model is notably\nproficient at certain forms of generalisation, but notably weak at others. We\nfurther show that the model’s ability to generalise improves markedly if it is\ntrained to predict symbolic explanations for its answers. Altogether, we\nintroduce and explore ways to both measure and induce stronger abstract\nreasoning in neural networks. Our freely-available dataset should motivate\nfurther progress in this direction.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "bartolo2020beat", "citations": "120", "year": "2020", "title":"Beat The AI: Investigating Adversarial Human Annotation For Reading Comprehension", "abstract": "<p>Innovations in annotation methodology have been a catalyst for Reading\nComprehension (RC) datasets and models. One recent trend to challenge current\nRC models is to involve a model in the annotation process: humans create\nquestions adversarially, such that the model fails to answer them correctly. In\nthis work we investigate this annotation methodology and apply it in three\ndifferent settings, collecting a total of 36,000 samples with progressively\nstronger models in the annotation loop. This allows us to explore questions\nsuch as the reproducibility of the adversarial effect, transfer from data\ncollected with varying model-in-the-loop strengths, and generalisation to data\ncollected without a model. We find that training on adversarially collected\nsamples leads to strong generalisation to non-adversarially collected datasets,\nyet with progressive performance deterioration with increasingly stronger\nmodels-in-the-loop. Furthermore, we find that stronger models can still learn\nfrom datasets collected with substantially weaker models-in-the-loop. When\ntrained on data collected with a BiDAF model in the loop, RoBERTa achieves\n39.9F1 on questions that it cannot answer when trained on SQuAD - only\nmarginally lower than when trained on data collected using RoBERTa itself\n(41.0F1).</p>\n", "tags": ["Datasets","Security","TACL","Training Techniques"] },
{"key": "bartolo2021improving", "citations": "62", "year": "2021", "title":"Improving Question Answering Model Robustness With Synthetic Adversarial Data Generation", "abstract": "<p>Despite recent progress, state-of-the-art question answering models remain\nvulnerable to a variety of adversarial attacks. While dynamic adversarial data\ncollection, in which a human annotator tries to write examples that fool a\nmodel-in-the-loop, can improve model robustness, this process is expensive\nwhich limits the scale of the collected data. In this work, we are the first to\nuse synthetic adversarial data generation to make question answering models\nmore robust to human adversaries. We develop a data generation pipeline that\nselects source passages, identifies candidate answers, generates questions,\nthen finally filters or re-labels them to improve quality. Using this approach,\nwe amplify a smaller human-written adversarial dataset to a much larger set of\nsynthetic question-answer pairs. By incorporating our synthetic data, we\nimprove the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve\nmodel generalisation on nine of the twelve MRQA datasets. We further conduct a\nnovel human-in-the-loop evaluation to show that our models are considerably\nmore robust to new human-written adversarial examples: crowdworkers can fool\nour model only 8.8% of the time on average, compared to 17.6% for a model\ntrained without synthetic data.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Security"] },
{"key": "bastianelli2020slurp", "citations": "105", "year": "2020", "title":"SLURP: A Spoken Language Understanding Resource Package", "abstract": "<p>Spoken Language Understanding infers semantic meaning directly from audio\ndata, and thus promises to reduce error propagation and misunderstandings in\nend-user applications. However, publicly available SLU resources are limited.\nIn this paper, we release SLURP, a new SLU package containing the following:\n(1) A new challenging dataset in English spanning 18 domains, which is\nsubstantially bigger and linguistically more diverse than existing datasets;\n(2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A\nnew transparent metric for entity labelling which enables a detailed error\nanalysis for identifying potential areas of improvement. SLURP is available at\nhttps: //github.com/pswietojanski/slurp.</p>\n", "tags": ["Applications","Datasets","EMNLP","Has Code"] },
{"key": "bastings2020elephant", "citations": "137", "year": "2020", "title":"The Elephant In The Interpretability Room: Why Use Attention As Explanation When We Have Saliency Methods?", "abstract": "<p>There is a recent surge of interest in using attention as explanation of\nmodel predictions, with mixed evidence on whether attention can be used as\nsuch. While attention conveniently gives us one weight per input token and is\neasily extracted, it is often unclear toward what goal it is used as\nexplanation. We find that often that goal, whether explicitly stated or not, is\nto find out what input tokens are the most relevant to a prediction, and that\nthe implied user for the explanation is a model developer. For this goal and\nuser, we argue that input saliency methods are better suited, and that there\nare no compelling reasons to use attention, despite the coincidence that it\nprovides a weight for each input. With this position paper, we hope to shift\nsome of the recent focus on attention to saliency methods, and for authors to\nclearly state the goal and user for their explanations.</p>\n", "tags": [] },
{"key": "battenberg2019location", "citations": "101", "year": "2020", "title":"Location-relative Attention Mechanisms For Robust Long-form Speech Synthesis", "abstract": "<p>Despite the ability to produce human-level speech for in-domain text,\nattention-based end-to-end text-to-speech (TTS) systems suffer from text\nalignment failures that increase in frequency for out-of-domain text. We show\nthat these failures can be addressed using simple location-relative attention\nmechanisms that do away with content-based query/key comparisons. We compare\ntwo families of attention mechanisms: location-relative GMM-based mechanisms\nand additive energy-based mechanisms. We suggest simple modifications to\nGMM-based attention that allow it to align quickly and consistently during\ntraining, and introduce a new location-relative attention mechanism to the\nadditive energy-based family, called Dynamic Convolution Attention (DCA). We\ncompare the various mechanisms in terms of alignment speed and consistency\nduring training, naturalness, and ability to generalize to long utterances, and\nconclude that GMM attention and DCA can generalize to very long utterances,\nwhile preserving naturalness for shorter, in-domain utterances.</p>\n", "tags": ["ICASSP","Model Architecture","Training Techniques"] },
{"key": "bauer2018commonsense", "citations": "173", "year": "2018", "title":"Commonsense For Generative Multi-hop Question Answering Tasks", "abstract": "<p>Reading comprehension QA tasks have seen a recent surge in popularity, yet\nmost works have focused on fact-finding extractive QA. We instead focus on a\nmore challenging multi-hop generative task (NarrativeQA), which requires the\nmodel to reason, gather, and synthesize disjoint pieces of information within\nthe context to generate an answer. This type of multi-step reasoning also often\nrequires understanding implicit relations, which humans resolve via external,\nbackground commonsense knowledge. We first present a strong generative baseline\nthat uses a multi-attention mechanism to perform multiple hops of reasoning and\na pointer-generator decoder to synthesize the answer. This model performs\nsubstantially better than previous generative models, and is competitive with\ncurrent state-of-the-art span prediction models. We next introduce a novel\nsystem for selecting grounded multi-hop relational commonsense information from\nConceptNet via a pointwise mutual information and term-frequency based scoring\nfunction. Finally, we effectively use this extracted commonsense information to\nfill in gaps of reasoning between context hops, using a selectively-gated\nattention mechanism. This boosts the model’s performance significantly (also\nverified via human evaluation), establishing a new state-of-the-art for the\ntask. We also show promising initial results of the generalizability of our\nbackground knowledge enhancements by demonstrating some improvement on\nQAngaroo-WikiHop, another multi-hop reasoning dataset.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "bautista2022scene", "citations": "137", "year": "2022", "title":"Scene Text Recognition With Permuted Autoregressive Sequence Models", "abstract": "<p>Context-aware STR methods typically use internal autoregressive (AR) language\nmodels (LM). Inherent limitations of AR models motivated two-stage methods\nwhich employ an external LM. The conditional independence of the external LM on\nthe input image may cause it to erroneously rectify correct predictions,\nleading to significant inefficiencies. Our method, PARSeq, learns an ensemble\nof internal AR LMs with shared weights using Permutation Language Modeling. It\nunifies context-free non-AR and context-aware AR inference, and iterative\nrefinement using bidirectional context. Using synthetic training data, PARSeq\nachieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and\nmore challenging datasets. It establishes new SOTA results (96.0% accuracy)\nwhen trained on real data. PARSeq is optimal on accuracy vs parameter count,\nFLOPS, and latency because of its simple, unified structure and parallel token\nprocessing. Due to its extensive use of attention, it is robust on\narbitrarily-oriented text which is common in real-world images. Code,\npretrained weights, and data are available at: https://github.com/baudm/parseq.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "beddiar2021data", "citations": "63", "year": "2021", "title":"Data Expansion Using Back Translation And Paraphrasing For Hate Speech Detection", "abstract": "<p>With proliferation of user generated contents in social media platforms,\nestablishing mechanisms to automatically identify toxic and abusive content\nbecomes a prime concern for regulators, researchers, and society. Keeping the\nbalance between freedom of speech and respecting each other dignity is a major\nconcern of social media platform regulators. Although, automatic detection of\noffensive content using deep learning approaches seems to provide encouraging\nresults, training deep learning-based models requires large amounts of\nhigh-quality labeled data, which is often missing. In this regard, we present\nin this paper a new deep learning-based method that fuses a Back Translation\nmethod, and a Paraphrasing technique for data augmentation. Our pipeline\ninvestigates different word-embedding-based architectures for classification of\nhate speech. The back translation technique relies on an encoder-decoder\narchitecture pre-trained on a large corpus and mostly used for machine\ntranslation. In addition, paraphrasing exploits the transformer model and the\nmixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are\ncompared to seek enhanced classification results. We evaluate our proposal on\nfive publicly available datasets; namely, AskFm corpus, Formspring dataset,\nWarner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The\nperformance of the proposal together with comparison to some related\nstate-of-art results demonstrate the effectiveness and soundness of our\nproposal.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "belinkov2017synthetic", "citations": "434", "year": "2017", "title":"Synthetic And Natural Noise Both Break Neural Machine Translation", "abstract": "<p>Character-based neural machine translation (NMT) models alleviate\nout-of-vocabulary issues, learn morphology, and move us closer to completely\nend-to-end translation systems. Unfortunately, they are also very brittle and\neasily falter when presented with noisy data. In this paper, we confront NMT\nmodels with synthetic and natural sources of noise. We find that\nstate-of-the-art models fail to translate even moderately noisy texts that\nhumans have no trouble comprehending. We explore two approaches to increase\nmodel robustness: structure-invariant word representations and robust training\non noisy texts. We find that a model based on a character convolutional neural\nnetwork is able to simultaneously learn representations robust to multiple\nkinds of noise.</p>\n", "tags": ["Security","Training Techniques"] },
{"key": "belinkov2017what", "citations": "281", "year": "2017", "title":"What Do Neural Machine Translation Models Learn About Morphology?", "abstract": "<p>Neural machine translation (MT) models obtain state-of-the-art performance\nwhile maintaining a simple, end-to-end architecture. However, little is known\nabout what these models learn about source and target languages during the\ntraining process. In this work, we analyze the representations learned by\nneural MT models at various levels of granularity and empirically evaluate the\nquality of the representations for learning morphology through extrinsic\npart-of-speech and morphological tagging tasks. We conduct a thorough\ninvestigation along several parameters: word-based vs. character-based\nrepresentations, depth of the encoding layer, the identity of the target\nlanguage, and encoder vs. decoder representations. Our data-driven,\nquantitative evaluation sheds light on important aspects in the neural MT\nsystem and its ability to capture word structure.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "belinkov2018evaluating", "citations": "127", "year": "2017", "title":"Evaluating Layers Of Representation In Neural Machine Translation On Part-of-speech And Semantic Tagging Tasks", "abstract": "<p>While neural machine translation (NMT) models provide improved translation\nquality in an elegant, end-to-end framework, it is less clear what they learn\nabout language. Recent work has started evaluating the quality of vector\nrepresentations learned by NMT models on morphological and syntactic tasks. In\nthis paper, we investigate the representations learned at different layers of\nNMT encoders. We train NMT systems on parallel data and use the trained models\nto extract features for training a classifier on two tasks: part-of-speech and\nsemantic tagging. We then measure the performance of the classifier as a proxy\nto the quality of the original NMT model for the given task. Our quantitative\nanalysis yields interesting insights regarding representation learning in NMT\nmodels. For instance, we find that higher layers are better at learning\nsemantics while lower layers tend to be better for part-of-speech tagging. We\nalso observe little effect of the target language on source-side\nrepresentations, especially with higher quality NMT models.</p>\n", "tags": ["Evaluation","Tools","Training Techniques"] },
{"key": "belinkov2019adversarial", "citations": "68", "year": "2019", "title":"On Adversarial Removal Of Hypothesis-only Bias In Natural Language Inference", "abstract": "<p>Popular Natural Language Inference (NLI) datasets have been shown to be\ntainted by hypothesis-only biases. Adversarial learning may help models ignore\nsensitive biases and spurious correlations in data. We evaluate whether\nadversarial learning can be used in NLI to encourage models to learn\nrepresentations free of hypothesis-only biases. Our analyses indicate that the\nrepresentations learned via adversarial learning may be less biased, with only\nsmall drops in NLI accuracy.</p>\n", "tags": ["Datasets","Ethics & Fairness","Security"] },
{"key": "belinkov2019linguistic", "citations": "70", "year": "2020", "title":"On The Linguistic Representational Power Of Neural Machine Translation Models", "abstract": "<p>Despite the recent success of deep neural networks in natural language\nprocessing (NLP), their interpretability remains a challenge. We analyze the\nrepresentations learned by neural machine translation models at various levels\nof granularity and evaluate their quality through relevant extrinsic\nproperties. In particular, we seek answers to the following questions: (i) How\naccurately is word-structure captured within the learned representations, an\nimportant aspect in translating morphologically-rich languages? (ii) Do the\nrepresentations capture long-range dependencies, and effectively handle\nsyntactically divergent languages? (iii) Do the representations capture lexical\nsemantics? We conduct a thorough investigation along several parameters: (i)\nWhich layers in the architecture capture each of these linguistic phenomena;\n(ii) How does the choice of translation unit (word, character, or subword unit)\nimpact the linguistic properties captured by the underlying representations?\n(iii) Do the encoder and decoder learn differently and independently? (iv) Do\nthe representations learned by multilingual NMT models capture the same amount\nof linguistic information as their bilingual counterparts? Our data-driven,\nquantitative evaluation illuminates important aspects in NMT models and their\nability to capture various linguistic phenomena. We show that deep NMT models\nlearn a non-trivial amount of linguistic information. Notable findings include:\ni) Word morphology and part-of-speech information are captured at the lower\nlayers of the model; (ii) In contrast, lexical semantics or non-local syntactic\nand semantic dependencies are better represented at the higher layers; (iii)\nRepresentations learned using characters are more informed about wordmorphology\ncompared to those learned using subword units; and (iv) Representations learned\nby multilingual models are richer compared to bilingual models.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "beltagy2019scibert", "citations": "2496", "year": "2019", "title":"Scibert: A Pretrained Language Model For Scientific Text", "abstract": "<p>Obtaining large-scale annotated data for NLP tasks in the scientific domain\nis challenging and expensive. We release SciBERT, a pretrained language model\nbased on BERT (Devlin et al., 2018) to address the lack of high-quality,\nlarge-scale labeled scientific data. SciBERT leverages unsupervised pretraining\non a large multi-domain corpus of scientific publications to improve\nperformance on downstream scientific NLP tasks. We evaluate on a suite of tasks\nincluding sequence tagging, sentence classification and dependency parsing,\nwith datasets from a variety of scientific domains. We demonstrate\nstatistically significant improvements over BERT and achieve new\nstate-of-the-art results on several of these tasks. The code and pretrained\nmodels are available at https://github.com/allenai/scibert/.</p>\n", "tags": ["Datasets","EMNLP","Has Code","Model Architecture"] },
{"key": "beltagy2020longformer", "citations": "2165", "year": "2020", "title":"Longformer: The Long-document Transformer", "abstract": "<p>Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer’s attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a\nLongformer variant for supporting long document generative sequence-to-sequence\ntasks, and demonstrate its effectiveness on the arXiv summarization dataset.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "bennun2018neural", "citations": "80", "year": "2018", "title":"Neural Code Comprehension: A Learnable Representation Of Code Semantics", "abstract": "<p>With the recent success of embeddings in natural language processing,\nresearch has been conducted into applying similar methods to code analysis.\nMost works attempt to process the code directly or use a syntactic tree\nrepresentation, treating it like sentences written in a natural language.\nHowever, none of the existing methods are sufficient to comprehend program\nsemantics robustly, due to structural features such as function calls,\nbranching, and interchangeable order of statements. In this paper, we propose a\nnovel processing technique to learn code semantics, and apply it to a variety\nof program analysis tasks. In particular, we stipulate that a robust\ndistributional hypothesis of code applies to both human- and machine-generated\nprograms. Following this hypothesis, we define an embedding space, inst2vec,\nbased on an Intermediate Representation (IR) of the code that is independent of\nthe source programming language. We provide a novel definition of contextual\nflow for this IR, leveraging both the underlying data- and control-flow of the\nprogram. We then analyze the embeddings qualitatively using analogies and\nclustering, and evaluate the learned representation on three different\nhigh-level tasks. We show that even without fine-tuning, a single RNN\narchitecture and fixed inst2vec embeddings outperform specialized approaches\nfor performance prediction (compute device mapping, optimal thread coarsening);\nand algorithm classification from raw code (104 classes), where we set a new\nstate-of-the-art.</p>\n", "tags": ["Fine-Tuning","Llm For Code","Model Architecture","Training Techniques"] },
{"key": "bentivogli2016neural", "citations": "290", "year": "2016", "title":"Neural Versus Phrase-based Machine Translation Quality: A Case Study", "abstract": "<p>Within the field of Statistical Machine Translation (SMT), the neural\napproach (NMT) has recently emerged as the first technology able to challenge\nthe long-standing dominance of phrase-based approaches (PBMT). In particular,\nat the IWSLT 2015 evaluation campaign, NMT outperformed well established\nstate-of-the-art PBMT systems on English-German, a language pair known to be\nparticularly hard because of morphology and syntactic differences. To\nunderstand in what respects NMT provides better translation quality than PBMT,\nwe perform a detailed analysis of neural versus phrase-based SMT outputs,\nleveraging high quality post-edits performed by professional translators on the\nIWSLT data. For the first time, our analysis provides useful insights on what\nlinguistic phenomena are best modeled by neural models – such as the\nreordering of verbs – while pointing out other aspects that remain to be\nimproved.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "benyounes2017mutan", "citations": "674", "year": "2017", "title":"MUTAN: Multimodal Tucker Fusion For Visual Question Answering", "abstract": "<p>Bilinear models provide an appealing framework for mixing and merging\ninformation in Visual Question Answering (VQA) tasks. They help to learn high\nlevel associations between question meaning and visual concepts in the image,\nbut they suffer from huge dimensionality issues. We introduce MUTAN, a\nmultimodal tensor-based Tucker decomposition to efficiently parametrize\nbilinear interactions between visual and textual representations. Additionally\nto the Tucker framework, we design a low-rank matrix-based decomposition to\nexplicitly constrain the interaction rank. With MUTAN, we control the\ncomplexity of the merging scheme while keeping nice interpretable fusion\nrelations. We show how our MUTAN model generalizes some of the latest VQA\narchitectures, providing state-of-the-art results.</p>\n", "tags": ["ICCV","Tools"] },
{"key": "benyounes2019block", "citations": "197", "year": "2019", "title":"BLOCK: Bilinear Superdiagonal Fusion For Visual Question Answering And Visual Relationship Detection", "abstract": "<p>Multimodal representation learning is gaining more and more interest within\nthe deep learning community. While bilinear models provide an interesting\nframework to find subtle combination of modalities, their number of parameters\ngrows quadratically with the input dimensions, making their practical\nimplementation within classical deep learning pipelines challenging. In this\npaper, we introduce BLOCK, a new multimodal fusion based on the\nblock-superdiagonal tensor decomposition. It leverages the notion of block-term\nranks, which generalizes both concepts of rank and mode ranks for tensors,\nalready used for multimodal fusion. It allows to define new ways for optimizing\nthe tradeoff between the expressiveness and complexity of the fusion model, and\nis able to represent very fine interactions between modalities while\nmaintaining powerful mono-modal representations. We demonstrate the practical\ninterest of our fusion model by using BLOCK for two challenging tasks: Visual\nQuestion Answering (VQA) and Visual Relationship Detection (VRD), where we\ndesign end-to-end learnable architectures for representing relevant\ninteractions between modalities. Through extensive experiments, we show that\nBLOCK compares favorably with respect to state-of-the-art multimodal fusion\nmodels for both VQA and VRD tasks. Our code is available at\nhttps://github.com/Cadene/block.bootstrap.pytorch.</p>\n", "tags": ["AAAI","Has Code"] },
{"key": "berg2021keyword", "citations": "95", "year": "2021", "title":"Keyword Transformer: A Self-attention Model For Keyword Spotting", "abstract": "<p>The Transformer architecture has been successful across many domains,\nincluding natural language processing, computer vision and speech recognition.\nIn keyword spotting, self-attention has primarily been used on top of\nconvolutional or recurrent encoders. We investigate a range of ways to adapt\nthe Transformer architecture to keyword spotting and introduce the Keyword\nTransformer (KWT), a fully self-attentional architecture that exceeds\nstate-of-the-art performance across multiple tasks without any pre-training or\nadditional data. Surprisingly, this simple architecture outperforms more\ncomplex models that mix convolutional, recurrent and attentive layers. KWT can\nbe used as a drop-in replacement for these models, setting two new benchmark\nrecords on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on\nthe 12 and 35-command tasks respectively.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "besta2023graph", "citations": "154", "year": "2024", "title":"Graph Of Thoughts: Solving Elaborate Problems With Large Language Models", "abstract": "<p>We introduce Graph of Thoughts (GoT): a framework that advances prompting\ncapabilities in large language models (LLMs) beyond those offered by paradigms\nsuch as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information generated by an LLM as\nan arbitrary graph, where units of information (“LLM thoughts”) are vertices,\nand edges correspond to dependencies between these vertices. This approach\nenables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback\nloops. We illustrate that GoT offers advantages over state of the art on\ndifferent tasks, for example increasing the quality of sorting by 62% over ToT,\nwhile simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible\nwith new thought transformations and thus can be used to spearhead new\nprompting schemes. This work brings the LLM reasoning closer to human thinking\nor brain mechanisms such as recurrence, both of which form complex networks.</p>\n", "tags": ["AAAI","Prompting","Tools"] },
{"key": "bhagavatula2019abductive", "citations": "240", "year": "2019", "title":"Abductive Commonsense Reasoning", "abstract": "<p>Abductive reasoning is inference to the most plausible explanation. For\nexample, if Jenny finds her house in a mess when she returns from work, and\nremembers that she left a window open, she can hypothesize that a thief broke\ninto her house and caused the mess, as the most plausible explanation. While\nabduction has long been considered to be at the core of how people interpret\nand read between the lines in natural language (Hobbs et al., 1988), there has\nbeen relatively little research in support of abductive natural language\ninference and generation. We present the first study that investigates the\nviability of language-based abductive reasoning. We introduce a challenge\ndataset, ART, that consists of over 20k commonsense narrative contexts and 200k\nexplanations. Based on this dataset, we conceptualize two new tasks – (i)\nAbductive NLI: a multiple-choice question answering task for choosing the more\nlikely explanation, and (ii) Abductive NLG: a conditional generation task for\nexplaining given observations in natural language. On Abductive NLI, the best\nmodel achieves 68.9% accuracy, well below human performance of 91.4%. On\nAbductive NLG, the current best language generators struggle even more, as they\nlack reasoning capabilities that are trivial for humans. Our analysis leads to\nnew insights into the types of reasoning that deep pre-trained language models\nfail to perform–despite their strong performance on the related but more\nnarrowly defined task of entailment NLI–pointing to interesting avenues for\nfuture research.</p>\n", "tags": [] },
{"key": "bhandare2019efficient", "citations": "64", "year": "2019", "title":"Efficient 8-bit Quantization Of Transformer Neural Machine Language Translation Model", "abstract": "<p>In this work, we quantize a trained Transformer machine language translation\nmodel leveraging INT8/VNNI instructions in the latest Intel\\(^\\circledR\\)\nXeon\\(^\\circledR\\) Cascade Lake processors to improve inference performance while\nmaintaining less than 0.5\\(%\\) drop in accuracy. To the best of our knowledge,\nthis is the first attempt in the industry to quantize the Transformer model.\nThis has high impact as it clearly demonstrates the various complexities of\nquantizing the language translation model. We present novel quantization\ntechniques directly in TensorFlow to opportunistically replace 32-bit floating\npoint (FP32) computations with 8-bit integers (INT8) and transform the FP32\ncomputational graph. We also present a bin-packing parallel batching technique\nto maximize CPU utilization. Overall, our optimizations with INT8/VNNI deliver\n1.5X improvement over the best FP32 performance. Furthermore, it reveals the\nopportunities and challenges to boost performance of quantized deep learning\ninference and establishes best practices to run inference with high efficiency\non Intel CPUs.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "bhatt2019explainable", "citations": "513", "year": "2020", "title":"Explainable Machine Learning In Deployment", "abstract": "<p>Explainable machine learning offers the potential to provide stakeholders\nwith insights into model behavior by using various methods such as feature\nimportance scores, counterfactual explanations, or influential training data.\nYet there is little understanding of how organizations use these methods in\npractice. This study explores how organizations view and use explainability for\nstakeholder consumption. We find that, currently, the majority of deployments\nare not for end users affected by the model but rather for machine learning\nengineers, who use explainability to debug the model itself. There is thus a\ngap between explainability in practice and the goal of transparency, since\nexplanations primarily serve internal stakeholders rather than external ones.\nOur study synthesizes the limitations of current explainability techniques that\nhamper their use for end users. To facilitate end user interaction, we develop\na framework for establishing clear goals for explainability. We end by\ndiscussing concerns raised regarding explainability.</p>\n", "tags": ["Applications","Ethics & Fairness","Training Techniques"] },
{"key": "bhattacharjee2021banglabert", "citations": "108", "year": "2022", "title":"Banglabert: Language Model Pretraining And Benchmarks For Low-resource Language Understanding Evaluation In Bangla", "abstract": "<p>In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+’) by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","NAACL"] },
{"key": "bhoopchand2016learning", "citations": "71", "year": "2016", "title":"Learning Python Code Suggestion With A Sparse Pointer Network", "abstract": "<p>To enhance developer productivity, all modern integrated development\nenvironments (IDEs) include code suggestion functionality that proposes likely\nnext tokens at the cursor. While current IDEs work well for statically-typed\nlanguages, their reliance on type annotations means that they do not provide\nthe same level of support for dynamic programming languages as for\nstatically-typed languages. Moreover, suggestion engines in modern IDEs do not\npropose expressions or multi-statement idiomatic code. Recent work has shown\nthat language models can improve code suggestion systems by learning from\nsoftware repositories. This paper introduces a neural language model with a\nsparse pointer network aimed at capturing very long-range dependencies. We\nrelease a large-scale code suggestion corpus of 41M lines of Python code\ncrawled from GitHub. On this corpus, we found standard neural language models\nto perform well at suggesting local phenomena, but struggle to refer to\nidentifiers that are introduced many tokens in the past. By augmenting a neural\nlanguage model with a pointer network specialized in referring to predefined\nclasses of identifiers, we obtain a much lower perplexity and a 5 percentage\npoints increase in accuracy for code suggestion compared to an LSTM baseline.\nIn fact, this increase in code suggestion accuracy is due to a 13 times more\naccurate prediction of identifiers. Furthermore, a qualitative analysis shows\nthis model indeed captures interesting long-range dependencies, like referring\nto a class member defined over 60 tokens in the past.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "bianchi2022easily", "citations": "150", "year": "2023", "title":"Easily Accessible Text-to-image Generation Amplifies Demographic Stereotypes At Large Scale", "abstract": "<p>Machine learning models that convert user-written text descriptions into\nimages are now widely available online and used by millions of users to\ngenerate millions of images a day. We investigate the potential for these\nmodels to amplify dangerous and complex stereotypes. We find a broad range of\nordinary prompts produce stereotypes, including prompts simply mentioning\ntraits, descriptors, occupations, or objects. For example, we find cases of\nprompting for basic traits or social roles resulting in images reinforcing\nwhiteness as ideal, prompting for occupations resulting in amplification of\nracial and gender disparities, and prompting for objects resulting in\nreification of American norms. Stereotypes are present regardless of whether\nprompts explicitly mention identity and demographic language or avoid such\nlanguage. Moreover, stereotypes persist despite mitigation strategies; neither\nuser attempts to counter stereotypes by requesting images with specific\ncounter-stereotypes nor institutional attempts to add system ``guardrails’’\nhave prevented the perpetuation of stereotypes. Our analysis justifies concerns\nregarding the impacts of today’s models, presenting striking exemplars, and\nconnecting these findings with deep insights into harms drawn from social\nscientific and humanist disciplines. This work contributes to the effort to\nshed light on the uniquely complex biases in language-vision models and\ndemonstrates the ways that the mass deployment of text-to-image generation\nmodels results in mass dissemination of stereotypes and resulting harms.</p>\n", "tags": ["Ethics & Fairness","Prompting"] },
{"key": "biderman2023pythia", "citations": "97", "year": "2023", "title":"Pythia: A Suite For Analyzing Large Language Models Across Training And Scaling", "abstract": "<p>How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\nhttps://github.com/EleutherAI/pythia.</p>\n", "tags": ["Ethics & Fairness","Few-Shot","Has Code","Tools","Training Techniques"] },
{"key": "bigscience2022bloom", "citations": "628", "year": "2022", "title":"BLOOM: A 176b-parameter Open-access Multilingual Language Model", "abstract": "<p>Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.</p>\n", "tags": ["Applications","Datasets","Ethics & Fairness","Model Architecture"] },
{"key": "bin2016bidirectional", "citations": "64", "year": "2016", "title":"Bidirectional Long-short Term Memory For Video Description", "abstract": "<p>Video captioning has been attracting broad research attention in multimedia\ncommunity. However, most existing approaches either ignore temporal information\namong video frames or just employ local contextual temporal knowledge. In this\nwork, we propose a novel video captioning framework, termed as\n<em>Bidirectional Long-Short Term Memory</em> (BiLSTM), which deeply captures\nbidirectional global temporal structure in video. Specifically, we first devise\na joint visual modelling approach to encode video data by combining a forward\nLSTM pass, a backward LSTM pass, together with visual features from\nConvolutional Neural Networks (CNNs). Then, we inject the derived video\nrepresentation into the subsequent language model for initialization. The\nbenefits are in two folds: 1) comprehensively preserving sequential and visual\ninformation; and 2) adaptively learning dense visual features and sparse\nsemantic representations for videos and sentences, respectively. We verify the\neffectiveness of our proposed video captioning framework on a commonly-used\nbenchmark, i.e., Microsoft Video Description (MSVD) corpus, and the\nexperimental results demonstrate that the superiority of the proposed approach\nas compared to several state-of-the-art methods.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "binz2022using", "citations": "276", "year": "2023", "title":"Using Cognitive Psychology To Understand GPT-3", "abstract": "<p>We study GPT-3, a recent large language model, using tools from cognitive\npsychology. More specifically, we assess GPT-3’s decision-making, information\nsearch, deliberation, and causal reasoning abilities on a battery of canonical\nexperiments from the literature. We find that much of GPT-3’s behavior is\nimpressive: it solves vignette-based tasks similarly or better than human\nsubjects, is able to make decent decisions from descriptions, outperforms\nhumans in a multi-armed bandit task, and shows signatures of model-based\nreinforcement learning. Yet we also find that small perturbations to\nvignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures\nof directed exploration, and that it fails miserably in a causal reasoning\ntask. These results enrich our understanding of current large language models\nand pave the way for future investigations using tools from cognitive\npsychology to study increasingly capable and opaque artificial agents.</p>\n", "tags": ["Agentic","Model Architecture","Reinforcement Learning","Tools"] },
{"key": "bird2020chatbot", "citations": "61", "year": "2021", "title":"Chatbot Interaction With Artificial Intelligence: Human Data Augmentation With T5 And Language Transformer Ensemble For Text Classification", "abstract": "<p>In this work, we present the Chatbot Interaction with Artificial Intelligence\n(CI-AI) framework as an approach to the training of deep learning chatbots for\ntask classification. The intelligent system augments human-sourced data via\nartificial paraphrasing in order to generate a large set of training data for\nfurther classical, attention, and language transformation-based learning\napproaches for Natural Language Processing. Human beings are asked to\nparaphrase commands and questions for task identification for further execution\nof a machine. The commands and questions are split into training and validation\nsets. A total of 483 responses were recorded. Secondly, the training set is\nparaphrased by the T5 model in order to augment it with further data. Seven\nstate-of-the-art transformer-based text classification algorithms (BERT,\nDistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are\nbenchmarked for both sets after fine-tuning on the training data for two\nepochs. We find that all models are improved when training data is augmented by\nthe T5 model, with an average increase of classification accuracy by 4.01%. The\nbest result was the RoBERTa model trained on T5 augmented data which achieved\n98.96% classification accuracy. Finally, we found that an ensemble of the five\nbest-performing transformer models via Logistic Regression of output label\npredictions led to an accuracy of 99.59% on the dataset of human responses. A\nhighly-performing model allows the intelligent system to interpret human\ncommands at the social-interaction level through a chatbot-like interface (e.g.\n“Robot, can we have a conversation?”) and allows for better accessibility to AI\nby non-technical users.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "bisk2019piqa", "citations": "418", "year": "2020", "title":"PIQA: Reasoning About Physical Commonsense In Natural Language", "abstract": "<p>To apply eyeshadow without a brush, should I use a cotton swab or a\ntoothpick? Questions requiring this kind of physical commonsense pose a\nchallenge to today’s natural language understanding systems. While recent\npretrained models (such as BERT) have made progress on question answering over\nmore abstract domains - such as news articles and encyclopedia entries, where\ntext is plentiful - in more physical domains, text is inherently limited due to\nreporting bias. Can AI systems learn to reliably answer physical common-sense\nquestions without experiencing the physical world? In this paper, we introduce\nthe task of physical commonsense reasoning and a corresponding benchmark\ndataset Physical Interaction: Question Answering or PIQA. Though humans find\nthe dataset easy (95% accuracy), large pretrained models struggle (77%). We\nprovide analysis about the dimensions of knowledge that existing models lack,\nwhich offers significant opportunities for future research.</p>\n", "tags": ["AAAI","Datasets","Ethics & Fairness","Evaluation","Model Architecture"] },
{"key": "bisk2020experience", "citations": "221", "year": "2020", "title":"Experience Grounds Language", "abstract": "<p>Language understanding research is held back by a failure to relate language\nto the physical world it describes and to the social interactions it\nfacilitates. Despite the incredible effectiveness of language processing models\nto tackle tasks after being trained on text alone, successful linguistic\ncommunication relies on a shared experience of the world. It is this shared\nexperience that makes utterances meaningful.\n  Natural language processing is a diverse field, and progress throughout its\ndevelopment has come from new representational theories, modeling techniques,\ndata collection paradigms, and tasks. We posit that the present success of\nrepresentation learning approaches trained on large, text-only corpora requires\nthe parallel tradition of research on the broader physical and social context\nof language to address the deeper questions of communication.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "biswas2024intelligent", "citations": "986", "year": "2024", "title":"Intelligent Clinical Documentation: Harnessing Generative AI For Patient-centric Clinical Note Generation", "abstract": "<p>Comprehensive clinical documentation is crucial for effective healthcare\ndelivery, yet it poses a significant burden on healthcare professionals,\nleading to burnout, increased medical errors, and compromised patient safety.\nThis paper explores the potential of generative AI (Artificial Intelligence) to\nstreamline the clinical documentation process, specifically focusing on\ngenerating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior,\nIntervention, Response, Plan) notes. We present a case study demonstrating the\napplication of natural language processing (NLP) and automatic speech\nrecognition (ASR) technologies to transcribe patient-clinician interactions,\ncoupled with advanced prompting techniques to generate draft clinical notes\nusing large language models (LLMs). The study highlights the benefits of this\napproach, including time savings, improved documentation quality, and enhanced\npatient-centered care. Additionally, we discuss ethical considerations, such as\nmaintaining patient confidentiality and addressing model biases, underscoring\nthe need for responsible deployment of generative AI in healthcare settings.\nThe findings suggest that generative AI has the potential to revolutionize\nclinical documentation practices, alleviating administrative burdens and\nenabling healthcare professionals to focus more on direct patient care.</p>\n", "tags": ["Prompting"] },
{"key": "biten2019scene", "citations": "227", "year": "2019", "title":"Scene Text Visual Question Answering", "abstract": "<p>Current visual question answering datasets do not consider the rich semantic\ninformation conveyed by text within an image. In this work, we present a new\ndataset, ST-VQA, that aims to highlight the importance of exploiting high-level\nsemantic information present in images as textual cues in the VQA process. We\nuse this dataset to define a series of tasks of increasing difficulty for which\nreading the scene text in the context provided by the visual information is\nnecessary to reason and generate an appropriate answer. We propose a new\nevaluation metric for these tasks to account both for reasoning errors as well\nas shortcomings of the text recognition module. In addition we put forward a\nseries of baseline methods, which provide further insight to the newly released\ndataset, and set the scene for further research.</p>\n", "tags": ["Datasets","Evaluation","ICCV"] },
{"key": "biten2021latr", "citations": "69", "year": "2022", "title":"Latr: Layout-aware Transformer For Scene-text VQA", "abstract": "<p>We propose a novel multimodal architecture for Scene Text Visual Question\nAnswering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA\nrequires models to reason over different modalities. Thus, we first investigate\nthe impact of each modality, and reveal the importance of the language module,\nespecially when enriched with layout information. Accounting for this, we\npropose a single objective pre-training scheme that requires only text and\nspatial cues. We show that applying this pre-training scheme on scanned\ndocuments has certain advantages over using natural images, despite the domain\ngap. Scanned documents are easy to procure, text-dense and have a variety of\nlayouts, helping the model learn various spatial cues (e.g. left-of, below\netc.) by tying together language and layout information. Compared to existing\napproaches, our method performs vocabulary-free decoding and, as shown,\ngeneralizes well beyond the training vocabulary. We further demonstrate that\nLaTr improves robustness towards OCR errors, a common reason for failure cases\nin STVQA. In addition, by leveraging a vision transformer, we eliminate the\nneed for an external object detector. LaTr outperforms state-of-the-art STVQA\nmethods on multiple datasets. In particular, +7.6% on TextVQA, +10.8% on ST-VQA\nand +4.0% on OCR-VQA (all absolute accuracy numbers).</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "black2022gpt", "citations": "294", "year": "2022", "title":"Gpt-neox-20b: An Open-source Autoregressive Language Model", "abstract": "<p>We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language\nmodel trained on the Pile, whose weights will be made freely and openly\navailable to the public through a permissive license. It is, to the best of our\nknowledge, the largest dense autoregressive model that has publicly available\nweights at the time of submission. In this work, we describe \\model{}’s\narchitecture and training and evaluate its performance on a range of\nlanguage-understanding, mathematics, and knowledge-based tasks. We find that\nGPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in\nperformance when evaluated five-shot than similarly sized GPT-3 and FairSeq\nmodels. We open-source the training and evaluation code, as well as the model\nweights, at https://github.com/EleutherAI/gpt-neox.</p>\n", "tags": ["Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "blattmann2023align", "citations": "280", "year": "2023", "title":"Align Your Latents: High-resolution Video Synthesis With Latent Diffusion Models", "abstract": "<p>Latent Diffusion Models (LDMs) enable high-quality image synthesis while\navoiding excessive compute demands by training a diffusion model in a\ncompressed lower-dimensional latent space. Here, we apply the LDM paradigm to\nhigh-resolution video generation, a particularly resource-intensive task. We\nfirst pre-train an LDM on images only; then, we turn the image generator into a\nvideo generator by introducing a temporal dimension to the latent space\ndiffusion model and fine-tuning on encoded image sequences, i.e., videos.\nSimilarly, we temporally align diffusion model upsamplers, turning them into\ntemporally consistent video super resolution models. We focus on two relevant\nreal-world applications: Simulation of in-the-wild driving data and creative\ncontent creation with text-to-video modeling. In particular, we validate our\nVideo LDM on real driving videos of resolution 512 x 1024, achieving\nstate-of-the-art performance. Furthermore, our approach can easily leverage\noff-the-shelf pre-trained image LDMs, as we only need to train a temporal\nalignment model in that case. Doing so, we turn the publicly available,\nstate-of-the-art text-to-image LDM Stable Diffusion into an efficient and\nexpressive text-to-video model with resolution up to 1280 x 2048. We show that\nthe temporal layers trained in this way generalize to different fine-tuned\ntext-to-image LDMs. Utilizing this property, we show the first results for\npersonalized text-to-video generation, opening exciting directions for future\ncontent creation. Project page:\nhttps://research.nvidia.com/labs/toronto-ai/VideoLDM/</p>\n", "tags": ["Applications","CVPR","Fine-Tuning","Training Techniques"] },
{"key": "blocklove2023chip", "citations": "68", "year": "2023", "title":"Chip-chat: Challenges And Opportunities In Conversational Hardware Design", "abstract": "<p>Modern hardware design starts with specifications provided in natural\nlanguage. These are then translated by hardware engineers into appropriate\nHardware Description Languages (HDLs) such as Verilog before synthesizing\ncircuit elements. Automating this translation could reduce sources of human\nerror from the engineering process. But, it is only recently that artificial\nintelligence (AI) has demonstrated capabilities for machine-based end-to-end\ndesign translations. Commercially-available instruction-tuned Large Language\nModels (LLMs) such as OpenAI’s ChatGPT and Google’s Bard claim to be able to\nproduce code in a variety of programming languages; but studies examining them\nfor hardware are still lacking. In this work, we thus explore the challenges\nfaced and opportunities presented when leveraging these recent advances in LLMs\nfor hardware design. Given that these <code class=\"language-plaintext highlighter-rouge\">conversational' LLMs perform best when\nused interactively, we perform a case study where a hardware engineer\nco-architects a novel 8-bit accumulator-based microprocessor architecture with\nthe LLM according to real-world hardware constraints. We then sent the\nprocessor to tapeout in a Skywater 130nm shuttle, meaning that this </code>Chip-Chat’\nresulted in what we believe to be the world’s first wholly-AI-written HDL for\ntapeout.</p>\n", "tags": ["Model Architecture"] },
{"key": "blodgett2020language", "citations": "717", "year": "2020", "title":"Language (technology) Is Power: A Critical Survey Of \"bias\" In NLP", "abstract": "<p>We survey 146 papers analyzing “bias” in NLP systems, finding that their\nmotivations are often vague, inconsistent, and lacking in normative reasoning,\ndespite the fact that analyzing “bias” is an inherently normative process. We\nfurther find that these papers’ proposed quantitative techniques for measuring\nor mitigating “bias” are poorly matched to their motivations and do not engage\nwith the relevant literature outside of NLP. Based on these findings, we\ndescribe the beginnings of a path forward by proposing three recommendations\nthat should guide work analyzing “bias” in NLP systems. These recommendations\nrest on a greater recognition of the relationships between language and social\nhierarchies, encouraging researchers and practitioners to articulate their\nconceptualizations of “bias”—i.e., what kinds of system behaviors are\nharmful, in what ways, to whom, and why, as well as the normative reasoning\nunderlying these statements—and to center work around the lived experiences\nof members of communities affected by NLP systems, while interrogating and\nreimagining the power relations between technologists and such communities.</p>\n", "tags": ["Ethics & Fairness","Survey Paper"] },
{"key": "bo2023rwkv", "citations": "117", "year": "2023", "title":"RWKV: Reinventing Rnns For The Transformer Era", "abstract": "<p>Transformers have revolutionized almost all natural language processing (NLP)\ntasks but suffer from memory and computational complexity that scales\nquadratically with sequence length. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and computational requirements but\nstruggle to match the same performance as Transformers due to limitations in\nparallelization and scalability. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that combines the efficient\nparallelizable training of transformers with the efficient inference of RNNs.\n  Our approach leverages a linear attention mechanism and allows us to\nformulate the model as either a Transformer or an RNN, thus parallelizing\ncomputations during training and maintains constant computational and memory\ncomplexity during inference. We scale our models as large as 14 billion\nparameters, by far the largest dense RNN ever trained, and find RWKV performs\non par with similarly sized Transformers, suggesting future work can leverage\nthis architecture to create more efficient models. This work presents a\nsignificant step towards reconciling trade-offs between computational\nefficiency and model performance in sequence processing tasks.</p>\n", "tags": ["EMNLP","Efficiency","Model Architecture","Training Techniques"] },
{"key": "bocklisch2017rasa", "citations": "218", "year": "2017", "title":"Rasa: Open Source Language Understanding And Dialogue Management", "abstract": "<p>We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source\npython libraries for building conversational software. Their purpose is to make\nmachine-learning based dialogue management and language understanding\naccessible to non-specialist software developers. In terms of design\nphilosophy, we aim for ease of use, and bootstrapping from minimal (or no)\ninitial training data. Both packages are extensively documented and ship with a\ncomprehensive suite of tests. The code is available at\nhttps://github.com/RasaHQ/</p>\n", "tags": ["Has Code","Tools","Training Techniques"] },
{"key": "boiko2023emergent", "citations": "60", "year": "2023", "title":"Emergent Autonomous Scientific Research Capabilities Of Large Language Models", "abstract": "<p>Transformer-based large language models are rapidly advancing in the field of\nmachine learning research, with applications spanning natural language,\nbiology, chemistry, and computer programming. Extreme scaling and reinforcement\nlearning from human feedback have significantly improved the quality of\ngenerated text, enabling these models to perform various tasks and reason about\ntheir choices. In this paper, we present an Intelligent Agent system that\ncombines multiple large language models for autonomous design, planning, and\nexecution of scientific experiments. We showcase the Agent’s scientific\nresearch capabilities with three distinct examples, with the most complex being\nthe successful performance of catalyzed cross-coupling reactions. Finally, we\ndiscuss the safety implications of such systems and propose measures to prevent\ntheir misuse.</p>\n", "tags": ["Agentic","Applications","Emergent Abilities","Model Architecture"] },
{"key": "bommarito2022gpt", "citations": "96", "year": "2022", "title":"GPT Takes The Bar Exam", "abstract": "<p>Nearly all jurisdictions in the United States require a professional license\nexam, commonly referred to as “the Bar Exam,” as a precondition for law\npractice. To even sit for the exam, most jurisdictions require that an\napplicant completes at least seven years of post-secondary education, including\nthree years at an accredited law school. In addition, most test-takers also\nundergo weeks to months of further, exam-specific preparation. Despite this\nsignificant investment of time and capital, approximately one in five\ntest-takers still score under the rate required to pass the exam on their first\ntry. In the face of a complex task that requires such depth of knowledge, what,\nthen, should we expect of the state of the art in “AI?” In this research, we\ndocument our experimental evaluation of the performance of OpenAI’s\n<code class=\"language-plaintext highlighter-rouge\">text-davinci-003</code> model, often-referred to as GPT-3.5, on the multistate\nmultiple choice (MBE) section of the exam. While we find no benefit in\nfine-tuning over GPT-3.5’s zero-shot performance at the scale of our training\ndata, we do find that hyperparameter optimization and prompt engineering\npositively impacted GPT-3.5’s zero-shot performance. For best prompt and\nparameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete\nNCBE MBE practice exam, significantly in excess of the 25% baseline guessing\nrate, and performs at a passing rate for both Evidence and Torts. GPT-3.5’s\nranking of responses is also highly-correlated with correctness; its top two\nand top three choices are correct 71% and 88% of the time, respectively,\nindicating very strong non-entailment performance. While our ability to\ninterpret these results is limited by nascent scientific understanding of LLMs\nand the proprietary nature of GPT, we believe that these results strongly\nsuggest that an LLM will pass the MBE component of the Bar Exam in the near\nfuture.</p>\n", "tags": ["Evaluation","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "bordes2016learning", "citations": "481", "year": "2016", "title":"Learning End-to-end Goal-oriented Dialog", "abstract": "<p>Traditional dialog systems used in goal-oriented applications require a lot\nof domain-specific handcrafting, which hinders scaling up to new domains.\nEnd-to-end dialog systems, in which all components are trained from the dialogs\nthemselves, escape this limitation. But the encouraging success recently\nobtained in chit-chat dialog may not carry over to goal-oriented settings. This\npaper proposes a testbed to break down the strengths and shortcomings of\nend-to-end dialog systems in goal-oriented applications. Set in the context of\nrestaurant reservation, our tasks require manipulating sentences and symbols,\nso as to properly conduct conversations, issue API calls and use the outputs of\nsuch calls. We show that an end-to-end dialog system based on Memory Networks\ncan reach promising, yet imperfect, performance and learn to perform\nnon-trivial operations. We confirm those results by comparing our system to a\nhand-crafted slot-filling baseline on data from the second Dialog State\nTracking Challenge (Henderson et al., 2014a). We show similar result patterns\non data extracted from an online concierge service.</p>\n", "tags": ["Applications","Dialogue & Multi Turn","Tools"] },
{"key": "borgeaud2021improving", "citations": "184", "year": "2021", "title":"Improving Language Models By Retrieving From Trillions Of Tokens", "abstract": "<p>We enhance auto-regressive language models by conditioning on document chunks\nretrieved from a large corpus, based on local similarity with preceding tokens.\nWith a \\(2\\) trillion token database, our Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite\nusing 25\\(\\times\\) fewer parameters. After fine-tuning, RETRO performance\ntranslates to downstream knowledge-intensive tasks such as question answering.\nRETRO combines a frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order of magnitude more\ndata than what is typically consumed during training. We typically train RETRO\nfrom scratch, yet can also rapidly RETROfit pre-trained transformers with\nretrieval and still achieve good performance. Our work opens up new avenues for\nimproving language models through explicit memory at unprecedented scale.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Retrieval Systems","Training Techniques"] },
{"key": "borji2023categorical", "citations": "359", "year": "2023", "title":"A Categorical Archive Of Chatgpt Failures", "abstract": "<p>Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT’s failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.</p>\n", "tags": ["Ethics & Fairness","Model Architecture","Security"] },
{"key": "born2022regression", "citations": "79", "year": "2023", "title":"Regression Transformer: Concurrent Sequence Regression And Generation For Molecular Language Modeling", "abstract": "<p>Despite significant progress of generative models in the natural sciences,\ntheir controllability remains challenging. One fundamentally missing aspect of\nmolecular or protein generative models is an inductive bias that can reflect\ncontinuous properties of interest. To that end, we propose the Regression\nTransformer (RT), a novel method that abstracts regression as a conditional\nsequence modeling problem. This introduces a new paradigm of multitask language\nmodels which seamlessly bridge sequence regression and conditional sequence\ngeneration.\n  We thoroughly demonstrate that, despite using a nominal-scale training\nobjective, the RT matches or surpasses the performance of conventional\nregression models in property prediction tasks of small molecules, proteins and\nchemical reactions. Critically, priming the same model with continuous\nproperties yields a highly competitive conditional generative model that\noutperforms specialized approaches in a substructure-constrained,\nproperty-driven molecule generation benchmark. Our dichotomous approach is\nfacilitated by a novel, alternating training scheme that enables the model to\ndecorate seed sequences by desired properties, e.g., to optimize reaction\nyield.\n  In sum, the RT is the first report of a multitask model that concurrently\nexcels at predictive and generative tasks in biochemistry. This finds\nparticular application in property-driven, local exploration of the chemical or\nprotein space and could pave the road toward foundation models in material\ndesign.\n  The code to reproduce all experiments of the paper is available at:\nhttps://github.com/IBM/regression-transformer</p>\n", "tags": ["Datasets","Evaluation","Has Code","Llm For Code","Model Architecture","Time Series","Training Techniques"] },
{"key": "borsos2022audiolm", "citations": "183", "year": "2023", "title":"Audiolm: A Language Modeling Approach To Audio Generation", "abstract": "<p>We introduce AudioLM, a framework for high-quality audio generation with\nlong-term consistency. AudioLM maps the input audio to a sequence of discrete\ntokens and casts audio generation as a language modeling task in this\nrepresentation space. We show how existing audio tokenizers provide different\ntrade-offs between reconstruction quality and long-term structure, and we\npropose a hybrid tokenization scheme to achieve both objectives. Namely, we\nleverage the discretized activations of a masked language model pre-trained on\naudio to capture long-term structure and the discrete codes produced by a\nneural audio codec to achieve high-quality synthesis. By training on large\ncorpora of raw audio waveforms, AudioLM learns to generate natural and coherent\ncontinuations given short prompts. When trained on speech, and without any\ntranscript or annotation, AudioLM generates syntactically and semantically\nplausible speech continuations while also maintaining speaker identity and\nprosody for unseen speakers. Furthermore, we demonstrate how our approach\nextends beyond speech by generating coherent piano music continuations, despite\nbeing trained without any symbolic representation of music.</p>\n", "tags": ["Tools","Training Techniques"] },
{"key": "boseop2021what", "citations": "62", "year": "2021", "title":"What Changes Can Large-scale Language Models Bring? Intensive Study On Hyperclova: Billions-scale Korean Generative Pretrained Transformers", "abstract": "<p>GPT-3 shows remarkable in-context learning ability of large-scale language\nmodels (LMs) trained on hundreds of billion scale data. Here we address some\nremaining issues less reported by the GPT-3 paper, such as a non-English LM,\nthe performances of different sized models, and the effect of recently\nintroduced prompt optimization on in-context learning. To achieve this, we\nintroduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric\ncorpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA\nwith our training configuration shows state-of-the-art in-context zero-shot and\nfew-shot learning performances on various downstream tasks in Korean. Also, we\nshow the performance benefits of prompt-based learning and demonstrate how it\ncan be integrated into the prompt engineering pipeline. Then we discuss the\npossibility of materializing the No Code AI paradigm by providing AI\nprototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,\nan interactive prompt engineering interface. Lastly, we demonstrate the\npotential of our methods with three successful in-house applications.</p>\n", "tags": ["Applications","Datasets","EMNLP","Few-Shot","In Context Learning","Model Architecture","Prompting","Training Techniques"] },
{"key": "bosselut2017simulating", "citations": "79", "year": "2017", "title":"Simulating Action Dynamics With Neural Process Networks", "abstract": "<p>Understanding procedural language requires anticipating the causal effects of\nactions, even when they are not explicitly stated. In this work, we introduce\nNeural Process Networks to understand procedural text through (neural)\nsimulation of action dynamics. Our model complements existing memory\narchitectures with dynamic entity tracking by explicitly modeling actions as\nstate transformers. The model updates the states of the entities by executing\nlearned action operators. Empirical results demonstrate that our proposed model\ncan reason about the unstated causal effects of actions, allowing it to provide\nmore accurate contextual information for understanding and generating\nprocedural text, all while offering more interpretable internal representations\nthan existing alternatives.</p>\n", "tags": [] },
{"key": "bosselut2018discourse", "citations": "80", "year": "2018", "title":"Discourse-aware Neural Rewards For Coherent Text Generation", "abstract": "<p>In this paper, we investigate the use of discourse-aware rewards with\nreinforcement learning to guide a model to generate long, coherent text. In\nparticular, we propose to learn neural rewards to model cross-sentence ordering\nas a means to approximate desired discourse structure. Empirical results\ndemonstrate that a generator trained with the learned reward produces more\ncoherent and less repetitive text than models trained with cross-entropy or\nwith reinforcement learning with commonly used scores as rewards.</p>\n", "tags": ["NAACL","Reinforcement Learning"] },
{"key": "bosselut2019dynamic", "citations": "86", "year": "2021", "title":"Dynamic Neuro-symbolic Knowledge Graph Construction For Zero-shot Commonsense Question Answering", "abstract": "<p>Understanding narratives requires reasoning about implicit world knowledge\nrelated to the causes, effects, and states of situations described in text. At\nthe core of this challenge is how to access contextually relevant knowledge on\ndemand and reason over it.\n  In this paper, we present initial studies toward zero-shot commonsense\nquestion answering by formulating the task as inference over dynamically\ngenerated commonsense knowledge graphs. In contrast to previous studies for\nknowledge integration that rely on retrieval of existing knowledge from static\nknowledge graphs, our study requires commonsense knowledge integration where\ncontextually relevant knowledge is often not present in existing knowledge\nbases. Therefore, we present a novel approach that generates\ncontextually-relevant symbolic knowledge structures on demand using generative\nneural commonsense knowledge models.\n  Empirical results on two datasets demonstrate the efficacy of our\nneuro-symbolic approach for dynamically constructing knowledge graphs for\nreasoning. Our approach achieves significant performance boosts over pretrained\nlanguage models and vanilla knowledge models, all while providing interpretable\nreasoning paths for its predictions.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "bostrom2020byte", "citations": "131", "year": "2020", "title":"Byte Pair Encoding Is Suboptimal For Language Model Pretraining", "abstract": "<p>The success of pretrained transformer language models (LMs) in natural\nlanguage processing has led to a wide range of pretraining setups. In\nparticular, these models employ a variety of subword tokenization methods, most\nnotably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the\nWordPiece method (Schuster and Nakajima, 2012), and unigram language modeling\n(Kudo, 2018), to segment text. However, to the best of our knowledge, the\nliterature does not contain a direct evaluation of the impact of tokenization\non language model pretraining. We analyze differences between BPE and unigram\nLM tokenization, finding that the latter method recovers subword units that\nalign more closely with morphology and avoids problems stemming from BPE’s\ngreedy construction procedure. We then compare the fine-tuned task performance\nof identical transformer masked language models pretrained with these\ntokenizations. Across downstream tasks and two languages (English and\nJapanese), we find that the unigram LM tokenization method matches or\noutperforms BPE. We hope that developers of future pretrained LMs will consider\nadopting the unigram LM method over the more prevalent BPE.</p>\n", "tags": ["EMNLP","Evaluation","Model Architecture"] },
{"key": "botha2018learning", "citations": "68", "year": "2018", "title":"Learning To Split And Rephrase From Wikipedia Edit History", "abstract": "<p>Split and rephrase is the task of breaking down a sentence into shorter ones\nthat together convey the same meaning. We extract a rich new dataset for this\ntask by mining Wikipedia’s edit history: WikiSplit contains one million\nnaturally occurring sentence rewrites, providing sixty times more distinct\nsplit examples and a ninety times larger vocabulary than the WebSplit corpus\nintroduced by Narayan et al. (2017) as a benchmark for this task. Incorporating\nWikiSplit as training data produces a model with qualitatively better\npredictions that score 32 BLEU points above the prior best result on the\nWebSplit benchmark.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "bouchacourt2018how", "citations": "81", "year": "2018", "title":"How Agents See Things: On Visual Representations In An Emergent Language Game", "abstract": "<p>There is growing interest in the language developed by agents interacting in\nemergent-communication settings. Earlier studies have focused on the agents’\nsymbol usage, rather than on their representation of visual input. In this\npaper, we consider the referential games of Lazaridou et al. (2017) and\ninvestigate the representations the agents develop during their evolving\ninteraction. We find that the agents establish successful communication by\ninducing visual representations that almost perfectly align with each other,\nbut, surprisingly, do not capture the conceptual properties of the objects\ndepicted in the input images. We conclude that, if we are interested in\ndeveloping language-like communication systems, we must pay more attention to\nthe visual semantics agents associate to the symbols they use.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "bouraoui2019inducing", "citations": "155", "year": "2020", "title":"Inducing Relational Knowledge From BERT", "abstract": "<p>One of the most remarkable properties of word embeddings is the fact that\nthey capture certain types of semantic and syntactic relationships. Recently,\npre-trained language models such as BERT have achieved groundbreaking results\nacross a wide range of Natural Language Processing tasks. However, it is\nunclear to what extent such models capture relational knowledge beyond what is\nalready captured by standard word embeddings. To explore this question, we\npropose a methodology for distilling relational knowledge from a pre-trained\nlanguage model. Starting from a few seed instances of a given relation, we\nfirst use a large text corpus to find sentences that are likely to express this\nrelation. We then use a subset of these extracted sentences as templates.\nFinally, we fine-tune a language model to predict whether a given word pair is\nlikely to be an instance of some relation, when given an instantiated template\nfor that relation as input.</p>\n", "tags": ["AAAI","Datasets","Model Architecture"] },
{"key": "bowman2016fast", "citations": "246", "year": "2016", "title":"A Fast Unified Model For Parsing And Sentence Understanding", "abstract": "<p>Tree-structured neural networks exploit valuable syntactic parse information\nas they interpret the meanings of sentences. However, they suffer from two key\ntechnical problems that make them slow and unwieldy for large-scale NLP tasks:\nthey usually operate on parsed sentences and they do not directly support\nbatched computation. We address these issues by introducing the Stack-augmented\nParser-Interpreter Neural Network (SPINN), which combines parsing and\ninterpretation within a single tree-sequence hybrid model by integrating\ntree-structured sentence interpretation into the linear sequential structure of\na shift-reduce parser. Our model supports batched computation for a speedup of\nup to 25 times over other tree-structured models, and its integrated parser can\noperate on unparsed data with little loss in accuracy. We evaluate it on the\nStanford NLI entailment task and show that it significantly outperforms other\nsentence-encoding models.</p>\n", "tags": [] },
{"key": "bowman2021what", "citations": "86", "year": "2021", "title":"What Will It Take To Fix Benchmarking In Natural Language Understanding?", "abstract": "<p>Evaluation for many natural language understanding (NLU) tasks is broken:\nUnreliable and biased systems score so highly on standard benchmarks that there\nis little room for researchers who develop better systems to demonstrate their\nimprovements. The recent trend to abandon IID benchmarks in favor of\nadversarially-constructed, out-of-distribution test sets ensures that current\nmodels will perform poorly, but ultimately only obscures the abilities that we\nwant our benchmarks to measure. In this position paper, we lay out four\ncriteria that we argue NLU benchmarks should meet. We argue most current\nbenchmarks fail at these criteria, and that adversarial data collection does\nnot meaningfully address the causes of these failures. Instead, restoring a\nhealthy evaluation ecosystem will require significant progress in the design of\nbenchmark datasets, the reliability with which they are annotated, their size,\nand the ways they handle social bias.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","NAACL"] },
{"key": "bowman2023eight", "citations": "77", "year": "2023", "title":"Eight Things To Know About Large Language Models", "abstract": "<p>The widespread public deployment of large language models (LLMs) in recent\nmonths has prompted a wave of new attention and engagement from advocates,\npolicymakers, and scholars from many fields. This attention is a timely\nresponse to the many urgent questions that this technology raises, but it can\nsometimes miss important considerations. This paper surveys the evidence for\neight potentially surprising such points:</p>\n<ol>\n  <li>LLMs predictably get more capable with increasing investment, even without\ntargeted innovation.</li>\n  <li>Many important LLM behaviors emerge unpredictably as a byproduct of\nincreasing investment.</li>\n  <li>LLMs often appear to learn and use representations of the outside world.</li>\n  <li>There are no reliable techniques for steering the behavior of LLMs.</li>\n  <li>Experts are not yet able to interpret the inner workings of LLMs.</li>\n  <li>Human performance on a task isn’t an upper bound on LLM performance.</li>\n  <li>LLMs need not express the values of their creators nor the values encoded\nin web text.</li>\n  <li>Brief interactions with LLMs are often misleading.</li>\n</ol>\n", "tags": ["Model Architecture"] },
{"key": "bošnjak2016programming", "citations": "64", "year": "2017", "title":"Programming With A Differentiable Forth Interpreter", "abstract": "<p>Given that in practice training data is scarce for all but a small set of\nproblems, a core question is how to incorporate prior knowledge into a model.\nIn this paper, we consider the case of prior procedural knowledge for neural\nnetworks, such as knowing how a program should traverse a sequence, but not\nwhat local actions should be performed at each step. To this end, we present an\nend-to-end differentiable interpreter for the programming language Forth which\nenables programmers to write program sketches with slots that can be filled\nwith behaviour trained from program input-output data. We can optimise this\nbehaviour directly through gradient descent techniques on user-specified\nobjectives, and also integrate the program into any larger neural computation\ngraph. We show empirically that our interpreter is able to effectively leverage\ndifferent levels of prior program structure and learn complex behaviours such\nas sequence sorting and addition. When connected to outputs of an LSTM and\ntrained jointly, our interpreter achieves state-of-the-art accuracy for\nend-to-end reasoning about quantities expressed in natural language stories.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "bradbury2016quasi", "citations": "350", "year": "2016", "title":"Quasi-recurrent Neural Networks", "abstract": "<p>Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep’s computation on the previous timestep’s\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.</p>\n", "tags": ["Time Series"] },
{"key": "brade2023promptify", "citations": "79", "year": "2023", "title":"Promptify: Text-to-image Generation Through Interactive Prompt Exploration With Large Language Models", "abstract": "<p>Text-to-image generative models have demonstrated remarkable capabilities in\ngenerating high-quality images based on textual prompts. However, crafting\nprompts that accurately capture the user’s creative intent remains challenging.\nIt often involves laborious trial-and-error procedures to ensure that the model\ninterprets the prompts in alignment with the user’s intention. To address the\nchallenges, we present Promptify, an interactive system that supports prompt\nexploration and refinement for text-to-image generative models. Promptify\nutilizes a suggestion engine powered by large language models to help users\nquickly explore and craft diverse prompts. Our interface allows users to\norganize the generated images flexibly, and based on their preferences,\nPromptify suggests potential changes to the original prompt. This feedback loop\nenables users to iteratively refine their prompts and enhance desired features\nwhile avoiding unwanted ones. Our user study shows that Promptify effectively\nfacilitates the text-to-image workflow and outperforms an existing baseline\ntool widely used for text-to-image generation.</p>\n", "tags": ["Prompting"] },
{"key": "bras2020adversarial", "citations": "146", "year": "2020", "title":"Adversarial Filters Of Dataset Biases", "abstract": "<p>Large neural models have demonstrated human-level performance on language and\nvision benchmarks, while their performance degrades considerably on adversarial\nor out-of-distribution samples. This raises the question of whether these\nmodels have learned to solve a dataset rather than the underlying task by\noverfitting to spurious dataset biases. We investigate one recently proposed\napproach, AFLite, which adversarially filters such dataset biases, as a means\nto mitigate the prevalent overestimation of machine performance. We provide a\ntheoretical understanding for AFLite, by situating it in the generalized\nframework for optimum bias reduction. We present extensive supporting evidence\nthat AFLite is broadly applicable for reduction of measurable dataset biases,\nand that models trained on the filtered datasets yield better generalization to\nout-of-distribution tasks. Finally, filtering results in a large drop in model\nperformance (e.g., from 92% to 62% for SNLI), while human performance still\nremains high. Our work thus shows that such filtered datasets can pose new\nresearch challenges for robust generalization by serving as upgraded\nbenchmarks.</p>\n", "tags": ["Datasets","Ethics & Fairness","Tools"] },
{"key": "bražinskas2020few", "citations": "60", "year": "2020", "title":"Few-shot Learning For Opinion Summarization", "abstract": "<p>Opinion summarization is the automatic creation of text reflecting subjective\ninformation expressed in multiple documents, such as user reviews of a product.\nThe task is practically important and has attracted a lot of attention.\nHowever, due to the high cost of summary production, datasets large enough for\ntraining supervised models are lacking. Instead, the task has been\ntraditionally approached with extractive methods that learn to select text\nfragments in an unsupervised or weakly-supervised way. Recently, it has been\nshown that abstractive summaries, potentially more fluent and better at\nreflecting conflicting information, can also be produced in an unsupervised\nfashion. However, these models, not being exposed to actual summaries, fail to\ncapture their essential properties. In this work, we show that even a handful\nof summaries is sufficient to bootstrap generation of the summary text with all\nexpected properties, such as writing style, informativeness, fluency, and\nsentiment preservation. We start by training a conditional Transformer language\nmodel to generate a new product review given other available reviews of the\nproduct. The model is also conditioned on review properties that are directly\nrelated to summaries; the properties are derived from reviews with no manual\neffort. In the second stage, we fine-tune a plug-in module that learns to\npredict property values on a handful of summaries. This lets us switch the\ngenerator to the summarization mode. We show on Amazon and Yelp datasets that\nour approach substantially outperforms previous extractive and abstractive\nmethods in automatic and human evaluation.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Few-Shot","Model Architecture","Training Techniques"] },
{"key": "brian2021power", "citations": "1746", "year": "2021", "title":"The Power Of Scale For Parameter-efficient Prompt Tuning", "abstract": "<p>In this work, we explore “prompt tuning”, a simple yet effective mechanism\nfor learning “soft prompts” to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3’s “few-shot” learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod “closes the gap” and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed “prefix tuning” of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.</p>\n", "tags": ["EMNLP","Few-Shot","Model Architecture","Prompting"] },
{"key": "britz2017massive", "citations": "466", "year": "2017", "title":"Massive Exploration Of Neural Machine Translation Architectures", "abstract": "<p>Neural Machine Translation (NMT) has shown remarkable progress over the past\nfew years with production systems now being deployed to end-users. One major\ndrawback of current architectures is that they are expensive to train,\ntypically requiring days to weeks of GPU time to converge. This makes\nexhaustive hyperparameter search, as is commonly done with other neural network\narchitectures, prohibitively expensive. In this work, we present the first\nlarge-scale analysis of NMT architecture hyperparameters. We report empirical\nresults and variance numbers for several hundred experimental runs,\ncorresponding to over 250,000 GPU hours on the standard WMT English to German\ntranslation task. Our experiments lead to novel insights and practical advice\nfor building and extending NMT architectures. As part of this contribution, we\nrelease an open-source NMT framework that enables researchers to easily\nexperiment with novel techniques and reproduce state of the art results.</p>\n", "tags": ["EMNLP","Model Architecture","Tools"] },
{"key": "brockschmidt2018generative", "citations": "84", "year": "2018", "title":"Generative Code Modeling With Graphs", "abstract": "<p>Generative models for source code are an interesting structured prediction\nproblem, requiring to reason about both hard syntactic and semantic constraints\nas well as about natural, likely programs. We present a novel model for this\nproblem that uses a graph to represent the intermediate state of the generated\noutput. The generative procedure interleaves grammar-driven expansion steps\nwith graph augmentation and neural message passing steps. An experimental\nevaluation shows that our new model can generate semantically meaningful\nexpressions, outperforming a range of strong baselines.</p>\n", "tags": ["Evaluation"] },
{"key": "brohan2023rt", "citations": "124", "year": "2023", "title":"RT-2: Vision-language-action Models Transfer Web Knowledge To Robotic Control", "abstract": "<p>We study how vision-language models trained on Internet-scale data can be\nincorporated directly into end-to-end robotic control to boost generalization\nand enable emergent semantic reasoning. Our goal is to enable a single\nend-to-end trained model to both learn to map robot observations to actions and\nenjoy the benefits of large-scale pretraining on language and vision-language\ndata from the web. To this end, we propose to co-fine-tune state-of-the-art\nvision-language models on both robotic trajectory data and Internet-scale\nvision-language tasks, such as visual question answering. In contrast to other\napproaches, we propose a simple, general recipe to achieve this goal: in order\nto fit both natural language responses and robotic actions into the same\nformat, we express the actions as text tokens and incorporate them directly\ninto the training set of the model in the same way as natural language tokens.\nWe refer to such category of models as vision-language-action models (VLA) and\ninstantiate an example of such a model, which we call RT-2. Our extensive\nevaluation (6k evaluation trials) shows that our approach leads to performant\nrobotic policies and enables RT-2 to obtain a range of emergent capabilities\nfrom Internet-scale training. This includes significantly improved\ngeneralization to novel objects, the ability to interpret commands not present\nin the robot training data (such as placing an object onto a particular number\nor icon), and the ability to perform rudimentary reasoning in response to user\ncommands (such as picking up the smallest or largest object, or the one closest\nto another object). We further show that incorporating chain of thought\nreasoning allows RT-2 to perform multi-stage semantic reasoning, for example\nfiguring out which object to pick up for use as an improvised hammer (a rock),\nor which type of drink is best suited for someone who is tired (an energy\ndrink).</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "brooks2022instructpix2pix", "citations": "603", "year": "2023", "title":"Instructpix2pix: Learning To Follow Image Editing Instructions", "abstract": "<p>We propose a method for editing images from human instructions: given an\ninput image and a written instruction that tells the model what to do, our\nmodel follows these instructions to edit the image. To obtain training data for\nthis problem, we combine the knowledge of two large pretrained models – a\nlanguage model (GPT-3) and a text-to-image model (Stable Diffusion) – to\ngenerate a large dataset of image editing examples. Our conditional diffusion\nmodel, InstructPix2Pix, is trained on our generated data, and generalizes to\nreal images and user-written instructions at inference time. Since it performs\nedits in the forward pass and does not require per example fine-tuning or\ninversion, our model edits images quickly, in a matter of seconds. We show\ncompelling editing results for a diverse collection of input images and written\ninstructions.</p>\n", "tags": ["CVPR","Datasets","Fine-Tuning","Instruction Following","Model Architecture","Training Techniques"] },
{"key": "broscheit2020investigating", "citations": "64", "year": "2019", "title":"Investigating Entity Knowledge In BERT With Simple Neural End-to-end Entity Linking", "abstract": "<p>A typical architecture for end-to-end entity linking systems consists of\nthree steps: mention detection, candidate generation and entity disambiguation.\nIn this study we investigate the following questions: (a) Can all those steps\nbe learned jointly with a model for contextualized text-representations, i.e.\nBERT (Devlin et al., 2019)? (b) How much entity knowledge is already contained\nin pretrained BERT? (c) Does additional entity knowledge improve BERT’s\nperformance in downstream tasks? To this end, we propose an extreme\nsimplification of the entity linking setup that works surprisingly well: simply\ncast it as a per token classification over the entire entity vocabulary (over\n700K classes in our case). We show on an entity linking benchmark that (i) this\nmodel improves the entity representations over plain BERT, (ii) that it\noutperforms entity linking architectures that optimize the tasks separately and\n(iii) that it only comes second to the current state-of-the-art that does\nmention detection and entity disambiguation jointly. Additionally, we\ninvestigate the usefulness of entity-aware token-representations in the\ntext-understanding benchmark GLUE, as well as the question answering benchmarks\nSQUAD V2 and SWAG and also the EN-DE WMT14 machine translation benchmark. To\nour surprise, we find that most of those benchmarks do not benefit from\nadditional entity knowledge, except for a task with very small training data,\nthe RTE task in GLUE, which improves by 2%.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "brown2019extrapolating", "citations": "129", "year": "2019", "title":"Extrapolating Beyond Suboptimal Demonstrations Via Inverse Reinforcement Learning From Observations", "abstract": "<p>A critical flaw of existing inverse reinforcement learning (IRL) methods is\ntheir inability to significantly outperform the demonstrator. This is because\nIRL typically seeks a reward function that makes the demonstrator appear\nnear-optimal, rather than inferring the underlying intentions of the\ndemonstrator that may have been poorly executed in practice. In this paper, we\nintroduce a novel reward-learning-from-observation algorithm, Trajectory-ranked\nReward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately)\nranked demonstrations in order to infer high-quality reward functions from a\nset of potentially poor demonstrations. When combined with deep reinforcement\nlearning, T-REX outperforms state-of-the-art imitation learning and IRL methods\non multiple Atari and MuJoCo benchmark tasks and achieves performance that is\noften more than twice the performance of the best demonstration. We also\ndemonstrate that T-REX is robust to ranking noise and can accurately\nextrapolate intention by simply watching a learner noisily improve at a task\nover time.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Reinforcement Learning"] },
{"key": "brown2020language", "citations": "13185", "year": "2020", "title":"Language Models Are Few-shot Learners", "abstract": "<p>Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3’s few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "brunner2019identifiability", "citations": "95", "year": "2019", "title":"On Identifiability In Transformers", "abstract": "<p>In this paper we delve deep in the Transformer architecture by investigating\ntwo of its core components: self-attention and contextual embeddings. In\nparticular, we study the identifiability of attention weights and token\nembeddings, and the aggregation of context into hidden tokens. We show that,\nfor sequences longer than the attention head dimension, attention weights are\nnot identifiable. We propose effective attention as a complementary tool for\nimproving explanatory interpretations based on attention. Furthermore, we show\nthat input tokens retain to a large degree their identity across the model. We\nalso find evidence suggesting that identity information is mainly encoded in\nthe angle of the embeddings and gradually decreases with depth. Finally, we\ndemonstrate strong mixing of input information in the generation of contextual\nembeddings by means of a novel quantification method based on gradient\nattribution. Overall, we show that self-attention distributions are not\ndirectly interpretable and present tools to better understand and further\ninvestigate Transformer models.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "bubeck2023sparks", "citations": "1090", "year": "2023", "title":"Sparks Of Artificial General Intelligence: Early Experiments With GPT-4", "abstract": "<p>Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google’s PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4’s\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4’s\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.</p>\n", "tags": ["Emergent Abilities","Model Architecture","Prompting"] },
{"key": "buch2022revisiting", "citations": "84", "year": "2022", "title":"Revisiting The \"video\" In Video-language Understanding", "abstract": "<p>What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Training Techniques"] },
{"key": "buck2017ask", "citations": "96", "year": "2018", "title":"Ask The Right Questions: Active Question Reformulation With Reinforcement Learning", "abstract": "<p>We frame Question Answering (QA) as a Reinforcement Learning task, an\napproach that we call Active Question Answering. We propose an agent that sits\nbetween the user and a black box QA system and learns to reformulate questions\nto elicit the best possible answers. The agent probes the system with,\npotentially many, natural language reformulations of an initial question and\naggregates the returned evidence to yield the best answer. The reformulation\nsystem is trained end-to-end to maximize answer quality using policy gradient.\nWe evaluate on SearchQA, a dataset of complex questions extracted from\nJeopardy!. The agent outperforms a state-of-the-art base model, playing the\nrole of the environment, and other benchmarks. We also analyze the language\nthat the agent has learned while interacting with the question answering\nsystem. We find that successful question reformulations look quite different\nfrom natural language paraphrases. The agent is able to discover non-trivial\nreformulation strategies that resemble classic information retrieval techniques\nsuch as term re-weighting (tf-idf) and stemming.</p>\n", "tags": ["Agentic","Datasets","ICLR","Reinforcement Learning"] },
{"key": "budzianowski2018multiwoz", "citations": "862", "year": "2018", "title":"Multiwoz -- A Large-scale Multi-domain Wizard-of-oz Dataset For Task-oriented Dialogue Modelling", "abstract": "<p>Even though machine learning has become the major scene in dialogue research\ncommunity, the real breakthrough has been blocked by the scale of data\navailable. To address this fundamental obstacle, we introduce the Multi-Domain\nWizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human\nwritten conversations spanning over multiple domains and topics. At a size of\n\\(10\\)k dialogues, it is at least one order of magnitude larger than all previous\nannotated task-oriented corpora. The contribution of this work apart from the\nopen-sourced dataset labelled with dialogue belief states and dialogue actions\nis two-fold: firstly, a detailed description of the data collection procedure\nalong with a summary of data structure and analysis is provided. The proposed\ndata-collection pipeline is entirely based on crowd-sourcing without the need\nof hiring professional annotators; secondly, a set of benchmark results of\nbelief tracking, dialogue act and response generation is reported, which shows\nthe usability of the data and sets a baseline for future studies.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "bugliarello2019enhancing", "citations": "68", "year": "2020", "title":"Enhancing Machine Translation With Dependency-aware Self-attention", "abstract": "<p>Most neural machine translation models only rely on pairs of parallel\nsentences, assuming syntactic information is automatically learned by an\nattention mechanism. In this work, we investigate different approaches to\nincorporate syntactic knowledge in the Transformer model and also propose a\nnovel, parameter-free, dependency-aware self-attention mechanism that improves\nits translation quality, especially for long sentences and in low-resource\nscenarios. We show the efficacy of each approach on WMT English-German and\nEnglish-Turkish, and WAT English-Japanese translation tasks.</p>\n", "tags": ["Model Architecture"] },
{"key": "bugliarello2020multimodal", "citations": "90", "year": "2021", "title":"Multimodal Pretraining Unmasked: A Meta-analysis And A Unified Framework Of Vision-and-language Berts", "abstract": "<p>Large-scale pretraining and task-specific fine-tuning is now the standard\nmethodology for many tasks in computer vision and natural language processing.\nRecently, a multitude of methods have been proposed for pretraining vision and\nlanguage BERTs to tackle challenges at the intersection of these two key areas\nof AI. These models can be categorised into either single-stream or dual-stream\nencoders. We study the differences between these two categories, and show how\nthey can be unified under a single theoretical framework. We then conduct\ncontrolled experiments to discern the empirical differences between five V&amp;L\nBERTs. Our experiments show that training data and hyperparameters are\nresponsible for most of the differences between the reported results, but they\nalso reveal that the embedding layer plays a crucial role in these massive\nmodels.</p>\n", "tags": ["Fine-Tuning","Survey Paper","TACL","Tools","Training Techniques"] },
{"key": "bunel2018leveraging", "citations": "72", "year": "2018", "title":"Leveraging Grammar And Reinforcement Learning For Neural Program Synthesis", "abstract": "<p>Program synthesis is the task of automatically generating a program\nconsistent with a specification. Recent years have seen proposal of a number of\nneural approaches for program synthesis, many of which adopt a sequence\ngeneration paradigm similar to neural machine translation, in which\nsequence-to-sequence models are trained to maximize the likelihood of known\nreference programs. While achieving impressive results, this strategy has two\nkey limitations. First, it ignores Program Aliasing: the fact that many\ndifferent programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many\nsemantically correct programs, which can adversely affect the synthesizer\nperformance. Second, this strategy overlooks the fact that programs have a\nstrict syntax that can be efficiently checked. To address the first limitation,\nwe perform reinforcement learning on top of a supervised model with an\nobjective that explicitly maximizes the likelihood of generating semantically\ncorrect programs. For addressing the second limitation, we introduce a training\nprocedure that directly maximizes the probability of generating syntactically\ncorrect programs that fulfill the specification. We show that our contributions\nlead to improved accuracy of the models, especially in cases where the training\ndata is limited.</p>\n", "tags": ["Agentic","Reinforcement Learning","Training Techniques"] },
{"key": "bunk2020diet", "citations": "105", "year": "2020", "title":"DIET: Lightweight Language Understanding For Dialogue Systems", "abstract": "<p>Large-scale pre-trained language models have shown impressive results on\nlanguage understanding benchmarks like GLUE and SuperGLUE, improving\nconsiderably over other pre-training methods like distributed representations\n(GloVe) and purely supervised approaches. We introduce the Dual Intent and\nEntity Transformer (DIET) architecture, and study the effectiveness of\ndifferent pre-trained representations on intent and entity prediction, two\ncommon dialogue language understanding tasks. DIET advances the state of the\nart on a complex multi-domain NLU dataset and achieves similarly high\nperformance on other simpler datasets. Surprisingly, we show that there is no\nclear benefit to using large pre-trained models for this task, and in fact DIET\nimproves upon the current state of the art even in a purely supervised setup\nwithout any pre-trained embeddings. Our best performing model outperforms\nfine-tuning BERT and is about six times faster to train.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "burlot2019using", "citations": "122", "year": "2018", "title":"Using Monolingual Data In Neural Machine Translation: A Systematic Study", "abstract": "<p>Neural Machine Translation (MT) has radically changed the way systems are\ndeveloped. A major difference with the previous generation (Phrase-Based MT) is\nthe way monolingual target data, which often abounds, is used in these two\nparadigms. While Phrase-Based MT can seamlessly integrate very large language\nmodels trained on billions of sentences, the best option for Neural MT\ndevelopers seems to be the generation of artificial parallel data through\n\\textsl{back-translation} - a technique that fails to fully take advantage of\nexisting datasets. In this paper, we conduct a systematic study of\nback-translation, comparing alternative uses of monolingual data, as well as\nmultiple data generation procedures. Our findings confirm that back-translation\nis very effective and give new explanations as to why this is the case. We also\nintroduce new data simulation techniques that are almost as effective, yet much\ncheaper to implement.</p>\n", "tags": ["Datasets"] },
{"key": "buys2017robust", "citations": "80", "year": "2017", "title":"Robust Incremental Neural Semantic Graph Parsing", "abstract": "<p>Parsing sentences to linguistically-expressive semantic representations is a\nkey goal of Natural Language Processing. Yet statistical parsing has focused\nalmost exclusively on bilexical dependencies or domain-specific logical forms.\nWe propose a neural encoder-decoder transition-based parser which is the first\nfull-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The\nmodel architecture uses stack-based embedding features, predicting graphs\njointly with unlexicalized predicates and their token alignments. Our parser is\nmore accurate than attention-based baselines on MRS, and on an additional\nAbstract Meaning Representation (AMR) benchmark, and GPU batch processing makes\nit an order of magnitude faster than a high-precision grammar-based parser.\nFurther, the 86.69% Smatch score of our MRS parser is higher than the\nupper-bound on AMR parsing, making MRS an attractive choice as a semantic\nrepresentation.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "byrne2019taskmaster", "citations": "143", "year": "2019", "title":"Taskmaster-1: Toward A Realistic And Diverse Dialog Dataset", "abstract": "<p>A significant barrier to progress in data-driven approaches to building\ndialog systems is the lack of high quality, goal-oriented conversational data.\nTo help satisfy this elementary requirement, we introduce the initial release\nof the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising\nsix domains. Two procedures were used to create this collection, each with\nunique advantages. The first involves a two-person, spoken “Wizard of Oz” (WOz)\napproach in which trained agents and crowdsourced workers interact to complete\nthe task while the second is “self-dialog” in which crowdsourced workers write\nthe entire dialog themselves. We do not restrict the workers to detailed\nscripts or to a small knowledge base and hence we observe that our dataset\ncontains more realistic and diverse conversations in comparison to existing\ndatasets. We offer several baseline models including state of the art neural\nseq2seq architectures with benchmark performance as well as qualitative human\nevaluations. Dialogs are labeled with API calls and arguments, a simple and\ncost effective approach which avoids the requirement of complex annotation\nschema. The layer of abstraction between the dialog model and the service\nprovider API allows for a given model to interact with multiple services that\nprovide similar functionally. Finally, the dataset will evoke interest in\nwritten vs. spoken language, discourse patterns, error handling and other\nlinguistic phenomena related to dialog system research, development and design.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Tools"] },
{"key": "bérard2018end", "citations": "128", "year": "2018", "title":"End-to-end Automatic Speech Translation Of Audiobooks", "abstract": "<p>We investigate end-to-end speech-to-text translation on a corpus of\naudiobooks specifically augmented for this task. Previous works investigated\nthe extreme case where source language transcription is not available during\nlearning nor decoding, but we also study a midway case where source language\ntranscription is available at training time only. In this case, a single model\nis trained to decode source speech into target text in a single pass.\nExperimental results show that it is possible to train compact and efficient\nend-to-end speech translation models in this setup. We also distribute the\ncorpus and hope that our speech translation baseline on this corpus will be\nchallenged in the future.</p>\n", "tags": ["Datasets","ICASSP","Training Techniques"] },
{"key": "caccia2018language", "citations": "105", "year": "2020", "title":"Language Gans Falling Short", "abstract": "<p>Generating high-quality text with sufficient diversity is essential for a\nwide range of Natural Language Generation (NLG) tasks. Maximum-Likelihood (MLE)\nmodels trained with teacher forcing have consistently been reported as weak\nbaselines, where poor performance is attributed to exposure bias (Bengio et\nal., 2015; Ranzato et al., 2015); at inference time, the model is fed its own\nprediction instead of a ground-truth token, which can lead to accumulating\nerrors and poor samples. This line of reasoning has led to an outbreak of\nadversarial based approaches for NLG, on the account that GANs do not suffer\nfrom exposure bias. In this work, we make several surprising observations which\ncontradict common beliefs. First, we revisit the canonical evaluation framework\nfor NLG, and point out fundamental flaws with quality-only evaluation: we show\nthat one can outperform such metrics using a simple, well-known temperature\nparameter to artificially reduce the entropy of the model’s conditional\ndistributions. Second, we leverage the control over the quality / diversity\ntrade-off given by this parameter to evaluate models over the whole\nquality-diversity spectrum and find MLE models constantly outperform the\nproposed GAN variants over the whole quality-diversity space. Our results have\nseveral implications: 1) The impact of exposure bias on sample quality is less\nsevere than previously thought, 2) temperature tuning provides a better quality\n/ diversity trade-off than adversarial training while being easier to train,\neasier to cross-validate, and less computationally expensive. Code to reproduce\nthe experiments is available at github.com/pclucas14/GansFallingShort</p>\n", "tags": ["Ethics & Fairness","Evaluation","Has Code","ICLR","Tools","Training Techniques"] },
{"key": "cachola2020tldr", "citations": "146", "year": "2020", "title":"TLDR: Extreme Summarization Of Scientific Documents", "abstract": "<p>We introduce TLDR generation, a new form of extreme summarization, for\nscientific papers. TLDR generation involves high source compression and\nrequires expert background knowledge and understanding of complex\ndomain-specific language. To facilitate study on this task, we introduce\nSciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR\ncontains both author-written and expert-derived TLDRs, where the latter are\ncollected using a novel annotation protocol that produces high-quality\nsummaries while minimizing annotation burden. We propose CATTS, a simple yet\neffective learning strategy for generating TLDRs that exploits titles as an\nauxiliary training signal. CATTS improves upon strong baselines under both\nautomated metrics and human evaluations. Data and code are publicly available\nat https://github.com/allenai/scitldr.</p>\n", "tags": ["EMNLP","Evaluation","Has Code"] },
{"key": "cadene2019murel", "citations": "306", "year": "2019", "title":"MUREL: Multimodal Relational Reasoning For Visual Question Answering", "abstract": "<p>Multimodal attentional networks are currently state-of-the-art models for\nVisual Question Answering (VQA) tasks involving real images. Although attention\nallows to focus on the visual content relevant to the question, this simple\nmechanism is arguably insufficient to model complex reasoning features required\nfor VQA or other high-level tasks.\n  In this paper, we propose MuRel, a multimodal relational network which is\nlearned end-to-end to reason over real images. Our first contribution is the\nintroduction of the MuRel cell, an atomic reasoning primitive representing\ninteractions between question and image regions by a rich vectorial\nrepresentation, and modeling region relations with pairwise combinations.\nSecondly, we incorporate the cell into a full MuRel network, which\nprogressively refines visual and question interactions, and can be leveraged to\ndefine visualization schemes finer than mere attention maps.\n  We validate the relevance of our approach with various ablation studies, and\nshow its superiority to attention-based methods on three datasets: VQA 2.0,\nVQA-CP v2 and TDIUC. Our final MuRel network is competitive to or outperforms\nstate-of-the-art results in this challenging context.\n  Our code is available: https://github.com/Cadene/murel.bootstrap.pytorch</p>\n", "tags": ["CVPR","Datasets","Has Code","Model Architecture"] },
{"key": "cadene2019rubi", "citations": "182", "year": "2019", "title":"Rubi: Reducing Unimodal Biases In Visual Question Answering", "abstract": "<p>Visual Question Answering (VQA) is the task of answering questions about an\nimage. Some VQA models often exploit unimodal biases to provide the correct\nanswer without using the image information. As a result, they suffer from a\nhuge drop in performance when evaluated on data outside their training set\ndistribution. This critical issue makes them unsuitable for real-world\nsettings.\n  We propose RUBi, a new learning strategy to reduce biases in any VQA model.\nIt reduces the importance of the most biased examples, i.e. examples that can\nbe correctly classified without looking at the image. It implicitly forces the\nVQA model to use the two input modalities instead of relying on statistical\nregularities between the question and the answer. We leverage a question-only\nmodel that captures the language biases by identifying when these unwanted\nregularities are used. It prevents the base VQA model from learning them by\ninfluencing its predictions. This leads to dynamically adjusting the loss in\norder to compensate for biases. We validate our contributions by surpassing the\ncurrent state-of-the-art results on VQA-CP v2. This dataset is specifically\ndesigned to assess the robustness of VQA models when exposed to different\nquestion biases at test time than what was seen during training.\n  Our code is available: github.com/cdancette/rubi.bootstrap.pytorch</p>\n", "tags": ["Datasets","Has Code","NEURIPS","Training Techniques"] },
{"key": "caglayan2016does", "citations": "77", "year": "2016", "title":"Does Multimodality Help Human And Machine For Translation And Image Captioning?", "abstract": "<p>This paper presents the systems developed by LIUM and CVC for the WMT16\nMultimodal Machine Translation challenge. We explored various comparative\nmethods, namely phrase-based systems and attentional recurrent neural networks\nmodels trained using monomodal or multimodal data. We also performed a human\nevaluation in order to estimate the usefulness of multimodal data for human\nmachine translation and image description generation. Our systems obtained the\nbest results for both tasks according to the automatic evaluation metrics BLEU\nand METEOR.</p>\n", "tags": ["Evaluation"] },
{"key": "caglayan2017lium", "citations": "73", "year": "2017", "title":"LIUM-CVC Submissions For WMT17 Multimodal Translation Task", "abstract": "<p>This paper describes the monomodal and multimodal Neural Machine Translation\nsystems developed by LIUM and CVC for WMT17 Shared Task on Multimodal\nTranslation. We mainly explored two multimodal architectures where either\nglobal visual features or convolutional feature maps are integrated in order to\nbenefit from visual context. Our final systems ranked first for both En-De and\nEn-Fr language pairs according to the automatic evaluation metrics METEOR and\nBLEU.</p>\n", "tags": ["Evaluation"] },
{"key": "caglayan2017nmtpy", "citations": "71", "year": "2017", "title":"NMTPY: A Flexible Toolkit For Advanced Neural Machine Translation Systems", "abstract": "<p>In this paper, we present nmtpy, a flexible Python toolkit based on Theano\nfor training Neural Machine Translation and other neural sequence-to-sequence\narchitectures. nmtpy decouples the specification of a network from the training\nand inference utilities to simplify the addition of a new architecture and\nreduce the amount of boilerplate code to be written. nmtpy has been used for\nLIUM’s top-ranked submissions to WMT Multimodal Machine Translation and News\nTranslation tasks in 2016 and 2017.</p>\n", "tags": ["Model Architecture","RAG","Tools","Training Techniques"] },
{"key": "caglayan2019probing", "citations": "146", "year": "2019", "title":"Probing The Need For Visual Context In Multimodal Machine Translation", "abstract": "<p>Current work on multimodal machine translation (MMT) has suggested that the\nvisual modality is either unnecessary or only marginally beneficial. We posit\nthat this is a consequence of the very simple, short and repetitive sentences\nused in the only available dataset for the task (Multi30K), rendering the\nsource text sufficient as context. In the general case, however, we believe\nthat it is possible to combine visual and textual information in order to\nground translations. In this paper we probe the contribution of the visual\nmodality to state-of-the-art MMT models by conducting a systematic analysis\nwhere we partially deprive the models from source-side textual context. Our\nresults show that under limited textual context, models are capable of\nleveraging the visual input to generate better translations. This contradicts\nthe current belief that MMT models disregard the visual modality because of\neither the quality of the image features or the way they are integrated into\nthe model.</p>\n", "tags": ["Datasets"] },
{"key": "cahyawijaya2021indonlg", "citations": "61", "year": "2021", "title":"Indonlg: Benchmark And Resources For Evaluating Indonesian Natural Language Generation", "abstract": "<p>Natural language generation (NLG) benchmarks provide an important avenue to\nmeasure progress and develop better NLG systems. Unfortunately, the lack of\npublicly available NLG benchmarks for low-resource languages poses a\nchallenging barrier for building NLG systems that work well for languages with\nlimited amounts of data. Here we introduce IndoNLG, the first benchmark to\nmeasure natural language generation (NLG) progress in three low-resource – yet\nwidely spoken – languages of Indonesia: Indonesian, Javanese, and Sundanese.\nAltogether, these languages are spoken by more than 100 million native\nspeakers, and hence constitute an important use case of NLG systems today.\nConcretely, IndoNLG covers six tasks: summarization, question answering,\nchit-chat, and three different pairs of machine translation (MT) tasks. We\ncollate a clean pretraining corpus of Indonesian, Sundanese, and Javanese\ndatasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and\nIndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on\nall tasks – despite using only one-fifth the parameters of a larger\nmultilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the\nimportance of pretraining on closely related, local languages to achieve more\nefficient learning and faster inference for very low-resource languages like\nJavanese and Sundanese.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "cai2018skeleton", "citations": "61", "year": "2019", "title":"Skeleton-to-response: Dialogue Generation Guided By Retrieval Memory", "abstract": "<p>For dialogue response generation, traditional generative models generate\nresponses solely from input queries. Such models rely on insufficient\ninformation for generating a specific response since a certain query could be\nanswered in multiple ways. Consequentially, those models tend to output generic\nand dull responses, impeding the generation of informative utterances.\nRecently, researchers have attempted to fill the information gap by exploiting\ninformation retrieval techniques. When generating a response for a current\nquery, similar dialogues retrieved from the entire training data are considered\nas an additional knowledge source. While this may harvest massive information,\nthe generative models could be overwhelmed, leading to undesirable performance.\nIn this paper, we propose a new framework which exploits retrieval results via\na skeleton-then-response paradigm. At first, a skeleton is generated by\nrevising the retrieved responses. Then, a novel generative model uses both the\ngenerated skeleton and the original query for response generation. Experimental\nresults show that our approaches significantly improve the diversity and\ninformativeness of the generated responses.</p>\n", "tags": ["Dialogue & Multi Turn","Memory & Context","Tools","Training Techniques"] },
{"key": "cai2019graph", "citations": "152", "year": "2020", "title":"Graph Transformer For Graph-to-sequence Learning", "abstract": "<p>The dominant graph-to-sequence transduction models employ graph neural\nnetworks for graph representation learning, where the structural information is\nreflected by the receptive field of neurons. Unlike graph neural networks that\nrestrict the information exchange between immediate neighborhood, we propose a\nnew model, known as Graph Transformer, that uses explicit relation encoding and\nallows direct communication between two distant nodes. It provides a more\nefficient way for global graph structure modeling. Experiments on the\napplications of text generation from Abstract Meaning Representation (AMR) and\nsyntax-based neural machine translation show the superiority of our proposed\nmodel. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU\non LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art\nresults by up to 2.2 points. On the syntax-based translation tasks, our model\nestablishes new single-model state-of-the-art BLEU scores, 21.3 for\nEnglish-to-German and 14.1 for English-to-Czech, improving over the existing\nbest results, including ensembles, by over 1 BLEU.</p>\n", "tags": ["AAAI","Applications","Model Architecture"] },
{"key": "cai2020amr", "citations": "92", "year": "2020", "title":"AMR Parsing Via Graph-sequence Iterative Inference", "abstract": "<p>We propose a new end-to-end model that treats AMR parsing as a series of dual\ndecisions on the input sequence and the incrementally constructed graph. At\neach time step, our model performs multiple rounds of attention, reasoning, and\ncomposition that aim to answer two critical questions: (1) which part of the\ninput \\textit{sequence} to abstract; and (2) where in the output \\textit{graph}\nto construct the new concept. We show that the answers to these two questions\nare mutually causalities. We design a model based on iterative inference that\nhelps achieve better answers in both perspectives, leading to greatly improved\nparsing accuracy. Our experimental results significantly outperform all\npreviously reported \\textsc{Smatch} scores by large margins. Remarkably,\nwithout the help of any large-scale pre-trained language model (e.g., BERT),\nour model already surpasses previous state-of-the-art using BERT. With the help\nof BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR\n2.0) and 75.4% on LDC2014T12 (AMR 1.0).</p>\n", "tags": ["Model Architecture"] },
{"key": "calixto2017doubly", "citations": "180", "year": "2017", "title":"Doubly-attentive Decoder For Multi-modal Neural Machine Translation", "abstract": "<p>We introduce a Multi-modal Neural Machine Translation model in which a\ndoubly-attentive decoder naturally incorporates spatial visual features\nobtained using pre-trained convolutional neural networks, bridging the gap\nbetween image description and translation. Our decoder learns to attend to\nsource-language words and parts of an image independently by means of two\nseparate attention mechanisms as it generates words in the target language. We\nfind that our model can efficiently exploit not just back-translated in-domain\nmulti-modal data but also large general-domain text-only MT corpora. We also\nreport state-of-the-art results on the Multi30k data set.</p>\n", "tags": ["Model Architecture"] },
{"key": "calixto2017incorporating", "citations": "154", "year": "2017", "title":"Incorporating Global Visual Features Into Attention-based Neural Machine Translation", "abstract": "<p>We introduce multi-modal, attention-based neural machine translation (NMT)\nmodels which incorporate visual features into different parts of both the\nencoder and the decoder. We utilise global image features extracted using a\npre-trained convolutional neural network and incorporate them (i) as words in\nthe source sentence, (ii) to initialise the encoder hidden state, and (iii) as\nadditional data to initialise the decoder hidden state. In our experiments, we\nevaluate how these different strategies to incorporate global image features\ncompare and which ones perform best. We also study the impact that adding\nsynthetic multi-modal, multilingual data brings and find that the additional\ndata have a positive impact on multi-modal models. We report new\nstate-of-the-art results and our best models also significantly improve on a\ncomparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k\ndata set according to all metrics evaluated. To the best of our knowledge, it\nis the first time a purely neural model significantly improves over a PBSMT\nmodel on all metrics evaluated on this data set.</p>\n", "tags": ["EMNLP","Evaluation","Model Architecture"] },
{"key": "calixto2018latent", "citations": "76", "year": "2019", "title":"Latent Variable Model For Multi-modal Translation", "abstract": "<p>In this work, we propose to model the interaction between visual and textual\nfeatures for multi-modal neural machine translation (MMT) through a latent\nvariable model. This latent variable can be seen as a multi-modal stochastic\nembedding of an image and its description in a foreign language. It is used in\na target-language decoder and also to predict image features. Importantly, our\nmodel formulation utilises visual and textual inputs during training but does\nnot require that images be available at test time. We show that our latent\nvariable MMT formulation improves considerably over strong baselines, including\na multi-task learning approach (Elliott and K'ad'ar, 2017) and a conditional\nvariational auto-encoder approach (Toyama et al., 2016). Finally, we show\nimprovements due to (i) predicting image features in addition to only\nconditioning on them, (ii) imposing a constraint on the minimum amount of\ninformation encoded in the latent variable, and (iii) by training on additional\ntarget-language image descriptions (i.e. synthetic data).</p>\n", "tags": ["Training Techniques"] },
{"key": "camachocollados2022tweetnlp", "citations": "75", "year": "2022", "title":"Tweetnlp: Cutting-edge Natural Language Processing For Social Media", "abstract": "<p>In this paper we present TweetNLP, an integrated platform for Natural\nLanguage Processing (NLP) in social media. TweetNLP supports a diverse set of\nNLP tasks, including generic focus areas such as sentiment analysis and named\nentity recognition, as well as social media-specific tasks such as emoji\nprediction and offensive language identification. Task-specific systems are\npowered by reasonably-sized Transformer-based language models specialized on\nsocial media text (in particular, Twitter) which can be run without the need\nfor dedicated hardware or cloud services. The main contributions of TweetNLP\nare: (1) an integrated Python library for a modern toolkit supporting social\nmedia analysis using our various task-specific models adapted to the social\ndomain; (2) an interactive online demo for codeless experimentation using our\nmodels; and (3) a tutorial covering a wide variety of typical social media\napplications.</p>\n", "tags": ["Applications","EMNLP","Model Architecture","Tools"] },
{"key": "camburu2018e", "citations": "278", "year": "2018", "title":"E-snli: Natural Language Inference With Natural Language Explanations", "abstract": "<p>In order for machine learning to garner widespread public adoption, models\nmust be able to provide interpretable and robust explanations for their\ndecisions, as well as learn from human-provided explanations at train time. In\nthis work, we extend the Stanford Natural Language Inference dataset with an\nadditional layer of human-annotated natural language explanations of the\nentailment relations. We further implement models that incorporate these\nexplanations into their training process and output them at test time. We show\nhow our corpus of explanations, which we call e-SNLI, can be used for various\ngoals, such as obtaining full sentence justifications of a model’s decisions,\nimproving universal sentence representations and transferring to out-of-domain\nNLI datasets. Our dataset thus opens up a range of research directions for\nusing natural language explanations, both for improving models and for\nasserting their trust.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "camgoz2020sign", "citations": "341", "year": "2020", "title":"Sign Language Transformers: Joint End-to-end Sign Language Recognition And Translation", "abstract": "<p>Prior work on Sign Language Translation has shown that having a mid-level\nsign gloss representation (effectively recognizing the individual signs)\nimproves the translation performance drastically. In fact, the current\nstate-of-the-art in translation requires gloss level tokenization in order to\nwork. We introduce a novel transformer based architecture that jointly learns\nContinuous Sign Language Recognition and Translation while being trainable in\nan end-to-end manner. This is achieved by using a Connectionist Temporal\nClassification (CTC) loss to bind the recognition and translation problems into\na single unified architecture. This joint approach does not require any\nground-truth timing information, simultaneously solving two co-dependant\nsequence-to-sequence learning problems and leads to significant performance\ngains.\n  We evaluate the recognition and translation performances of our approaches on\nthe challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report\nstate-of-the-art sign language recognition and translation results achieved by\nour Sign Language Transformers. Our translation networks outperform both sign\nvideo to spoken language and gloss to spoken language translation models, in\nsome cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We\nalso share new baseline translation results using transformer networks for\nseveral other text-to-text sign language translation tasks.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "campagna2019genie", "citations": "65", "year": "2019", "title":"Genie: A Generator Of Natural Language Semantic Parsers For Virtual Assistant Commands", "abstract": "<p>To understand diverse natural language commands, virtual assistants today are\ntrained with numerous labor-intensive, manually annotated sentences. This paper\npresents a methodology and the Genie toolkit that can handle new compound\ncommands with significantly less manual effort. We advocate formalizing the\ncapability of virtual assistants with a Virtual Assistant Programming Language\n(VAPL) and using a neural semantic parser to translate natural language into\nVAPL code. Genie needs only a small realistic set of input sentences for\nvalidating the neural model. Developers write templates to synthesize data;\nGenie uses crowdsourced paraphrases and data augmentation, along with the\nsynthesized data, to train a semantic parser. We also propose design principles\nthat make VAPL languages amenable to natural language translation. We apply\nthese principles to revise ThingTalk, the language used by the Almond virtual\nassistant. We use Genie to build the first semantic parser that can support\ncompound virtual assistants commands with unquoted free-form parameters. Genie\nachieves a 62% accuracy on realistic user inputs. We demonstrate Genie’s\ngenerality by showing a 19% and 31% improvement over the previous state of the\nart on a music skill, aggregate functions, and access control.</p>\n", "tags": [] },
{"key": "campagna2020zero", "citations": "77", "year": "2020", "title":"Zero-shot Transfer Learning With Synthesized Data For Multi-domain Dialogue State Tracking", "abstract": "<p>Zero-shot transfer learning for multi-domain dialogue state tracking can\nallow us to handle new domains without incurring the high cost of data\nacquisition. This paper proposes new zero-short transfer learning technique for\ndialogue state tracking where the in-domain training data are all synthesized\nfrom an abstract dialogue model and the ontology of the domain. We show that\ndata augmentation through synthesized data can improve the accuracy of\nzero-shot learning for both the TRADE model and the BERT-based SUMBT model on\nthe MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data\non the SUMBT model can reach about 2/3 of the accuracy obtained with the full\ntraining dataset. We improve the zero-shot learning state of the art on average\nacross domains by 21%.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "campbell2017nlp2code", "citations": "64", "year": "2017", "title":"Nlp2code: Code Snippet Content Assist Via Natural Language Tasks", "abstract": "<p>Developers increasingly take to the Internet for code snippets to integrate\ninto their programs. To save developers the time required to switch from their\ndevelopment environments to a web browser in the quest for a suitable code\nsnippet, we introduce NLP2Code, a content assist for code snippets. Unlike\nrelated tools, NLP2Code integrates directly into the source code editor and\nprovides developers with a content assist feature to close the vocabulary gap\nbetween developers’ needs and code snippet meta data. Our preliminary\nevaluation of NLP2Code shows that the majority of invocations lead to code\nsnippets rated as helpful by users and that the tool is able to support a wide\nrange of tasks.</p>\n", "tags": ["Evaluation","Tools"] },
{"key": "can2023wizardlm", "citations": "79", "year": "2023", "title":"Wizardlm: Empowering Large Language Models To Follow Complex Instructions", "abstract": "<p>Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna’s testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM</p>\n", "tags": ["Evaluation","Fine-Tuning","Has Code","Instruction Following","Model Architecture","Training Techniques"] },
{"key": "cao2016joint", "citations": "65", "year": "2016", "title":"Joint Copying And Restricted Generation For Paraphrase", "abstract": "<p>Many natural language generation tasks, such as abstractive summarization and\ntext simplification, are paraphrase-orientated. In these tasks, copying and\nrewriting are two main writing modes. Most previous sequence-to-sequence\n(Seq2Seq) models use a single decoder and neglect this fact. In this paper, we\ndevelop a novel Seq2Seq model to fuse a copying decoder and a restricted\ngenerative decoder. The copying decoder finds the position to be copied based\non a typical attention model. The generative decoder produces words limited in\nthe source-specific vocabulary. To combine the two decoders and determine the\nfinal output, we develop a predictor to predict the mode of copying or\nrewriting. This predictor can be guided by the actual writing mode in the\ntraining data. We conduct extensive experiments on two different paraphrase\ndatasets. The result shows that our model outperforms the state-of-the-art\napproaches in terms of both informativeness and language quality.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "cao2020behind", "citations": "100", "year": "2020", "title":"Behind The Scene: Revealing The Secrets Of Pre-trained Vision-and-language Models", "abstract": "<p>Recent Transformer-based large-scale pre-trained models have revolutionized\nvision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER\nhave significantly lifted state of the art across a wide range of V+L\nbenchmarks with joint image-text pre-training. However, little is known about\nthe inner mechanisms that destine their impressive success. To reveal the\nsecrets behind the scene of these powerful models, we present VALUE\n(Vision-And-Language Understanding Evaluation), a set of meticulously designed\nprobing tasks (e.g., Visual Coreference Resolution, Visual Relation Detection,\nLinguistic Probing Tasks) generalizable to standard pre-trained V+L models,\naiming to decipher the inner workings of multimodal pre-training (e.g., the\nimplicit knowledge garnered in individual attention heads, the inherent\ncross-modal alignment learned through contextualized multimodal embeddings).\nThrough extensive analysis of each archetypal model architecture via these\nprobing tasks, our key observations are: (i) Pre-trained models exhibit a\npropensity for attending over text rather than images during inference. (ii)\nThere exists a subset of attention heads that are tailored for capturing\ncross-modal interactions. (iii) Learned attention matrix in pre-trained models\ndemonstrates patterns coherent with the latent alignment between image regions\nand textual words. (iv) Plotted attention patterns reveal\nvisually-interpretable relations among image regions. (v) Pure linguistic\nknowledge is also effectively encoded in the attention heads. These are\nvaluable insights serving to guide future work towards designing better model\narchitecture and objectives for multimodal pre-training.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "cao2020multilingual", "citations": "160", "year": "2020", "title":"Multilingual Alignment Of Contextual Word Representations", "abstract": "<p>We propose procedures for evaluating and strengthening contextual embedding\nalignment and show that they are useful in analyzing and improving multilingual\nBERT. In particular, after our proposed alignment procedure, BERT exhibits\nsignificantly improved zero-shot performance on XNLI compared to the base\nmodel, remarkably matching pseudo-fully-supervised translate-train models for\nBulgarian and Greek. Further, to measure the degree of alignment, we introduce\na contextual version of word retrieval and show that it correlates well with\ndownstream zero-shot transfer. Using this word retrieval task, we also analyze\nBERT and find that it exhibits systematic deficiencies, e.g. worse alignment\nfor open-class parts-of-speech and word pairs written in different scripts,\nthat are corrected by the alignment procedure. These results support contextual\nalignment as a useful concept for understanding large multilingual pre-trained\nmodels.</p>\n", "tags": ["Model Architecture"] },
{"key": "cao2021knowledgeable", "citations": "66", "year": "2021", "title":"Knowledgeable Or Educated Guess? Revisiting Language Models As Knowledge Bases", "abstract": "<p>Previous literatures show that pre-trained masked language models (MLMs) such\nas BERT can achieve competitive factual knowledge extraction performance on\nsome datasets, indicating that MLMs can potentially be a reliable knowledge\nsource. In this paper, we conduct a rigorous study to explore the underlying\npredicting mechanisms of MLMs over different extraction paradigms. By\ninvestigating the behaviors of MLMs, we find that previous decent performance\nmainly owes to the biased prompts which overfit dataset artifacts. Furthermore,\nincorporating illustrative cases and external contexts improve knowledge\nprediction mainly due to entity type guidance and golden answer leakage. Our\nfindings shed light on the underlying predicting mechanisms of MLMs, and\nstrongly question the previous conclusion that current MLMs can potentially\nserve as reliable factual knowledge bases.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "cao2023assessing", "citations": "67", "year": "2023", "title":"Assessing Cross-cultural Alignment Between Chatgpt And Human Societies: An Empirical Study", "abstract": "<p>The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "cao2023comprehensive", "citations": "280", "year": "2023", "title":"A Comprehensive Survey Of Ai-generated Content (AIGC): A History Of Generative AI From GAN To Chatgpt", "abstract": "<p>Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant\nattention from society. As a result, many individuals have become interested in\nrelated resources and are seeking to uncover the background and secrets behind\nits impressive performance. In fact, ChatGPT and other Generative AI (GAI)\ntechniques belong to the category of Artificial Intelligence Generated Content\n(AIGC), which involves the creation of digital content, such as images, music,\nand natural language, through AI models. The goal of AIGC is to make the\ncontent creation process more efficient and accessible, allowing for the\nproduction of high-quality content at a faster pace. AIGC is achieved by\nextracting and understanding intent information from instructions provided by\nhuman, and generating the content according to its knowledge and the intent\ninformation. In recent years, large-scale models have become increasingly\nimportant in AIGC as they provide better intent extraction and thus, improved\ngeneration results. With the growth of data and the size of the models, the\ndistribution that the model can learn becomes more comprehensive and closer to\nreality, leading to more realistic and high-quality content generation. This\nsurvey provides a comprehensive review on the history of generative models, and\nbasic components, recent advances in AIGC from unimodal interaction and\nmultimodal interaction. From the perspective of unimodality, we introduce the\ngeneration tasks and relative models of text and image. From the perspective of\nmultimodality, we introduce the cross-application between the modalities\nmentioned above. Finally, we discuss the existing open problems and future\nchallenges in AIGC.</p>\n", "tags": ["Model Architecture","Survey Paper"] },
{"key": "card2020little", "citations": "81", "year": "2020", "title":"With Little Power Comes Great Responsibility", "abstract": "<p>Despite its importance to experimental design, statistical power (the\nprobability that, given a real effect, an experiment will reject the null\nhypothesis) has largely been ignored by the NLP community. Underpowered\nexperiments make it more difficult to discern the difference between\nstatistical noise and meaningful model improvements, and increase the chances\nof exaggerated findings. By meta-analyzing a set of existing NLP papers and\ndatasets, we characterize typical power for a variety of settings and conclude\nthat underpowered experiments are common in the NLP literature. In particular,\nfor several tasks in the popular GLUE benchmark, small test sets mean that most\nattempted comparisons to state of the art models will not be adequately\npowered. Similarly, based on reasonable assumptions, we find that the most\ntypical experimental design for human rating studies will be underpowered to\ndetect small model differences, of the sort that are frequently studied. For\nmachine translation, we find that typical test sets of 2000 sentences have\napproximately 75% power to detect differences of 1 BLEU point. To improve the\nsituation going forward, we give an overview of best practices for power\nanalysis in NLP and release a series of notebooks to assist with future power\nanalyses.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "carlini2022quantifying", "citations": "125", "year": "2022", "title":"Quantifying Memorization Across Neural Language Models", "abstract": "<p>Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n  We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes more complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.</p>\n", "tags": ["Ethics & Fairness","Memory & Context","Prompting","Training Techniques"] },
{"key": "casanueva2020efficient", "citations": "292", "year": "2020", "title":"Efficient Intent Detection With Dual Sentence Encoders", "abstract": "<p>Building conversational systems in new domains and with added functionality\nrequires resource-efficient models that work under low-data regimes (i.e., in\nfew-shot setups). Motivated by these requirements, we introduce intent\ndetection methods backed by pretrained dual sentence encoders such as USE and\nConveRT. We demonstrate the usefulness and wide applicability of the proposed\nintent detectors, showing that: 1) they outperform intent detectors based on\nfine-tuning the full BERT-Large model or using BERT as a fixed black-box\nencoder on three diverse intent detection data sets; 2) the gains are\nespecially pronounced in few-shot setups (i.e., with only 10 or 30 annotated\nexamples per intent); 3) our intent detectors can be trained in a matter of\nminutes on a single CPU; and 4) they are stable across different hyperparameter\nsettings. In hope of facilitating and democratizing research focused on\nintention detection, we release our code, as well as a new challenging\nsingle-domain intent detection dataset comprising 13,083 annotated examples\nover 77 intents.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "caselli2020hatebert", "citations": "182", "year": "2021", "title":"Hatebert: Retraining BERT For Abusive Language Detection In English", "abstract": "<p>In this paper, we introduce HateBERT, a re-trained BERT model for abusive\nlanguage detection in English. The model was trained on RAL-E, a large-scale\ndataset of Reddit comments in English from communities banned for being\noffensive, abusive, or hateful that we have collected and made available to the\npublic. We present the results of a detailed comparison between a general\npre-trained language model and the abuse-inclined version obtained by\nretraining with posts from the banned communities on three English datasets for\noffensive, abusive language and hate speech detection tasks. In all datasets,\nHateBERT outperforms the corresponding general BERT model. We also discuss a\nbattery of experiments comparing the portability of the generic pre-trained\nlanguage model and its corresponding abusive language-inclined counterpart\nacross the datasets, indicating that portability is affected by compatibility\nof the annotated phenomena.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture"] },
{"key": "castro2018dopamine", "citations": "175", "year": "2018", "title":"Dopamine: A Research Framework For Deep Reinforcement Learning", "abstract": "<p>Deep reinforcement learning (deep RL) research has grown significantly in\nrecent years. A number of software offerings now exist that provide stable,\ncomprehensive implementations for benchmarking. At the same time, recent deep\nRL research has become more diverse in its goals. In this paper we introduce\nDopamine, a new research framework for deep RL that aims to support some of\nthat diversity. Dopamine is open-source, TensorFlow-based, and provides compact\nand reliable implementations of some state-of-the-art deep RL agents. We\ncomplement this offering with a taxonomy of the different research objectives\nin deep RL research. While by no means exhaustive, our analysis highlights the\nheterogeneity of research in the field, and the value of frameworks such as\nours.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning","Tools"] },
{"key": "caswell2019tagged", "citations": "188", "year": "2019", "title":"Tagged Back-translation", "abstract": "<p>Recent work in Neural Machine Translation (NMT) has shown significant quality\ngains from noised-beam decoding during back-translation, a method to generate\nsynthetic parallel data. We show that the main role of such synthetic noise is\nnot to diversify the source side, as previously suggested, but simply to\nindicate to the model that the given source is synthetic. We propose a simpler\nalternative to noising techniques, consisting of tagging back-translated source\nsentences with an extra token. Our results on WMT outperform noised\nback-translation in English-Romanian and match performance on English-German,\nre-defining state-of-the-art in the former.</p>\n", "tags": [] },
{"key": "cañete2023spanish", "citations": "242", "year": "2023", "title":"Spanish Pre-trained BERT Model And Evaluation Data", "abstract": "<p>The Spanish language is one of the top 5 spoken languages in the world.\nNevertheless, finding resources to train or evaluate Spanish language models is\nnot an easy task. In this paper we help bridge this gap by presenting a\nBERT-based language model pre-trained exclusively on Spanish data. As a second\ncontribution, we also compiled several tasks specifically for the Spanish\nlanguage in a single repository much in the spirit of the GLUE benchmark. By\nfine-tuning our pre-trained Spanish model, we obtain better results compared to\nother BERT-based models pre-trained on multilingual corpora for most of the\ntasks, even achieving a new state-of-the-art on some of them. We have publicly\nreleased our model, the pre-training data, and the compilation of the Spanish\nbenchmarks.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "celikyilmaz2018deep", "citations": "337", "year": "2018", "title":"Deep Communicating Agents For Abstractive Summarization", "abstract": "<p>We present deep communicating agents in an encoder-decoder architecture to\naddress the challenges of representing a long document for abstractive\nsummarization. With deep communicating agents, the task of encoding a long text\nis divided across multiple collaborating agents, each in charge of a subsection\nof the input text. These encoders are connected to a single decoder, trained\nend-to-end using reinforcement learning to generate a focused and coherent\nsummary. Empirical results demonstrate that multiple communicating encoders\nlead to a higher quality summary compared to several strong baselines,\nincluding those based on a single encoder or multiple non-communicating\nencoders.</p>\n", "tags": ["Agentic","Model Architecture","NAACL","Reinforcement Learning"] },
{"key": "celikyilmaz2020evaluation", "citations": "200", "year": "2020", "title":"Evaluation Of Text Generation: A Survey", "abstract": "<p>The paper surveys evaluation methods of natural language generation (NLG)\nsystems that have been developed in the last few years. We group NLG evaluation\nmethods into three categories: (1) human-centric evaluation metrics, (2)\nautomatic metrics that require no training, and (3) machine-learned metrics.\nFor each category, we discuss the progress that has been made and the\nchallenges still being faced, with a focus on the evaluation of recently\nproposed NLG tasks and neural NLG models. We then present two examples for\ntask-specific NLG evaluations for automatic text summarization and long text\ngeneration, and conclude the paper by proposing future research directions.</p>\n", "tags": ["Evaluation","Survey Paper","Training Techniques"] },
{"key": "cer2017semeval", "citations": "345", "year": "2017", "title":"Semeval-2017 Task 1: Semantic Textual Similarity - Multilingual And Cross-lingual Focused Evaluation", "abstract": "<p>Semantic Textual Similarity (STS) measures the meaning similarity of\nsentences. Applications include machine translation (MT), summarization,\ngeneration, question answering (QA), short answer grading, semantic search,\ndialog and conversational systems. The STS shared task is a venue for assessing\nthe current state-of-the-art. The 2017 task focuses on multilingual and\ncross-lingual pairs with one sub-track exploring MT quality estimation (MTQE)\ndata. The task obtained strong participation from 31 teams, with 17\nparticipating in all language tracks. We summarize performance and review a\nselection of well performing methods. Analysis highlights common errors,\nproviding insight into the limitations of existing models. To support ongoing\nwork on semantic representations, the STS Benchmark is introduced as a new\nshared training and evaluation set carefully selected from the corpus of\nEnglish STS shared task data (2012-2017).</p>\n", "tags": ["Applications","Datasets","Evaluation","Training Techniques"] },
{"key": "cer2018universal", "citations": "1366", "year": "2018", "title":"Universal Sentence Encoder", "abstract": "<p>We present models for encoding sentences into embedding vectors that\nspecifically target transfer learning to other NLP tasks. The models are\nefficient and result in accurate performance on diverse transfer tasks. Two\nvariants of the encoding models allow for trade-offs between accuracy and\ncompute resources. For both variants, we investigate and report the\nrelationship between model complexity, resource consumption, the availability\nof transfer task training data, and task performance. Comparisons are made with\nbaselines that use word level transfer learning via pretrained word embeddings\nas well as baselines do not use any transfer learning. We find that transfer\nlearning using sentence embeddings tends to outperform word level transfer.\nWith transfer learning via sentence embeddings, we observe surprisingly good\nperformance with minimal amounts of supervised training data for a transfer\ntask. We obtain encouraging results on Word Embedding Association Tests (WEAT)\ntargeted at detecting model bias. Our pre-trained sentence encoding models are\nmade freely available for download and on TF Hub.</p>\n", "tags": ["Ethics & Fairness","Fine-Tuning","Training Techniques"] },
{"key": "chakraborty2022natgen", "citations": "69", "year": "2022", "title":"Natgen: Generative Pre-training By \"naturalizing\" Source Code", "abstract": "<p>Pre-trained Generative Language models (e.g. PLBART, CodeT5, SPT-Code) for\nsource code yielded strong results on several tasks in the past few years,\nincluding code generation and translation. These models have adopted varying\npre-training objectives to learn statistics of code construction from very\nlarge-scale corpora in a self-supervised fashion; the success of pre-trained\nmodels largely hinges on these pre-training objectives. This paper proposes a\nnew pre-training objective, “Naturalizing” of source code, exploiting code’s\nbimodal, dual-channel (formal &amp; natural channels) nature. Unlike natural\nlanguage, code’s bimodal, dual-channel nature allows us to generate\nsemantically equivalent code at scale. We introduce six classes of semantic\npreserving transformations to introduce un-natural forms of code, and then\nforce our model to produce more natural original programs written by\ndevelopers. Learning to generate equivalent, but more natural code, at scale,\nover large corpora of open-source code, without explicit manual supervision,\nhelps the model learn to both ingest &amp; generate code. We fine-tune our model in\nthree generative Software Engineering tasks: code generation, code translation,\nand code refinement with limited human-curated labeled data and achieve\nstate-of-the-art performance rivaling CodeT5. We show that our pre-trained\nmodel is especially competitive at zero-shot and few-shot learning, and better\nat learning code properties (e.g., syntax, data flow).</p>\n", "tags": ["Few-Shot","Llm For Code","Training Techniques"] },
{"key": "chalkidis2019neural", "citations": "247", "year": "2019", "title":"Neural Legal Judgment Prediction In English", "abstract": "<p>Legal judgment prediction is the task of automatically predicting the outcome\nof a court case, given a text describing the case’s facts. Previous work on\nusing neural models for this task has focused on Chinese; only feature-based\nmodels (e.g., using bags of words and topics) have been considered in English.\nWe release a new English legal judgment prediction dataset, containing cases\nfrom the European Court of Human Rights. We evaluate a broad variety of neural\nmodels on the new dataset, establishing strong baselines that surpass previous\nfeature-based models in three tasks: (1) binary violation classification; (2)\nmulti-label classification; (3) case importance prediction. We also explore if\nmodels are biased towards demographic information via data anonymization. As a\nside-product, we propose a hierarchical version of BERT, which bypasses BERT’s\nlength limitation.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "chalkidis2020legal", "citations": "558", "year": "2020", "title":"LEGAL-BERT: The Muppets Straight Out Of Law School", "abstract": "<p>BERT has achieved impressive performance in several NLP tasks. However, there\nhas been limited investigation on its adaptation guidelines in specialised\ndomains. Here we focus on the legal domain, where we explore several approaches\nfor applying BERT models to downstream legal tasks, evaluating on multiple\ndatasets. Our findings indicate that the previous guidelines for pre-training\nand fine-tuning, often blindly followed, do not always generalize well in the\nlegal domain. Thus we propose a systematic investigation of the available\nstrategies when applying BERT in specialised domains. These are: (a) use the\noriginal BERT out of the box, (b) adapt BERT by additional pre-training on\ndomain-specific corpora, and (c) pre-train BERT from scratch on domain-specific\ncorpora. We also propose a broader hyper-parameter search space when\nfine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT\nmodels intended to assist legal NLP research, computational law, and legal\ntechnology applications.</p>\n", "tags": ["Applications","Datasets","EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "chalkidis2021lexglue", "citations": "81", "year": "2022", "title":"Lexglue: A Benchmark Dataset For Legal Language Understanding In English", "abstract": "<p>Laws and their interpretations, legal arguments and agreements\\ are typically\nexpressed in writing, leading to the production of vast corpora of legal text.\nTheir analysis, which is at the center of legal practice, becomes increasingly\nelaborate as these collections grow in size. Natural language understanding\n(NLU) technologies can be a valuable tool to support legal practitioners in\nthese endeavors. Their usefulness, however, largely depends on whether current\nstate-of-the-art models can generalize across various tasks in the legal\ndomain. To answer this currently open question, we introduce the Legal General\nLanguage Understanding Evaluation (LexGLUE) benchmark, a collection of datasets\nfor evaluating model performance across a diverse set of legal NLU tasks in a\nstandardized way. We also provide an evaluation and analysis of several generic\nand legal-oriented models demonstrating that the latter consistently offer\nperformance improvements across multiple tasks.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "chalmers2023could", "citations": "99", "year": "2023", "title":"Could A Large Language Model Be Conscious?", "abstract": "<p>There has recently been widespread discussion of whether large language\nmodels might be sentient. Should we take this idea seriously? I will break down\nthe strongest reasons for and against. Given mainstream assumptions in the\nscience of consciousness, there are significant obstacles to consciousness in\ncurrent models: for example, their lack of recurrent processing, a global\nworkspace, and unified agency. At the same time, it is quite possible that\nthese obstacles will be overcome in the next decade or so. I conclude that\nwhile it is somewhat unlikely that current large language models are conscious,\nwe should take seriously the possibility that successors to large language\nmodels may be conscious in the not-too-distant future.</p>\n", "tags": [] },
{"key": "chan2019kermit", "citations": "78", "year": "2019", "title":"KERMIT: Generative Insertion-based Modeling For Sequences", "abstract": "<p>We present KERMIT, a simple insertion-based approach to generative modeling\nfor sequences and sequence pairs. KERMIT models the joint distribution and its\ndecompositions (i.e., marginals and conditionals) using a single neural network\nand, unlike much prior work, does not rely on a prespecified factorization of\nthe data distribution. During training, one can feed KERMIT paired data \\((x,\ny)\\) to learn the joint distribution \\(p(x, y)\\), and optionally mix in unpaired\ndata \\(x\\) or \\(y\\) to refine the marginals \\(p(x)\\) or \\(p(y)\\). During inference, we\nhave access to the conditionals \\(p(x \\mid y)\\) and \\(p(y \\mid x)\\) in both\ndirections. We can also sample from the joint distribution or the marginals.\nThe model supports both serial fully autoregressive decoding and parallel\npartially autoregressive decoding, with the latter exhibiting an empirically\nlogarithmic runtime. We demonstrate through experiments in machine translation,\nrepresentation learning, and zero-shot cloze question answering that our\nunified approach is capable of matching or exceeding the performance of\ndedicated state-of-the-art systems across a wide range of tasks without the\nneed for problem-specific architectural adaptation.</p>\n", "tags": ["Training Techniques"] },
{"key": "chan2019neural", "citations": "83", "year": "2019", "title":"Neural Keyphrase Generation Via Reinforcement Learning With Adaptive Rewards", "abstract": "<p>Generating keyphrases that summarize the main points of a document is a\nfundamental task in natural language processing. Although existing generative\nmodels are capable of predicting multiple keyphrases for an input document as\nwell as determining the number of keyphrases to generate, they still suffer\nfrom the problem of generating too few keyphrases. To address this problem, we\npropose a reinforcement learning (RL) approach for keyphrase generation, with\nan adaptive reward function that encourages a model to generate both sufficient\nand accurate keyphrases. Furthermore, we introduce a new evaluation method that\nincorporates name variations of the ground-truth keyphrases using the Wikipedia\nknowledge base. Thus, our evaluation method can more robustly evaluate the\nquality of predicted keyphrases. Extensive experiments on five real-world\ndatasets of different scales demonstrate that our RL approach consistently and\nsignificantly improves the performance of the state-of-the-art generative\nmodels with both conventional and new evaluation methods.</p>\n", "tags": ["Datasets","Evaluation","Reinforcement Learning"] },
{"key": "chan2021speechstew", "citations": "70", "year": "2021", "title":"Speechstew: Simply Mix All Available Speech Recognition Data To Train One Large Neural Network", "abstract": "<p>We present SpeechStew, a speech recognition model that is trained on a\ncombination of various publicly available speech recognition datasets: AMI,\nBroadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and\nWall Street Journal. SpeechStew simply mixes all of these datasets together,\nwithout any special re-weighting or re-balancing of the datasets. SpeechStew\nachieves SoTA or near SoTA results across a variety of tasks, without the use\nof an external language model. Our results include 9.0% WER on AMI-IHM, 4.7%\nWER on Switchboard, 8.3% WER on CallHome, and 1.3% on WSJ, which\nsignificantly outperforms prior work with strong external language models. We\nalso demonstrate that SpeechStew learns powerful transfer learning\nrepresentations. We fine-tune SpeechStew on a noisy low resource speech\ndataset, CHiME-6. We achieve 38.9% WER without a language model, which\ncompares to 38.6% WER to a strong HMM baseline with a language model.</p>\n", "tags": ["Datasets","Fine-Tuning"] },
{"key": "chan2023ai", "citations": "247", "year": "2023", "title":"The AI Generation Gap: Are Gen Z Students More Interested In Adopting Generative AI Such As Chatgpt In Teaching And Learning Than Their Gen X And Millennial Generation Teachers?", "abstract": "<p>This study aimed to explore the experiences, perceptions, knowledge,\nconcerns, and intentions of Gen Z students with Gen X and Gen Y teachers\nregarding the use of generative AI (GenAI) in higher education. A sample of\nstudents and teachers were recruited to investigate the above using a survey\nconsisting of both open and closed questions. The findings showed that Gen Z\nparticipants were generally optimistic about the potential benefits of GenAI,\nincluding enhanced productivity, efficiency, and personalized learning, and\nexpressed intentions to use GenAI for various educational purposes. Gen X and\nGen Y teachers acknowledged the potential benefits of GenAI but expressed\nheightened concerns about overreliance, ethical and pedagogical implications,\nemphasizing the need for proper guidelines and policies to ensure responsible\nuse of the technology. The study highlighted the importance of combining\ntechnology with traditional teaching methods to provide a more effective\nlearning experience. Implications of the findings include the need to develop\nevidence-based guidelines and policies for GenAI integration, foster critical\nthinking and digital literacy skills among students, and promote responsible\nuse of GenAI technologies in higher education.</p>\n", "tags": ["Survey Paper"] },
{"key": "chang2019taming", "citations": "191", "year": "2020", "title":"Taming Pretrained Transformers For Extreme Multi-label Text Classification", "abstract": "<p>We consider the extreme multi-label text classification (XMC) problem: given\nan input text, return the most relevant labels from a large label collection.\nFor example, the input text could be a product description on Amazon.com and\nthe labels could be product categories. XMC is an important yet challenging\nproblem in the NLP community. Recently, deep pretrained transformer models have\nachieved state-of-the-art performance on many NLP tasks including sentence\nclassification, albeit with small label sets. However, naively applying deep\ntransformer models to the XMC problem leads to sub-optimal performance due to\nthe large output space and the label sparsity issue. In this paper, we propose\nX-Transformer, the first scalable approach to fine-tuning deep transformer\nmodels for the XMC problem. The proposed method achieves new state-of-the-art\nresults on four XMC benchmark datasets. In particular, on a Wiki dataset with\naround 0.5 million labels, the prec@1 of X-Transformer is 77.28%, a substantial\nimprovement over state-of-the-art XMC approaches Parabel (linear) and\nAttentionXML (neural), which achieve 68.70% and 76.95% precision@1,\nrespectively. We further apply X-Transformer to a product2query dataset from\nAmazon and gained 10.7% relative improvement on prec@1 over Parabel.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","KDD","Model Architecture","Training Techniques"] },
{"key": "chang2020convokit", "citations": "75", "year": "2020", "title":"Convokit: A Toolkit For The Analysis Of Conversations", "abstract": "<p>This paper describes the design and functionality of ConvoKit, an open-source\ntoolkit for analyzing conversations and the social interactions embedded\nwithin. ConvoKit provides an unified framework for representing and\nmanipulating conversational data, as well as a large and diverse collection of\nconversational datasets. By providing an intuitive interface for exploring and\ninteracting with conversational data, this toolkit lowers the technical\nbarriers for the broad adoption of computational methods for conversational\nanalysis.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "chang2020pre", "citations": "117", "year": "2020", "title":"Pre-training Tasks For Embedding-based Large-scale Retrieval", "abstract": "<p>We consider the large-scale query-document retrieval problem: given a query\n(e.g., a question), return the set of relevant documents (e.g., paragraphs\ncontaining the answer) from a large document corpus. This problem is often\nsolved in two steps. The retrieval phase first reduces the solution space,\nreturning a subset of candidate documents. The scoring phase then re-ranks the\ndocuments. Critically, the retrieval algorithm not only desires high recall but\nalso requires to be highly efficient, returning candidates in time sublinear to\nthe number of documents. Unlike the scoring phase witnessing significant\nadvances recently due to the BERT-style pre-training tasks on cross-attention\nmodels, the retrieval phase remains less well studied. Most previous works rely\non classic Information Retrieval (IR) methods such as BM-25 (token matching +\nTF-IDF weights). These models only accept sparse handcrafted features and can\nnot be optimized for different downstream tasks of interest. In this paper, we\nconduct a comprehensive study on the embedding-based retrieval models. We show\nthat the key ingredient of learning a strong embedding-based Transformer model\nis the set of pre-training tasks. With adequately designed paragraph-level\npre-training tasks, the Transformer models can remarkably improve over the\nwidely-used BM-25 as well as embedding models without Transformers. The\nparagraph-level pre-training tasks we studied are Inverse Cloze Task (ICT),\nBody First Selection (BFS), Wiki Link Prediction (WLP), and the combination of\nall three.</p>\n", "tags": ["Datasets","Model Architecture","Retrieval Systems","Training Techniques"] },
{"key": "chang2023muse", "citations": "96", "year": "2023", "title":"Muse: Text-to-image Generation Via Masked Generative Transformers", "abstract": "<p>We present Muse, a text-to-image Transformer model that achieves\nstate-of-the-art image generation performance while being significantly more\nefficient than diffusion or autoregressive models. Muse is trained on a masked\nmodeling task in discrete token space: given the text embedding extracted from\na pre-trained large language model (LLM), Muse is trained to predict randomly\nmasked image tokens. Compared to pixel-space diffusion models, such as Imagen\nand DALL-E 2, Muse is significantly more efficient due to the use of discrete\ntokens and requiring fewer sampling iterations; compared to autoregressive\nmodels, such as Parti, Muse is more efficient due to the use of parallel\ndecoding. The use of a pre-trained LLM enables fine-grained language\nunderstanding, translating to high-fidelity image generation and the\nunderstanding of visual concepts such as objects, their spatial relationships,\npose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M,\nwith an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88\non zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also\ndirectly enables a number of image editing applications without the need to\nfine-tune or invert the model: inpainting, outpainting, and mask-free editing.\nMore results are available at https://muse-model.github.io</p>\n", "tags": ["Applications","Evaluation","Has Code","Model Architecture"] },
{"key": "chang2023survey", "citations": "912", "year": "2024", "title":"A Survey On Evaluation Of Large Language Models", "abstract": "<p>Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the <code class=\"language-plaintext highlighter-rouge\">where' and </code>how’ questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.</p>\n", "tags": ["Applications","Evaluation","Survey Paper"] },
{"key": "changpinyo2021conceptual", "citations": "424", "year": "2021", "title":"Conceptual 12M: Pushing Web-scale Image-text Pre-training To Recognize Long-tail Visual Concepts", "abstract": "<p>The availability of large-scale image captioning and visual question\nanswering datasets has contributed significantly to recent successes in\nvision-and-language pre-training. However, these datasets are often collected\nwith overrestrictive requirements inherited from their original target tasks\n(e.g., image caption generation), which limit the resulting dataset scale and\ndiversity. We take a step further in pushing the limits of vision-and-language\npre-training data by relaxing the data collection pipeline used in Conceptual\nCaptions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M\n(CC12M), a dataset with 12 million image-text pairs specifically meant to be\nused for vision-and-language pre-training. We perform an analysis of this\ndataset and benchmark its effectiveness against CC3M on multiple downstream\ntasks with an emphasis on long-tail visual recognition. Our results clearly\nillustrate the benefit of scaling up pre-training data for vision-and-language\ntasks, as indicated by the new state-of-the-art results on both the nocaps and\nConceptual Captions benchmarks.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Training Techniques"] },
{"key": "chao2019bert", "citations": "114", "year": "2019", "title":"BERT-DST: Scalable End-to-end Dialogue State Tracking With Bidirectional Encoder Representations From Transformer", "abstract": "<p>An important yet rarely tackled problem in dialogue state tracking (DST) is\nscalability for dynamic ontology (e.g., movie, restaurant) and unseen slot\nvalues. We focus on a specific condition, where the ontology is unknown to the\nstate tracker, but the target slot value (except for none and dontcare),\npossibly unseen during training, can be found as word segment in the dialogue\ncontext. Prior approaches often rely on candidate generation from n-gram\nenumeration or slot tagger outputs, which can be inefficient or suffer from\nerror propagation. We propose BERT-DST, an end-to-end dialogue state tracker\nwhich directly extracts slot values from the dialogue context. We use BERT as\ndialogue context encoder whose contextualized language representations are\nsuitable for scalable DST to identify slot values from their semantic context.\nFurthermore, we employ encoder parameter sharing across all slots with two\nadvantages: (1) Number of parameters does not grow linearly with the ontology.\n(2) Language representation knowledge can be transferred among slots. Empirical\nevaluation shows BERT-DST with cross-slot parameter sharing outperforms prior\nwork on the benchmark scalable DST datasets Sim-M and Sim-R, and achieves\ncompetitive performance on the standard DSTC2 and WOZ 2.0 datasets.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "chaplot2017gated", "citations": "119", "year": "2017", "title":"Gated-attention Architectures For Task-oriented Language Grounding", "abstract": "<p>To perform tasks specified by natural language instructions, autonomous\nagents need to extract semantically meaningful representations of language and\nmap it to visual elements and actions in the environment. This problem is\ncalled task-oriented language grounding. We propose an end-to-end trainable\nneural architecture for task-oriented language grounding in 3D environments\nwhich assumes no prior linguistic or perceptual knowledge and requires only raw\npixels from the environment and the natural language instruction as input. The\nproposed model combines the image and text representations using a\nGated-Attention mechanism and learns a policy to execute the natural language\ninstruction using standard reinforcement and imitation learning methods. We\nshow the effectiveness of the proposed model on unseen instructions as well as\nunseen maps, both quantitatively and qualitatively. We also introduce a novel\nenvironment based on a 3D game engine to simulate the challenges of\ntask-oriented language grounding over a rich set of instructions and\nenvironment states.</p>\n", "tags": ["Model Architecture"] },
{"key": "chard2018dlhub", "citations": "71", "year": "2019", "title":"Dlhub: Model And Data Serving For Science", "abstract": "<p>While the Machine Learning (ML) landscape is evolving rapidly, there has been\na relative lag in the development of the “learning systems” needed to enable\nbroad adoption. Furthermore, few such systems are designed to support the\nspecialized requirements of scientific ML. Here we present the Data and\nLearning Hub for science (DLHub), a multi-tenant system that provides both\nmodel repository and serving capabilities with a focus on science applications.\nDLHub addresses two significant shortcomings in current systems. First, its\nselfservice model repository allows users to share, publish, verify, reproduce,\nand reuse models, and addresses concerns related to model reproducibility by\npackaging and distributing models and all constituent components. Second, it\nimplements scalable and low-latency serving capabilities that can leverage\nparallel and distributed computing resources to democratize access to published\nmodels through a simple web interface. Unlike other model serving frameworks,\nDLHub can store and serve any Python 3-compatible model or processing function,\nplus multiple-function pipelines. We show that relative to other model serving\nsystems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides\ngreater capabilities, comparable performance without memoization and batching,\nand significantly better performance when the latter two techniques can be\nemployed. We also describe early uses of DLHub for scientific applications.</p>\n", "tags": ["Applications","Has Code"] },
{"key": "chatterjee2018diverse", "citations": "67", "year": "2018", "title":"Diverse And Coherent Paragraph Generation From Images", "abstract": "<p>Paragraph generation from images, which has gained popularity recently, is an\nimportant task for video summarization, editing, and support of the disabled.\nTraditional image captioning methods fall short on this front, since they\naren’t designed to generate long informative descriptions. Moreover, the\nvanilla approach of simply concatenating multiple short sentences, possibly\nsynthesized from a classical image captioning system, doesn’t embrace the\nintricacies of paragraphs: coherent sentences, globally consistent structure,\nand diversity. To address those challenges, we propose to augment paragraph\ngeneration techniques with ‘coherence vectors’, ‘global topic vectors’, and\nmodeling of the inherent ambiguity of associating paragraphs with images, via a\nvariational auto-encoder formulation. We demonstrate the effectiveness of the\ndeveloped approach on two datasets, outperforming existing state-of-the-art\ntechniques on both.</p>\n", "tags": ["Datasets"] },
{"key": "chaudhry2019tiny", "citations": "331", "year": "2019", "title":"On Tiny Episodic Memories In Continual Learning", "abstract": "<p>In continual learning (CL), an agent learns from a stream of tasks leveraging\nprior experience to transfer knowledge to future tasks. It is an ideal\nframework to decrease the amount of supervision in the existing learning\nalgorithms. But for a successful knowledge transfer, the learner needs to\nremember how to perform previous tasks. One way to endow the learner the\nability to perform tasks seen in the past is to store a small memory, dubbed\nepisodic memory, that stores few examples from previous tasks and then to\nreplay these examples when training for future tasks. In this work, we\nempirically analyze the effectiveness of a very small episodic memory in a CL\nsetup where each training example is only seen once. Surprisingly, across four\nrather different supervised learning benchmarks adapted to CL, a very simple\nbaseline, that jointly trains on both examples from the current task as well as\nexamples stored in the episodic memory, significantly outperforms specifically\ndesigned CL approaches with and without episodic memory. Interestingly, we find\nthat repetitive training on even tiny memories of past tasks does not harm\ngeneralization, on the contrary, it improves it, with gains between 7% and\n17% when the memory is populated with a single example per class.</p>\n", "tags": ["Agentic","Tools","Training Techniques"] },
{"key": "che2020n", "citations": "98", "year": "2021", "title":"N-LTP: An Open-source Neural Language Technology Platform For Chinese", "abstract": "<p>We introduce \\texttt{N-LTP}, an open-source neural language technology\nplatform supporting six fundamental Chinese NLP tasks: {lexical analysis}\n(Chinese word segmentation, part-of-speech tagging, and named entity\nrecognition), {syntactic parsing} (dependency parsing), and {semantic parsing}\n(semantic dependency parsing and semantic role labeling). Unlike the existing\nstate-of-the-art toolkits, such as \\texttt{Stanza}, that adopt an independent\nmodel for each task, \\texttt{N-LTP} adopts the multi-task framework by using a\nshared pre-trained model, which has the advantage of capturing the shared\nknowledge across relevant Chinese tasks. In addition, a knowledge distillation\nmethod \\cite{DBLP:journals/corr/abs-1907-04829} where the single-task model\nteaches the multi-task model is further introduced to encourage the multi-task\nmodel to surpass its single-task teacher. Finally, we provide a collection of\neasy-to-use APIs and a visualization tool to make users to use and view the\nprocessing results more easily and directly. To the best of our knowledge, this\nis the first toolkit to support six Chinese NLP fundamental tasks. Source code,\ndocumentation, and pre-trained models are available at\nhttps://github.com/HIT-SCIR/ltp.</p>\n", "tags": ["EMNLP","Has Code","Tools"] },
{"key": "chefer2021generic", "citations": "176", "year": "2021", "title":"Generic Attention-model Explainability For Interpreting Bi-modal And Encoder-decoder Transformers", "abstract": "<p>Transformers are increasingly dominating multi-modal reasoning tasks, such as\nvisual question answering, achieving state-of-the-art results thanks to their\nability to contextualize information using the self-attention and co-attention\nmechanisms. These attention modules also play a role in other computer vision\ntasks including object detection and image segmentation. Unlike Transformers\nthat only use self-attention, Transformers with co-attention require to\nconsider multiple attention maps in parallel in order to highlight the\ninformation that is relevant to the prediction in the model’s input. In this\nwork, we propose the first method to explain prediction by any\nTransformer-based architecture, including bi-modal Transformers and\nTransformers with co-attentions. We provide generic solutions and apply these\nto the three most commonly used of these architectures: (i) pure\nself-attention, (ii) self-attention combined with co-attention, and (iii)\nencoder-decoder attention. We show that our method is superior to all existing\nmethods which are adapted from single modality explainability.</p>\n", "tags": ["ICCV","Model Architecture"] },
{"key": "chefer2023attend", "citations": "205", "year": "2023", "title":"Attend-and-excite: Attention-based Semantic Guidance For Text-to-image Diffusion Models", "abstract": "<p>Recent text-to-image generative models have demonstrated an unparalleled\nability to generate diverse and creative imagery guided by a target text\nprompt. While revolutionary, current state-of-the-art diffusion models may\nstill fail in generating images that fully convey the semantics in the given\ntext prompt. We analyze the publicly available Stable Diffusion model and\nassess the existence of catastrophic neglect, where the model fails to generate\none or more of the subjects from the input prompt. Moreover, we find that in\nsome cases the model also fails to correctly bind attributes (e.g., colors) to\ntheir corresponding subjects. To help mitigate these failure cases, we\nintroduce the concept of Generative Semantic Nursing (GSN), where we seek to\nintervene in the generative process on the fly during inference time to improve\nthe faithfulness of the generated images. Using an attention-based formulation\nof GSN, dubbed Attend-and-Excite, we guide the model to refine the\ncross-attention units to attend to all subject tokens in the text prompt and\nstrengthen - or excite - their activations, encouraging the model to generate\nall subjects described in the text prompt. We compare our approach to\nalternative approaches and demonstrate that it conveys the desired concepts\nmore faithfully across a range of text prompts.</p>\n", "tags": ["Model Architecture","Prompting"] },
{"key": "chen2016enhanced", "citations": "981", "year": "2017", "title":"Enhanced LSTM For Natural Language Inference", "abstract": "<p>Reasoning and inference are central to human and artificial intelligence.\nModeling inference in human language is very challenging. With the availability\nof large annotated data (Bowman et al., 2015), it has recently become feasible\nto train neural network based inference models, which have shown to be very\neffective. In this paper, we present a new state-of-the-art result, achieving\nthe accuracy of 88.6% on the Stanford Natural Language Inference Dataset.\nUnlike the previous top models that use very complicated network architectures,\nwe first demonstrate that carefully designing sequential inference models based\non chain LSTMs can outperform all previous models. Based on this, we further\nshow that by explicitly considering recursive architectures in both local\ninference modeling and inference composition, we achieve additional\nimprovement. Particularly, incorporating syntactic parsing information\ncontributes to our best result—it further improves the performance even when\nadded to the already very strong model.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "chen2016guided", "citations": "85", "year": "2016", "title":"Guided Alignment Training For Topic-aware Neural Machine Translation", "abstract": "<p>In this paper, we propose an effective way for biasing the attention\nmechanism of a sequence-to-sequence neural machine translation (NMT) model\ntowards the well-studied statistical word alignment models. We show that our\nnovel guided alignment training approach improves translation quality on\nreal-life e-commerce texts consisting of product titles and descriptions,\novercoming the problems posed by many unknown words and a large type/token\nratio. We also show that meta-data associated with input texts such as topic or\ncategory information can significantly improve translation quality when used as\nan additional signal to the decoder part of the network. With both novel\nfeatures, the BLEU score of the NMT system on a product title set improves from\n18.6 to 21.3%. Even larger MT quality gains are obtained through domain\nadaptation of a general domain NMT system to e-commerce data. The developed NMT\nsystem also performs well on the IWSLT speech translation task, where an\nensemble of four variant systems outperforms the phrase-based baseline by 2.1%\nBLEU absolute.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "chen2016sca", "citations": "1911", "year": "2017", "title":"SCA-CNN: Spatial And Channel-wise Attention In Convolutional Networks For Image Captioning", "abstract": "<p>Visual attention has been successfully applied in structural prediction tasks\nsuch as visual captioning and question answering. Existing visual attention\nmodels are generally spatial, i.e., the attention is modeled as spatial\nprobabilities that re-weight the last conv-layer feature map of a CNN encoding\nan input image. However, we argue that such spatial attention does not\nnecessarily conform to the attention mechanism — a dynamic feature extractor\nthat combines contextual fixations over time, as CNN features are naturally\nspatial, channel-wise and multi-layer. In this paper, we introduce a novel\nconvolutional neural network dubbed SCA-CNN that incorporates Spatial and\nChannel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN\ndynamically modulates the sentence generation context in multi-layer feature\nmaps, encoding where (i.e., attentive spatial locations at multiple layers) and\nwhat (i.e., attentive channels) the visual attention is. We evaluate the\nproposed SCA-CNN architecture on three benchmark image captioning datasets:\nFlickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN\nsignificantly outperforms state-of-the-art visual attention-based image\ncaptioning methods.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture"] },
{"key": "chen2016thorough", "citations": "463", "year": "2016", "title":"A Thorough Examination Of The Cnn/daily Mail Reading Comprehension Task", "abstract": "<p>Enabling a computer to understand a document so that it can answer\ncomprehension questions is a central, yet unsolved goal of NLP. A key factor\nimpeding its solution by machine learned systems is the limited availability of\nhuman-annotated data. Hermann et al. (2015) seek to solve this problem by\ncreating over a million training examples by pairing CNN and Daily Mail news\narticles with their summarized bullet points, and show that a neural network\ncan then be trained to give good performance on this task. In this paper, we\nconduct a thorough examination of this new reading comprehension task. Our\nprimary aim is to understand what depth of language understanding is required\nto do well on this task. We approach this from one side by doing a careful\nhand-analysis of a small subset of the problems and from the other by showing\nthat simple, carefully designed systems can obtain accuracies of 73.6% and\n76.6% on these two datasets, exceeding current state-of-the-art results by\n7-10% and approaching what we believe is the ceiling for performance on this\ntask.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "chen2017attacking", "citations": "115", "year": "2018", "title":"Attacking Visual Language Grounding With Adversarial Examples: A Case Study On Neural Image Captioning", "abstract": "<p>Visual language grounding is widely studied in modern neural image captioning\nsystems, which typically adopts an encoder-decoder framework consisting of two\nprincipal components: a convolutional neural network (CNN) for image feature\nextraction and a recurrent neural network (RNN) for language caption\ngeneration. To study the robustness of language grounding to adversarial\nperturbations in machine vision and perception, we propose Show-and-Fool, a\nnovel algorithm for crafting adversarial examples in neural image captioning.\nThe proposed algorithm provides two evaluation approaches, which check whether\nneural image captioning systems can be mislead to output some randomly chosen\ncaptions or keywords. Our extensive experiments show that our algorithm can\nsuccessfully craft visually-similar adversarial examples with randomly targeted\ncaptions or keywords, and the adversarial examples can be made highly\ntransferable to other image captioning systems. Consequently, our approach\nleads to new robustness implications of neural image captioning and novel\ninsights in visual language grounding.</p>\n", "tags": ["Evaluation","Security","Tools"] },
{"key": "chen2017improved", "citations": "130", "year": "2017", "title":"Improved Neural Machine Translation With A Syntax-aware Encoder And Decoder", "abstract": "<p>Most neural machine translation (NMT) models are based on the sequential\nencoder-decoder framework, which makes no use of syntactic information. In this\npaper, we improve this model by explicitly incorporating source-side syntactic\ntrees. More specifically, we propose (1) a bidirectional tree encoder which\nlearns both sequential and tree structured representations; (2) a tree-coverage\nmodel that lets the attention depend on the source-side syntax. Experiments on\nChinese-English translation demonstrate that our proposed models outperform the\nsequential attentional model as well as a stronger baseline with a bottom-up\ntree encoder and word coverage.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "chen2017neural", "citations": "282", "year": "2018", "title":"Neural Natural Language Inference Models Enhanced With External Knowledge", "abstract": "<p>Modeling natural language inference is a very challenging task. With the\navailability of large annotated data, it has recently become feasible to train\ncomplex models such as neural-network-based inference models, which have shown\nto achieve the state-of-the-art performance. Although there exist relatively\nlarge annotated data, can machines learn all knowledge needed to perform\nnatural language inference (NLI) from these data? If not, how can\nneural-network-based NLI models benefit from external knowledge and how to\nbuild NLI models to leverage it? In this paper, we enrich the state-of-the-art\nneural natural language inference models with external knowledge. We\ndemonstrate that the proposed models improve neural NLI models to achieve the\nstate-of-the-art performance on the SNLI and MultiNLI datasets.</p>\n", "tags": ["Datasets"] },
{"key": "chen2017recurrent", "citations": "94", "year": "2017", "title":"Recurrent Neural Network-based Sentence Encoder With Gated Attention For Natural Language Inference", "abstract": "<p>The RepEval 2017 Shared Task aims to evaluate natural language understanding\nmodels for sentence representation, in which a sentence is represented as a\nfixed-length vector with neural networks and the quality of the representation\nis tested with a natural language inference task. This paper describes our\nsystem (alpha) that is ranked among the top in the Shared Task, on both the\nin-domain test set (obtaining a 74.9% accuracy) and on the cross-domain test\nset (also attaining a 74.9% accuracy), demonstrating that the model generalizes\nwell to the cross-domain data. Our model is equipped with intra-sentence\ngated-attention composition which helps achieve a better performance. In\naddition to submitting our model to the Shared Task, we have also tested it on\nthe Stanford Natural Language Inference (SNLI) dataset. We obtain an accuracy\nof 85.5%, which is the best reported result on SNLI when cross-sentence\nattention is not allowed, the same condition enforced in RepEval 2017.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "chen2017survey", "citations": "394", "year": "2017", "title":"A Survey On Dialogue Systems: Recent Advances And New Frontiers", "abstract": "<p>Dialogue systems have attracted more and more attention. Recent advances on\ndialogue systems are overwhelmingly contributed by deep learning techniques,\nwhich have been employed to enhance a wide range of big data applications such\nas computer vision, natural language processing, and recommender systems. For\ndialogue systems, deep learning can leverage a massive amount of data to learn\nmeaningful feature representations and response generation strategies, while\nrequiring a minimum amount of hand-crafting. In this article, we give an\noverview to these recent advances on dialogue systems from various perspectives\nand discuss some possible research directions. In particular, we generally\ndivide existing dialogue systems into task-oriented and non-task-oriented\nmodels, then detail how deep learning techniques help them with representative\nalgorithms and finally discuss some appealing research directions that can\nbring the dialogue system research into a new frontier.</p>\n", "tags": ["Applications","Dialogue & Multi Turn","Model Architecture","Survey Paper"] },
{"key": "chen2017syntax", "citations": "74", "year": "2018", "title":"Syntax-directed Attention For Neural Machine Translation", "abstract": "<p>Attention mechanism, including global attention and local attention, plays a\nkey role in neural machine translation (NMT). Global attention attends to all\nsource words for word prediction. In comparison, local attention selectively\nlooks at fixed-window source words. However, alignment weights for the current\ntarget word often decrease to the left and right by linear distance centering\non the aligned source position and neglect syntax-directed distance\nconstraints. In this paper, we extend local attention with syntax-distance\nconstraint, to focus on syntactically related source words with the predicted\ntarget word, thus learning a more effective context vector for word prediction.\nMoreover, we further propose a double context NMT architecture, which consists\nof a global context vector and a syntax-directed context vector over the global\nattention, to provide more translation performance for NMT from source\nrepresentation. The experiments on the large-scale Chinese-to-English and\nEnglish-to-Germen translation tasks show that the proposed approach achieves a\nsubstantial and significant improvement over the baseline system.</p>\n", "tags": ["AAAI","Model Architecture"] },
{"key": "chen2017teacher", "citations": "110", "year": "2017", "title":"A Teacher-student Framework For Zero-resource Neural Machine Translation", "abstract": "<p>While end-to-end neural machine translation (NMT) has made remarkable\nprogress recently, it still suffers from the data scarcity problem for\nlow-resource language pairs and domains. In this paper, we propose a method for\nzero-resource NMT by assuming that parallel sentences have close probabilities\nof generating a sentence in a third language. Based on this assumption, our\nmethod is able to train a source-to-target NMT model (“student”) without\nparallel corpora available, guided by an existing pivot-to-target NMT model\n(“teacher”) on a source-pivot parallel corpus. Experimental results show that\nthe proposed method significantly improves over a baseline pivot-based model by\n+3.0 BLEU points across various language pairs.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "chen2018best", "citations": "397", "year": "2018", "title":"The Best Of Both Worlds: Combining Recent Advances In Neural Machine Translation", "abstract": "<p>The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)\nmodeling for Machine Translation (MT). The classic RNN-based approaches to MT\nwere first out-performed by the convolutional seq2seq model, which was then\nout-performed by the more recent Transformer model. Each of these new\napproaches consists of a fundamental architecture accompanied by a set of\nmodeling and training techniques that are in principle applicable to other\nseq2seq architectures. In this paper, we tease apart the new architectures and\ntheir accompanying techniques in two ways. First, we identify several key\nmodeling and training techniques, and apply them to the RNN architecture,\nyielding a new RNMT+ model that outperforms all of the three fundamental\narchitectures on the benchmark WMT’14 English to French and English to German\ntasks. Second, we analyze the properties of each fundamental seq2seq\narchitecture and devise new hybrid architectures intended to combine their\nstrengths. Our hybrid models obtain further improvements, outperforming the\nRNMT+ model on both benchmark datasets.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "chen2018emoji", "citations": "71", "year": "2019", "title":"Emoji-powered Representation Learning For Cross-lingual Sentiment Classification", "abstract": "<p>Sentiment classification typically relies on a large amount of labeled data.\nIn practice, the availability of labels is highly imbalanced among different\nlanguages, e.g., more English texts are labeled than texts in any other\nlanguages, which creates a considerable inequality in the quality of related\ninformation services received by users speaking different languages. To tackle\nthis problem, cross-lingual sentiment classification approaches aim to transfer\nknowledge learned from one language that has abundant labeled examples (i.e.,\nthe source language, usually English) to another language with fewer labels\n(i.e., the target language). The source and the target languages are usually\nbridged through off-the-shelf machine translation tools. Through such a\nchannel, cross-language sentiment patterns can be successfully learned from\nEnglish and transferred into the target languages. This approach, however,\noften fails to capture sentiment knowledge specific to the target language, and\nthus compromises the accuracy of the downstream classification task. In this\npaper, we employ emojis, which are widely available in many languages, as a new\nchannel to learn both the cross-language and the language-specific sentiment\npatterns. We propose a novel representation learning method that uses emoji\nprediction as an instrument to learn respective sentiment-aware representations\nfor each language. The learned representations are then integrated to\nfacilitate cross-lingual sentiment classification. The proposed method\ndemonstrates state-of-the-art performance on benchmark datasets, which is\nsustained even when sentiment labels are scarce.</p>\n", "tags": ["Datasets","Evaluation","Tools"] },
{"key": "chen2018enhancing", "citations": "60", "year": "2018", "title":"Enhancing Sentence Embedding With Generalized Pooling", "abstract": "<p>Pooling is an essential component of a wide variety of sentence\nrepresentation and embedding models. This paper explores generalized pooling\nmethods to enhance sentence embedding. We propose vector-based multi-head\nattention that includes the widely used max pooling, mean pooling, and scalar\nself-attention as special cases. The model benefits from properly designed\npenalization terms to reduce redundancy in multi-head attention. We evaluate\nthe proposed model on three different tasks: natural language inference (NLI),\nauthor profiling, and sentiment classification. The experiments show that the\nproposed model achieves significant improvement over strong\nsentence-encoding-based methods, resulting in state-of-the-art performances on\nfour datasets. The proposed approach can be easily implemented for more\nproblems than we discuss in this paper.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "chen2018factual", "citations": "85", "year": "2018", "title":"\"factual\" Or \"emotional\": Stylized Image Captioning With Adaptive Learning And Attention", "abstract": "<p>Generating stylized captions for an image is an emerging topic in image\ncaptioning. Given an image as input, it requires the system to generate a\ncaption that has a specific style (e.g., humorous, romantic, positive, and\nnegative) while describing the image content semantically accurately. In this\npaper, we propose a novel stylized image captioning model that effectively\ntakes both requirements into consideration. To this end, we first devise a new\nvariant of LSTM, named style-factual LSTM, as the building block of our model.\nIt uses two groups of matrices to capture the factual and stylized knowledge,\nrespectively, and automatically learns the word-level weights of the two groups\nbased on previous context. In addition, when we train the model to capture\nstylized elements, we propose an adaptive learning approach based on a\nreference factual model, it provides factual knowledge to the model as the\nmodel learns from stylized caption labels, and can adaptively compute how much\ninformation to supply at each time step. We evaluate our model on two stylized\nimage captioning datasets, which contain humorous/romantic captions and\npositive/negative captions, respectively. Experiments shows that our proposed\nmodel outperforms the state-of-the-art approaches, without using extra ground\ntruth supervision.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "chen2018fast", "citations": "623", "year": "2018", "title":"Fast Abstractive Summarization With Reinforce-selected Sentence Rewriting", "abstract": "<p>Inspired by how humans summarize long documents, we propose an accurate and\nfast summarization model that first selects salient sentences and then rewrites\nthem abstractively (i.e., compresses and paraphrases) to generate a concise\noverall summary. We use a novel sentence-level policy gradient method to bridge\nthe non-differentiable computation between these two neural networks in a\nhierarchical way, while maintaining language fluency. Empirically, we achieve\nthe new state-of-the-art on all metrics (including human evaluation) on the\nCNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.\nMoreover, by first operating at the sentence-level and then the word-level, we\nenable parallel decoding of our neural generative model that results in\nsubstantially faster (10-20x) inference speed as well as 4x faster training\nconvergence than previous long-paragraph encoder-decoder models. We also\ndemonstrate the generalization of our model on the test-only DUC-2002 dataset,\nwhere we achieve higher scores than a state-of-the-art model.</p>\n", "tags": ["Datasets","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "chen2018incorporating", "citations": "67", "year": "2019", "title":"Incorporating Structured Commonsense Knowledge In Story Completion", "abstract": "<p>The ability to select an appropriate story ending is the first step towards\nperfect narrative comprehension. Story ending prediction requires not only the\nexplicit clues within the context, but also the implicit knowledge (such as\ncommonsense) to construct a reasonable and consistent story. However, most\nprevious approaches do not explicitly use background commonsense knowledge. We\npresent a neural story ending selection model that integrates three types of\ninformation: narrative sequence, sentiment evolution and commonsense knowledge.\nExperiments show that our model outperforms state-of-the-art approaches on a\npublic dataset, ROCStory Cloze Task , and the performance gain from adding the\nadditional commonsense knowledge is significant.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "chen2018large", "citations": "123", "year": "2019", "title":"Large-scale Interactive Recommendation With Tree-structured Policy Gradient", "abstract": "<p>Reinforcement learning (RL) has recently been introduced to interactive\nrecommender systems (IRS) because of its nature of learning from dynamic\ninteractions and planning for long-run performance. As IRS is always with\nthousands of items to recommend (i.e., thousands of actions), most existing\nRL-based methods, however, fail to handle such a large discrete action space\nproblem and thus become inefficient. The existing work that tries to deal with\nthe large discrete action space problem by utilizing the deep deterministic\npolicy gradient framework suffers from the inconsistency between the continuous\naction representation (the output of the actor network) and the real discrete\naction. To avoid such inconsistency and achieve high efficiency and\nrecommendation effectiveness, in this paper, we propose a Tree-structured\nPolicy Gradient Recommendation (TPGR) framework, where a balanced hierarchical\nclustering tree is built over the items and picking an item is formulated as\nseeking a path from the root to a certain leaf of the tree. Extensive\nexperiments on carefully-designed environments based on two real-world datasets\ndemonstrate that our model provides superior recommendation performance and\nsignificant efficiency improvement over state-of-the-art methods.</p>\n", "tags": ["AAAI","Datasets","Efficiency","Reinforcement Learning","Tools"] },
{"key": "chen2018sequencer", "citations": "343", "year": "2019", "title":"Sequencer: Sequence-to-sequence Learning For End-to-end Program Repair", "abstract": "<p>This paper presents a novel end-to-end approach to program repair based on\nsequence-to-sequence learning. We devise, implement, and evaluate a system,\ncalled SequenceR, for fixing bugs based on sequence-to-sequence learning on\nsource code. This approach uses the copy mechanism to overcome the unlimited\nvocabulary problem that occurs with big code. Our system is data-driven; we\ntrain it on 35,578 samples, carefully curated from commits to open-source\nrepositories. We evaluate it on 4,711 independent real bug fixes, as well on\nthe Defects4J benchmark used in program repair research. SequenceR is able to\nperfectly predict the fixed line for 950/4711 testing samples, and find correct\npatches for 14 bugs in Defects4J. It captures a wide range of repair operators\nwithout any domain-specific top-down design.</p>\n", "tags": ["Datasets","Evaluation","Llm For Code"] },
{"key": "chen2018tree", "citations": "91", "year": "2018", "title":"Tree-to-tree Neural Networks For Program Translation", "abstract": "<p>Program translation is an important tool to migrate legacy code in one\nlanguage into an ecosystem built in a different language. In this work, we are\nthe first to employ deep neural networks toward tackling this problem. We\nobserve that program translation is a modular procedure, in which a sub-tree of\nthe source tree is translated into the corresponding target sub-tree at each\nstep. To capture this intuition, we design a tree-to-tree neural network to\ntranslate a source tree into a target one. Meanwhile, we develop an attention\nmechanism for the tree-to-tree model, so that when the decoder expands one\nnon-terminal in the target tree, the attention mechanism locates the\ncorresponding sub-tree in the source tree to guide the expansion of the\ndecoder. We evaluate the program translation capability of our tree-to-tree\nmodel against several state-of-the-art approaches. Compared against other\nneural translation models, we observe that our approach is consistently better\nthan the baselines with a margin of up to 15 points. Further, our approach can\nimprove the previous state-of-the-art program translation approaches by a\nmargin of 20 points on the translation of real-world projects.</p>\n", "tags": ["Model Architecture"] },
{"key": "chen2018twitter", "citations": "60", "year": "2018", "title":"Twitter Sentiment Analysis Via Bi-sense Emoji Embedding And Attention-based LSTM", "abstract": "<p>Sentiment analysis on large-scale social media data is important to bridge\nthe gaps between social media contents and real world activities including\npolitical election prediction, individual and public emotional status\nmonitoring and analysis, and so on. Although textual sentiment analysis has\nbeen well studied based on platforms such as Twitter and Instagram, analysis of\nthe role of extensive emoji uses in sentiment analysis remains light. In this\npaper, we propose a novel scheme for Twitter sentiment analysis with extra\nattention on emojis. We first learn bi-sense emoji embeddings under positive\nand negative sentimental tweets individually, and then train a sentiment\nclassifier by attending on these bi-sense emoji embeddings with an\nattention-based long short-term memory network (LSTM). Our experiments show\nthat the bi-sense embedding is effective for extracting sentiment-aware\nembeddings of emojis and outperforms the state-of-the-art models. We also\nvisualize the attentions to show that the bi-sense emoji embedding provides\nbetter guidance on the attention mechanism to obtain a more robust\nunderstanding of the semantics and sentiments.</p>\n", "tags": ["Model Architecture"] },
{"key": "chen2019behavior", "citations": "325", "year": "2019", "title":"Behavior Sequence Transformer For E-commerce Recommendation In Alibaba", "abstract": "<p>Deep learning based methods have been widely used in industrial\nrecommendation systems (RSs). Previous works adopt an Embedding&amp;MLP paradigm:\nraw features are embedded into low-dimensional vectors, which are then fed on\nto MLP for final recommendations. However, most of these works just concatenate\ndifferent features, ignoring the sequential nature of users’ behaviors. In this\npaper, we propose to use the powerful Transformer model to capture the\nsequential signals underlying users’ behavior sequences for recommendation in\nAlibaba. Experimental results demonstrate the superiority of the proposed\nmodel, which is then deployed online at Taobao and obtain significant\nimprovements in online Click-Through-Rate (CTR) comparing to two baselines.</p>\n", "tags": ["Applications","Model Architecture"] },
{"key": "chen2019bert", "citations": "418", "year": "2019", "title":"BERT For Joint Intent Classification And Slot Filling", "abstract": "<p>Intent classification and slot filling are two essential tasks for natural\nlanguage understanding. They often suffer from small-scale human-labeled\ntraining data, resulting in poor generalization capability, especially for rare\nwords. Recently a new language representation model, BERT (Bidirectional\nEncoder Representations from Transformers), facilitates pre-training deep\nbidirectional representations on large-scale unlabeled corpora, and has created\nstate-of-the-art models for a wide variety of natural language processing tasks\nafter simple fine-tuning. However, there has not been much effort on exploring\nBERT for natural language understanding. In this work, we propose a joint\nintent classification and slot filling model based on BERT. Experimental\nresults demonstrate that our proposed model achieves significant improvement on\nintent classification accuracy, slot filling F1, and sentence-level semantic\nframe accuracy on several public benchmark datasets, compared to the\nattention-based recurrent neural network models and slot-gated models.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "chen2019bidirectional", "citations": "120", "year": "2019", "title":"Bidirectional Attentive Memory Networks For Question Answering Over Knowledge Bases", "abstract": "<p>When answering natural language questions over knowledge bases (KBs),\ndifferent question components and KB aspects play different roles. However,\nmost existing embedding-based methods for knowledge base question answering\n(KBQA) ignore the subtle inter-relationships between the question and the KB\n(e.g., entity types, relation paths and context). In this work, we propose to\ndirectly model the two-way flow of interactions between the questions and the\nKB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring\nno external resources and only very few hand-crafted features, on the\nWebQuestions benchmark, our method significantly outperforms existing\ninformation-retrieval based methods, and remains competitive with\n(hand-crafted) semantic parsing based methods. Also, since we use attention\nmechanisms, our method offers better interpretability compared to other\nbaselines.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "chen2019controllable", "citations": "105", "year": "2019", "title":"Controllable Paraphrase Generation With A Syntactic Exemplar", "abstract": "<p>Prior work on controllable text generation usually assumes that the\ncontrolled attribute can take on one of a small set of values known a priori.\nIn this work, we propose a novel task, where the syntax of a generated sentence\nis controlled rather by a sentential exemplar. To evaluate quantitatively with\nstandard metrics, we create a novel dataset with human annotations. We also\ndevelop a variational model with a neural module specifically designed for\ncapturing syntactic knowledge and several multitask training objectives to\npromote disentangled representation learning. Empirically, the proposed model\nis observed to achieve improvements over baselines and learn to capture\ndesirable characteristics.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "chen2019distilling", "citations": "117", "year": "2020", "title":"Distilling Knowledge Learned In BERT For Text Generation", "abstract": "<p>Large-scale pre-trained language model such as BERT has achieved great\nsuccess in language understanding tasks. However, it remains an open question\nhow to utilize BERT for language generation. In this paper, we present a novel\napproach, Conditional Masked Language Modeling (C-MLM), to enable the\nfinetuning of BERT on target generation tasks. The finetuned BERT (teacher) is\nexploited as extra supervision to improve conventional Seq2Seq models (student)\nfor better text generation performance. By leveraging BERT’s idiosyncratic\nbidirectional nature, distilling knowledge learned in BERT can encourage\nauto-regressive Seq2Seq models to plan ahead, imposing global sequence-level\nsupervision for coherent text generation. Experiments show that the proposed\napproach significantly outperforms strong Transformer baselines on multiple\nlanguage generation tasks such as machine translation and text summarization.\nOur proposed model also achieves new state of the art on IWSLT German-English\nand English-Vietnamese MT datasets. Code is available at\nhttps://github.com/ChenRocks/Distill-BERT-Textgen.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "chen2019few", "citations": "114", "year": "2020", "title":"Few-shot NLG With Pre-trained Language Model", "abstract": "<p>Neural-based end-to-end approaches to natural language generation (NLG) from\nstructured data or knowledge are data-hungry, making their adoption for\nreal-world applications difficult with limited data. In this work, we propose\nthe new task of \\textit{few-shot natural language generation}. Motivated by how\nhumans tend to summarize tabular data, we propose a simple yet effective\napproach and show that it not only demonstrates strong performance but also\nprovides good generalization across domains. The design of the model\narchitecture is based on two aspects: content selection from input data and\nlanguage modeling to compose coherent sentences, which can be acquired from\nprior knowledge. With just 200 training examples, across multiple domains, we\nshow that our approach achieves very reasonable performances and outperforms\nthe strongest baseline by an average of over 8.0 BLEU points improvement. Our\ncode and data can be found at https://github.com/czyssrs/Few-Shot-NLG</p>\n", "tags": ["Applications","Few-Shot","Has Code","Model Architecture","Training Techniques"] },
{"key": "chen2019multi", "citations": "69", "year": "2019", "title":"Multi-hop Question Answering Via Reasoning Chains", "abstract": "<p>Multi-hop question answering requires models to gather information from\ndifferent parts of a text to answer a question. Most current approaches learn\nto address this task in an end-to-end way with neural networks, without\nmaintaining an explicit representation of the reasoning process. We propose a\nmethod to extract a discrete reasoning chain over the text, which consists of a\nseries of sentences leading to the answer. We then feed the extracted chains to\na BERT-based QA model to do final answer prediction. Critically, we do not rely\non gold annotated chains or “supporting facts:” at training time, we derive\npseudogold reasoning chains using heuristics based on named entity recognition\nand coreference resolution. Nor do we rely on these annotations at test time,\nas our model learns to extract chains from raw text alone. We test our approach\non two recently proposed large multi-hop question answering datasets: WikiHop\nand HotpotQA, and achieve state-of-art performance on WikiHop and strong\nperformance on HotpotQA. Our analysis shows the properties of chains that are\ncrucial for high performance: in particular, modeling extraction sequentially\nis important, as is dealing with each candidate sentence in a context-aware\nway. Furthermore, human evaluation shows that our extracted chains allow humans\nto give answers with high confidence, indicating that these are a strong\nintermediate abstraction for this task.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "chen2019reinforcement", "citations": "74", "year": "2019", "title":"Reinforcement Learning Based Graph-to-sequence Model For Natural Question Generation", "abstract": "<p>Natural question generation (QG) aims to generate questions from a passage\nand an answer. Previous works on QG either (i) ignore the rich structure\ninformation hidden in text, (ii) solely rely on cross-entropy loss that leads\nto issues like exposure bias and inconsistency between train/test measurement,\nor (iii) fail to fully exploit the answer information. To address these\nlimitations, in this paper, we propose a reinforcement learning (RL) based\ngraph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq\ngenerator with a novel Bidirectional Gated Graph Neural Network based encoder\nto embed the passage, and a hybrid evaluator with a mixed objective combining\nboth cross-entropy and RL losses to ensure the generation of syntactically and\nsemantically valid text. We also introduce an effective Deep Alignment Network\nfor incorporating the answer information into the passage at both the word and\ncontextual levels. Our model is end-to-end trainable and achieves new\nstate-of-the-art scores, outperforming existing methods by a significant margin\non the standard SQuAD benchmark.</p>\n", "tags": ["Datasets","Evaluation","Reinforcement Learning"] },
{"key": "chen2019semantically", "citations": "134", "year": "2019", "title":"Semantically Conditioned Dialog Response Generation Via Hierarchical Disentangled Self-attention", "abstract": "<p>Semantically controlled neural response generation on limited-domain has\nachieved great performance. However, moving towards multi-domain large-scale\nscenarios are shown to be difficult because the possible combinations of\nsemantic inputs grow exponentially with the number of domains. To alleviate\nsuch scalability issue, we exploit the structure of dialog acts to build a\nmulti-layer hierarchical graph, where each act is represented as a root-to-leaf\nroute on the graph. Then, we incorporate such graph structure prior as an\ninductive bias to build a hierarchical disentangled self-attention network,\nwhere we disentangle attention heads to model designated nodes on the dialog\nact graph. By activating different (disentangled) heads at each layer,\ncombinatorially many dialog act semantics can be modeled to control the neural\nresponse generation. On the large-scale Multi-Domain-WOZ dataset, our model can\nyield a significant improvement over the baselines on various automatic and\nhuman evaluation metrics.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "chen2019towards", "citations": "184", "year": "2019", "title":"Towards Knowledge-based Recommender Dialog System", "abstract": "<p>In this paper, we propose a novel end-to-end framework called KBRD, which\nstands for Knowledge-Based Recommender Dialog System. It integrates the\nrecommender system and the dialog generation system. The dialog system can\nenhance the performance of the recommendation system by introducing\nknowledge-grounded information about users’ preferences, and the recommender\nsystem can improve that of the dialog generation system by providing\nrecommendation-aware vocabulary bias. Experimental results demonstrate that our\nproposed model has significant advantages over the baselines in both the\nevaluation of dialog generation and recommendation. A series of analyses show\nthat the two systems can bring mutual benefits to each other, and the\nintroduced knowledge contributes to both their performances.</p>\n", "tags": ["EMNLP","Evaluation","Tools"] },
{"key": "chen2019understanding", "citations": "102", "year": "2019", "title":"Understanding Dataset Design Choices For Multi-hop Reasoning", "abstract": "<p>Learning multi-hop reasoning has been a key challenge for reading\ncomprehension models, leading to the design of datasets that explicitly focus\non it. Ideally, a model should not be able to perform well on a multi-hop\nquestion answering task without doing multi-hop reasoning. In this paper, we\ninvestigate two recently proposed datasets, WikiHop and HotpotQA. First, we\nexplore sentence-factored models for these tasks; by design, these models\ncannot do multi-hop reasoning, but they are still able to solve a large number\nof examples in both datasets. Furthermore, we find spurious correlations in the\nunmasked version of WikiHop, which make it easy to achieve high performance\nconsidering only the questions and answers. Finally, we investigate one key\ndifference between these datasets, namely span-based vs. multiple-choice\nformulations of the QA task. Multiple-choice versions of both datasets can be\neasily gamed, and two models we examine only marginally exceed a baseline in\nthis setting. Overall, while these datasets are useful testbeds,\nhigh-performing models may not be learning as much multi-hop reasoning as\npreviously thought.</p>\n", "tags": ["Datasets"] },
{"key": "chen2019uniter", "citations": "1537", "year": "2020", "title":"UNITER: Universal Image-text Representation Learning", "abstract": "<p>Joint image-text embedding is the bedrock for most Vision-and-Language (V+L)\ntasks, where multimodality inputs are simultaneously processed for joint visual\nand textual understanding. In this paper, we introduce UNITER, a UNiversal\nImage-TExt Representation, learned through large-scale pre-training over four\nimage-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU\nCaptions), which can power heterogeneous downstream V+L tasks with joint\nmultimodal embeddings. We design four pre-training tasks: Masked Language\nModeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text\nMatching (ITM), and Word-Region Alignment (WRA). Different from previous work\nthat applies joint random masking to both modalities, we use conditional\nmasking on pre-training tasks (i.e., masked language/region modeling is\nconditioned on full observation of image/text). In addition to ITM for global\nimage-text alignment, we also propose WRA via the use of Optimal Transport (OT)\nto explicitly encourage fine-grained alignment between words and image regions\nduring pre-training. Comprehensive analysis shows that both conditional masking\nand OT-based WRA contribute to better pre-training. We also conduct a thorough\nablation study to find an optimal combination of pre-training tasks. Extensive\nexperiments show that UNITER achieves new state of the art across six V+L tasks\n(over nine datasets), including Visual Question Answering, Image-Text\nRetrieval, Referring Expression Comprehension, Visual Commonsense Reasoning,\nVisual Entailment, and NLVR\\(^2\\). Code is available at\nhttps://github.com/ChenRocks/UNITER.</p>\n", "tags": ["Datasets","Has Code","Training Techniques"] },
{"key": "chen2020adabert", "citations": "69", "year": "2020", "title":"Adabert: Task-adaptive BERT Compression With Differentiable Neural Architecture Search", "abstract": "<p>Large pre-trained language models such as BERT have shown their effectiveness\nin various natural language processing tasks. However, the huge parameter size\nmakes them difficult to be deployed in real-time applications that require\nquick inference with limited resources. Existing methods compress BERT into\nsmall models while such compression is task-independent, i.e., the same\ncompressed BERT for all different downstream tasks. Motivated by the necessity\nand benefits of task-oriented BERT compression, we propose a novel compression\nmethod, AdaBERT, that leverages differentiable Neural Architecture Search to\nautomatically compress BERT into task-adaptive small models for specific tasks.\nWe incorporate a task-oriented knowledge distillation loss to provide search\nhints and an efficiency-aware loss as search constraints, which enables a good\ntrade-off between efficiency and effectiveness for task-adaptive BERT\ncompression. We evaluate AdaBERT on several NLP tasks, and the results\ndemonstrate that those task-adaptive compressed models are 12.7x to 29.3x\nfaster than BERT in inference time and 11.5x to 17.0x smaller in terms of\nparameter size, while comparable performance is maintained.</p>\n", "tags": ["Applications","Efficiency","IJCAI","Model Architecture"] },
{"key": "chen2020adversarial", "citations": "156", "year": "2020", "title":"Adversarial Robustness: From Self-supervised Pre-training To Fine-tuning", "abstract": "<p>Pretrained models from self-supervision are prevalently used in fine-tuning\ndownstream tasks faster or for better accuracy. However, gaining robustness\nfrom pretraining is left unexplored. We introduce adversarial training into\nself-supervision, to provide general-purpose robust pre-trained models for the\nfirst time. We find these robust pre-trained models can benefit the subsequent\nfine-tuning in two ways: i) boosting final model robustness; ii) saving the\ncomputation cost, if proceeding towards adversarial fine-tuning. We conduct\nextensive experiments to demonstrate that the proposed framework achieves large\nperformance margins (eg, 3.83% on robust accuracy and 1.3% on standard\naccuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end\nadversarial training baseline. Moreover, we find that different self-supervised\npre-trained models have a diverse adversarial vulnerability. It inspires us to\nensemble several pretraining tasks, which boosts robustness more. Our ensemble\nstrategy contributes to a further improvement of 3.59% on robust accuracy,\nwhile maintaining a slightly higher standard accuracy on CIFAR-10. Our codes\nare available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.</p>\n", "tags": ["CVPR","Fine-Tuning","Has Code","Security","Training Techniques"] },
{"key": "chen2020counterfactual", "citations": "313", "year": "2020", "title":"Counterfactual Samples Synthesizing For Robust Visual Question Answering", "abstract": "<p>Despite Visual Question Answering (VQA) has realized impressive progress over\nthe last few years, today’s VQA models tend to capture superficial linguistic\ncorrelations in the train set and fail to generalize to the test set with\ndifferent QA distributions. To reduce the language biases, several recent works\nintroduce an auxiliary question-only model to regularize the training of\ntargeted VQA model, and achieve dominating performance on VQA-CP. However,\nsince the complexity of design, current methods are unable to equip the\nensemble-based models with two indispensable characteristics of an ideal VQA\nmodel: 1) visual-explainable: the model should rely on the right visual regions\nwhen making decisions. 2) question-sensitive: the model should be sensitive to\nthe linguistic variations in question. To this end, we propose a model-agnostic\nCounterfactual Samples Synthesizing (CSS) training scheme. The CSS generates\nnumerous counterfactual training samples by masking critical objects in images\nor words in questions, and assigning different ground-truth answers. After\ntraining with the complementary samples (ie, the original and generated\nsamples), the VQA models are forced to focus on all critical objects and words,\nwhich significantly improves both visual-explainable and question-sensitive\nabilities. In return, the performance of these models is further boosted.\nExtensive ablations have shown the effectiveness of CSS. Particularly, by\nbuilding on top of the model LMH, we achieve a record-breaking performance of\n58.95% on VQA-CP v2, with 6.5% gains.</p>\n", "tags": ["CVPR","Evaluation","Training Techniques"] },
{"key": "chen2020generating", "citations": "76", "year": "2020", "title":"Generating Hierarchical Explanations On Text Classification Via Feature Interaction Detection", "abstract": "<p>Generating explanations for neural networks has become crucial for their\napplications in real-world with respect to reliability and trustworthiness. In\nnatural language processing, existing methods usually provide important\nfeatures which are words or phrases selected from an input text as an\nexplanation, but ignore the interactions between them. It poses challenges for\nhumans to interpret an explanation and connect it to model prediction. In this\nwork, we build hierarchical explanations by detecting feature interactions.\nSuch explanations visualize how words and phrases are combined at different\nlevels of the hierarchy, which can help users understand the decision-making of\nblack-box models. The proposed method is evaluated with three neural text\nclassifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic\nand human evaluations. Experiments show the effectiveness of the proposed\nmethod in providing explanations that are both faithful to models and\ninterpretable to humans.</p>\n", "tags": ["Applications","Datasets","Evaluation","Model Architecture"] },
{"key": "chen2020hybridqa", "citations": "183", "year": "2020", "title":"Hybridqa: A Dataset Of Multi-hop Question Answering Over Tabular And Textual Data", "abstract": "<p>Existing question answering datasets focus on dealing with homogeneous\ninformation, based either only on text or KB/Table information alone. However,\nas human knowledge is distributed over heterogeneous forms, using homogeneous\ninformation alone might lead to severe coverage problems. To fill in the gap,\nwe present HybridQA https://github.com/wenhuchen/HybridQA, a new large-scale\nquestion-answering dataset that requires reasoning on heterogeneous\ninformation. Each question is aligned with a Wikipedia table and multiple\nfree-form corpora linked with the entities in the table. The questions are\ndesigned to aggregate both tabular information and text information, i.e., lack\nof either form would render the question unanswerable. We test with three\ndifferent models: 1) a table-only model. 2) text-only model. 3) a hybrid model\nthat combines heterogeneous information to find the answer. The experimental\nresults show that the EM scores obtained by two baselines are below 20%, while\nthe hybrid model can achieve an EM over 40%. This gap suggests the necessity\nto aggregate heterogeneous information in HybridQA. However, the hybrid model’s\nscore is still far behind human performance. Hence, HybridQA can serve as a\nchallenging benchmark to study question answering with heterogeneous\ninformation.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code"] },
{"key": "chen2020kgpt", "citations": "102", "year": "2020", "title":"KGPT: Knowledge-grounded Pre-training For Data-to-text Generation", "abstract": "<p>Data-to-text generation has recently attracted substantial interests due to\nits wide applications. Existing methods have shown impressive performance on an\narray of tasks. However, they rely on a significant amount of labeled data for\neach task, which is costly to acquire and thus limits their application to new\ntasks and domains. In this paper, we propose to leverage pre-training and\ntransfer learning to address this issue. We propose a knowledge-grounded\npre-training (KGPT), which consists of two parts, 1) a general\nknowledge-grounded generation model to generate knowledge-enriched text. 2) a\npre-training paradigm on a massive knowledge-grounded text corpus crawled from\nthe web. The pre-trained model can be fine-tuned on various data-to-text\ngeneration tasks to generate task-specific text. We adopt three settings,\nnamely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.\nUnder the fully-supervised setting, our model can achieve remarkable gains over\nthe known baselines. Under zero-shot setting, our model without seeing any\nexamples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.\nUnder the few-shot setting, our model only needs about one-fifteenth as many\nlabeled examples to achieve the same level of performance as baseline models.\nThese experiments consistently prove the strong generalization ability of our\nproposed framework https://github.com/wenhuchen/KGPT.</p>\n", "tags": ["Applications","Datasets","EMNLP","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "chen2020learning", "citations": "81", "year": "2020", "title":"Learning Variational Word Masks To Improve The Interpretability Of Neural Text Classifiers", "abstract": "<p>To build an interpretable neural text classifier, most of the prior work has\nfocused on designing inherently interpretable models or finding faithful\nexplanations. A new line of work on improving model interpretability has just\nstarted, and many existing methods require either prior information or human\nannotations as additional inputs in training. To address this limitation, we\npropose the variational word mask (VMASK) method to automatically learn\ntask-specific important words and reduce irrelevant information on\nclassification, which ultimately improves the interpretability of model\npredictions. The proposed method is evaluated with three neural text\nclassifiers (CNN, LSTM, and BERT) on seven benchmark text classification\ndatasets. Experiments show the effectiveness of VMASK in improving both model\nprediction accuracy and interpretability.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "chen2020logical", "citations": "116", "year": "2020", "title":"Logical Natural Language Generation From Open-domain Tables", "abstract": "<p>Neural natural language generation (NLG) models have recently shown\nremarkable progress in fluency and coherence. However, existing studies on\nneural NLG are primarily focused on surface-level realizations with limited\nemphasis on logical inference, an important aspect of human thinking and\nlanguage. In this paper, we suggest a new NLG task where a model is tasked with\ngenerating natural language statements that can be <em>logically entailed</em> by\nthe facts in an open-domain semi-structured table. To facilitate the study of\nthe proposed logical NLG problem, we use the existing TabFact dataset\n\\cite{chen2019tabfact} featured with a wide range of logical/symbolic\ninferences as our testbed, and propose new automatic metrics to evaluate the\nfidelity of generation models w.r.t.\\ logical inference. The new task poses\nchallenges to the existing monotonic generation frameworks due to the mismatch\nbetween sequence order and logical order. In our experiments, we\ncomprehensively survey different generation architectures (LSTM, Transformer,\nPre-Trained LM) trained with different algorithms (RL, Adversarial Training,\nCoarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained\nLM can significantly boost both the fluency and logical fidelity metrics, 2) RL\nand Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine\ngeneration can help partially alleviate the fidelity issue while maintaining\nhigh language fluency. The code and data are available at\nhttps://github.com/wenhuchen/LogicNLG.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "chen2020low", "citations": "65", "year": "2020", "title":"Low-resource Domain Adaptation For Compositional Task-oriented Semantic Parsing", "abstract": "<p>Task-oriented semantic parsing is a critical component of virtual assistants,\nwhich is responsible for understanding the user’s intents (set reminder, play\nmusic, etc.). Recent advances in deep learning have enabled several approaches\nto successfully parse more complex queries (Gupta et al., 2018; Rongali et\nal.,2020), but these models require a large amount of annotated training data\nto parse queries on new domains (e.g. reminder, music).\n  In this paper, we focus on adapting task-oriented semantic parsers to\nlow-resource domains, and propose a novel method that outperforms a supervised\nneural model at a 10-fold data reduction. In particular, we identify two\nfundamental factors for low-resource domain adaptation: better representation\nlearning and better training techniques. Our representation learning uses BART\n(Lewis et al., 2019) to initialize our model which outperforms encoder-only\npre-trained representations used in previous work. Furthermore, we train with\noptimization-based meta-learning (Finn et al., 2017) to improve generalization\nto low-resource domains. This approach significantly outperforms all baseline\nmethods in the experiments on a newly collected multi-domain task-oriented\nsemantic parsing dataset (TOPv2), which we release to the public.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "chen2020recall", "citations": "127", "year": "2020", "title":"Recall And Learn: Fine-tuning Deep Pretrained Language Models With Less Forgetting", "abstract": "<p>Deep pretrained language models have achieved great success in the way of\npretraining first and then fine-tuning. But such a sequential transfer learning\nparadigm often confronts the catastrophic forgetting problem and leads to\nsub-optimal performance. To fine-tune with less forgetting, we propose a recall\nand learn mechanism, which adopts the idea of multi-task learning and jointly\nlearns pretraining tasks and downstream tasks. Specifically, we propose a\nPretraining Simulation mechanism to recall the knowledge from pretraining tasks\nwithout data, and an Objective Shifting mechanism to focus the learning on\ndownstream tasks gradually. Experiments show that our method achieves\nstate-of-the-art performance on the GLUE benchmark. Our method also enables\nBERT-base to achieve better performance than directly fine-tuning of\nBERT-large. Further, we provide the open-source RecAdam optimizer, which\nintegrates the proposed mechanisms into Adam optimizer, to facility the NLP\ncommunity.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "chen2021decision", "citations": "342", "year": "2021", "title":"Decision Transformer: Reinforcement Learning Via Sequence Modeling", "abstract": "<p>We introduce a framework that abstracts Reinforcement Learning (RL) as a\nsequence modeling problem. This allows us to draw upon the simplicity and\nscalability of the Transformer architecture, and associated advances in\nlanguage modeling such as GPT-x and BERT. In particular, we present Decision\nTransformer, an architecture that casts the problem of RL as conditional\nsequence modeling. Unlike prior approaches to RL that fit value functions or\ncompute policy gradients, Decision Transformer simply outputs the optimal\nactions by leveraging a causally masked Transformer. By conditioning an\nautoregressive model on the desired return (reward), past states, and actions,\nour Decision Transformer model can generate future actions that achieve the\ndesired return. Despite its simplicity, Decision Transformer matches or exceeds\nthe performance of state-of-the-art model-free offline RL baselines on Atari,\nOpenAI Gym, and Key-to-Door tasks.</p>\n", "tags": ["Agentic","Model Architecture","Reinforcement Learning","Time Series","Tools"] },
{"key": "chen2021empirical", "citations": "101", "year": "2023", "title":"An Empirical Survey Of Data Augmentation For Limited Data Learning In NLP", "abstract": "<p>NLP has achieved great progress in the past decade through the use of neural\nmodels and large labeled datasets. The dependence on abundant data prevents NLP\nmodels from being applied to low-resource settings or novel tasks where\nsignificant time, money, or expertise is required to label massive amounts of\ntextual data. Recently, data augmentation methods have been explored as a means\nof improving data efficiency in NLP. To date, there has been no systematic\nempirical overview of data augmentation for NLP in the limited labeled data\nsetting, making it difficult to understand which methods work in which\nsettings. In this paper, we provide an empirical survey of recent progress on\ndata augmentation for NLP in the limited labeled data setting, summarizing the\nlandscape of methods (including token-level augmentations, sentence-level\naugmentations, adversarial augmentations, and hidden-space augmentations) and\ncarrying out experiments on 11 datasets covering topics/news classification,\ninference tasks, paraphrasing tasks, and single-sentence tasks. Based on the\nresults, we draw several conclusions to help practitioners choose appropriate\naugmentations in different settings and discuss the current challenges and\nfuture directions for limited data learning in NLP.</p>\n", "tags": ["Datasets","Survey Paper","TACL"] },
{"key": "chen2021evaluating", "citations": "1240", "year": "2021", "title":"Evaluating Large Language Models Trained On Code", "abstract": "<p>We introduce Codex, a GPT language model fine-tuned on publicly available\ncode from GitHub, and study its Python code-writing capabilities. A distinct\nproduction version of Codex powers GitHub Copilot. On HumanEval, a new\nevaluation set we release to measure functional correctness for synthesizing\nprograms from docstrings, our model solves 28.8% of the problems, while GPT-3\nsolves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling\nfrom the model is a surprisingly effective strategy for producing working\nsolutions to difficult prompts. Using this method, we solve 70.2% of our\nproblems with 100 samples per problem. Careful investigation of our model\nreveals its limitations, including difficulty with docstrings describing long\nchains of operations and with binding operations to variables. Finally, we\ndiscuss the potential broader impacts of deploying powerful code generation\ntechnologies, covering safety, security, and economics.</p>\n", "tags": ["Evaluation","Has Code","Llm For Code","Model Architecture","Security"] },
{"key": "chen2021industry", "citations": "65", "year": "2021", "title":"Industry Scale Semi-supervised Learning For Natural Language Understanding", "abstract": "<p>This paper presents a production Semi-Supervised Learning (SSL) pipeline\nbased on the student-teacher framework, which leverages millions of unlabeled\nexamples to improve Natural Language Understanding (NLU) tasks. We investigate\ntwo questions related to the use of unlabeled data in production SSL context:\n1) how to select samples from a huge unlabeled data pool that are beneficial\nfor SSL training, and 2) how do the selected data affect the performance of\ndifferent state-of-the-art SSL techniques. We compare four widely used SSL\ntechniques, Pseudo-Label (PL), Knowledge Distillation (KD), Virtual Adversarial\nTraining (VAT) and Cross-View Training (CVT) in conjunction with two data\nselection methods including committee-based selection and submodular\noptimization based selection. We further examine the benefits and drawbacks of\nthese techniques when applied to intent classification (IC) and named entity\nrecognition (NER) tasks, and provide guidelines specifying when each of these\nmethods might be beneficial to improve large scale NLU systems.</p>\n", "tags": ["Efficiency","NAACL","Tools","Training Techniques"] },
{"key": "chen2021knowprompt", "citations": "292", "year": "2022", "title":"Knowprompt: Knowledge-aware Prompt-tuning With Synergistic Optimization For Relation Extraction", "abstract": "<p>Recently, prompt-tuning has achieved promising results for specific few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces\n(i.e., templates) into the input and transform a classification task into a\nmasked language modeling problem. However, for relation extraction, determining\nan appropriate prompt template requires domain expertise, and it is cumbersome\nand time-consuming to obtain a suitable label word. Furthermore, there exists\nabundant semantic and prior knowledge among the relation labels that cannot be\nignored. To this end, we focus on incorporating knowledge among relation labels\ninto prompt-tuning for relation extraction and propose a Knowledge-aware\nPrompt-tuning approach with synergistic optimization (KnowPrompt).\nSpecifically, we inject latent knowledge contained in relation labels into\nprompt construction with learnable virtual type words and answer words. Then,\nwe synergistically optimize their representation with structured constraints.\nExtensive experimental results on five datasets with standard and low-resource\nsettings demonstrate the effectiveness of our approach. Our code and datasets\nare available in https://github.com/zjunlp/KnowPrompt for reproducibility.</p>\n", "tags": ["Datasets","Efficiency","Few-Shot","Has Code","Prompting"] },
{"key": "chen2021temporal", "citations": "82", "year": "2021", "title":"Temporal Meta-path Guided Explainable Recommendation", "abstract": "<p>This paper utilizes well-designed item-item path modelling between\nconsecutive items with attention mechanisms to sequentially model dynamic\nuser-item evolutions on dynamic knowledge graph for explainable\nrecommendations. Compared with existing works that use heavy recurrent neural\nnetworks to model temporal information, we propose simple but effective neural\nnetworks to capture user historical item features and path-based context to\ncharacterise next purchased item. Extensive evaluations of TMER on three\nreal-world benchmark datasets show state-of-the-art performance compared\nagainst recent strong baselines.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "chen2021visualgpt", "citations": "110", "year": "2022", "title":"Visualgpt: Data-efficient Adaptation Of Pretrained Language Models For Image Captioning", "abstract": "<p>The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.</p>\n", "tags": ["Applications","CVPR","Datasets","Efficiency","Has Code","Model Architecture","Training Techniques"] },
{"key": "chen2022codet", "citations": "61", "year": "2022", "title":"Codet: Code Generation With Generated Tests", "abstract": "<p>The task of generating code solutions for a given programming problem can\nbenefit from the use of pre-trained language models such as Codex, which can\nproduce multiple diverse samples. However, a major challenge for this task is\nto select the most appropriate solution from the multiple samples generated by\nthe pre-trained language models. A natural way to evaluate the quality and\ncorrectness of a code solution is to run it against a set of test cases, but\nthe manual creation of such test cases is often costly and time-consuming. In\nthis paper, we propose a novel method, CodeT, that leverages the same\npre-trained language models to automatically generate test cases for the code\nsamples, thus reducing the human effort and increasing the coverage of the test\nscenarios. CodeT then executes the code samples using the generated test cases,\nand performs a dual execution agreement, which considers both the consistency\nof the outputs against the generated test cases and the agreement of the\noutputs with other code samples. We conduct comprehensive experiments on four\nbenchmarks, HumanEval, MBPP, APPS and CodeContests, using five different\npre-trained language models with varying sizes and capabilities. Our results\nshow that CodeT can significantly improve the performance of code solution\nselection over previous methods, achieving remarkable and consistent gains\nacross different models and benchmarks. For instance, CodeT improves the pass@1\nmetric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%\nover the code-davinci-002 model, and an absolute improvement of more than 20%\nover the previous state-of-the-art results.</p>\n", "tags": ["Llm For Code"] },
{"key": "chen2022hybrid", "citations": "113", "year": "2022", "title":"Hybrid Transformer With Multi-level Fusion For Multimodal Knowledge Graph Completion", "abstract": "<p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual\nknowledge, have recently been successfully applied to tasks such as information\nretrieval, question answering, and recommendation system. Since most MKGs are\nfar from complete, extensive knowledge graph completion studies have been\nproposed focusing on the multimodal entity, relation extraction and link\nprediction. However, different tasks and modalities require changes to the\nmodel architecture, and not all images/objects are relevant to text input,\nwhich hinders the applicability to diverse real-world scenarios. In this paper,\nwe propose a hybrid transformer with multi-level fusion to address those\nissues. Specifically, we leverage a hybrid transformer architecture with\nunified input-output for diverse multimodal knowledge graph completion tasks.\nMoreover, we propose multi-level fusion, which integrates visual and text\nrepresentation via coarse-grained prefix-guided interaction and fine-grained\ncorrelation-aware fusion modules. We conduct extensive experiments to validate\nthat our MKGformer can obtain SOTA performance on four datasets of multimodal\nlink prediction, multimodal RE, and multimodal NER. Code is available in\nhttps://github.com/zjunlp/MKGformer.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","SIGIR"] },
{"key": "chen2022intent", "citations": "261", "year": "2022", "title":"Intent Contrastive Learning For Sequential Recommendation", "abstract": "<p>Users’ interactions with items are driven by various intents (e.g., preparing\nfor holiday gifts, shopping for fishing equipment, etc.).However, users’\nunderlying intents are often unobserved/latent, making it challenging to\nleverage such latent intents forSequentialrecommendation(SR). To investigate\nthe benefits of latent intents and leverage them effectively for\nrecommendation, we proposeIntentContrastiveLearning(ICL), a general learning\nparadigm that leverages a latent intent variable into SR. The core idea is to\nlearn users’ intent distribution functions from unlabeled user behavior\nsequences and optimize SR models with contrastive self-supervised learning\n(SSL) by considering the learned intents to improve recommendation.\nSpecifically, we introduce a latent variable to represent users’ intents and\nlearn the distribution function of the latent variable via clustering. We\npropose to leverage the learned intents into SR models via contrastive SSL,\nwhich maximizes the agreement between a view of sequence and its corresponding\nintent. The training is alternated between intent representation learning and\nthe SR model optimization steps within the generalized expectation-maximization\n(EM) framework. Fusing user intent information into SR also improves model\nrobustness. Experiments conducted on four real-world datasets demonstrate the\nsuperiority of the proposed learning paradigm, which improves performance, and\nrobustness against data sparsity and noisy interaction issues.</p>\n", "tags": ["Datasets","Efficiency","Tools","Training Techniques"] },
{"key": "chen2022open", "citations": "62", "year": "2023", "title":"Open-vocabulary Queryable Scene Representations For Real World Planning", "abstract": "<p>Large language models (LLMs) have unlocked new capabilities of task planning\nfrom human instructions. However, prior attempts to apply LLMs to real-world\nrobotic tasks are limited by the lack of grounding in the surrounding scene. In\nthis paper, we develop NLMap, an open-vocabulary and queryable scene\nrepresentation to address this problem. NLMap serves as a framework to gather\nand integrate contextual information into LLM planners, allowing them to see\nand query available objects in the scene before generating a\ncontext-conditioned plan. NLMap first establishes a natural language queryable\nscene representation with Visual Language models (VLMs). An LLM based object\nproposal module parses instructions and proposes involved objects to query the\nscene representation for object availability and location. An LLM planner then\nplans with such information about the scene. NLMap allows robots to operate\nwithout a fixed list of objects nor executable options, enabling real robot\noperation unachievable by previous methods. Project website:\nhttps://nlmap-saycan.github.io</p>\n", "tags": ["Has Code","ICRA","Tools"] },
{"key": "chen2022pali", "citations": "156", "year": "2022", "title":"Pali: A Jointly-scaled Multilingual Language-image Model", "abstract": "<p>Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. We present PaLI (Pathways Language and Image model), a\nmodel that extends this approach to the joint modeling of language and vision.\nPaLI generates text based on visual and textual inputs, and with this interface\nperforms many vision, language, and multimodal tasks, in many languages. To\ntrain PaLI, we make use of large pre-trained encoder-decoder language models\nand Vision Transformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the\nbenefits from even larger-capacity vision models. To train PaLI, we create a\nlarge multilingual mix of pretraining tasks, based on a new image-text training\nset containing 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.</p>\n", "tags": ["Training Techniques"] },
{"key": "chen2022program", "citations": "94", "year": "2022", "title":"Program Of Thoughts Prompting: Disentangling Computation From Reasoning For Numerical Reasoning Tasks", "abstract": "<p>Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step <code class=\"language-plaintext highlighter-rouge\">thought' process. To disentangle computation from reasoning, we\npropose </code>Program of Thoughts’ (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in Github https://github.com/wenhuchen/Program-of-Thoughts</p>\n", "tags": ["Datasets","Few-Shot","Has Code","Prompting"] },
{"key": "chen2022vision", "citations": "179", "year": "2022", "title":"Vision Transformer Adapter For Dense Predictions", "abstract": "<p>This work investigates a simple yet powerful dense prediction task adapter\nfor Vision Transformer (ViT). Unlike recently advanced variants that\nincorporate vision-specific inductive biases into their architectures, the\nplain ViT suffers inferior performance on dense predictions due to weak prior\nassumptions. To address this issue, we propose the ViT-Adapter, which allows\nplain ViT to achieve comparable performance to vision-specific transformers.\nSpecifically, the backbone in our framework is a plain ViT that can learn\npowerful representations from large-scale multi-modal data. When transferring\nto downstream tasks, a pre-training-free adapter is used to introduce the\nimage-related inductive biases into the model, making it suitable for these\ntasks. We verify ViT-Adapter on multiple dense prediction tasks, including\nobject detection, instance segmentation, and semantic segmentation. Notably,\nwithout using extra detection data, our ViT-Adapter-L yields state-of-the-art\n60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter\ncould serve as an alternative for vision-specific transformers and facilitate\nfuture research. The code and models will be released at\nhttps://github.com/czczup/ViT-Adapter.</p>\n", "tags": ["Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "chen2022vlp", "citations": "138", "year": "2023", "title":"VLP: A Survey On Vision-language Pre-training", "abstract": "<p>In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey focused on VLP. We hope that this survey can shed\nlight on future research in the VLP field.</p>\n", "tags": ["Datasets","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "chen2023meditron", "citations": "60", "year": "2023", "title":"MEDITRON-70B: Scaling Medical Pretraining For Large Language Models", "abstract": "<p>Large language models (LLMs) can potentially democratize access to medical\nknowledge. While many efforts have been made to harness and improve LLMs’\nmedical knowledge and reasoning capacities, the resulting models are either\nclosed-source (e.g., PaLM, GPT-4) or limited in scale (&lt;= 13B parameters),\nwhich restricts their abilities. In this work, we improve access to large-scale\nmedical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B\nparameters adapted to the medical domain. MEDITRON builds on Llama-2 (through\nour adaptation of Nvidia’s Megatron-LM distributed trainer), and extends\npretraining on a comprehensively curated medical corpus, including selected\nPubMed articles, abstracts, and internationally-recognized medical guidelines.\nEvaluations using four major medical benchmarks show significant performance\ngains over several state-of-the-art baselines before and after task-specific\nfinetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the\nbest public baseline in its parameter class and 3% over the strongest baseline\nwe finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B\noutperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of\nMed-PaLM-2. We release our code for curating the medical pretraining corpus and\nthe MEDITRON model weights to drive open-source development of more capable\nmedical LLMs.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "chen2023teaching", "citations": "61", "year": "2023", "title":"Teaching Large Language Models To Self-debug", "abstract": "<p>Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Llm For Code"] },
{"key": "chen2024xtrimopglm", "citations": "65", "year": "2023", "title":"Xtrimopglm: Unified 100b-scale Pre-trained Transformer For Deciphering The Language Of Protein", "abstract": "<p>Protein language models have shown remarkable success in learning biological\ninformation from protein sequences. However, most existing models are limited\nby either autoencoding or autoregressive pre-training objectives, which makes\nthem struggle to handle protein understanding and generation tasks\nconcurrently. We propose a unified protein language model, xTrimoPGLM, to\naddress these two types of tasks simultaneously through an innovative\npre-training framework. Our key technical contribution is an exploration of the\ncompatibility and the potential for joint optimization of the two types of\nobjectives, which has led to a strategy for training xTrimoPGLM at an\nunprecedented scale of 100 billion parameters and 1 trillion training tokens.\nOur extensive experiments reveal that 1) xTrimoPGLM significantly outperforms\nother advanced baselines in 18 protein understanding benchmarks across four\ncategories. The model also facilitates an atomic-resolution view of protein\nstructures, leading to an advanced 3D structural prediction model that\nsurpasses existing language model-based tools. 2) xTrimoPGLM not only can\ngenerate de novo protein sequences following the principles of natural ones,\nbut also can perform programmable generation after supervised fine-tuning (SFT)\non curated sequences. These results highlight the substantial capability and\nversatility of xTrimoPGLM in understanding and generating protein sequences,\ncontributing to the evolving landscape of foundation models in protein science.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "cheng2016long", "citations": "1036", "year": "2016", "title":"Long Short-term Memory-networks For Machine Reading", "abstract": "<p>In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "cheng2016semi", "citations": "91", "year": "2016", "title":"Semi-supervised Learning For Neural Machine Translation", "abstract": "<p>While end-to-end neural machine translation (NMT) has made remarkable\nprogress recently, NMT systems only rely on parallel corpora for parameter\nestimation. Since parallel corpora are usually limited in quantity, quality,\nand coverage, especially for low-resource languages, it is appealing to exploit\nmonolingual corpora to improve NMT. We propose a semi-supervised approach for\ntraining NMT models on the concatenation of labeled (parallel corpora) and\nunlabeled (monolingual corpora) data. The central idea is to reconstruct the\nmonolingual corpora using an autoencoder, in which the source-to-target and\ntarget-to-source translation models serve as the encoder and decoder,\nrespectively. Our approach can not only exploit the monolingual corpora of the\ntarget language, but also of the source language. Experiments on the\nChinese-English dataset show that our approach achieves significant\nimprovements over state-of-the-art SMT and NMT systems.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "cheng2016wide", "citations": "2990", "year": "2016", "title":"Wide & Deep Learning For Recommender Systems", "abstract": "<p>Generalized linear models with nonlinear feature transformations are widely\nused for large-scale regression and classification problems with sparse inputs.\nMemorization of feature interactions through a wide set of cross-product\nfeature transformations are effective and interpretable, while generalization\nrequires more feature engineering effort. With less feature engineering, deep\nneural networks can generalize better to unseen feature combinations through\nlow-dimensional dense embeddings learned for the sparse features. However, deep\nneural networks with embeddings can over-generalize and recommend less relevant\nitems when the user-item interactions are sparse and high-rank. In this paper,\nwe present Wide &amp; Deep learning—jointly trained wide linear models and deep\nneural networks—to combine the benefits of memorization and generalization\nfor recommender systems. We productionized and evaluated the system on Google\nPlay, a commercial mobile app store with over one billion active users and over\none million apps. Online experiment results show that Wide &amp; Deep significantly\nincreased app acquisitions compared with wide-only and deep-only models. We\nhave also open-sourced our implementation in TensorFlow.</p>\n", "tags": [] },
{"key": "cheng2018sequential", "citations": "61", "year": "2020", "title":"Sequential Attention GAN For Interactive Image Editing", "abstract": "<p>Most existing text-to-image synthesis tasks are static single-turn\ngeneration, based on pre-defined textual descriptions of images. To explore\nmore practical and interactive real-life applications, we introduce a new task</p>\n<ul>\n  <li>Interactive Image Editing, where users can guide an agent to edit images via\nmulti-turn textual commands on-the-fly. In each session, the agent takes a\nnatural language description from the user as the input and modifies the image\ngenerated in the previous turn to a new design, following the user description.\nThe main challenges in this sequential and interactive image generation task\nare two-fold: 1) contextual consistency between a generated image and the\nprovided textual description; 2) step-by-step region-level modification to\nmaintain visual consistency across the generated image sequence in each\nsession. To address these challenges, we propose a novel Sequential Attention\nGenerative Adversarial Net-work (SeqAttnGAN), which applies a neural state\ntracker to encode the previous image and the textual description in each turn\nof the sequence, and uses a GAN framework to generate a modified version of the\nimage that is consistent with the preceding images and coherent with the\ndescription. To achieve better region-specific refinement, we also introduce a\nsequential attention mechanism into the model. To benchmark on the new task, we\nintroduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain\nmulti-turn sessions with image-description sequences in the fashion domain.\nExperiments on both datasets show that the proposed SeqAttnGANmodel outperforms\nstate-of-the-art approaches on the interactive image editing task across all\nevaluation metrics including visual quality, image sequence coherence, and\ntext-image consistency.</li>\n</ul>\n", "tags": ["Applications","Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "cheng2018towards", "citations": "182", "year": "2018", "title":"Towards Robust Neural Machine Translation", "abstract": "<p>Small perturbations in the input can severely distort intermediate\nrepresentations and thus impact translation quality of neural machine\ntranslation (NMT) models. In this paper, we propose to improve the robustness\nof NMT models with adversarial stability training. The basic idea is to make\nboth the encoder and decoder in NMT models robust against input perturbations\nby enabling them to behave similarly for the original input and its perturbed\ncounterpart. Experimental results on Chinese-English, English-German and\nEnglish-French translation tasks show that our approaches can not only achieve\nsignificant improvements over strong NMT systems but also improve the\nrobustness of NMT models.</p>\n", "tags": ["Training Techniques"] },
{"key": "cheng2019robust", "citations": "233", "year": "2019", "title":"Robust Neural Machine Translation With Doubly Adversarial Inputs", "abstract": "<p>Neural machine translation (NMT) often suffers from the vulnerability to\nnoisy perturbations in the input. We propose an approach to improving the\nrobustness of NMT models, which consists of two parts: (1) attack the\ntranslation model with adversarial source examples; (2) defend the translation\nmodel with adversarial target inputs to improve its robustness against the\nadversarial source inputs.For the generation of adversarial inputs, we propose\na gradient-based method to craft adversarial examples informed by the\ntranslation loss over the clean inputs.Experimental results on Chinese-English\nand English-German translation tasks demonstrate that our approach achieves\nsignificant improvements (\\(2.8\\) and \\(1.6\\) BLEU points) over Transformer on\nstandard clean benchmarks as well as exhibiting higher robustness on noisy\ndata.</p>\n", "tags": ["Model Architecture","Security"] },
{"key": "cheng2020advaug", "citations": "97", "year": "2020", "title":"Advaug: Robust Adversarial Augmentation For Neural Machine Translation", "abstract": "<p>In this paper, we propose a new adversarial augmentation method for Neural\nMachine Translation (NMT). The main idea is to minimize the vicinal risk over\nvirtual sentences sampled from two vicinity distributions, of which the crucial\none is a novel vicinity distribution for adversarial sentences that describes a\nsmooth interpolated embedding space centered around observed training sentence\npairs. We then discuss our approach, AdvAug, to train NMT models using the\nembeddings of virtual sentences in sequence-to-sequence learning. Experiments\non Chinese-English, English-French, and English-German translation benchmarks\nshow that AdvAug achieves significant improvements over the Transformer (up to\n4.9 BLEU points), and substantially outperforms other data augmentation\ntechniques (e.g. back-translation) without using extra corpora.</p>\n", "tags": ["Model Architecture","Security","Training Techniques"] },
{"key": "cheng2022vista", "citations": "66", "year": "2022", "title":"Vista: Vision And Scene Text Aggregation For Cross-modal Retrieval", "abstract": "<p>Visual appearance is considered to be the most important cue to understand\nimages for cross-modal retrieval, while sometimes the scene text appearing in\nimages can provide valuable information to understand the visual semantics.\nMost of existing cross-modal retrieval approaches ignore the usage of scene\ntext information and directly adding this information may lead to performance\ndegradation in scene text free scenarios. To address this issue, we propose a\nfull transformer architecture to unify these cross-modal retrieval scenarios in\na single \\(\\textbf{Vi}\\)sion and \\(\\textbf{S}\\)cene \\(\\textbf{T}\\)ext\n\\(\\textbf{A}\\)ggregation framework (ViSTA). Specifically, ViSTA utilizes\ntransformer blocks to directly encode image patches and fuse scene text\nembedding to learn an aggregated visual representation for cross-modal\nretrieval. To tackle the modality missing problem of scene text, we propose a\nnovel fusion token based transformer aggregation approach to exchange the\nnecessary scene text information only through the fusion token and concentrate\non the most important features in each modality. To further strengthen the\nvisual modality, we develop dual contrastive learning losses to embed both\nimage-text pairs and fusion-text pairs into a common cross-modal space.\nCompared to existing methods, ViSTA enables to aggregate relevant scene text\nsemantics with visual appearance, and hence improve results under both scene\ntext free and scene text aware scenarios. Experimental results show that ViSTA\noutperforms other methods by at least \\(\\bf{8.4}%\\) at Recall@1 for scene text\naware retrieval task. Compared with state-of-the-art scene text free retrieval\nmethods, ViSTA can achieve better accuracy on Flicker30K and MSCOCO while\nrunning at least three times faster during the inference stage, which validates\nthe effectiveness of the proposed framework.</p>\n", "tags": ["CVPR","Model Architecture","Tools"] },
{"key": "chenglei2022prompting", "citations": "62", "year": "2022", "title":"Prompting GPT-3 To Be Reliable", "abstract": "<p>Large language models (LLMs) show impressive abilities via few-shot\nprompting. Commercialized APIs such as OpenAI GPT-3 further increase their use\nin real-world language applications. However, the crucial problem of how to\nimprove the reliability of GPT-3 is still under-explored. While reliability is\na broad and vaguely defined term, we decompose reliability into four main\nfacets that correspond to the existing framework of ML safety and are\nwell-recognized to be important: generalizability, social biases, calibration,\nand factuality. Our core contribution is to establish simple and effective\nprompts that improve GPT-3’s reliability as it: 1) generalizes\nout-of-distribution, 2) balances demographic distribution and uses natural\nlanguage instructions to reduce social biases, 3) calibrates output\nprobabilities, and 4) updates the LLM’s factual knowledge and reasoning chains.\nWith appropriate prompts, GPT-3 is more reliable than smaller-scale supervised\nmodels on all these facets. We release all processed datasets, evaluation\nscripts, and model predictions. Our systematic empirical study not only sheds\nnew insights on the reliability of prompting LLMs, but more importantly, our\nprompting strategies can help practitioners more reliably use LLMs like GPT-3.</p>\n", "tags": ["Applications","Datasets","Evaluation","Few-Shot","Model Architecture","Prompting","Tools"] },
{"key": "chengyi2023neural", "citations": "141", "year": "2023", "title":"Neural Codec Language Models Are Zero-shot Text To Speech Synthesizers", "abstract": "<p>We introduce a language modeling approach for text to speech synthesis (TTS).\nSpecifically, we train a neural codec language model (called Vall-E) using\ndiscrete codes derived from an off-the-shelf neural audio codec model, and\nregard TTS as a conditional language modeling task rather than continuous\nsignal regression as in previous work. During the pre-training stage, we scale\nup the TTS training data to 60K hours of English speech which is hundreds of\ntimes larger than existing systems. Vall-E emerges in-context learning\ncapabilities and can be used to synthesize high-quality personalized speech\nwith only a 3-second enrolled recording of an unseen speaker as an acoustic\nprompt. Experiment results show that Vall-E significantly outperforms the\nstate-of-the-art zero-shot TTS system in terms of speech naturalness and\nspeaker similarity. In addition, we find Vall-E could preserve the speaker’s\nemotion and acoustic environment of the acoustic prompt in synthesis. See\nhttps://aka.ms/valle for demos of our work.</p>\n", "tags": ["In Context Learning","Prompting","Training Techniques"] },
{"key": "cherry2018revisiting", "citations": "98", "year": "2018", "title":"Revisiting Character-based Neural Machine Translation With Capacity And Compression", "abstract": "<p>Translating characters instead of words or word-fragments has the potential\nto simplify the processing pipeline for neural machine translation (NMT), and\nimprove results by eliminating hyper-parameters and manual feature engineering.\nHowever, it results in longer sequences in which each symbol contains less\ninformation, creating both modeling and computational challenges. In this\npaper, we show that the modeling problem can be solved by standard\nsequence-to-sequence architectures of sufficient depth, and that deep models\noperating at the character level outperform identical models operating over\nword fragments. This result implies that alternative architectures for handling\ncharacter input are better viewed as methods for reducing computation time than\nas improved ways of modeling longer sequences. From this perspective, we\nevaluate several techniques for character-level NMT, verify that they do not\nmatch the performance of our deep character baseline model, and evaluate the\nperformance versus computation time tradeoffs they offer. Within this\nframework, we also perform the first evaluation for NMT of conditional\ncomputation over time, in which the model learns which timesteps can be\nskipped, rather than having them be dictated by a fixed schedule specified\nbefore training begins.</p>\n", "tags": ["EMNLP","Evaluation","Tools","Training Techniques"] },
{"key": "cherti2022reproducible", "citations": "233", "year": "2023", "title":"Reproducible Scaling Laws For Contrastive Language-image Learning", "abstract": "<p>Scaling up neural networks has led to remarkable performance across a wide\nrange of tasks. Moreover, performance often follows reliable scaling laws as a\nfunction of training set size, model size, and compute, which offers valuable\nguidance as large-scale experiments are becoming increasingly expensive.\nHowever, previous work on scaling laws has primarily used private data \\&amp;\nmodels or focused on uni-modal language or vision learning. To address these\nlimitations, we investigate scaling laws for contrastive language-image\npre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP\nrepository. Our large-scale experiments involve models trained on up to two\nbillion image-text pairs and identify power law scaling for multiple downstream\ntasks including zero-shot classification, retrieval, linear probing, and\nend-to-end fine-tuning. We find that the training distribution plays a key role\nin scaling laws as the OpenAI and OpenCLIP models exhibit different scaling\nbehavior despite identical model architectures and similar training recipes. We\nopen-source our evaluation workflow and all models, including the largest\npublic CLIP models, to ensure reproducibility and make scaling laws research\nmore accessible. Source code and instructions to reproduce this study will be\navailable at https://github.com/LAION-AI/scaling-laws-openclip</p>\n", "tags": ["CVPR","Datasets","Efficiency","Evaluation","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "chevalierboisvert2018babyai", "citations": "68", "year": "2018", "title":"Babyai: A Platform To Study The Sample Efficiency Of Grounded Language Learning", "abstract": "<p>Allowing humans to interactively train artificial agents to understand\nlanguage instructions is desirable for both practical and scientific reasons,\nbut given the poor data efficiency of the current learning methods, this goal\nmay require substantial research efforts. Here, we introduce the BabyAI\nresearch platform to support investigations towards including humans in the\nloop for grounded language learning. The BabyAI platform comprises an\nextensible suite of 19 levels of increasing difficulty. The levels gradually\nlead the agent towards acquiring a combinatorially rich synthetic language\nwhich is a proper subset of English. The platform also provides a heuristic\nexpert agent for the purpose of simulating a human teacher. We report baseline\nresults and estimate the amount of human involvement that would be required to\ntrain a neural network-based agent on some of the BabyAI levels. We put forward\nstrong evidence that current deep learning methods are not yet sufficiently\nsample efficient when it comes to learning a language with compositional\nproperties.</p>\n", "tags": ["Efficiency","Tools"] },
{"key": "chi2019cross", "citations": "142", "year": "2020", "title":"Cross-lingual Natural Language Generation Via Pre-training", "abstract": "<p>In this work we focus on transferring supervision signals of natural language\ngeneration (NLG) tasks between multiple languages. We propose to pretrain the\nencoder and the decoder of a sequence-to-sequence model under both monolingual\nand cross-lingual settings. The pre-training objective encourages the model to\nrepresent different languages in the shared space, so that we can conduct\nzero-shot cross-lingual transfer. After the pre-training procedure, we use\nmonolingual data to fine-tune the pre-trained model on downstream NLG tasks.\nThen the sequence-to-sequence model trained in a single language can be\ndirectly evaluated beyond that language (i.e., accepting multi-lingual input\nand producing multi-lingual output). Experimental results on question\ngeneration and abstractive summarization show that our model outperforms the\nmachine-translation-based pipeline methods for zero-shot cross-lingual\ngeneration. Moreover, cross-lingual transfer improves NLG performance of\nlow-resource languages by leveraging rich-resource language data. Our\nimplementation and data are available at https://github.com/CZWin32768/xnlg.</p>\n", "tags": ["AAAI","Has Code","Training Techniques"] },
{"key": "chi2020infoxlm", "citations": "224", "year": "2021", "title":"Infoxlm: An Information-theoretic Framework For Cross-lingual Language Model Pre-training", "abstract": "<p>In this work, we present an information-theoretic framework that formulates\ncross-lingual language model pre-training as maximizing mutual information\nbetween multilingual-multi-granularity texts. The unified view helps us to\nbetter understand the existing methods for learning cross-lingual\nrepresentations. More importantly, inspired by the framework, we propose a new\npre-training task based on contrastive learning. Specifically, we regard a\nbilingual sentence pair as two views of the same meaning and encourage their\nencoded representations to be more similar than the negative examples. By\nleveraging both monolingual and parallel corpora, we jointly train the pretext\ntasks to improve the cross-lingual transferability of pre-trained models.\nExperimental results on several benchmarks show that our approach achieves\nconsiderably better performance. The code and pre-trained models are available\nat https://aka.ms/infoxlm.</p>\n", "tags": ["NAACL","Tools","Training Techniques"] },
{"key": "chi2021xlm", "citations": "61", "year": "2022", "title":"XLM-E: Cross-lingual Language Model Pre-training Via ELECTRA", "abstract": "<p>In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.</p>\n", "tags": ["Training Techniques"] },
{"key": "chia2022relationprompt", "citations": "67", "year": "2022", "title":"Relationprompt: Leveraging Prompts To Generate Synthetic Data For Zero-shot Relation Triplet Extraction", "abstract": "<p>Despite the importance of relation extraction in building and representing\nknowledge, less research is focused on generalizing to unseen relations types.\nWe introduce the task setting of Zero-Shot Relation Triplet Extraction\n(ZeroRTE) to encourage further research in low-resource relation extraction\nmethods. Given an input sentence, each extracted triplet consists of the head\nentity, relation label, and tail entity where the relation label is not seen at\nthe training stage. To solve ZeroRTE, we propose to synthesize relation\nexamples by prompting language models to generate structured texts. Concretely,\nwe unify language model prompts and structured text approaches to design a\nstructured prompt template for generating synthetic relation samples when\nconditioning on relation label prompts (RelationPrompt). To overcome the\nlimitation for extracting multiple relation triplets in a sentence, we design a\nnovel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL\ndatasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot\nrelation classification. Our code and data are available at\ngithub.com/declare-lab/RelationPrompt.</p>\n", "tags": ["Datasets","Has Code","Prompting","Training Techniques"] },
{"key": "chiang2018semantically", "citations": "93", "year": "2019", "title":"Semantically-aligned Equation Generation For Solving And Reasoning Math Word Problems", "abstract": "<p>Solving math word problems is a challenging task that requires accurate\nnatural language understanding to bridge natural language texts and math\nexpressions. Motivated by the intuition about how human generates the equations\ngiven the problem texts, this paper presents a neural approach to automatically\nsolve math word problems by operating symbols according to their semantic\nmeanings in texts. This paper views the process of generating equation as a\nbridge between the semantic world and the symbolic world, where the proposed\nneural math solver is based on an encoder-decoder framework. In the proposed\nmodel, the encoder is designed to understand the semantics of problems, and the\ndecoder focuses on tracking semantic meanings of the generated symbols and then\ndeciding which symbol to generate next. The preliminary experiments are\nconducted in a dataset Math23K, and our model significantly outperforms both\nthe state-of-the-art single model and the best non-retrieval-based model over\nabout 10% accuracy, demonstrating the effectiveness of bridging the symbolic\nand semantic worlds from math word problems.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "chiang2023can", "citations": "127", "year": "2023", "title":"Can Large Language Models Be An Alternative To Human Evaluations?", "abstract": "<p>Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.</p>\n", "tags": ["Evaluation"] },
{"key": "child2019generating", "citations": "631", "year": "2019", "title":"Generating Long Sequences With Sparse Transformers", "abstract": "<p>Transformers are powerful sequence models, but require time and memory that\ngrows quadratically with the sequence length. In this paper we introduce sparse\nfactorizations of the attention matrix which reduce this to \\(O(n \\sqrt{n})\\). We\nalso introduce a) a variation on architecture and initialization to train\ndeeper networks, b) the recomputation of attention matrices to save memory, and\nc) fast attention kernels for training. We call networks with these changes\nSparse Transformers, and show they can model sequences tens of thousands of\ntimesteps long using hundreds of layers. We use the same architecture to model\nimages, audio, and text from raw bytes, setting a new state of the art for\ndensity modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate\nunconditional samples that demonstrate global coherence and great diversity,\nand show it is possible in principle to use self-attention to model sequences\nof length one million or more.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "chintagunta2021medically", "citations": "100", "year": "2021", "title":"Medically Aware GPT-3 As A Data Generator For Medical Dialogue Summarization", "abstract": "<p>In medical dialogue summarization, summaries must be coherent and must\ncapture all the medically relevant information in the dialogue. However,\nlearning effective models for summarization require large amounts of labeled\ndata which is especially hard to obtain. We present an algorithm to create\nsynthetic training data with an explicit focus on capturing medically relevant\ninformation. We utilize GPT-3 as the backbone of our algorithm and scale 210\nhuman labeled examples to yield results comparable to using 6400 human labeled\nexamples (~30x) leveraging low-shot learning and an ensemble method. In\ndetailed experiments, we show that this approach produces high quality training\ndata that can further be combined with human labeled data to get summaries that\nare strongly preferable to those produced by models trained on human data alone\nboth in terms of medical accuracy and coherency.</p>\n", "tags": ["Dialogue & Multi Turn","Model Architecture","Training Techniques"] },
{"key": "chisholm2017learning", "citations": "97", "year": "2017", "title":"Learning To Generate One-sentence Biographies From Wikidata", "abstract": "<p>We investigate the generation of one-sentence Wikipedia biographies from\nfacts derived from Wikidata slot-value pairs. We train a recurrent neural\nnetwork sequence-to-sequence model with attention to select facts and generate\ntextual summaries. Our model incorporates a novel secondary objective that\nhelps ensure it generates sentences that contain the input facts. The model\nachieves a BLEU score of 41, improving significantly upon the vanilla\nsequence-to-sequence model and scoring roughly twice that of a simple template\nbaseline. Human preference evaluation suggests the model is nearly as good as\nthe Wikipedia reference. Manual analysis explores content selection, suggesting\nthe model can trade the ability to infer knowledge against the risk of\nhallucinating incorrect information.</p>\n", "tags": ["Evaluation","Model Architecture","NAACL"] },
{"key": "chiu2017monotonic", "citations": "146", "year": "2018", "title":"Monotonic Chunkwise Attention", "abstract": "<p>Sequence-to-sequence models with soft attention have been successfully\napplied to a wide variety of problems, but their decoding process incurs a\nquadratic time and space cost and is inapplicable to real-time sequence\ntransduction. To address these issues, we propose Monotonic Chunkwise Attention\n(MoChA), which adaptively splits the input sequence into small chunks over\nwhich soft attention is computed. We show that models utilizing MoChA can be\ntrained efficiently with standard backpropagation while allowing online and\nlinear-time decoding at test time. When applied to online speech recognition,\nwe obtain state-of-the-art results and match the performance of a model using\nan offline soft attention mechanism. In document summarization experiments\nwhere we do not expect monotonic alignments, we show significantly improved\nperformance compared to a baseline monotonic attention-based model.</p>\n", "tags": ["Model Architecture"] },
{"key": "cho2019mixture", "citations": "63", "year": "2019", "title":"Mixture Content Selection For Diverse Sequence Generation", "abstract": "<p>Generating diverse sequences is important in many NLP applications such as\nquestion generation or summarization that exhibit semantically one-to-many\nrelationships between source and the target sequences. We present a method to\nexplicitly separate diversification from generation using a general\nplug-and-play module (called SELECTOR) that wraps around and guides an existing\nencoder-decoder model. The diversification stage uses a mixture of experts to\nsample different binary masks on the source sequence for diverse content\nselection. The generation stage uses a standard encoder-decoder model given\neach selected content from the source sequence. Due to the non-differentiable\nnature of discrete sampling and the lack of ground truth labels for binary\nmask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM\nfor training. In question generation (SQuAD) and abstractive summarization\n(CNN-DM), our method demonstrates significant improvements in accuracy,\ndiversity and training efficiency, including state-of-the-art top-1 accuracy in\nboth datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a\nstate of the art model. Our code is publicly available at\nhttps://github.com/clovaai/FocusSeq2Seq.</p>\n", "tags": ["Applications","EMNLP","Has Code","Training Techniques"] },
{"key": "cho2021unifying", "citations": "148", "year": "2021", "title":"Unifying Vision-and-language Tasks Via Text Generation", "abstract": "<p>Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5</p>\n", "tags": ["Has Code","Model Architecture","Tools"] },
{"key": "cho2022dall", "citations": "74", "year": "2023", "title":"Dall-eval: Probing The Reasoning Skills And Social Biases Of Text-to-image Generation Models", "abstract": "<p>Recently, DALL-E, a multimodal transformer language model, and its variants,\nincluding diffusion models, have shown high-quality text-to-image generation\ncapabilities. However, despite the realistic image generation results, there\nhas not been a detailed analysis of how to evaluate such models. In this work,\nwe investigate the visual reasoning capabilities and social biases of different\ntext-to-image models, covering both multimodal transformer language models and\ndiffusion models. First, we measure three visual reasoning skills: object\nrecognition, object counting, and spatial relation understanding. For this, we\npropose PaintSkills, a compositional diagnostic evaluation dataset that\nmeasures these skills. Despite the high-fidelity image generation capability, a\nlarge gap exists between the performance of recent models and the upper bound\naccuracy in object counting and spatial relation understanding skills. Second,\nwe assess the gender and skin tone biases by measuring the gender/skin tone\ndistribution of generated images across various professions and attributes. We\ndemonstrate that recent text-to-image generation models learn specific biases\nabout gender and skin tone from web image-text pairs. We hope our work will\nhelp guide future progress in improving text-to-image generation models on\nvisual reasoning skills and learning socially unbiased representations. Code\nand data: https://github.com/j-min/DallEval</p>\n", "tags": ["Datasets","Evaluation","Has Code","ICCV","Model Architecture"] },
{"key": "choi2018fine", "citations": "196", "year": "2018", "title":"Fine-grained Attention Mechanism For Neural Machine Translation", "abstract": "<p>Neural machine translation (NMT) has been a new paradigm in machine\ntranslation, and the attention mechanism has become the dominant approach with\nthe state-of-the-art records in many language pairs. While there are variants\nof the attention mechanism, all of them use only temporal attention where one\nscalar value is assigned to one context vector corresponding to a source word.\nIn this paper, we propose a fine-grained (or 2D) attention mechanism where each\ndimension of a context vector will receive a separate attention score. In\nexperiments with the task of En-De and En-Fi translation, the fine-grained\nattention method improves the translation quality in terms of BLEU score. In\naddition, our alignment analysis reveals how the fine-grained attention\nmechanism exploits the internal structure of context vectors.</p>\n", "tags": ["Model Architecture"] },
{"key": "choi2018quac", "citations": "686", "year": "2018", "title":"Quac : Question Answering In Context", "abstract": "<p>We present QuAC, a dataset for Question Answering in Context that contains\n14K information-seeking QA dialogs (100K questions in total). The dialogs\ninvolve two crowd workers: (1) a student who poses a sequence of freeform\nquestions to learn as much as possible about a hidden Wikipedia text, and (2) a\nteacher who answers the questions by providing short excerpts from the text.\nQuAC introduces challenges not found in existing machine comprehension\ndatasets: its questions are often more open-ended, unanswerable, or only\nmeaningful within the dialog context, as we show in a detailed qualitative\nevaluation. We also report results for a number of reference models, including\na recently state-of-the-art reading comprehension architecture extended to\nmodel dialog context. Our best model underperforms humans by 20 F1, suggesting\nthat there is significant room for future work on this data. Dataset, baseline,\nand leaderboard available at http://quac.ai.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "choi2018ultra", "citations": "214", "year": "2018", "title":"Ultra-fine Entity Typing", "abstract": "<p>We introduce a new entity typing task: given a sentence with an entity\nmention, the goal is to predict a set of free-form phrases (e.g. skyscraper,\nsongwriter, or criminal) that describe appropriate types for the target entity.\nThis formulation allows us to use a new type of distant supervision at large\nscale: head words, which indicate the type of the noun phrases they appear in.\nWe show that these ultra-fine types can be crowd-sourced, and introduce new\nevaluation sets that are much more diverse and fine-grained than existing\nbenchmarks. We present a model that can predict open types, and is trained\nusing a multitask objective that pools our new head-word supervision with prior\nsupervision from entity linking. Experimental results demonstrate that our\nmodel is effective in predicting entity types at varying granularity; it\nachieves state of the art performance on an existing fine-grained entity typing\nbenchmark, and sets baselines for our newly-introduced datasets. Our data and\nmodel can be downloaded from: http://nlp.cs.washington.edu/entity_type</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "choi2021evaluation", "citations": "93", "year": "2021", "title":"Evaluation Of BERT And ALBERT Sentence Embedding Performance On Downstream NLP Tasks", "abstract": "<p>Contextualized representations from a pre-trained language model are central\nto achieve a high performance on downstream NLP task. The pre-trained BERT and\nA Lite BERT (ALBERT) models can be fine-tuned to give state-ofthe-art results\nin sentence-pair regressions such as semantic textual similarity (STS) and\nnatural language inference (NLI). Although BERT-based models yield the [CLS]\ntoken vector as a reasonable sentence embedding, the search for an optimal\nsentence embedding scheme remains an active research area in computational\nlinguistics. This paper explores on sentence embedding models for BERT and\nALBERT. In particular, we take a modified BERT network with siamese and triplet\nnetwork structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to\ncreate Sentence-ALBERT (SALBERT). We also experiment with an outer CNN\nsentence-embedding network for SBERT and SALBERT. We evaluate performances of\nall sentence-embedding models considered using the STS and NLI datasets. The\nempirical results indicate that our CNN architecture improves ALBERT models\nsubstantially more than BERT models for STS benchmark. Despite significantly\nfewer model parameters, ALBERT sentence embedding is highly competitive to BERT\nin downstream NLP evaluations.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "chorowski2016towards", "citations": "338", "year": "2017", "title":"Towards Better Decoding And Language Model Integration In Sequence To Sequence Models", "abstract": "<p>The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER.</p>\n", "tags": ["Datasets","INTERSPEECH","Model Architecture","Tools"] },
{"key": "chronopoulou2019embarrassingly", "citations": "112", "year": "2019", "title":"An Embarrassingly Simple Approach For Transfer Learning From Pretrained Language Models", "abstract": "<p>A growing number of state-of-the-art transfer learning methods employ\nlanguage models pretrained on large generic corpora. In this paper we present a\nconceptually simple and effective transfer learning approach that addresses the\nproblem of catastrophic forgetting. Specifically, we combine the task-specific\noptimization function with an auxiliary language model objective, which is\nadjusted during the training process. This preserves language regularities\ncaptured by language models, while enabling sufficient adaptation for solving\nthe target task. Our method does not require pretraining or finetuning separate\ncomponents of the network and we train our models end-to-end in a single step.\nWe present results on a variety of challenging affective and text\nclassification tasks, surpassing well established transfer learning methods\nwith greater level of complexity.</p>\n", "tags": ["Efficiency","Fine-Tuning","Training Techniques"] },
{"key": "chuang2019speechbert", "citations": "78", "year": "2020", "title":"Speechbert: An Audio-and-text Jointly Learned Language Model For End-to-end Spoken Question Answering", "abstract": "<p>While various end-to-end models for spoken language understanding tasks have\nbeen explored recently, this paper is probably the first known attempt to\nchallenge the very difficult task of end-to-end spoken question answering\n(SQA). Learning from the very successful BERT model for various text processing\ntasks, here we proposed an audio-and-text jointly learned SpeechBERT model.\nThis model outperformed the conventional approach of cascading ASR with the\nfollowing text question answering (TQA) model on datasets including ASR errors\nin answer spans, because the end-to-end model was shown to be able to extract\ninformation out of audio data before ASR produced errors. When ensembling the\nproposed end-to-end model with the cascade architecture, even better\nperformance was achieved. In addition to the potential of end-to-end SQA, the\nSpeechBERT can also be considered for many other spoken language understanding\ntasks just as BERT for many text processing tasks.</p>\n", "tags": ["Datasets","INTERSPEECH","Model Architecture"] },
{"key": "chung2016character", "citations": "238", "year": "2016", "title":"A Character-level Decoder Without Explicit Segmentation For Neural Machine Translation", "abstract": "<p>The existing machine translation systems, whether phrase-based or neural,\nhave relied almost exclusively on word-level modelling with explicit\nsegmentation. In this paper, we ask a fundamental question: can neural machine\ntranslation generate a character sequence without any explicit segmentation? To\nanswer this question, we evaluate an attention-based encoder-decoder with a\nsubword-level encoder and a character-level decoder on four language\npairs–En-Cs, En-De, En-Ru and En-Fi– using the parallel corpora from WMT’15.\nOur experiments show that the models with a character-level decoder outperform\nthe ones with a subword-level decoder on all of the four language pairs.\nFurthermore, the ensembles of neural models with a character-level decoder\noutperform the state-of-the-art non-neural machine translation systems on\nEn-Cs, En-De and En-Fi and perform comparably on En-Ru.</p>\n", "tags": ["Model Architecture"] },
{"key": "chung2018semi", "citations": "97", "year": "2019", "title":"Semi-supervised Training For Improving Data Efficiency In End-to-end Speech Synthesis", "abstract": "<p>Although end-to-end text-to-speech (TTS) models such as Tacotron have shown\nexcellent results, they typically require a sizable set of high-quality &lt;text,\naudio&gt; pairs for training, which are expensive to collect. In this paper, we\npropose a semi-supervised training framework to improve the data efficiency of\nTacotron. The idea is to allow Tacotron to utilize textual and acoustic\nknowledge contained in large, publicly-available text and speech corpora.\nImportantly, these external data are unpaired and potentially noisy.\nSpecifically, first we embed each word in the input text into word vectors and\ncondition the Tacotron encoder on them. We then use an unpaired speech corpus\nto pre-train the Tacotron decoder in the acoustic domain. Finally, we fine-tune\nthe model using available paired data. We demonstrate that the proposed\nframework enables Tacotron to generate intelligible speech using less than half\nan hour of paired training data.</p>\n", "tags": ["Efficiency","ICASSP","Tools","Training Techniques"] },
{"key": "chung2019conan", "citations": "163", "year": "2019", "title":"CONAN -- Counter Narratives Through Nichesourcing: A Multilingual Dataset Of Responses To Fight Online Hate Speech", "abstract": "<p>Although there is an unprecedented effort to provide adequate responses in\nterms of laws and policies to hate content on social media platforms, dealing\nwith hatred online is still a tough problem. Tackling hate speech in the\nstandard way of content deletion or user suspension may be charged with\ncensorship and overblocking. One alternate strategy, that has received little\nattention so far by the research community, is to actually oppose hate content\nwith counter-narratives (i.e. informed textual responses). In this paper, we\ndescribe the creation of the first large-scale, multilingual, expert-based\ndataset of hate speech/counter-narrative pairs. This dataset has been built\nwith the effort of more than 100 operators from three different NGOs that\napplied their training and expertise to the task. Together with the collected\ndata we also provide additional annotations about expert demographics, hate and\nresponse type, and data augmentation through translation and paraphrasing.\nFinally, we provide initial experiments to assess the quality of our data.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "chung2020rethinking", "citations": "69", "year": "2020", "title":"Rethinking Embedding Coupling In Pre-trained Language Models", "abstract": "<p>We re-evaluate the standard practice of sharing weights between input and\noutput embeddings in state-of-the-art pre-trained language models. We show that\ndecoupled embeddings provide increased modeling flexibility, allowing us to\nsignificantly improve the efficiency of parameter allocation in the input\nembedding of multilingual models. By reallocating the input embedding\nparameters in the Transformer layers, we achieve dramatically better\nperformance on standard natural language understanding tasks with the same\nnumber of parameters during fine-tuning. We also show that allocating\nadditional capacity to the output embedding provides benefits to the model that\npersist through the fine-tuning stage even though the output embedding is\ndiscarded after pre-training. Our analysis shows that larger output embeddings\nprevent the model’s last layers from overspecializing to the pre-training task\nand encourage Transformer representations to be more general and more\ntransferable to other tasks and languages. Harnessing these findings, we are\nable to train models that achieve strong performance on the XTREME benchmark\nwithout increasing the number of parameters at the fine-tuning stage.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "chung2021w2v", "citations": "190", "year": "2021", "title":"W2v-bert: Combining Contrastive Learning And Masked Language Modeling For Self-supervised Speech Pre-training", "abstract": "<p>Motivated by the success of masked language modeling~(MLM) in pre-training\nnatural language processing models, we propose w2v-BERT that explores MLM for\nself-supervised speech representation learning. w2v-BERT is a framework that\ncombines contrastive learning and MLM, where the former trains the model to\ndiscretize input continuous speech signals into a finite set of discriminative\nspeech tokens, and the latter trains the model to learn contextualized speech\nrepresentations via solving a masked prediction task consuming the discretized\ntokens. In contrast to existing MLM-based speech pre-training frameworks such\nas HuBERT, which relies on an iterative re-clustering and re-training process,\nor vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can\nbe optimized in an end-to-end fashion by solving the two self-supervised\ntasks~(the contrastive task and MLM) simultaneously. Our experiments show that\nw2v-BERT achieves competitive results compared to current state-of-the-art\npre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k\ncorpus as the unsupervised data. In particular, when compared to published\nmodels such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5%\nto~10% relative WER reduction on the test-clean and test-other subsets. When\napplied to the Google’s Voice Search traffic dataset, w2v-BERT outperforms our\ninternal conformer-based wav2vec~2.0 by more than~30% relatively.</p>\n", "tags": ["ASRU","Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "chung2022scaling", "citations": "1044", "year": "2022", "title":"Scaling Instruction-finetuned Language Models", "abstract": "<p>Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Few-Shot","Prompting"] },
{"key": "chunting2023lima", "citations": "100", "year": "2023", "title":"LIMA: Less Is More For Alignment", "abstract": "<p>Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.</p>\n", "tags": ["Agentic","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "clark2017simple", "citations": "422", "year": "2018", "title":"Simple And Effective Multi-paragraph Reading Comprehension", "abstract": "<p>We consider the problem of adapting neural paragraph-level question answering\nmodels to the case where entire documents are given as input. Our proposed\nsolution trains models to produce well calibrated confidence scores for their\nresults on individual paragraphs. We sample multiple paragraphs from the\ndocuments during training, and use a shared-normalization training objective\nthat encourages the model to produce globally correct output. We combine this\nmethod with a state-of-the-art pipeline for training models on document QA\ndata. Experiments demonstrate strong performance on several document QA\ndatasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion\nof TriviaQA, a large improvement from the 56.7 F1 of the previous best system.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "clark2019boolq", "citations": "203", "year": "2019", "title":"Boolq: Exploring The Surprising Difficulty Of Natural Yes/no Questions", "abstract": "<p>In this paper we study yes/no questions that are naturally occurring —\nmeaning that they are generated in unprompted and unconstrained settings. We\nbuild a reading comprehension dataset, BoolQ, of such questions, and show that\nthey are unexpectedly challenging. They often query for complex, non-factoid\ninformation, and require difficult entailment-like inference to solve. We also\nexplore the effectiveness of a range of transfer learning baselines. We find\nthat transferring from entailment data is more effective than transferring from\nparaphrase or extractive QA data, and that it, surprisingly, continues to be\nvery beneficial even when starting from massive pre-trained language models\nsuch as BERT. Our best method trains BERT on MultiNLI and then re-trains it on\nour train set. It achieves 80.4% accuracy compared to 90% accuracy of human\nannotators (and 62% majority-baseline), leaving a significant gap for future\nwork.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture"] },
{"key": "clark2019f", "citations": "81", "year": "2019", "title":"From 'F' To 'A' On The N.Y. Regents Science Exams: An Overview Of The Aristo Project", "abstract": "<p>AI has achieved remarkable mastery over games such as Chess, Go, and Poker,\nand even Jeopardy, but the rich variety of standardized exams has remained a\nlandmark challenge. Even in 2016, the best AI system achieved merely 59.3% on\nan 8th Grade science exam challenge. This paper reports unprecedented success\non the Grade 8 New York Regents Science Exam, where for the first time a system\nscores more than 90% on the exam’s non-diagram, multiple choice (NDMC)\nquestions. In addition, our Aristo system, building upon the success of recent\nlanguage models, exceeded 83% on the corresponding Grade 12 Science Exam NDMC\nquestions. The results, on unseen test questions, are robust across different\ntest years and different variations of this kind of test. They demonstrate that\nmodern NLP methods can result in mastery on this task. While not a full\nsolution to general question-answering (the questions are multiple choice, and\nthe domain is restricted to 8th Grade science), it represents a significant\nmilestone for the field.</p>\n", "tags": [] },
{"key": "clark2019what", "citations": "133", "year": "2019", "title":"What Does BERT Look At? An Analysis Of Bert's Attention", "abstract": "<p>Large pre-trained neural networks such as BERT have had great recent success\nin NLP, motivating a growing body of research investigating what aspects of\nlanguage they are able to learn from unlabeled data. Most recent analysis has\nfocused on model outputs (e.g., language model surprisal) or internal vector\nrepresentations (e.g., probing classifiers). Complementary to these works, we\npropose methods for analyzing the attention mechanisms of pre-trained models\nand apply them to BERT. BERT’s attention heads exhibit patterns such as\nattending to delimiter tokens, specific positional offsets, or broadly\nattending over the whole sentence, with heads in the same layer often\nexhibiting similar behaviors. We further show that certain attention heads\ncorrespond well to linguistic notions of syntax and coreference. For example,\nwe find heads that attend to the direct objects of verbs, determiners of nouns,\nobjects of prepositions, and coreferent mentions with remarkably high accuracy.\nLastly, we propose an attention-based probing classifier and use it to further\ndemonstrate that substantial syntactic information is captured in BERT’s\nattention.</p>\n", "tags": ["Model Architecture"] },
{"key": "clark2020transformers", "citations": "159", "year": "2020", "title":"Transformers As Soft Reasoners Over Language", "abstract": "<p>Beginning with McCarthy’s Advice Taker (1959), AI has pursued the goal of\nproviding a system with explicit, general knowledge and having the system\nreason over that knowledge. However, expressing the knowledge in a formal\n(logical or probabilistic) representation has been a major obstacle to this\nresearch. This paper investigates a modern approach to this problem where the\nfacts and rules are provided as natural language sentences, thus bypassing a\nformal representation. We train transformers to reason (or emulate reasoning)\nover these sentences using synthetically generated data. Our models, that we\ncall RuleTakers, provide the first empirical demonstration that this kind of\nsoft reasoning over language is learnable, can achieve high (99%) accuracy, and\ngeneralizes to test data requiring substantially deeper chaining than seen\nduring training (95%+ scores). We also demonstrate that the models transfer\nwell to two hand-authored rulebases, and to rulebases paraphrased into more\nnatural language. These findings are significant as it suggests a new role for\ntransformers, namely as limited “soft theorem provers” operating over explicit\ntheories in language. This in turn suggests new possibilities for\nexplainability, correctability, and counterfactual reasoning in\nquestion-answering.</p>\n", "tags": ["IJCAI","Training Techniques"] },
{"key": "clark2020tydi", "citations": "113", "year": "2020", "title":"Tydi QA: A Benchmark For Information-seeking Question Answering In Typologically Diverse Languages", "abstract": "<p>Confidently making progress on multilingual modeling requires challenging,\ntrustworthy evaluations. We present TyDi QA—a question answering dataset\ncovering 11 typologically diverse languages with 204K question-answer pairs.\nThe languages of TyDi QA are diverse with regard to their typology—the set of\nlinguistic features each language expresses—such that we expect models\nperforming well on this set to generalize across a large number of the world’s\nlanguages. We present a quantitative analysis of the data quality and\nexample-level qualitative linguistic analyses of observed language phenomena\nthat would not be found in English-only corpora. To provide a realistic\ninformation-seeking task and avoid priming effects, questions are written by\npeople who want to know the answer, but don’t know the answer yet, and the data\nis collected directly in each language without the use of translation.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "clark2021canine", "citations": "103", "year": "2022", "title":"CANINE: Pre-training An Efficient Tokenization-free Encoder For Language Representation", "abstract": "<p>Pipelined NLP systems have largely been superseded by end-to-end neural\nmodeling, yet nearly all commonly-used models still require an explicit\ntokenization step. While recent tokenization approaches based on data-derived\nsubword lexicons are less brittle than manually engineered tokenizers, these\ntechniques are not equally suited to all languages, and the use of any fixed\nvocabulary may limit a model’s ability to adapt. In this paper, we present\nCANINE, a neural encoder that operates directly on character sequences, without\nexplicit tokenization or vocabulary, and a pre-training strategy that operates\neither directly on characters or optionally uses subwords as a soft inductive\nbias. To use its finer-grained input effectively and efficiently, CANINE\ncombines downsampling, which reduces the input sequence length, with a deep\ntransformer stack, which encodes context. CANINE outperforms a comparable mBERT\nmodel by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite\nhaving 28% fewer model parameters.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","TACL","Training Techniques"] },
{"key": "clement2020pymt5", "citations": "93", "year": "2020", "title":"Pymt5: Multi-mode Translation Of Natural Language And Python Code With Transformers", "abstract": "<p>Simultaneously modeling source code and natural language has many exciting\napplications in automated software development and understanding. Pursuant to\nachieving such technology, we introduce PyMT5, the Python method text-to-text\ntransfer transformer, which is trained to translate between all pairs of Python\nmethod feature combinations: a single model that can both predict whole methods\nfrom natural language documentation strings (docstrings) and summarize code\ninto docstrings of any common style. We present an analysis and modeling effort\nof a large-scale parallel corpus of 26 million Python methods and 7.7 million\nmethod-docstring pairs, demonstrating that for docstring and method generation,\nPyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which\nwere English pre-trained or randomly initialized. On the CodeSearchNet test\nset, our best model predicts 92.1% syntactically correct method bodies,\nachieved a BLEU score of 8.59 for method generation and 16.3 for docstring\ngeneration (summarization), and achieved a ROUGE-L F-score of 24.8 for method\ngeneration and 36.7 for docstring generation.</p>\n", "tags": ["Applications","Datasets","EMNLP","Model Architecture"] },
{"key": "clinchant2019use", "citations": "100", "year": "2019", "title":"On The Use Of BERT For Neural Machine Translation", "abstract": "<p>Exploiting large pretrained models for various NMT tasks have gained a lot of\nvisibility recently. In this work we study how BERT pretrained models could be\nexploited for supervised Neural Machine Translation. We compare various ways to\nintegrate pretrained BERT model with NMT model and study the impact of the\nmonolingual data used for BERT training on the final translation quality. We\nuse WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian\ndatasets for these experiments. In addition to standard task test set\nevaluation, we perform evaluation on out-of-domain test sets and noise injected\ntest sets, in order to assess how BERT pretrained representations affect model\nrobustness.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "cobbe2021training", "citations": "415", "year": "2021", "title":"Training Verifiers To Solve Math Word Problems", "abstract": "<p>State-of-the-art language models can match human performance on many tasks,\nbut they still struggle to robustly perform multi-step mathematical reasoning.\nTo diagnose the failures of current models and support research, we introduce\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\nword problems. We find that even the largest transformer models fail to achieve\nhigh test performance, despite the conceptual simplicity of this problem\ndistribution. To increase performance, we propose training verifiers to judge\nthe correctness of model completions. At test time, we generate many candidate\nsolutions and select the one ranked highest by the verifier. We demonstrate\nthat verification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "coenen2019visualizing", "citations": "218", "year": "2019", "title":"Visualizing And Measuring The Geometry Of BERT", "abstract": "<p>Transformer architectures show significant promise for natural language\nprocessing. Given that a single pretrained model can be fine-tuned to perform\nwell on many different tasks, these networks appear to extract generally useful\nlinguistic features. A natural question is how such networks represent this\ninformation internally. This paper describes qualitative and quantitative\ninvestigations of one particularly effective model, BERT. At a high level,\nlinguistic features seem to be represented in separate semantic and syntactic\nsubspaces. We find evidence of a fine-grained geometric representation of word\nsenses. We also present empirical descriptions of syntactic representations in\nboth attention matrices and individual word embeddings, as well as a\nmathematical argument to explain the geometry of these representations.</p>\n", "tags": ["Model Architecture"] },
{"key": "cohan2019pretrained", "citations": "109", "year": "2019", "title":"Pretrained Language Models For Sequential Sentence Classification", "abstract": "<p>As a step toward better document-level understanding, we explore\nclassification of a sequence of sentences into their corresponding categories,\na task that requires understanding sentences in context of the document. Recent\nsuccessful models for this task have used hierarchical models to contextualize\nsentence representations, and Conditional Random Fields (CRFs) to incorporate\ndependencies between subsequent labels. In this work, we show that pretrained\nlanguage models, BERT (Devlin et al., 2018) in particular, can be used for this\ntask to capture contextual dependencies without the need for hierarchical\nencoding nor a CRF. Specifically, we construct a joint sentence representation\nthat allows BERT Transformer layers to directly utilize contextual information\nfrom all words in all sentences. Our approach achieves state-of-the-art results\non four datasets, including a new dataset of structured scientific abstracts.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "cohan2020specter", "citations": "306", "year": "2020", "title":"SPECTER: Document-level Representation Learning Using Citation-informed Transformers", "abstract": "<p>Representation learning is a critical ingredient for natural language\nprocessing systems. Recent Transformer language models like BERT learn powerful\ntextual representations, but these models are targeted towards token- and\nsentence-level training objectives and do not leverage information on\ninter-document relatedness, which limits their document-level representation\npower. For applications on scientific documents, such as classification and\nrecommendation, the embeddings power strong performance on end tasks. We\npropose SPECTER, a new method to generate document-level embedding of\nscientific documents based on pretraining a Transformer language model on a\npowerful signal of document-level relatedness: the citation graph. Unlike\nexisting pretrained language models, SPECTER can be easily applied to\ndownstream applications without task-specific fine-tuning. Additionally, to\nencourage further research on document-level models, we introduce SciDocs, a\nnew evaluation benchmark consisting of seven document-level tasks ranging from\ncitation prediction, to document classification and recommendation. We show\nthat SPECTER outperforms a variety of competitive baselines on the benchmark.</p>\n", "tags": ["Applications","Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "cohn2016incorporating", "citations": "181", "year": "2016", "title":"Incorporating Structural Alignment Biases Into An Attentional Neural Translation Model", "abstract": "<p>Neural encoder-decoder models of machine translation have achieved impressive\nresults, rivalling traditional translation models. However their modelling\nformulation is overly simplistic, and omits several key inductive biases built\ninto traditional models. In this paper we extend the attentional neural\ntranslation model to include structural biases from word based alignment\nmodels, including positional bias, Markov conditioning, fertility and agreement\nover translation directions. We show improvements over a baseline attentional\nmodel and standard phrase-based model over several language pairs, evaluating\non difficult languages in a low resource setting.</p>\n", "tags": ["Ethics & Fairness","NAACL"] },
{"key": "colin2019exploring", "citations": "8206", "year": "2019", "title":"Exploring The Limits Of Transfer Learning With A Unified Text-to-text Transformer", "abstract": "<p>Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus’’, we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "colombo2019affect", "citations": "107", "year": "2019", "title":"Affect-driven Dialog Generation", "abstract": "<p>The majority of current systems for end-to-end dialog generation focus on\nresponse quality without an explicit control over the affective content of the\nresponses. In this paper, we present an affect-driven dialog system, which\ngenerates emotional responses in a controlled manner using a continuous\nrepresentation of emotions. The system achieves this by modeling emotions at a\nword and sequence level using: (1) a vector representation of the desired\nemotion, (2) an affect regularizer, which penalizes neutral words, and (3) an\naffect sampling method, which forces the neural network to generate diverse\nwords that are emotionally relevant. During inference, we use a reranking\nprocedure that aims to extract the most emotionally relevant responses using a\nhuman-in-the-loop optimization process. We study the performance of our system\nin terms of both quantitative (BLEU score and response diversity), and\nqualitative (emotional appropriateness) measures.</p>\n", "tags": [] },
{"key": "conati2019toward", "citations": "115", "year": "2021", "title":"Toward Personalized XAI: A Case Study In Intelligent Tutoring Systems", "abstract": "<p>Our research is a step toward ascertaining the need for personalization, in\nXAI, and we do so in the context of investigating the value of explanations of\nAI-driven hints and feedback are useful in Intelligent Tutoring Systems (ITS).\nWe added an explanation functionality for the adaptive hints provided by the\nAdaptive CSP (ACSP) applet, an interactive simulation that helps students learn\nan algorithm for constraint satisfaction problems by providing AI-driven hints\nadapted to their predicted level of learning. We present the design of the\nexplanation functionality and the results of a controlled study to evaluate its\nimpact on students’ learning and perception of the ACPS hints. The study\nincludes an analysis of how these outcomes are modulated by several user\ncharacteristics such as personality traits and cognitive abilities, to asses if\nexplanations should be personalized to these characteristics. Our results\nindicate that providing explanations increase students’ trust in the ACPS\nhints, perceived usefulness of the hints, and intention to use them again. In\naddition, we show that students’ access of the explanation and learning gains\nare modulated by user characteristics, providing insights toward designing\npersonalized Explainable AI (XAI) for ITS.</p>\n", "tags": ["AAAI"] },
{"key": "conneau2018xnli", "citations": "938", "year": "2018", "title":"XNLI: Evaluating Cross-lingual Sentence Representations", "abstract": "<p>State-of-the-art natural language processing systems rely on supervision in\nthe form of annotated data to learn competent models. These models are\ngenerally trained on data in a single language (usually English), and cannot be\ndirectly used beyond that language. Since collecting data in every language is\nnot realistic, there has been a growing interest in cross-lingual language\nunderstanding (XLU) and low-resource cross-language transfer. In this work, we\nconstruct an evaluation set for XLU by extending the development and test sets\nof the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15\nlanguages, including low-resource languages such as Swahili and Urdu. We hope\nthat our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence\nunderstanding by providing an informative standard evaluation task. In\naddition, we provide several baselines for multilingual sentence understanding,\nincluding two based on machine translation systems, and two that use parallel\ndata to train aligned multilingual bag-of-words and LSTM encoders. We find that\nXNLI represents a practical and challenging evaluation suite, and that directly\ntranslating the test data yields the best performance among available\nbaselines.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "conneau2019unsupervised", "citations": "4154", "year": "2020", "title":"Unsupervised Cross-lingual Representation Learning At Scale", "abstract": "<p>This paper shows that pretraining multilingual language models at scale leads\nto significant performance gains for a wide range of cross-lingual transfer\ntasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our\nmodel, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a\nvariety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,\n+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs\nparticularly well on low-resource languages, improving 15.7% in XNLI accuracy\nfor Swahili and 11.4% for Urdu over previous XLM models. We also present a\ndetailed empirical analysis of the key factors that are required to achieve\nthese gains, including the trade-offs between (1) positive transfer and\ncapacity dilution and (2) the performance of high and low resource languages at\nscale. Finally, we show, for the first time, the possibility of multilingual\nmodeling without sacrificing per-language performance; XLM-R is very\ncompetitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make our code, data and models publicly available.</p>\n", "tags": ["Model Architecture"] },
{"key": "conneau2022fleurs", "citations": "84", "year": "2023", "title":"FLEURS: Few-shot Learning Evaluation Of Universal Representations Of Speech", "abstract": "<p>We introduce FLEURS, the Few-shot Learning Evaluation of Universal\nRepresentations of Speech benchmark. FLEURS is an n-way parallel speech dataset\nin 102 languages built on top of the machine translation FLoRes-101 benchmark,\nwith approximately 12 hours of speech supervision per language. FLEURS can be\nused for a variety of speech tasks, including Automatic Speech Recognition\n(ASR), Speech Language Identification (Speech LangID), Translation and\nRetrieval. In this paper, we provide baselines for the tasks based on\nmultilingual pre-trained models like mSLAM. The goal of FLEURS is to enable\nspeech technology in more languages and catalyze research in low-resource\nspeech understanding.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","SLT"] },
{"key": "cornia2019meshed", "citations": "983", "year": "2020", "title":"Meshed-memory Transformer For Image Captioning", "abstract": "<p>Transformer-based architectures represent the state of the art in sequence\nmodeling tasks like machine translation and language understanding. Their\napplicability to multi-modal contexts like image captioning, however, is still\nlargely under-explored. With the aim of filling this gap, we present M\\(^2\\) - a\nMeshed Transformer with Memory for Image Captioning. The architecture improves\nboth the image encoding and the language generation steps: it learns a\nmulti-level representation of the relationships between image regions\nintegrating learned a priori knowledge, and uses a mesh-like connectivity at\ndecoding stage to exploit low- and high-level features. Experimentally, we\ninvestigate the performance of the M\\(^2\\) Transformer and different\nfully-attentive models in comparison with recurrent ones. When tested on COCO,\nour proposal achieves a new state of the art in single-model and ensemble\nconfigurations on the “Karpathy” test split and on the online test server. We\nalso assess its performances when describing objects unseen in the training\nset. Trained models and code for reproducing the experiments are publicly\navailable at: https://github.com/aimagelab/meshed-memory-transformer.</p>\n", "tags": ["CVPR","Has Code","Model Architecture","Training Techniques"] },
{"key": "correia2019adaptively", "citations": "192", "year": "2019", "title":"Adaptively Sparse Transformers", "abstract": "<p>Attention mechanisms have become ubiquitous in NLP. Recent architectures,\nnotably the Transformer, learn powerful context-aware word representations\nthrough layered, multi-headed attention. The multiple heads learn diverse types\nof word relationships. However, with standard softmax attention, all attention\nheads are dense, assigning a non-zero weight to all context words. In this\nwork, we introduce the adaptively sparse Transformer, wherein attention heads\nhave flexible, context-dependent sparsity patterns. This sparsity is\naccomplished by replacing softmax with \\(\\alpha\\)-entmax: a differentiable\ngeneralization of softmax that allows low-scoring words to receive precisely\nzero weight. Moreover, we derive a method to automatically learn the \\(\\alpha\\)\nparameter – which controls the shape and sparsity of \\(\\alpha\\)-entmax –\nallowing attention heads to choose between focused or spread-out behavior. Our\nadaptively sparse Transformer improves interpretability and head diversity when\ncompared to softmax Transformers on machine translation datasets. Findings of\nthe quantitative and qualitative analysis of our approach include that heads in\ndifferent layers learn different sparsity preferences and tend to be more\ndiverse in their attention distributions than softmax Transformers.\nFurthermore, at no cost in accuracy, sparsity in attention heads helps to\nuncover different head specializations.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "costa2017automatic", "citations": "86", "year": "2018", "title":"Automatic Generation Of Natural Language Explanations", "abstract": "<p>An important task for recommender system is to generate explanations\naccording to a user’s preferences. Most of the current methods for explainable\nrecommendations use structured sentences to provide descriptions along with the\nrecommendations they produce. However, those methods have neglected the\nreview-oriented way of writing a text, even though it is known that these\nreviews have a strong influence over user’s decision.\n  In this paper, we propose a method for the automatic generation of natural\nlanguage explanations, for predicting how a user would write about an item,\nbased on user ratings from different items’ features. We design a\ncharacter-level recurrent neural network (RNN) model, which generates an item’s\nreview explanations using long-short term memories (LSTM). The model generates\ntext reviews given a combination of the review and ratings score that express\nopinions about different factors or aspects of an item. Our network is trained\non a sub-sample from the large real-world dataset BeerAdvocate. Our empirical\nevaluation using natural language processing metrics shows the generated text’s\nquality is close to a real user written review, identifying negation,\nmisspellings, and domain specific vocabulary.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "costajussà2016character", "citations": "233", "year": "2016", "title":"Character-based Neural Machine Translation", "abstract": "<p>Neural Machine Translation (MT) has reached state-of-the-art results.\nHowever, one of the main challenges that neural MT still faces is dealing with\nvery large vocabularies and morphologically rich languages. In this paper, we\npropose a neural MT system using character-based embeddings in combination with\nconvolutional and highway layers to replace the standard lookup-based word\nrepresentations. The resulting unlimited-vocabulary and affix-aware source word\nembeddings are tested in a state-of-the-art neural MT based on an\nattention-based bidirectional recurrent neural network. The proposed MT scheme\nprovides improved results even when the source language is not morphologically\nrich. Improvements up to 3 BLEU points are obtained in the German-English WMT\ntask.</p>\n", "tags": ["Model Architecture"] },
{"key": "cotterell2017conll", "citations": "202", "year": "2017", "title":"Conll-sigmorphon 2017 Shared Task: Universal Morphological Reinflection In 52 Languages", "abstract": "<p>The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation\nrequired systems to be trained and tested in each of 52 typologically diverse\nlanguages. In sub-task 1, submitted systems were asked to predict a specific\ninflected form of a given lemma. In sub-task 2, systems were given a lemma and\nsome of its specific inflected forms, and asked to complete the inflectional\nparadigm by predicting all of the remaining inflected forms. Both sub-tasks\nincluded high, medium, and low-resource conditions. Sub-task 1 received 24\nsystem submissions, while sub-task 2 received 3 system submissions. Following\nthe success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared\ntask, all but one of the submissions included a neural component. The results\nshow that high performance can be achieved with small training datasets, so\nlong as models have appropriate inductive bias or make use of additional\nunlabeled data or synthetic data. However, different biasing and data\naugmentation resulted in disjoint sets of inflected forms being predicted\ncorrectly, suggesting that there is room for future improvement.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "cotterell2018are", "citations": "95", "year": "2018", "title":"Are All Languages Equally Hard To Language-model?", "abstract": "<p>For general modeling methods applied to diverse languages, a natural question\nis: how well should we expect our models to work on languages with differing\ntypological profiles? In this work, we develop an evaluation framework for fair\ncross-linguistic comparison of language models, using translated text so that\nall models are asked to predict approximately the same information. We then\nconduct a study on 21 languages, demonstrating that in some languages, the\ntextual expression of the information is harder to predict with both \\(n\\)-gram\nand LSTM language models. We show complex inflectional morphology to be a cause\nof performance differences among languages.</p>\n", "tags": ["Evaluation","Model Architecture","NAACL","Tools"] },
{"key": "coulombe2018text", "citations": "82", "year": "2018", "title":"Text Data Augmentation Made Simple By Leveraging NLP Cloud Apis", "abstract": "<p>In practice, it is common to find oneself with far too little text data to\ntrain a deep neural network. This “Big Data Wall” represents a challenge for\nminority language communities on the Internet, organizations, laboratories and\ncompanies that compete the GAFAM (Google, Amazon, Facebook, Apple, Microsoft).\nWhile most of the research effort in text data augmentation aims on the\nlong-term goal of finding end-to-end learning solutions, which is equivalent to\n“using neural networks to feed neural networks”, this engineering work focuses\non the use of practical, robust, scalable and easy-to-implement data\naugmentation pre-processing techniques similar to those that are successful in\ncomputer vision. Several text augmentation techniques have been experimented.\nSome existing ones have been tested for comparison purposes such as noise\ninjection or the use of regular expressions. Others are modified or improved\ntechniques like lexical replacement. Finally more innovative ones, such as the\ngeneration of paraphrases using back-translation or by the transformation of\nsyntactic trees, are based on robust, scalable, and easy-to-use NLP Cloud APIs.\nAll the text augmentation techniques studied, with an amplification factor of\nonly 5, increased the accuracy of the results in a range of 4.3% to 21.6%, with\nsignificant statistical fluctuations, on a standardized task of text polarity\nprediction. Some standard deep neural network architectures were tested: the\nmultilayer perceptron (MLP), the long short-term memory recurrent network\n(LSTM) and the bidirectional LSTM (biLSTM). Classical XGBoost algorithm has\nbeen tested with up to 2.5% improvements.</p>\n", "tags": ["Model Architecture"] },
{"key": "craswell2020overview", "citations": "90", "year": "2020", "title":"Overview Of The TREC 2019 Deep Learning Track", "abstract": "<p>The Deep Learning Track is a new track for TREC 2019, with the goal of\nstudying ad hoc ranking in a large data regime. It is the first track with\nlarge human-labeled training sets, introducing two sets corresponding to two\ntasks, each with rigorous TREC-style blind evaluation and reusable test sets.\nThe document retrieval task has a corpus of 3.2 million documents with 367\nthousand training queries, for which we generate a reusable test set of 43\nqueries. The passage retrieval task has a corpus of 8.8 million passages with\n503 thousand training queries, for which we generate a reusable test set of 43\nqueries. This year 15 groups submitted a total of 75 runs, using various\ncombinations of deep learning, transfer learning and traditional IR ranking\nmethods. Deep learning runs significantly outperformed traditional IR runs.\nPossible explanations for this result are that we introduced large training\ndata and we included deep models trained on such data in our judging pools,\nwhereas some past studies did not have such training data or pooling.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "craswell2021overview", "citations": "128", "year": "2021", "title":"Overview Of The TREC 2020 Deep Learning Track", "abstract": "<p>This is the second year of the TREC Deep Learning Track, with the goal of\nstudying ad hoc ranking in the large training data regime. We again have a\ndocument retrieval task and a passage retrieval task, each with hundreds of\nthousands of human-labeled training queries. We evaluate using single-shot\nTREC-style evaluation, to give us a picture of which ranking methods work best\nwhen large data is available, with much more comprehensive relevance labeling\non the small number of test queries. This year we have further evidence that\nrankers with BERT-style pretraining outperform other rankers in the large data\nregime.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "crego2016systran", "citations": "89", "year": "2016", "title":"Systran's Pure Neural Machine Translation Systems", "abstract": "<p>Since the first online demonstration of Neural Machine Translation (NMT) by\nLISA, NMT development has recently moved from laboratory to production systems\nas demonstrated by several entities announcing roll-out of NMT engines to\nreplace their existing technologies. NMT systems have a large number of\ntraining configurations and the training process of such systems is usually\nvery long, often a few weeks, so role of experimentation is critical and\nimportant to share. In this work, we present our approach to production-ready\nsystems simultaneously with release of online demonstrators covering a large\nvariety of languages (12 languages, for 32 language pairs). We explore\ndifferent practical choices: an efficient and evolutive open-source framework;\ndata preparation; network architecture; additional implemented features; tuning\nfor production; etc. We discuss about evaluation methodology, present our first\nfindings and we finally outline further work.\n  Our ultimate goal is to share our expertise to build competitive production\nsystems for “generic” translation. We aim at contributing to set up a\ncollaborative framework to speed-up adoption of the technology, foster further\nresearch efforts and enable the delivery and adoption to/by industry of\nuse-case specific engines integrated in real production workflows. Mastering of\nthe technology would allow us to build translation engines suited for\nparticular needs, outperforming current simplest/uniform systems.</p>\n", "tags": ["Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "crisan2022interactive", "citations": "63", "year": "2022", "title":"Interactive Model Cards: A Human-centered Approach To Model Documentation", "abstract": "<p>Deep learning models for natural language processing (NLP) are increasingly\nadopted and deployed by analysts without formal training in NLP or machine\nlearning (ML). However, the documentation intended to convey the model’s\ndetails and appropriate use is tailored primarily to individuals with ML or NLP\nexpertise. To address this gap, we conduct a design inquiry into interactive\nmodel cards, which augment traditionally static model cards with affordances\nfor exploring model documentation and interacting with the models themselves.\nOur investigation consists of an initial conceptual study with experts in ML,\nNLP, and AI Ethics, followed by a separate evaluative study with non-expert\nanalysts who use ML models in their work. Using a semi-structured interview\nformat coupled with a think-aloud protocol, we collected feedback from a total\nof 30 participants who engaged with different versions of standard and\ninteractive model cards. Through a thematic analysis of the collected data, we\nidentified several conceptual dimensions that summarize the strengths and\nlimitations of standard and interactive model cards, including: stakeholders;\ndesign; guidance; understandability &amp; interpretability; sensemaking &amp;\nskepticism; and trust &amp; safety. Our findings demonstrate the importance of\ncarefully considered design and interactivity for orienting and supporting\nnon-expert analysts using deep learning models, along with a need for\nconsideration of broader sociotechnical contexts and organizational dynamics.\nWe have also identified design elements, such as language, visual cues, and\nwarnings, among others, that support interactivity and make non-interactive\ncontent accessible. We summarize our findings as design guidelines and discuss\ntheir implications for a human-centered approach towards AI/ML documentation.</p>\n", "tags": ["Ethics & Fairness","Training Techniques"] },
{"key": "croitoru2021teachtext", "citations": "108", "year": "2021", "title":"TEACHTEXT: Crossmodal Generalized Distillation For Text-video Retrieval", "abstract": "<p>In recent years, considerable progress on the task of text-video retrieval\nhas been achieved by leveraging large-scale pretraining on visual and audio\ndatasets to construct powerful video encoders. By contrast, despite the natural\nsymmetry, the design of effective algorithms for exploiting large-scale\nlanguage pretraining remains under-explored. In this work, we are the first to\ninvestigate the design of such algorithms and propose a novel generalized\ndistillation method, TeachText, which leverages complementary cues from\nmultiple text encoders to provide an enhanced supervisory signal to the\nretrieval model. Moreover, we extend our method to video side modalities and\nshow that we can effectively reduce the number of used modalities at test time\nwithout compromising performance. Our approach advances the state of the art on\nseveral video retrieval benchmarks by a significant margin and adds no\ncomputational overhead at test time. Last but not least, we show an effective\napplication of our method for eliminating noise from retrieval datasets. Code\nand data can be found at https://www.robots.ox.ac.uk/~vgg/research/teachtext/.</p>\n", "tags": ["Datasets","Efficiency","ICCV"] },
{"key": "crothers2022machine", "citations": "88", "year": "2023", "title":"Machine Generated Text: A Comprehensive Survey Of Threat Models And Detection Methods", "abstract": "<p>Machine generated text is increasingly difficult to distinguish from human\nauthored text. Powerful open-source models are freely available, and\nuser-friendly tools that democratize access to generative models are\nproliferating. ChatGPT, which was released shortly after the first edition of\nthis survey, epitomizes these trends. The great potential of state-of-the-art\nnatural language generation (NLG) systems is tempered by the multitude of\navenues for abuse. Detection of machine generated text is a key countermeasure\nfor reducing abuse of NLG models, with significant technical challenges and\nnumerous open problems. We provide a survey that includes both 1) an extensive\nanalysis of threat models posed by contemporary NLG systems, and 2) the most\ncomplete review of machine generated text detection methods to date. This\nsurvey places machine generated text within its cybersecurity and social\ncontext, and provides strong guidance for future work addressing the most\ncritical threat models, and ensuring detection systems themselves demonstrate\ntrustworthiness through fairness, robustness, and accountability.</p>\n", "tags": ["Ethics & Fairness","Security","Survey Paper","Tools"] },
{"key": "crowley2017moonshine", "citations": "75", "year": "2018", "title":"Moonshine: Distilling With Cheap Convolutions", "abstract": "<p>Many engineers wish to deploy modern neural networks in memory-limited\nsettings; but the development of flexible methods for reducing memory use is in\nits infancy, and there is little knowledge of the resulting cost-benefit. We\npropose structural model distillation for memory reduction using a strategy\nthat produces a student architecture that is a simple transformation of the\nteacher architecture: no redesign is needed, and the same hyperparameters can\nbe used. Using attention transfer, we provide Pareto curves/tables for\ndistillation of residual networks with four benchmark datasets, indicating the\nmemory versus accuracy payoff. We show that substantial memory savings are\npossible with very little loss of accuracy, and confirm that distillation\nprovides student network performance that is better than training that student\narchitecture directly on data.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Model Architecture","Training Techniques"] },
{"key": "crowson2022vqgan", "citations": "199", "year": "2022", "title":"VQGAN-CLIP: Open Domain Image Generation And Editing With Natural Language Guidance", "abstract": "<p>Generating and editing images from open domain text prompts is a challenging\ntask that heretofore has required expensive and specially trained models. We\ndemonstrate a novel methodology for both tasks which is capable of producing\nimages of high visual quality from text prompts of significant semantic\ncomplexity without any training by using a multimodal encoder to guide image\ngenerations. We demonstrate on a variety of tasks how using CLIP [37] to guide\nVQGAN [11] produces higher visual quality outputs than prior, less flexible\napproaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being\ntrained for the tasks presented. Our code is available in a public repository.</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "csaky2019improving", "citations": "64", "year": "2019", "title":"Improving Neural Conversational Models With Entropy-based Data Filtering", "abstract": "<p>Current neural network-based conversational models lack diversity and\ngenerate boring responses to open-ended utterances. Priors such as persona,\nemotion, or topic provide additional information to dialog models to aid\nresponse generation, but annotating a dataset with priors is expensive and such\nannotations are rarely available. While previous methods for improving the\nquality of open-domain response generation focused on either the underlying\nmodel or the training objective, we present a method of filtering dialog\ndatasets by removing generic utterances from training data using a simple\nentropy-based approach that does not require human supervision. We conduct\nextensive experiments with different variations of our method, and compare\ndialog models across 17 evaluation metrics to show that training on datasets\nfiltered this way results in better conversational quality as chatbots learn to\noutput more diverse responses.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "cuayáhuitl2016simpleds", "citations": "70", "year": "2016", "title":"Simpleds: A Simple Deep Reinforcement Learning Dialogue System", "abstract": "<p>This paper presents ‘SimpleDS’, a simple and publicly available dialogue\nsystem trained with deep reinforcement learning. In contrast to previous\nreinforcement learning dialogue systems, this system avoids manual feature\nengineering by performing action selection directly from raw text of the last\nsystem and (noisy) user responses. Our initial results, in the restaurant\ndomain, show that it is indeed possible to induce reasonable dialogue behaviour\nwith an approach that aims for high levels of automation in dialogue control\nfor intelligent interactive agents.</p>\n", "tags": ["Agentic","Dialogue & Multi Turn","Reinforcement Learning"] },
{"key": "cuayáhuitl2019ensemble", "citations": "74", "year": "2019", "title":"Ensemble-based Deep Reinforcement Learning For Chatbots", "abstract": "<p>Trainable chatbots that exhibit fluent and human-like conversations remain a\nbig challenge in artificial intelligence. Deep Reinforcement Learning (DRL) is\npromising for addressing this challenge, but its successful application remains\nan open question. This article describes a novel ensemble-based approach\napplied to value-based DRL chatbots, which use finite action sets as a form of\nmeaning representation. In our approach, while dialogue actions are derived\nfrom sentence clustering, the training datasets in our ensemble are derived\nfrom dialogue clustering. The latter aim to induce specialised agents that\nlearn to interact in a particular style. In order to facilitate neural chatbot\ntraining using our proposed approach, we assume dialogue data in raw text only\n– without any manually-labelled data. Experimental results using chitchat data\nreveal that (1) near human-like dialogue policies can be induced, (2)\ngeneralisation to unseen data is a difficult problem, and (3) training an\nensemble of chatbot agents is essential for improved performance over using a\nsingle agent. In addition to evaluations using held-out data, our results are\nfurther supported by a human evaluation that rated dialogues in terms of\nfluency, engagingness and consistency – which revealed that our proposed\ndialogue rewards strongly correlate with human judgements.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "cui2016attention", "citations": "404", "year": "2017", "title":"Attention-over-attention Neural Networks For Reading Comprehension", "abstract": "<p>Cloze-style queries are representative problems in reading comprehension.\nOver the past few months, we have seen much progress that utilizing neural\nnetwork approach to solve Cloze-style questions. In this paper, we present a\nnovel model called attention-over-attention reader for the Cloze-style reading\ncomprehension task. Our model aims to place another attention mechanism over\nthe document-level attention, and induces “attended attention” for final\npredictions. Unlike the previous works, our neural network model requires less\npre-defined hyper-parameters and uses an elegant architecture for modeling.\nExperimental results show that the proposed attention-over-attention model\nsignificantly outperforms various state-of-the-art systems by a large margin in\npublic datasets, such as CNN and Children’s Book Test datasets.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "cui2018span", "citations": "164", "year": "2019", "title":"A Span-extraction Dataset For Chinese Machine Reading Comprehension", "abstract": "<p>Machine Reading Comprehension (MRC) has become enormously popular recently\nand has attracted a lot of attention. However, the existing reading\ncomprehension datasets are mostly in English. In this paper, we introduce a\nSpan-Extraction dataset for Chinese machine reading comprehension to add\nlanguage diversities in this area. The dataset is composed by near 20,000 real\nquestions annotated on Wikipedia paragraphs by human experts. We also annotated\na challenge set which contains the questions that need comprehensive\nunderstanding and multi-sentence inference throughout the context. We present\nseveral baseline systems as well as anonymous submissions for demonstrating the\ndifficulties in this dataset. With the release of the dataset, we hosted the\nSecond Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC\n2018). We hope the release of the dataset could further accelerate the Chinese\nmachine reading comprehension research. Resources are available:\nhttps://github.com/ymcui/cmrc2018</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "cui2019pre", "citations": "488", "year": "2021", "title":"Pre-training With Whole Word Masking For Chinese BERT", "abstract": "<p>Bidirectional Encoder Representations from Transformers (BERT) has shown\nmarvelous improvements across various NLP tasks, and its consecutive variants\nhave been proposed to further improve the performance of the pre-trained\nlanguage models. In this paper, we aim to first introduce the whole word\nmasking (wwm) strategy for Chinese BERT, along with a series of Chinese\npre-trained language models. Then we also propose a simple but effective model\ncalled MacBERT, which improves upon RoBERTa in several ways. Especially, we\npropose a new masking strategy called MLM as correction (Mac). To demonstrate\nthe effectiveness of these models, we create a series of Chinese pre-trained\nlanguage models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc.\nWe carried out extensive experiments on ten Chinese NLP tasks to evaluate the\ncreated Chinese pre-trained language models as well as the proposed MacBERT.\nExperimental results show that MacBERT could achieve state-of-the-art\nperformances on many NLP tasks, and we also ablate details with several\nfindings that may help future research. We open-source our pre-trained language\nmodels for further facilitating our research community. Resources are\navailable: https://github.com/ymcui/Chinese-BERT-wwm</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "cui2020mutual", "citations": "111", "year": "2020", "title":"Mutual: A Dataset For Multi-turn Dialogue Reasoning", "abstract": "<p>Non-task oriented dialogue systems have achieved great success in recent\nyears due to largely accessible conversation data and the development of deep\nlearning techniques. Given a context, current systems are able to yield a\nrelevant and fluent response, but sometimes make logical mistakes because of\nweak reasoning capabilities. To facilitate the conversation reasoning research,\nwe introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning,\nconsisting of 8,860 manually annotated dialogues based on Chinese student\nEnglish listening comprehension exams. Compared to previous benchmarks for\nnon-task oriented dialogue systems, MuTual is much more challenging since it\nrequires a model that can handle various reasoning problems. Empirical results\nshow that state-of-the-art methods only reach 71%, which is far behind the\nhuman performance of 94%, indicating that there is ample room for improving\nreasoning ability. MuTual is available at https://github.com/Nealcly/MuTual.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Has Code"] },
{"key": "cui2020revisiting", "citations": "634", "year": "2020", "title":"Revisiting Pre-trained Models For Chinese Natural Language Processing", "abstract": "<p>Bidirectional Encoder Representations from Transformers (BERT) has shown\nmarvelous improvements across various NLP tasks, and consecutive variants have\nbeen proposed to further improve the performance of the pre-trained language\nmodels. In this paper, we target on revisiting Chinese pre-trained language\nmodels to examine their effectiveness in a non-English language and release the\nChinese pre-trained language model series to the community. We also propose a\nsimple but effective model called MacBERT, which improves upon RoBERTa in\nseveral ways, especially the masking strategy that adopts MLM as correction\n(Mac). We carried out extensive experiments on eight Chinese NLP tasks to\nrevisit the existing pre-trained language models as well as the proposed\nMacBERT. Experimental results show that MacBERT could achieve state-of-the-art\nperformances on many NLP tasks, and we also ablate details with several\nfindings that may help future research. Resources available:\nhttps://github.com/ymcui/MacBERT</p>\n", "tags": ["EMNLP","Has Code","Model Architecture"] },
{"key": "cui2022prototypical", "citations": "60", "year": "2022", "title":"Prototypical Verbalizer For Prompt-based Few-shot Tuning", "abstract": "<p>Prompt-based tuning for pre-trained language models (PLMs) has shown its\neffectiveness in few-shot learning. Typically, prompt-based tuning wraps the\ninput text into a cloze question. To make predictions, the model maps the\noutput words to labels via a verbalizer, which is either manually designed or\nautomatically built. However, manual verbalizers heavily depend on\ndomain-specific prior knowledge and human efforts, while finding appropriate\nlabel words automatically still remains challenging.In this work, we propose\nthe prototypical verbalizer (ProtoVerb) which is built directly from training\ndata. Specifically, ProtoVerb learns prototype vectors as verbalizers by\ncontrastive learning. In this way, the prototypes summarize training instances\nand are able to enclose rich class-level semantics. We conduct experiments on\nboth topic classification and entity typing tasks, and the results demonstrate\nthat ProtoVerb significantly outperforms current automatic verbalizers,\nespecially when training data is extremely scarce. More surprisingly, ProtoVerb\nconsistently boosts prompt-based tuning even on untuned PLMs, indicating an\nelegant non-tuning way to utilize PLMs. Our codes are avaliable at\nhttps://github.com/thunlp/OpenPrompt.</p>\n", "tags": ["Few-Shot","Fine-Tuning","Has Code","Prompting","Training Techniques"] },
{"key": "cui2023survey", "citations": "103", "year": "2024", "title":"A Survey On Multimodal Large Language Models For Autonomous Driving", "abstract": "<p>With the emergence of Large Language Models (LLMs) and Vision Foundation\nModels (VFMs), multimodal AI systems benefiting from large models have the\npotential to equally perceive the real world, make decisions, and control tools\nas humans. In recent months, LLMs have shown widespread attention in autonomous\ndriving and map systems. Despite its immense potential, there is still a lack\nof a comprehensive understanding of key challenges, opportunities, and future\nendeavors to apply in LLM driving systems. In this paper, we present a\nsystematic investigation in this field. We first introduce the background of\nMultimodal Large Language Models (MLLMs), the multimodal models development\nusing LLMs, and the history of autonomous driving. Then, we overview existing\nMLLM tools for driving, transportation, and map systems together with existing\ndatasets and benchmarks. Moreover, we summarized the works in The 1st WACV\nWorkshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD),\nwhich is the first workshop of its kind regarding LLMs in autonomous driving.\nTo further promote the development of this field, we also discuss several\nimportant problems regarding using MLLMs in autonomous driving systems that\nneed to be solved by both academia and industry.</p>\n", "tags": ["Applications","Datasets","Model Architecture","Survey Paper","Tools"] },
{"key": "côté2018textworld", "citations": "138", "year": "2019", "title":"Textworld: A Learning Environment For Text-based Games", "abstract": "<p>We introduce TextWorld, a sandbox learning environment for the training and\nevaluation of RL agents on text-based games. TextWorld is a Python library that\nhandles interactive play-through of text games, as well as backend functions\nlike state tracking and reward assignment. It comes with a curated list of\ngames whose features and challenges we have analyzed. More significantly, it\nenables users to handcraft or automatically generate new games. Its generative\nmechanisms give precise control over the difficulty, scope, and language of\nconstructed games, and can be used to relax challenges inherent to commercial\ntext games like partial observability and sparse rewards. By generating sets of\nvaried but similar games, TextWorld can also be used to study generalization\nand transfer learning. We cast text-based games in the Reinforcement Learning\nformalism, use our framework to develop a set of benchmark games, and evaluate\nseveral baseline agents on this set and the curated list.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Fine-Tuning","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "dai2019deeper", "citations": "233", "year": "2019", "title":"Deeper Text Understanding For IR With Contextual Neural Language Modeling", "abstract": "<p>Neural networks provide new possibilities to automatically learn complex\nlanguage patterns and query-document relations. Neural IR models have achieved\npromising results in learning query-document relevance patterns, but few\nexplorations have been done on understanding the text content of a query or a\ndocument. This paper studies leveraging a recently-proposed contextual neural\nlanguage model, BERT, to provide deeper text understanding for IR. Experimental\nresults demonstrate that the contextual text representations from BERT are more\neffective than traditional word embeddings. Compared to bag-of-words retrieval\nmodels, the contextual language model can better leverage language structures,\nbringing large improvements on queries written in natural languages. Combining\nthe text understanding ability with search knowledge leads to an enhanced\npre-trained BERT model that can benefit related search tasks where training\ndata are limited.</p>\n", "tags": ["Model Architecture","SIGIR","Training Techniques"] },
{"key": "dai2019transformer", "citations": "2932", "year": "2019", "title":"Transformer-xl: Attentive Language Models Beyond A Fixed-length Context", "abstract": "<p>Transformers have a potential of learning longer-term dependency, but are\nlimited by a fixed-length context in the setting of language modeling. We\npropose a novel neural architecture Transformer-XL that enables learning\ndependency beyond a fixed length without disrupting temporal coherence. It\nconsists of a segment-level recurrence mechanism and a novel positional\nencoding scheme. Our method not only enables capturing longer-term dependency,\nbut also resolves the context fragmentation problem. As a result,\nTransformer-XL learns dependency that is 80% longer than RNNs and 450% longer\nthan vanilla Transformers, achieves better performance on both short and long\nsequences, and is up to 1,800+ times faster than vanilla Transformers during\nevaluation. Notably, we improve the state-of-the-art results of bpc/perplexity\nto 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion\nWord, and 54.5 on Penn Treebank (without finetuning). When trained only on\nWikiText-103, Transformer-XL manages to generate reasonably coherent, novel\ntext articles with thousands of tokens. Our code, pretrained models, and\nhyperparameters are available in both Tensorflow and PyTorch.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "dai2020funnel", "citations": "104", "year": "2020", "title":"Funnel-transformer: Filtering Out Sequential Redundancy For Efficient Language Processing", "abstract": "<p>With the success of language pretraining, it is highly desirable to develop\nmore efficient architectures of good scalability that can exploit the abundant\nunlabeled data at a lower cost. To improve the efficiency, we examine the\nmuch-overlooked redundancy in maintaining a full-length token-level\npresentation, especially for tasks that only require a single-vector\npresentation of the sequence. With this intuition, we propose\nFunnel-Transformer which gradually compresses the sequence of hidden states to\na shorter one and hence reduces the computation cost. More importantly, by\nre-investing the saved FLOPs from length reduction in constructing a deeper or\nwider model, we further improve the model capacity. In addition, to perform\ntoken-level predictions as required by common pretraining objectives,\nFunnel-Transformer is able to recover a deep representation for each token from\nthe reduced hidden sequence via a decoder. Empirically, with comparable or\nfewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide\nvariety of sequence-level prediction tasks, including text classification,\nlanguage understanding, and reading comprehension. The code and pretrained\ncheckpoints are available at https://github.com/laiguokun/Funnel-Transformer.</p>\n", "tags": ["Efficiency","Has Code","Model Architecture"] },
{"key": "dai2021knowledge", "citations": "83", "year": "2022", "title":"Knowledge Neurons In Pretrained Transformers", "abstract": "<p>Large-scale pretrained language models are surprisingly good at recalling\nfactual knowledge presented in the training corpus. In this paper, we present\npreliminary studies on how factual knowledge is stored in pretrained\nTransformers by introducing the concept of knowledge neurons. Specifically, we\nexamine the fill-in-the-blank cloze task for BERT. Given a relational fact, we\npropose a knowledge attribution method to identify the neurons that express the\nfact. We find that the activation of such knowledge neurons is positively\ncorrelated to the expression of their corresponding facts. In our case studies,\nwe attempt to leverage knowledge neurons to edit (such as update, and erase)\nspecific factual knowledge without fine-tuning. Our results shed light on\nunderstanding the storage of knowledge within pretrained Transformers. The code\nis available at https://github.com/Hunter-DDM/knowledge-neurons.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "dai2023auggpt", "citations": "84", "year": "2023", "title":"Auggpt: Leveraging Chatgpt For Text Data Augmentation", "abstract": "<p>Text data augmentation is an effective strategy for overcoming the challenge\nof limited sample sizes in many natural language processing (NLP) tasks. This\nchallenge is especially prominent in the few-shot learning scenario, where the\ndata in the target domain is generally much scarcer and of lowered quality. A\nnatural and widely-used strategy to mitigate such challenges is to perform data\naugmentation to better capture the data invariance and increase the sample\nsize. However, current text data augmentation methods either can’t ensure the\ncorrect labeling of the generated data (lacking faithfulness) or can’t ensure\nsufficient diversity in the generated data (lacking compactness), or both.\nInspired by the recent success of large language models, especially the\ndevelopment of ChatGPT, which demonstrated improved language comprehension\nabilities, in this work, we propose a text data augmentation approach based on\nChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples\ninto multiple conceptually similar but semantically different samples. The\naugmented samples can then be used in downstream model training. Experiment\nresults on few-shot learning text classification tasks show the superior\nperformance of the proposed AugGPT approach over state-of-the-art text data\naugmentation methods in terms of testing accuracy and distribution of the\naugmented samples.</p>\n", "tags": ["Few-Shot","Training Techniques"] },
{"key": "dai2023instructblip", "citations": "230", "year": "2023", "title":"Instructblip: Towards General-purpose Vision-language Models With Instruction Tuning", "abstract": "<p>Large-scale pre-training and instruction tuning have been successful at\ncreating general-purpose language models with broad competence. However,\nbuilding general-purpose vision-language models is challenging due to the rich\ninput distributions and task diversity resulting from the additional visual\ninput. Although vision-language pretraining has been widely studied,\nvision-language instruction tuning remains under-explored. In this paper, we\nconduct a systematic and comprehensive study on vision-language instruction\ntuning based on the pretrained BLIP-2 models. We gather 26 publicly available\ndatasets, covering a wide variety of tasks and capabilities, and transform them\ninto instruction tuning format. Additionally, we introduce an instruction-aware\nQuery Transformer, which extracts informative features tailored to the given\ninstruction. Trained on 13 held-in datasets, InstructBLIP attains\nstate-of-the-art zero-shot performance across all 13 held-out datasets,\nsubstantially outperforming BLIP-2 and larger Flamingo models. Our models also\nlead to state-of-the-art performance when finetuned on individual downstream\ntasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts).\nFurthermore, we qualitatively demonstrate the advantages of InstructBLIP over\nconcurrent multimodal models. All InstructBLIP models are open-sourced at\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "dalvi2018incremental", "citations": "108", "year": "2018", "title":"Incremental Decoding And Training Methods For Simultaneous Translation In Neural Machine Translation", "abstract": "<p>We address the problem of simultaneous translation by modifying the Neural MT\ndecoder to operate with dynamically built encoder and attention. We propose a\ntunable agent which decides the best segmentation strategy for a user-defined\nBLEU loss and Average Proportion (AP) constraint. Our agent outperforms\npreviously proposed Wait-if-diff and Wait-if-worse agents (Cho and Esipova,\n2016) on BLEU with a lower latency. Secondly we proposed data-driven changes to\nNeural MT training to better match the incremental decoding framework.</p>\n", "tags": ["Model Architecture","NAACL","Training Techniques"] },
{"key": "dalvi2018what", "citations": "137", "year": "2019", "title":"What Is One Grain Of Sand In The Desert? Analyzing Individual Neurons In Deep NLP Models", "abstract": "<p>Despite the remarkable evolution of deep neural networks in natural language\nprocessing (NLP), their interpretability remains a challenge. Previous work\nlargely focused on what these models learn at the representation level. We\nbreak this analysis down further and study individual dimensions (neurons) in\nthe vector representation learned by end-to-end neural models in NLP tasks. We\npropose two methods: Linguistic Correlation Analysis, based on a supervised\nmethod to extract the most relevant neurons with respect to an extrinsic task,\nand Cross-model Correlation Analysis, an unsupervised method to extract salient\nneurons w.r.t. the model itself. We evaluate the effectiveness of our\ntechniques by ablating the identified neurons and reevaluating the network’s\nperformance for two tasks: neural machine translation (NMT) and neural language\nmodeling (NLM). We further present a comprehensive analysis of neurons with the\naim to address the following questions: i) how localized or distributed are\ndifferent linguistic properties in the models? ii) are certain neurons\nexclusive to some properties and not others? iii) is the information more or\nless distributed in NMT vs. NLM? and iv) how important are the neurons\nidentified through the linguistic correlation method to the overall task? Our\ncode is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019).</p>\n", "tags": ["AAAI"] },
{"key": "dalvi2021explaining", "citations": "66", "year": "2021", "title":"Explaining Answers With Entailment Trees", "abstract": "<p>Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by showing the line of reasoning from what is known to the\nanswer, rather than simply showing a fragment of textual evidence (a\n“rationale’”). If this could be done, new opportunities for understanding and\ndebugging the system’s reasoning become possible. Our approach is to generate\nexplanations in the form of entailment trees, namely a tree of multipremise\nentailment steps from facts that are known, through intermediate conclusions,\nto the hypothesis of interest (namely the question + answer). To train a model\nwith this skill, we created ENTAILMENTBANK, the first dataset to contain\nmultistep entailment trees. Given a hypothesis (question + answer), we define\nthree increasingly difficult explanation tasks: generate a valid entailment\ntree given (a) all relevant sentences (b) all relevant and some irrelevant\nsentences, or (c) a corpus. We show that a strong language model can partially\nsolve these tasks, in particular when the relevant sentences are included in\nthe input (e.g., 35% of trees for (a) are perfect), and with indications of\ngeneralization to other domains. This work is significant as it provides a new\ntype of dataset (multistep entailments) and baselines, offering a new avenue\nfor the community to generate richer, more systematic explanations.</p>\n", "tags": ["EMNLP"] },
{"key": "dam2016deep", "citations": "74", "year": "2016", "title":"A Deep Language Model For Software Code", "abstract": "<p>Existing language models such as n-grams for software code often fail to\ncapture a long context where dependent code elements scatter far apart. In this\npaper, we propose a novel approach to build a language model for software code\nto address this particular issue. Our language model, partly inspired by human\nmemory, is built upon the powerful deep learning-based Long Short Term Memory\narchitecture that is capable of learning long-term dependencies which occur\nfrequently in software code. Results from our intrinsic evaluation on a corpus\nof Java projects have demonstrated the effectiveness of our language model.\nThis work contributes to realizing our vision for DeepSoft, an end-to-end,\ngeneric deep learning-based framework for modeling software and its development\nprocess.</p>\n", "tags": ["Datasets","Evaluation","Memory & Context","Model Architecture","Tools"] },
{"key": "damour2020underspecification", "citations": "375", "year": "2020", "title":"Underspecification Presents Challenges For Credibility In Modern Machine Learning", "abstract": "<p>ML models often exhibit unexpectedly poor behavior when they are deployed in\nreal-world domains. We identify underspecification as a key reason for these\nfailures. An ML pipeline is underspecified when it can return many predictors\nwith equivalently strong held-out performance in the training domain.\nUnderspecification is common in modern ML pipelines, such as those based on\ndeep learning. Predictors returned by underspecified pipelines are often\ntreated as equivalent based on their training domain performance, but we show\nhere that such predictors can behave very differently in deployment domains.\nThis ambiguity can lead to instability and poor model behavior in practice, and\nis a distinct failure mode from previously identified issues arising from\nstructural mismatch between training and deployment domains. We show that this\nproblem appears in a wide variety of practical ML pipelines, using examples\nfrom computer vision, medical imaging, natural language processing, clinical\nrisk prediction based on electronic health records, and medical genomics. Our\nresults show the need to explicitly account for underspecification in modeling\npipelines that are intended for real-world deployment in any domain.</p>\n", "tags": ["Training Techniques"] },
{"key": "dang2022beyond", "citations": "60", "year": "2022", "title":"Beyond Text Generation: Supporting Writers With Continuous Automatic Text Summaries", "abstract": "<p>We propose a text editor to help users plan, structure and reflect on their\nwriting process. It provides continuously updated paragraph-wise summaries as\nmargin annotations, using automatic text summarization. Summary levels range\nfrom full text, to selected (central) sentences, down to a collection of\nkeywords. To understand how users interact with this system during writing, we\nconducted two user studies (N=4 and N=8) in which people wrote analytic essays\nabout a given topic and article. As a key finding, the summaries gave users an\nexternal perspective on their writing and helped them to revise the content and\nscope of their drafted paragraphs. People further used the tool to quickly gain\nan overview of the text and developed strategies to integrate insights from the\nautomated summaries. More broadly, this work explores and highlights the value\nof designing AI tools for writers, with Natural Language Processing (NLP)\ncapabilities that go beyond direct text generation and correction.</p>\n", "tags": ["Tools"] },
{"key": "danilevsky2020survey", "citations": "116", "year": "2020", "title":"A Survey Of The State Of Explainable AI For Natural Language Processing", "abstract": "<p>Recent years have seen important advances in the quality of state-of-the-art\nmodels, but this has come at the expense of models becoming less interpretable.\nThis survey presents an overview of the current state of Explainable AI (XAI),\nconsidered within the domain of Natural Language Processing (NLP). We discuss\nthe main categorization of explanations, as well as the various ways\nexplanations can be arrived at and visualized. We detail the operations and\nexplainability techniques currently available for generating explanations for\nNLP model predictions, to serve as a resource for model developers in the\ncommunity. Finally, we point out the current gaps and encourage directions for\nfuture work in this important research area.</p>\n", "tags": ["Survey Paper"] },
{"key": "daniluk2017frustratingly", "citations": "80", "year": "2017", "title":"Frustratingly Short Attention Spans In Neural Language Modeling", "abstract": "<p>Neural language models predict the next token using a latent representation\nof the immediate token history. Recently, various methods for augmenting neural\nlanguage models with an attention mechanism over a differentiable memory have\nbeen proposed. For predicting the next token, these models query information\nfrom a memory of the recent history which can facilitate learning mid- and\nlong-range dependencies. However, conventional attention mechanisms used in\nmemory-augmented neural language models produce a single output vector per time\nstep. This vector is used both for predicting the next token as well as for the\nkey and value of a differentiable memory of a token history. In this paper, we\npropose a neural language model with a key-value attention mechanism that\noutputs separate representations for the key and value of a differentiable\nmemory, as well as for encoding the next-word distribution. This model\noutperforms existing memory-augmented neural language models on two corpora.\nYet, we found that our method mainly utilizes a memory of the five most recent\noutput representations. This led to the unexpected main finding that a much\nsimpler model based only on the concatenation of recent output representations\nfrom previous time steps is on par with more sophisticated memory-augmented\nneural language models.</p>\n", "tags": ["Model Architecture"] },
{"key": "danny2023palm", "citations": "299", "year": "2023", "title":"Palm-e: An Embodied Multimodal Language Model", "abstract": "<p>Large language models excel at a wide range of complex tasks. However,\nenabling general inference in the real world, e.g., for robotics problems,\nraises the challenge of grounding. We propose embodied language models to\ndirectly incorporate real-world continuous sensor modalities into language\nmodels and thereby establish the link between words and percepts. Input to our\nembodied language model are multi-modal sentences that interleave visual,\ncontinuous state estimation, and textual input encodings. We train these\nencodings end-to-end, in conjunction with a pre-trained large language model,\nfor multiple embodied tasks including sequential robotic manipulation planning,\nvisual question answering, and captioning. Our evaluations show that PaLM-E, a\nsingle large embodied multimodal model, can address a variety of embodied\nreasoning tasks, from a variety of observation modalities, on multiple\nembodiments, and further, exhibits positive transfer: the model benefits from\ndiverse joint training across internet-scale language, vision, and\nvisual-language domains. Our largest model, PaLM-E-562B with 562B parameters,\nin addition to being trained on robotics tasks, is a visual-language generalist\nwith state-of-the-art performance on OK-VQA, and retains generalist language\ncapabilities with increasing scale.</p>\n", "tags": ["Training Techniques"] },
{"key": "dao2022flashattention", "citations": "295", "year": "2022", "title":"Flashattention: Fast And Memory-efficient Exact Attention With Io-awareness", "abstract": "<p>Transformers are slow and memory-hungry on long sequences, since the time and\nmemory complexity of self-attention are quadratic in sequence length.\nApproximate attention methods have attempted to address this problem by trading\noff model quality to reduce the compute complexity, but often do not achieve\nwall-clock speedup. We argue that a missing principle is making attention\nalgorithms IO-aware – accounting for reads and writes between levels of GPU\nmemory. We propose FlashAttention, an IO-aware exact attention algorithm that\nuses tiling to reduce the number of memory reads/writes between GPU high\nbandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of\nFlashAttention, showing that it requires fewer HBM accesses than standard\nattention, and is optimal for a range of SRAM sizes. We also extend\nFlashAttention to block-sparse attention, yielding an approximate attention\nalgorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15%\nend-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the\nMLPerf 1.1 training speed record, 3\\(\\times\\) speedup on GPT-2 (seq. length 1K),\nand 2.4\\(\\times\\) speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding\nhigher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on\nlong-document classification) and entirely new capabilities: the first\nTransformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%\naccuracy).</p>\n", "tags": ["Efficiency","Model Architecture","Tools","Training Techniques"] },
{"key": "dao2023flashattention", "citations": "85", "year": "2023", "title":"Flashattention-2: Faster Attention With Better Parallelism And Work Partitioning", "abstract": "<p>Scaling Transformers to longer sequence lengths has been a major problem in\nthe last several years, promising to improve performance in language modeling\nand high-resolution image understanding, as well as to unlock new applications\nin code, audio, and video generation. The attention layer is the main\nbottleneck in scaling to longer sequences, as its runtime and memory increase\nquadratically in the sequence length. FlashAttention exploits the asymmetric\nGPU memory hierarchy to bring significant memory saving (linear instead of\nquadratic) and runtime speedup (2-4\\(\\times\\) compared to optimized baselines),\nwith no approximation. However, FlashAttention is still not nearly as fast as\noptimized matrix-multiply (GEMM) operations, reaching only 25-40% of the\ntheoretical maximum FLOPs/s. We observe that the inefficiency is due to\nsuboptimal work partitioning between different thread blocks and warps on the\nGPU, causing either low-occupancy or unnecessary shared memory reads/writes. We\npropose FlashAttention-2, with better work partitioning to address these\nissues. In particular, we (1) tweak the algorithm to reduce the number of\nnon-matmul FLOPs (2) parallelize the attention computation, even for a single\nhead, across different thread blocks to increase occupancy, and (3) within each\nthread block, distribute the work between warps to reduce communication through\nshared memory. These yield around 2\\(\\times\\) speedup compared to FlashAttention,\nreaching 50-73% of the theoretical maximum FLOPs/s on A100 and getting close\nto the efficiency of GEMM operations. We empirically validate that when used\nend-to-end to train GPT-style models, FlashAttention-2 reaches training speed\nof up to 225 TFLOPs/s per A100 GPU (72% model FLOPs utilization).</p>\n", "tags": ["Applications","Efficiency","Model Architecture","Training Techniques"] },
{"key": "das2016visual", "citations": "552", "year": "2017", "title":"Visual Dialog", "abstract": "<p>We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders – Late Fusion, Hierarchical Recurrent Encoder and Memory Network –\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first ‘visual\nchatbot’! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org</p>\n", "tags": ["Agentic","Datasets","Evaluation"] },
{"key": "das2017embodied", "citations": "371", "year": "2018", "title":"Embodied Question Answering", "abstract": "<p>We present a new AI task – Embodied Question Answering (EmbodiedQA) – where\nan agent is spawned at a random location in a 3D environment and asked a\nquestion (“What color is the car?”). In order to answer, the agent must first\nintelligently navigate to explore the environment, gather information through\nfirst-person (egocentric) vision, and then answer the question (“orange”).\n  This challenging task requires a range of AI skills – active perception,\nlanguage understanding, goal-driven navigation, commonsense reasoning, and\ngrounding of language into actions. In this work, we develop the environments,\nend-to-end-trained reinforcement learning agents, and evaluation protocols for\nEmbodiedQA.</p>\n", "tags": ["Agentic","CVPR","Evaluation","Reinforcement Learning"] },
{"key": "das2017learning", "citations": "327", "year": "2017", "title":"Learning Cooperative Visual Dialog Agents With Deep Reinforcement Learning", "abstract": "<p>We introduce the first goal-driven training for visual question answering and\ndialog agents. Specifically, we pose a cooperative ‘image guessing’ game\nbetween two agents – Qbot and Abot – who communicate in natural language\ndialog so that Qbot can select an unseen image from a lineup of images. We use\ndeep reinforcement learning (RL) to learn the policies of these agents\nend-to-end – from pixels to multi-agent multi-round dialog to game reward.\n  We demonstrate two experimental results.\n  First, as a ‘sanity check’ demonstration of pure RL (from scratch), we show\nresults on a synthetic world, where the agents communicate in ungrounded\nvocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find\nthat two bots invent their own communication protocol and start using certain\nsymbols to ask/answer about certain visual attributes (shape/color/style).\nThus, we demonstrate the emergence of grounded language and communication among\n‘visual’ dialog agents with no human supervision.\n  Second, we conduct large-scale real-image experiments on the VisDial dataset,\nwhere we pretrain with supervised dialog data and show that the RL ‘fine-tuned’\nagents significantly outperform SL agents. Interestingly, the RL Qbot learns to\nask questions that Abot is good at, ultimately resulting in more informative\ndialog and a better team.</p>\n", "tags": ["Dialogue & Multi Turn","ICCV","Reinforcement Learning","Training Techniques"] },
{"key": "das2017question", "citations": "143", "year": "2017", "title":"Question Answering On Knowledge Bases And Text Using Universal Schema And Memory Networks", "abstract": "<p>Existing question answering methods infer answers either from a knowledge\nbase or from raw text. While knowledge base (KB) methods are good at answering\ncompositional questions, their performance is often affected by the\nincompleteness of the KB. Au contraire, web text contains millions of facts\nthat are absent in the KB, however in an unstructured form. {\\it Universal\nschema} can support reasoning on the union of both structured KBs and\nunstructured text by aligning them in a common embedded space. In this paper we\nextend universal schema to natural language question answering, employing\n<em>memory networks</em> to attend to the large body of facts in the combination\nof text and KB. Our models can be trained in an end-to-end fashion on\nquestion-answer pairs. Evaluation results on \\spades fill-in-the-blank question\nanswering dataset show that exploiting universal schema for question answering\nis better than using either a KB or text alone. This model also outperforms the\ncurrent state-of-the-art by 8.5 \\(F_1\\) points.\\footnote{Code and data available\nin https://rajarshd.github.io/TextKBQA}</p>\n", "tags": ["Datasets","Evaluation","Has Code"] },
{"key": "das2018neural", "citations": "78", "year": "2018", "title":"Neural Modular Control For Embodied Question Answering", "abstract": "<p>We present a modular approach for learning policies for navigation over long\nplanning horizons from language input. Our hierarchical policy operates at\nmultiple timescales, where the higher-level master policy proposes subgoals to\nbe executed by specialized sub-policies. Our choice of subgoals is\ncompositional and semantic, i.e. they can be sequentially combined in arbitrary\norderings, and assume human-interpretable descriptions (e.g. ‘exit room’, ‘find\nkitchen’, ‘find refrigerator’, etc.).\n  We use imitation learning to warm-start policies at each level of the\nhierarchy, dramatically increasing sample efficiency, followed by reinforcement\nlearning. Independent reinforcement learning at each level of hierarchy enables\nsub-policies to adapt to consequences of their actions and recover from errors.\nSubsequent joint hierarchical training enables the master policy to adapt to\nthe sub-policies.\n  On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al.,\n2018), requiring navigating diverse realistic indoor environments, our approach\noutperforms prior work by a significant margin, both in terms of navigation and\nquestion answering.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "das2019multi", "citations": "113", "year": "2019", "title":"Multi-step Retriever-reader Interaction For Scalable Open-domain Question Answering", "abstract": "<p>This paper introduces a new framework for open-domain question answering in\nwhich the retriever and the reader iteratively interact with each other. The\nframework is agnostic to the architecture of the machine reading model, only\nrequiring access to the token-level hidden representations of the reader. The\nretriever uses fast nearest neighbor search to scale to corpora containing\nmillions of paragraphs. A gated recurrent unit updates the query at each step\nconditioned on the state of the reader and the reformulated query is used to\nre-rank the paragraphs by the retriever. We conduct analysis and show that\niterative interaction helps in retrieving informative paragraphs from the\ncorpus. Finally, we show that our multi-step-reasoning framework brings\nconsistent improvement when applied to two widely used reader architectures\nDrQA and BiDAF on various large open-domain datasets — TriviaQA-unfiltered,\nQuasarT, SearchQA, and SQuAD-Open.</p>\n", "tags": ["Datasets","Model Architecture","Retrieval Systems","Tools"] },
{"key": "dasigi2019quoref", "citations": "163", "year": "2019", "title":"Quoref: A Reading Comprehension Dataset With Questions Requiring Coreferential Reasoning", "abstract": "<p>Machine comprehension of texts longer than a single sentence often requires\ncoreference resolution. However, most current reading comprehension benchmarks\ndo not contain complex coreferential phenomena and hence fail to evaluate the\nability of models to resolve coreference. We present a new crowdsourced dataset\ncontaining more than 24K span-selection questions that require resolving\ncoreference among entities in over 4.7K English paragraphs from Wikipedia.\nObtaining questions focused on such phenomena is challenging, because it is\nhard to avoid lexical cues that shortcut complex reasoning. We deal with this\nissue by using a strong baseline model as an adversary in the crowdsourcing\nloop, which helps crowdworkers avoid writing questions with exploitable surface\ncues. We show that state-of-the-art reading comprehension models perform\nsignificantly worse than humans on this benchmark—the best model performance\nis 70.5 F1, while the estimated human performance is 93.4 F1.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "dathathri2019plug", "citations": "439", "year": "2019", "title":"Plug And Play Language Models: A Simple Approach To Controlled Text Generation", "abstract": "<p>Large transformer-based language models (LMs) trained on huge text corpora\nhave shown unparalleled generation capabilities. However, controlling\nattributes of the generated language (e.g. switching topic or sentiment) is\ndifficult without modifying the model architecture or fine-tuning on\nattribute-specific data and entailing the significant cost of retraining. We\npropose a simple alternative: the Plug and Play Language Model (PPLM) for\ncontrollable language generation, which combines a pretrained LM with one or\nmore simple attribute classifiers that guide text generation without any\nfurther training of the LM. In the canonical scenario we present, the attribute\nmodels are simple classifiers consisting of a user-specified bag of words or a\nsingle learned layer with 100,000 times fewer parameters than the LM. Sampling\nentails a forward and backward pass in which gradients from the attribute model\npush the LM’s hidden activations and thus guide the generation. Model samples\ndemonstrate control over a range of topics and sentiment styles, and extensive\nautomated and human annotated evaluations show attribute alignment and fluency.\nPPLMs are flexible in that any combination of differentiable attribute models\nmay be used to steer text generation, which will allow for diverse and creative\napplications beyond the examples given in this paper.</p>\n", "tags": ["Applications","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "datta2019align2ground", "citations": "89", "year": "2019", "title":"Align2ground: Weakly Supervised Phrase Grounding Guided By Image-caption Alignment", "abstract": "<p>We address the problem of grounding free-form textual phrases by using weak\nsupervision from image-caption pairs. We propose a novel end-to-end model that\nuses caption-to-image retrieval as a <code class=\"language-plaintext highlighter-rouge\">downstream' task to guide the process of\nphrase localization. Our method, as a first step, infers the latent\ncorrespondences between regions-of-interest (RoIs) and phrases in the caption\nand creates a discriminative image representation using these matched RoIs. In\na subsequent step, this (learned) representation is aligned with the caption.\nOur key contribution lies in building this </code>caption-conditioned’ image encoding\nwhich tightly couples both the tasks and allows the weak supervision to\neffectively guide visual grounding. We provide an extensive empirical and\nqualitative analysis to investigate the different components of our proposed\nmodel and compare it with competitive baselines. For phrase localization, we\nreport an improvement of 4.9% (absolute) over the prior state-of-the-art on the\nVisualGenome dataset. We also report results that are at par with the\nstate-of-the-art on the downstream caption-to-image retrieval task on COCO and\nFlickr30k datasets.</p>\n", "tags": ["Datasets","ICCV"] },
{"key": "dauphin2016language", "citations": "933", "year": "2016", "title":"Language Modeling With Gated Convolutional Networks", "abstract": "<p>The pre-dominant approach to language modeling to date is based on recurrent\nneural networks. Their success on this task is often linked to their ability to\ncapture unbounded context. In this paper we develop a finite context approach\nthrough stacked convolutions, which can be more efficient since they allow\nparallelization over sequential tokens. We propose a novel simplified gating\nmechanism that outperforms Oord et al (2016) and investigate the impact of key\narchitectural decisions. The proposed approach achieves state-of-the-art on the\nWikiText-103 benchmark, even though it features long-term dependencies, as well\nas competitive results on the Google Billion Words benchmark. Our model reduces\nthe latency to score a sentence by an order of magnitude compared to a\nrecurrent baseline. To our knowledge, this is the first time a non-recurrent\napproach is competitive with strong recurrent models on these large scale\nlanguage tasks.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "dautume2019episodic", "citations": "105", "year": "2019", "title":"Episodic Memory In Lifelong Language Learning", "abstract": "<p>We introduce a lifelong language learning setup where a model needs to learn\nfrom a stream of text examples without any dataset identifier. We propose an\nepisodic memory model that performs sparse experience replay and local\nadaptation to mitigate catastrophic forgetting in this setup. Experiments on\ntext classification and question answering demonstrate the complementary\nbenefits of sparse experience replay and local adaptation to allow the model to\ncontinuously learn from new datasets. We also show that the space complexity of\nthe episodic memory module can be reduced significantly (~50-90%) by randomly\nchoosing which examples to store in memory with a minimal decrease in\nperformance. We consider an episodic memory component as a crucial building\nblock of general linguistic intelligence and see our model as a first step in\nthat direction.</p>\n", "tags": ["Datasets"] },
{"key": "daza2020inductive", "citations": "84", "year": "2021", "title":"Inductive Entity Representations From Text Via Link Prediction", "abstract": "<p>Knowledge Graphs (KG) are of vital importance for multiple applications on\nthe web, including information retrieval, recommender systems, and metadata\nannotation. Regardless of whether they are built manually by domain experts or\nwith automatic pipelines, KGs are often incomplete. Recent work has begun to\nexplore the use of textual descriptions available in knowledge graphs to learn\nvector representations of entities in order to preform link prediction.\nHowever, the extent to which these representations learned for link prediction\ngeneralize to other tasks is unclear. This is important given the cost of\nlearning such representations. Ideally, we would prefer representations that do\nnot need to be trained again when transferring to a different task, while\nretaining reasonable performance.\n  In this work, we propose a holistic evaluation protocol for entity\nrepresentations learned via a link prediction objective. We consider the\ninductive link prediction and entity classification tasks, which involve\nentities not seen during training. We also consider an information retrieval\ntask for entity-oriented search. We evaluate an architecture based on a\npretrained language model, that exhibits strong generalization to entities not\nobserved during training, and outperforms related state-of-the-art methods (22%\nMRR improvement in link prediction on average). We further provide evidence\nthat the learned representations transfer well to other tasks without\nfine-tuning. In the entity classification task we obtain an average improvement\nof 16% in accuracy compared with baselines that also employ pre-trained models.\nIn the information retrieval task, we obtain significant improvements of up to\n8.8% in NDCG@10 for natural language queries. We thus show that the learned\nrepresentations are not limited KG-specific tasks, and have greater\ngeneralization properties than evaluated in previous work.</p>\n", "tags": ["Applications","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "dazeley2021levels", "citations": "93", "year": "2021", "title":"Levels Of Explainable Artificial Intelligence For Human-aligned Conversational Explanations", "abstract": "<p>Over the last few years there has been rapid research growth into eXplainable\nArtificial Intelligence (XAI) and the closely aligned Interpretable Machine\nLearning (IML). Drivers for this growth include recent legislative changes and\nincreased investments by industry and governments, along with increased concern\nfrom the general public. People are affected by autonomous decisions every day\nand the public need to understand the decision-making process to accept the\noutcomes. However, the vast majority of the applications of XAI/IML are focused\non providing low-level <code class=\"language-plaintext highlighter-rouge\">narrow' explanations of how an individual decision was\nreached based on a particular datum. While important, these explanations rarely\nprovide insights into an agent's: beliefs and motivations; hypotheses of other\n(human, animal or AI) agents' intentions; interpretation of external cultural\nexpectations; or, processes used to generate its own explanation. Yet all of\nthese factors, we propose, are essential to providing the explanatory depth\nthat people require to accept and trust the AI's decision-making. This paper\naims to define levels of explanation and describe how they can be integrated to\ncreate a human-aligned conversational explanation system. In so doing, this\npaper will survey current approaches and discuss the integration of different\ntechnologies to achieve these levels with Broad eXplainable Artificial\nIntelligence (Broad-XAI), and thereby move towards high-level </code>strong’\nexplanations.</p>\n", "tags": ["Agentic","Applications","Survey Paper"] },
{"key": "decao2018question", "citations": "256", "year": "2019", "title":"Question Answering By Reasoning Across Documents With Graph Convolutional Networks", "abstract": "<p>Most research in reading comprehension has focused on answering questions\nbased on individual documents or even single paragraphs. We introduce a neural\nmodel which integrates and reasons relying on information spread within\ndocuments and across multiple documents. We frame it as an inference problem on\na graph. Mentions of entities are nodes of this graph while edges encode\nrelations between different mentions (e.g., within- and cross-document\nco-reference). Graph convolutional networks (GCNs) are applied to these graphs\nand trained to perform multi-step reasoning. Our Entity-GCN method is scalable\nand compact, and it achieves state-of-the-art results on a multi-document\nquestion answering dataset, WikiHop (Welbl et al., 2018).</p>\n", "tags": ["Datasets"] },
{"key": "decao2021editing", "citations": "108", "year": "2021", "title":"Editing Factual Knowledge In Language Models", "abstract": "<p>The factual knowledge acquired during pre-training and stored in the\nparameters of Language Models (LMs) can be useful in downstream tasks (e.g.,\nquestion answering or textual inference). However, some facts can be\nincorrectly induced or become obsolete over time. We present KnowledgeEditor, a\nmethod which can be used to edit this knowledge and, thus, fix ‘bugs’ or\nunexpected predictions without the need for expensive re-training or\nfine-tuning. Besides being computationally efficient, KnowledgeEditordoes not\nrequire any modifications in LM pre-training (e.g., the use of meta-learning).\nIn our approach, we train a hyper-network with constrained optimization to\nmodify a fact without affecting the rest of the knowledge; the trained\nhyper-network is then used to predict the weight update at test time. We show\nKnowledgeEditor’s efficacy with two popular architectures and\nknowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for question answering. With our method,\nchanging a prediction on the specific wording of a query tends to result in a\nconsistent change in predictions also for its paraphrases. We show that this\ncan be further encouraged by exploiting (e.g., automatically-generated)\nparaphrases during training. Interestingly, our hyper-network can be regarded\nas a ‘probe’ revealing which components need to be changed to manipulate\nfactual knowledge; our analysis shows that the updates tend to be concentrated\non a small subset of components. Source code available at\nhttps://github.com/nicola-decao/KnowledgeEditor</p>\n", "tags": ["EMNLP","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "deepseekai2025deepseek", "citations": "102", "year": "2025", "title":"Deepseek-r1: Incentivizing Reasoning Capability In Llms Via Reinforcement Learning", "abstract": "<p>We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.</p>\n", "tags": ["Agentic","Fine-Tuning","Reinforcement Learning","Training Techniques"] },
{"key": "dehghani2018universal", "citations": "412", "year": "2018", "title":"Universal Transformers", "abstract": "<p>Recurrent neural networks (RNNs) sequentially process data by updating their\nstate with each new data point, and have long been the de facto choice for\nsequence modeling tasks. However, their inherently sequential computation makes\nthem slow to train. Feed-forward and convolutional architectures have recently\nbeen shown to achieve superior results on some sequence modeling tasks such as\nmachine translation, with the added advantage that they concurrently process\nall inputs in the sequence, leading to easy parallelization and faster training\ntimes. Despite these successes, however, popular feed-forward sequence models\nlike the Transformer fail to generalize in many simple tasks that recurrent\nmodels handle with ease, e.g. copying strings or even simple logical inference\nwhen the string or formula lengths exceed those observed at training time. We\npropose the Universal Transformer (UT), a parallel-in-time self-attentive\nrecurrent sequence model which can be cast as a generalization of the\nTransformer model and which addresses these issues. UTs combine the\nparallelizability and global receptive field of feed-forward sequence models\nlike the Transformer with the recurrent inductive bias of RNNs. We also add a\ndynamic per-position halting mechanism and find that it improves accuracy on\nseveral tasks. In contrast to the standard Transformer, under certain\nassumptions, UTs can be shown to be Turing-complete. Our experiments show that\nUTs outperform standard Transformers on a wide range of algorithmic and\nlanguage understanding tasks, including the challenging LAMBADA language\nmodeling task where UTs achieve a new state of the art, and machine translation\nwhere UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De\ndataset.</p>\n", "tags": ["Datasets","Ethics & Fairness","Model Architecture","Time Series","Training Techniques"] },
{"key": "deitke2022procthor", "citations": "61", "year": "2022", "title":"Procthor: Large-scale Embodied AI Using Procedural Generation", "abstract": "<p>Massive datasets and high-capacity models have driven many recent\nadvancements in computer vision and natural language understanding. This work\npresents a platform to enable similar success stories in Embodied AI. We\npropose ProcTHOR, a framework for procedural generation of Embodied AI\nenvironments. ProcTHOR enables us to sample arbitrarily large datasets of\ndiverse, interactive, customizable, and performant virtual environments to\ntrain and evaluate embodied agents across navigation, interaction, and\nmanipulation tasks. We demonstrate the power and potential of ProcTHOR via a\nsample of 10,000 generated houses and a simple neural model. Models trained\nusing only RGB images on ProcTHOR, with no explicit mapping and no human task\nsupervision produce state-of-the-art results across 6 embodied AI benchmarks\nfor navigation, rearrangement, and arm manipulation, including the presently\nrunning Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We\nalso demonstrate strong 0-shot results on these benchmarks, via pre-training on\nProcTHOR with no fine-tuning on the downstream benchmark, often beating\nprevious state-of-the-art systems that access the downstream training data.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Tools","Training Techniques"] },
{"key": "delobelle2020robbert", "citations": "178", "year": "2020", "title":"Robbert: A Dutch Roberta-based Language Model", "abstract": "<p>Pre-trained language models have been dominating the field of natural\nlanguage processing in recent years, and have led to significant performance\ngains for various complex natural language tasks. One of the most prominent\npre-trained language models is BERT, which was released as an English as well\nas a multilingual version. Although multilingual BERT performs well on many\ntasks, recent studies show that BERT models trained on a single language\nsignificantly outperform the multilingual version. Training a Dutch BERT model\nthus has a lot of potential for a wide range of Dutch NLP tasks. While previous\napproaches have used earlier implementations of BERT to train a Dutch version\nof BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch\nlanguage model called RobBERT. We measured its performance on various tasks as\nwell as the importance of the fine-tuning dataset size. We also evaluated the\nimportance of language-specific tokenizers and the model’s fairness. We found\nthat RobBERT improves state-of-the-art results for various tasks, and\nespecially significantly outperforms other models when dealing with smaller\ndatasets. These results indicate that it is a powerful pre-trained model for a\nlarge variety of Dutch language tasks. The pre-trained and fine-tuned models\nare publicly available to support further downstream Dutch NLP applications.</p>\n", "tags": ["Applications","Datasets","EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "demszky2018transforming", "citations": "133", "year": "2018", "title":"Transforming Question Answering Datasets Into Natural Language Inference Datasets", "abstract": "<p>Existing datasets for natural language inference (NLI) have propelled\nresearch on language understanding. We propose a new method for automatically\nderiving NLI datasets from the growing abundance of large-scale question\nanswering datasets. Our approach hinges on learning a sentence transformation\nmodel which converts question-answer pairs into their declarative forms.\nDespite being primarily trained on a single QA dataset, we show that it can be\nsuccessfully applied to a variety of other QA resources. Using this system, we\nautomatically derive a new freely available dataset of over 500k NLI examples\n(QA-NLI), and show that it exhibits a wide range of inference phenomena rarely\nseen in previous NLI datasets.</p>\n", "tags": ["Datasets"] },
{"key": "demszky2020goemotions", "citations": "489", "year": "2020", "title":"Goemotions: A Dataset Of Fine-grained Emotions", "abstract": "<p>Understanding emotion expressed in language has a wide range of applications,\nfrom building empathetic chatbots to detecting harmful online behavior.\nAdvancement in this area can be improved using large-scale datasets with a\nfine-grained typology, adaptable to multiple downstream tasks. We introduce\nGoEmotions, the largest manually annotated dataset of 58k English Reddit\ncomments, labeled for 27 emotion categories or Neutral. We demonstrate the high\nquality of the annotations via Principal Preserved Component Analysis. We\nconduct transfer learning experiments with existing emotion benchmarks to show\nthat our dataset generalizes well to other domains and different emotion\ntaxonomies. Our BERT-based model achieves an average F1-score of .46 across our\nproposed taxonomy, leaving much room for improvement.</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Model Architecture"] },
{"key": "deng2016image", "citations": "87", "year": "2016", "title":"Image-to-markup Generation With Coarse-to-fine Attention", "abstract": "<p>We present a neural encoder-decoder model to convert images into\npresentational markup based on a scalable coarse-to-fine attention mechanism.\nOur method is evaluated in the context of image-to-LaTeX generation, and we\nintroduce a new dataset of real-world rendered mathematical expressions paired\nwith LaTeX markup. We show that unlike neural OCR techniques using CTC-based\nmodels, attention-based approaches can tackle this non-standard OCR task. Our\napproach outperforms classical mathematical OCR systems by a large margin on\nin-domain rendered data, and, with pretraining, also performs well on\nout-of-domain handwritten data. To reduce the inference complexity associated\nwith the attention-based approaches, we introduce a new coarse-to-fine\nattention layer that selects a support region before applying attention.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "deng2018latent", "citations": "88", "year": "2018", "title":"Latent Alignment And Variational Attention", "abstract": "<p>Neural attention has become central to many state-of-the-art models in\nnatural language processing and related domains. Attention networks are an\neasy-to-train and effective method for softly simulating alignment; however,\nthe approach does not marginalize over latent alignments in a probabilistic\nsense. This property makes it difficult to compare attention to other alignment\napproaches, to compose it with probabilistic models, and to perform posterior\ninference conditioned on observed data. A related latent approach, hard\nattention, fixes these issues, but is generally harder to train and less\naccurate. This work considers variational attention networks, alternatives to\nsoft and hard attention for learning latent variable alignment models, with\ntighter approximation bounds based on amortized variational inference. We\nfurther propose methods for reducing the variance of gradients to make these\napproaches computationally feasible. Experiments show that for machine\ntranslation and visual question answering, inefficient exact latent variable\nmodels outperform standard neural attention, but these gains go away when using\nhard attention based training. On the other hand, variational attention retains\nmost of the performance gain but with training speed comparable to neural\nattention.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "deng2020residual", "citations": "61", "year": "2020", "title":"Residual Energy-based Models For Text Generation", "abstract": "<p>Text generation is ubiquitous in many NLP tasks, from summarization, to\ndialogue and machine translation. The dominant parametric approach is based on\nlocally normalized models which predict one word at a time. While these work\nremarkably well, they are plagued by exposure bias due to the greedy nature of\nthe generation process. In this work, we investigate un-normalized energy-based\nmodels (EBMs) which operate not at the token but at the sequence level. In\norder to make training tractable, we first work in the residual of a pretrained\nlocally normalized language model and second we train using noise contrastive\nestimation. Furthermore, since the EBM works at the sequence level, we can\nleverage pretrained bi-directional contextual representations, such as BERT and\nRoBERTa. Our experiments on two large language modeling datasets show that\nresidual EBMs yield lower perplexity compared to locally normalized baselines.\nMoreover, generation via importance sampling is very efficient and of higher\nquality than the baseline models according to human evaluation.</p>\n", "tags": ["Datasets","Evaluation","ICLR","Model Architecture","Training Techniques"] },
{"key": "deng2022exploring", "citations": "63", "year": "2022", "title":"Exploring How Machine Learning Practitioners (try To) Use Fairness Toolkits", "abstract": "<p>Recent years have seen the development of many open-source ML fairness\ntoolkits aimed at helping ML practitioners assess and address unfairness in\ntheir systems. However, there has been little research investigating how ML\npractitioners actually use these toolkits in practice. In this paper, we\nconducted the first in-depth empirical exploration of how industry\npractitioners (try to) work with existing fairness toolkits. In particular, we\nconducted think-aloud interviews to understand how participants learn about and\nuse fairness toolkits, and explored the generality of our findings through an\nanonymous online survey. We identified several opportunities for fairness\ntoolkits to better address practitioner needs and scaffold them in using\ntoolkits effectively and responsibly. Based on these findings, we highlight\nimplications for the design of future open-source fairness toolkits that can\nsupport practitioners in better contextualizing, communicating, and\ncollaborating around ML fairness efforts.</p>\n", "tags": ["Ethics & Fairness","Survey Paper"] },
{"key": "deng2022large", "citations": "115", "year": "2023", "title":"Large Language Models Are Zero-shot Fuzzers: Fuzzing Deep-learning Libraries Via Large Language Models", "abstract": "<p>Detecting bugs in Deep Learning (DL) libraries (e.g., TensorFlow/PyTorch) is\ncritical for almost all downstream DL systems in ensuring effectiveness/safety\nfor end users. Meanwhile, traditional fuzzing techniques can be hardly\neffective for such a challenging domain since the input DL programs need to\nsatisfy both the input language (e.g., Python) syntax/semantics and the DL API\ninput/shape constraints for tensor computations.\n  To address these limitations, we propose TitanFuzz - the first approach to\ndirectly leveraging Large Language Models (LLMs) to generate input programs for\nfuzzing DL libraries. LLMs are titanic models trained on billions of code\nsnippets and can auto-regressively generate human-like code snippets. Our key\ninsight is that modern LLMs can also include numerous code snippets invoking DL\nlibrary APIs in their training corpora, and thus can implicitly learn both\nlanguage syntax/semantics and intricate DL API constraints for valid DL program\ngeneration. More specifically, we use both generative and infilling LLMs (e.g.,\nCodex/InCoder) to generate and mutate valid/diverse input DL programs for\nfuzzing. Our experimental results demonstrate that TitanFuzz can achieve\n30.38%/50.84% higher code coverage than state-of-the-art fuzzers on\nTensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 41\nalready confirmed as previously unknown bugs.\n  This paper demonstrates that modern titanic LLMs can be leveraged to directly\nperform both generation-based and mutation-based fuzzing studied for decades,\nwhile being fully automated, generalizable, and applicable to domains\nchallenging for traditional approaches (such as DL systems). We hope TitanFuzz\ncan stimulate more work in this promising direction of LLMs for fuzzing.</p>\n", "tags": ["Security","Tools","Training Techniques"] },
{"key": "deng2022rlprompt", "citations": "89", "year": "2022", "title":"Rlprompt: Optimizing Discrete Text Prompts With Reinforcement Learning", "abstract": "<p>Prompting has shown impressive success in enabling large pretrained language\nmodels (LMs) to perform diverse NLP tasks, especially when only few downstream\ndata are available. Automatically finding the optimal prompt for each task,\nhowever, is challenging. Most existing work resorts to tuning soft prompt\n(e.g., embeddings) which falls short of interpretability, reusability across\nLMs, and applicability when gradients are not accessible. Discrete prompt, on\nthe other hand, is difficult to optimize, and is often created by “enumeration\n(e.g., paraphrasing)-then-selection” heuristics that do not explore the prompt\nspace systematically. This paper proposes RLPrompt, an efficient discrete\nprompt optimization approach with reinforcement learning (RL). RLPrompt\nformulates a parameter-efficient policy network that generates the desired\ndiscrete prompt after training with reward. To overcome the complexity and\nstochasticity of reward signals by the large LM environment, we incorporate\neffective reward stabilization that substantially enhances the training\nefficiency. RLPrompt is flexibly applicable to different types of LMs, such as\nmasked (e.g., BERT) and left-to-right models (e.g., GPTs), for both\nclassification and generation tasks. Experiments on few-shot classification and\nunsupervised text style transfer show superior performance over a wide range of\nexisting finetuning or prompting methods. Interestingly, the resulting\noptimized prompts are often ungrammatical gibberish text; and surprisingly,\nthose gibberish prompts are transferrable between different LMs to retain\nsignificant performance, indicating LM prompting may not follow human language\npatterns.</p>\n", "tags": ["EMNLP","Efficiency","Few-Shot","Model Architecture","Prompting","Reinforcement Learning","Training Techniques"] },
{"key": "denkowski2017stronger", "citations": "106", "year": "2017", "title":"Stronger Baselines For Trustable Results In Neural Machine Translation", "abstract": "<p>Interest in neural machine translation has grown rapidly as its effectiveness\nhas been demonstrated across language and data scenarios. New research\nregularly introduces architectural and algorithmic improvements that lead to\nsignificant gains over “vanilla” NMT implementations. However, these new\ntechniques are rarely evaluated in the context of previously published\ntechniques, specifically those that are widely used in state-of-theart\nproduction and shared-task systems. As a result, it is often difficult to\ndetermine whether improvements from research will carry over to systems\ndeployed for real-world use. In this work, we recommend three specific methods\nthat are relatively easy to implement and result in much stronger experimental\nsystems. Beyond reporting significantly higher BLEU scores, we conduct an\nin-depth analysis of where improvements originate and what inherent weaknesses\nof basic NMT models are being addressed. We then compare the relative gains\nafforded by several other techniques proposed in the literature when starting\nwith vanilla systems versus our stronger baselines, showing that experimental\nconclusions may change depending on the baseline chosen. This indicates that\nchoosing a strong baseline is crucial for reporting reliable experimental\nresults.</p>\n", "tags": [] },
{"key": "denny2022conversing", "citations": "186", "year": "2023", "title":"Conversing With Copilot: Exploring Prompt Engineering For Solving CS1 Problems Using Natural Language", "abstract": "<p>GitHub Copilot is an artificial intelligence model for automatically\ngenerating source code from natural language problem descriptions. Since June\n2022, Copilot has officially been available for free to all students as a\nplug-in to development environments like Visual Studio Code. Prior work\nexploring OpenAI Codex, the underlying model that powers Copilot, has shown it\nperforms well on typical CS1 problems thus raising concerns about the impact it\nwill have on how introductory programming courses are taught. However, little\nis known about the types of problems for which Copilot does not perform well,\nor about the natural language interactions that a student might have with\nCopilot when resolving errors. We explore these questions by evaluating the\nperformance of Copilot on a publicly available dataset of 166 programming\nproblems. We find that it successfully solves around half of these problems on\nits very first attempt, and that it solves 60% of the remaining problems using\nonly natural language changes to the problem description. We argue that this\ntype of prompt engineering, which we believe will become a standard interaction\nbetween human and Copilot when it initially fails, is a potentially useful\nlearning activity that promotes computational thinking skills, and is likely to\nchange the nature of code writing skill development.</p>\n", "tags": ["Has Code","Prompting"] },
{"key": "denny2022least", "citations": "237", "year": "2022", "title":"Least-to-most Prompting Enables Complex Reasoning In Large Language Models", "abstract": "<p>Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Prompting","Training Techniques"] },
{"key": "denny2023computing", "citations": "122", "year": "2024", "title":"Computing Education In The Era Of Generative AI", "abstract": "<p>The computing education community has a rich history of pedagogical\ninnovation designed to support students in introductory courses, and to support\nteachers in facilitating student learning. Very recent advances in artificial\nintelligence have resulted in code generation models that can produce source\ncode from natural language problem descriptions – with impressive accuracy in\nmany cases. The wide availability of these models and their ease of use has\nraised concerns about potential impacts on many aspects of society, including\nthe future of computing education. In this paper, we discuss the challenges and\nopportunities such models present to computing educators, with a focus on\nintroductory programming classrooms. We summarize the results of two recent\narticles, the first evaluating the performance of code generation models on\ntypical introductory-level programming problems, and the second exploring the\nquality and novelty of learning resources generated by these models. We\nconsider likely impacts of such models upon pedagogical practice in the context\nof the most recent advances at the time of writing.</p>\n", "tags": ["Llm For Code"] },
{"key": "denny2023prompt", "citations": "70", "year": "2024", "title":"Prompt Problems: A New Programming Exercise For The Generative AI Era", "abstract": "<p>Large Language Models (LLMs) are revolutionizing the field of computing\neducation with their powerful code-generating capabilities. Traditional\npedagogical practices have focused on code writing tasks, but there is now a\nshift in importance towards code reading, comprehension and evaluation of\nLLM-generated code. Alongside this shift, an important new skill is emerging –\nthe ability to solve programming tasks by constructing good prompts for\ncode-generating models. In this work we introduce a new type of programming\nexercise to hone this nascent skill: ‘Prompt Problems’. Prompt Problems are\ndesigned to help students learn how to write effective prompts for AI code\ngenerators. A student solves a Prompt Problem by crafting a natural language\nprompt which, when provided as input to an LLM, outputs code that successfully\nsolves a specified programming task. We also present a new web-based tool\ncalled Promptly which hosts a repository of Prompt Problems and supports the\nautomated evaluation of prompt-generated code. We deploy Promptly for the first\ntime in one CS1 and one CS2 course and describe our experiences, which include\nstudent perceptions of this new type of activity and their interactions with\nthe tool. We find that students are enthusiastic about Prompt Problems, and\nappreciate how the problems engage their computational thinking skills and\nexpose them to new programming constructs. We discuss ideas for the future\ndevelopment of new variations of Prompt Problems, and the need to carefully\nstudy their integration into classroom practice.</p>\n", "tags": ["Evaluation","Has Code","Prompting"] },
{"key": "derose2020attention", "citations": "72", "year": "2020", "title":"Attention Flows: Analyzing And Comparing Attention Mechanisms In Language Models", "abstract": "<p>Advances in language modeling have led to the development of deep\nattention-based models that are performant across a wide variety of natural\nlanguage processing (NLP) problems. These language models are typified by a\npre-training process on large unlabeled text corpora and subsequently\nfine-tuned for specific tasks. Although considerable work has been devoted to\nunderstanding the attention mechanisms of pre-trained models, it is less\nunderstood how a model’s attention mechanisms change when trained for a target\nNLP task. In this paper, we propose a visual analytics approach to\nunderstanding fine-tuning in attention-based language models. Our\nvisualization, Attention Flows, is designed to support users in querying,\ntracing, and comparing attention within layers, across layers, and amongst\nattention heads in Transformer-based language models. To help users gain\ninsight on how a classification decision is made, our design is centered on\ndepicting classification-based attention at the deepest layer and how attention\nfrom prior layers flows throughout words in the input. Attention Flows supports\nthe analysis of a single model, as well as the visual comparison between\npre-trained and fine-tuned models via their similarities and differences. We\nuse Attention Flows to study attention mechanisms in various sentence\nunderstanding tasks and highlight how attention evolves to address the nuances\nof solving these tasks.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "desai2020calibration", "citations": "168", "year": "2020", "title":"Calibration Of Pre-trained Transformers", "abstract": "<p>Pre-trained Transformers are now ubiquitous in natural language processing,\nbut despite their high end-task performance, little is known empirically about\nwhether they are calibrated. Specifically, do these models’ posterior\nprobabilities provide an accurate empirical measure of how likely the model is\nto be correct on a given example? We focus on BERT and RoBERTa in this work,\nand analyze their calibration across three tasks: natural language inference,\nparaphrase detection, and commonsense reasoning. For each task, we consider\nin-domain as well as challenging out-of-domain settings, where models face more\nexamples they should be uncertain about. We show that: (1) when used\nout-of-the-box, pre-trained models are calibrated in-domain, and compared to\nbaselines, their calibration error out-of-domain can be as much as 3.5x lower;\n(2) temperature scaling is effective at further reducing calibration error\nin-domain, and using label smoothing to deliberately increase empirical\nuncertainty helps calibrate posteriors out-of-domain.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "deshpande2023toxicity", "citations": "92", "year": "2023", "title":"Toxicity In Chatgpt: Analyzing Persona-assigned Language Models", "abstract": "<p>Large language models (LLMs) have shown incredible capabilities and\ntranscended the natural language processing (NLP) community, with adoption\nthroughout many services like healthcare, therapy, education, and customer\nservice. Since users include people with critical information needs like\nstudents or patients engaging with chatbots, the safety of these systems is of\nprime importance. Therefore, a clear understanding of the capabilities and\nlimitations of LLMs is necessary. To this end, we systematically evaluate\ntoxicity in over half a million generations of ChatGPT, a popular\ndialogue-based LLM. We find that setting the system parameter of ChatGPT by\nassigning it a persona, say that of the boxer Muhammad Ali, significantly\nincreases the toxicity of generations. Depending on the persona assigned to\nChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect\nstereotypes, harmful dialogue, and hurtful opinions. This may be potentially\ndefamatory to the persona and harmful to an unsuspecting user. Furthermore, we\nfind concerning patterns where specific entities (e.g., certain races) are\ntargeted more than others (3x more) irrespective of the assigned persona, that\nreflect inherent discriminatory biases in the model. We hope that our findings\ninspire the broader AI community to rethink the efficacy of current safety\nguardrails and develop better techniques that lead to robust, safe, and\ntrustworthy AI systems.</p>\n", "tags": ["EMNLP","Ethics & Fairness"] },
{"key": "dettmers2022llm", "citations": "87", "year": "2022", "title":"Llm.int8(): 8-bit Matrix Multiplication For Transformers At Scale", "abstract": "<p>Large language models have been widely adopted but require significant GPU\nmemory for inference. We develop a procedure for Int8 matrix multiplication for\nfeed-forward and attention projection layers in transformers, which cut the\nmemory needed for inference by half while retaining full precision performance.\nWith our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted\nto Int8, and used immediately without performance degradation. This is made\npossible by understanding and working around properties of highly systematic\nemergent features in transformer language models that dominate attention and\ntransformer predictive performance. To cope with these features, we develop a\ntwo-part quantization procedure, LLM.int8(). We first use vector-wise\nquantization with separate normalization constants for each inner product in\nthe matrix multiplication, to quantize most of the features. However, for the\nemergent outliers, we also include a new mixed-precision decomposition scheme,\nwhich isolates the outlier feature dimensions into a 16-bit matrix\nmultiplication while still more than 99.9% of values are multiplied in 8-bit.\nUsing LLM.int8(), we show empirically it is possible to perform inference in\nLLMs with up to 175B parameters without any performance degradation. This\nresult makes such models much more accessible, for example making it possible\nto use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our\nsoftware.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "dettmers2023qlora", "citations": "365", "year": "2023", "title":"Qlora: Efficient Finetuning Of Quantized Llms", "abstract": "<p>We present QLoRA, an efficient finetuning approach that reduces memory usage\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\na frozen, 4-bit quantized pretrained language model into Low Rank\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\nsingle GPU. QLoRA introduces a number of innovations to save memory without\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\ninformation theoretically optimal for normally distributed weights (b) double\nquantization to reduce the average memory footprint by quantizing the\nquantization constants, and (c) paged optimziers to manage memory spikes. We\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\ninstruction following and chatbot performance across 8 instruction datasets,\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\nshow that QLoRA finetuning on a small high-quality dataset leads to\nstate-of-the-art results, even when using smaller models than the previous\nSoTA. We provide a detailed analysis of chatbot performance based on both human\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\nalternative to human evaluation. Furthermore, we find that current chatbot\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\nChatGPT. We release all of our models and code, including CUDA kernels for\n4-bit training.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "devlin2017robustfill", "citations": "112", "year": "2017", "title":"Robustfill: Neural Program Learning Under Noisy I/O", "abstract": "<p>The problem of automatically generating a computer program from some\nspecification has been studied since the early days of AI. Recently, two\ncompeting approaches for automatic program learning have received significant\nattention: (1) neural program synthesis, where a neural network is conditioned\non input/output (I/O) examples and learns to generate a program, and (2) neural\nprogram induction, where a neural network generates new outputs directly using\na latent program representation.\n  Here, for the first time, we directly compare both approaches on a\nlarge-scale, real-world learning task. We additionally contrast to rule-based\nprogram synthesis, which uses hand-crafted semantics to guide the program\ngeneration. Our neural models use a modified attention RNN to allow encoding of\nvariable-sized sets of I/O pairs. Our best synthesis model achieves 92%\naccuracy on a real-world test set, compared to the 34% accuracy of the previous\nbest neural synthesis approach. The synthesis model also outperforms a\ncomparable induction model on this task, but we more importantly demonstrate\nthat the strength of each approach is highly dependent on the evaluation metric\nand end-user application. Finally, we show that we can train our neural models\nto remain very robust to the type of noise expected in real-world data (e.g.,\ntypos), while a highly-engineered rule-based system fails entirely.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "devlin2018bert", "citations": "38038", "year": "2018", "title":"BERT: Pre-training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "<p>We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "devries2016guesswhat", "citations": "421", "year": "2017", "title":"Guesswhat?! Visual Object Discovery Through Multi-modal Dialogue", "abstract": "<p>We introduce GuessWhat?!, a two-player guessing game as a testbed for\nresearch on the interplay of computer vision and dialogue systems. The goal of\nthe game is to locate an unknown object in a rich image scene by asking a\nsequence of questions. Higher-level image understanding, like spatial reasoning\nand language grounding, is required to solve the proposed task. Our key\ncontribution is the collection of a large-scale dataset consisting of 150K\nhuman-played games with a total of 800K visual question-answer pairs on 66K\nimages. We explain our design decisions in collecting the dataset and introduce\nthe oracle and questioner tasks that are associated with the two players of the\ngame. We prototyped deep learning models to establish initial baselines of the\nintroduced tasks.</p>\n", "tags": ["CVPR","Datasets","Dialogue & Multi Turn"] },
{"key": "devries2017modulating", "citations": "235", "year": "2017", "title":"Modulating Early Visual Processing By Language", "abstract": "<p>It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the <em>entire visual processing</em> by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial.</p>\n", "tags": [] },
{"key": "devries2018talk", "citations": "117", "year": "2018", "title":"Talk The Walk: Navigating New York City Through Grounded Dialogue", "abstract": "<p>We introduce “Talk The Walk”, the first large-scale dialogue dataset grounded\nin action and perception. The task involves two agents (a “guide” and a\n“tourist”) that communicate via natural language in order to achieve a common\ngoal: having the tourist navigate to a given target location. The task and\ndataset, which are described in detail, are challenging and their full solution\nis an open problem that we pose to the community. We (i) focus on the task of\ntourist localization and develop the novel Masked Attention for Spatial\nConvolutions (MASC) mechanism that allows for grounding tourist utterances into\nthe guide’s map, (ii) show it yields significant improvements for both emergent\nand natural language communication, and (iii) using this method, we establish\nnon-trivial baselines on the full task.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "devries2019bertje", "citations": "224", "year": "2019", "title":"Bertje: A Dutch BERT Model", "abstract": "<p>The transformer-based pre-trained language model BERT has helped to improve\nstate-of-the-art performance on many natural language processing (NLP) tasks.\nUsing the same architecture and parameters, we developed and evaluated a\nmonolingual Dutch BERT model called BERTje. Compared to the multilingual BERT\nmodel, which includes Dutch but is only based on Wikipedia text, BERTje is\nbased on a large and diverse dataset of 2.4 billion tokens. BERTje consistently\noutperforms the equally-sized multilingual BERT model on downstream NLP tasks\n(part-of-speech tagging, named-entity recognition, semantic role labeling, and\nsentiment analysis). Our pre-trained Dutch BERT model is made available at\nhttps://github.com/wietsedv/bertje.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "deyao2023minigpt", "citations": "359", "year": "2023", "title":"Minigpt-4: Enhancing Vision-language Understanding With Advanced Large Language Models", "abstract": "<p>The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. However, the technical details behind GPT-4 continue to\nremain undisclosed. We believe that the enhanced multi-modal generation\ncapabilities of GPT-4 stem from the utilization of sophisticated large language\nmodels (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a\nfrozen visual encoder with a frozen advanced LLM, Vicuna, using one projection\nlayer. Our work, for the first time, uncovers that properly aligning the visual\nfeatures with an advanced large language model can possess numerous advanced\nmulti-modal abilities demonstrated by GPT-4, such as detailed image description\ngeneration and website creation from hand-drawn drafts. Furthermore, we also\nobserve other emerging capabilities in MiniGPT-4, including writing stories and\npoems inspired by given images, teaching users how to cook based on food\nphotos, and so on. In our experiment, we found that the model trained on short\nimage caption pairs could produce unnatural language outputs (e.g., repetition\nand fragmentation). To address this problem, we curate a detailed image\ndescription dataset in the second stage to finetune the model, which\nconsequently improves the model’s generation reliability and overall usability.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "deyoung2019eraser", "citations": "447", "year": "2020", "title":"ERASER: A Benchmark To Evaluate Rationalized NLP Models", "abstract": "<p>State-of-the-art models in NLP are now predominantly based on deep neural\nnetworks that are opaque in terms of how they come to make predictions. This\nlimitation has increased interest in designing more interpretable deep models\nfor NLP that reveal the `reasoning’ behind model outputs. But work in this\ndirection has been conducted on different datasets and tasks with\ncorrespondingly unique aims and metrics; this makes it difficult to track\nprogress. We propose the Evaluating Rationales And Simple English Reasoning\n(ERASER) benchmark to advance research on interpretable models in NLP. This\nbenchmark comprises multiple datasets and tasks for which human annotations of\n“rationales” (supporting evidence) have been collected. We propose several\nmetrics that aim to capture how well the rationales provided by models align\nwith human rationales, and also how faithful these rationales are (i.e., the\ndegree to which provided rationales influenced the corresponding predictions).\nOur hope is that releasing this benchmark facilitates progress on designing\nmore interpretable NLP systems. The benchmark, code, and documentation are\navailable at https://www.eraserbenchmark.com/</p>\n", "tags": ["Evaluation","Has Code"] },
{"key": "dhar2018learning", "citations": "411", "year": "2019", "title":"Learning Without Memorizing", "abstract": "<p>Incremental learning (IL) is an important task aimed at increasing the\ncapability of a trained model, in terms of the number of classes recognizable\nby the model. The key problem in this task is the requirement of storing data\n(e.g. images) associated with existing classes, while teaching the classifier\nto learn new classes. However, this is impractical as it increases the memory\nrequirement at every incremental step, which makes it impossible to implement\nIL algorithms on edge devices with limited memory. Hence, we propose a novel\napproach, called `Learning without Memorizing (LwM)’, to preserve the\ninformation about existing (base) classes, without storing any of their data,\nwhile making the classifier progressively learn the new classes. In LwM, we\npresent an information preserving penalty: Attention Distillation Loss\n(\\(L_{AD}\\)), and demonstrate that penalizing the changes in classifiers’\nattention maps helps to retain information of the base classes, as new classes\nare added. We show that adding \\(L_{AD}\\) to the distillation loss which is an\nexisting information preserving loss consistently outperforms the\nstate-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in\nterms of the overall accuracy of base and incrementally learned classes.</p>\n", "tags": ["CVPR","Datasets","Efficiency","Model Architecture"] },
{"key": "dhariwal2020jukebox", "citations": "231", "year": "2020", "title":"Jukebox: A Generative Model For Music", "abstract": "<p>We introduce Jukebox, a model that generates music with singing in the raw\naudio domain. We tackle the long context of raw audio using a multi-scale\nVQ-VAE to compress it to discrete codes, and modeling those using\nautoregressive Transformers. We show that the combined model at scale can\ngenerate high-fidelity and diverse songs with coherence up to multiple minutes.\nWe can condition on artist and genre to steer the musical and vocal style, and\non unaligned lyrics to make the singing more controllable. We are releasing\nthousands of non cherry-picked samples at https://jukebox.openai.com, along\nwith model weights and code at https://github.com/openai/jukebox</p>\n", "tags": ["Has Code"] },
{"key": "dhingra2016gated", "citations": "358", "year": "2017", "title":"Gated-attention Readers For Text Comprehension", "abstract": "<p>In this paper we study the problem of answering cloze-style questions over\ndocuments. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop\narchitecture with a novel attention mechanism, which is based on multiplicative\ninteractions between the query embedding and the intermediate states of a\nrecurrent neural network document reader. This enables the reader to build\nquery-specific representations of tokens in the document for accurate answer\nselection. The GA Reader obtains state-of-the-art results on three benchmarks\nfor this task–the CNN \\&amp; Daily Mail news stories and the Who Did What dataset.\nThe effectiveness of multiplicative interaction is demonstrated by an ablation\nstudy, and by comparing to alternative compositional operators for implementing\nthe gated-attention. The code is available at\nhttps://github.com/bdhingra/ga-reader.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "dhingra2016towards", "citations": "315", "year": "2017", "title":"Towards End-to-end Reinforcement Learning Of Dialogue Agents For Information Access", "abstract": "<p>This paper proposes KB-InfoBot – a multi-turn dialogue agent which helps\nusers search Knowledge Bases (KBs) without composing complicated queries. Such\ngoal-oriented dialogue agents typically need to interact with an external\ndatabase to access real-world knowledge. Previous systems achieved this by\nissuing a symbolic query to the KB to retrieve entries based on their\nattributes. However, such symbolic operations break the differentiability of\nthe system and prevent end-to-end training of neural dialogue agents. In this\npaper, we address this limitation by replacing symbolic queries with an induced\n“soft” posterior distribution over the KB that indicates which entities the\nuser is interested in. Integrating the soft retrieval process with a\nreinforcement learner leads to higher task success rate and reward in both\nsimulations and against real users. We also present a fully neural end-to-end\nagent, trained entirely from user feedback, and discuss its application towards\npersonalized dialogue agents. The source code is available at\nhttps://github.com/MiuLab/KB-InfoBot.</p>\n", "tags": ["Dialogue & Multi Turn","Has Code","Reinforcement Learning","Training Techniques"] },
{"key": "dhingra2017quasar", "citations": "149", "year": "2017", "title":"Quasar: Datasets For Question Answering By Search And Reading", "abstract": "<p>We present two new large-scale datasets aimed at evaluating systems designed\nto comprehend a natural language query and extract its answer from a large\ncorpus of text. The Quasar-S dataset consists of 37000 cloze-style\n(fill-in-the-gap) queries constructed from definitions of software entity tags\non the popular website Stack Overflow. The posts and comments on the website\nserve as the background corpus for answering the cloze questions. The Quasar-T\ndataset consists of 43000 open-domain trivia questions and their answers\nobtained from various internet sources. ClueWeb09 serves as the background\ncorpus for extracting these answers. We pose these datasets as a challenge for\ntwo related subtasks of factoid Question Answering: (1) searching for relevant\npieces of text that include the correct answer to a query, and (2) reading the\nretrieved text to answer the query. We also describe a retrieval system for\nextracting relevant sentences and documents from the corpus given a query, and\ninclude these in the release for researchers wishing to only focus on (2). We\nevaluate several baselines on both datasets, ranging from simple heuristics to\npowerful neural models, and show that these lag behind human performance by\n16.4% and 32.1% for Quasar-S and -T respectively. The datasets are available at\nhttps://github.com/bdhingra/quasar .</p>\n", "tags": ["Datasets","Has Code","Retrieval Systems"] },
{"key": "dhingra2018simple", "citations": "69", "year": "2018", "title":"Simple And Effective Semi-supervised Question Answering", "abstract": "<p>Recent success of deep learning models for the task of extractive Question\nAnswering (QA) is hinged on the availability of large annotated corpora.\nHowever, large domain specific annotated corpora are limited and expensive to\nconstruct. In this work, we envision a system where the end user specifies a\nset of base documents and only a few labelled examples. Our system exploits the\ndocument structure to create cloze-style questions from these base documents;\npre-trains a powerful neural network on the cloze style questions; and further\nfine-tunes the model on the labeled examples. We evaluate our proposed system\nacross three diverse datasets from different domains, and find it to be highly\neffective with very little labeled data. We attain more than 50% F1 score on\nSQuAD and TriviaQA with less than a thousand labelled examples. We are also\nreleasing a set of 3.2M cloze-style questions for practitioners to use while\nbuilding QA systems.</p>\n", "tags": ["Datasets","NAACL","Training Techniques"] },
{"key": "dhingra2019handling", "citations": "142", "year": "2019", "title":"Handling Divergent Reference Texts When Evaluating Table-to-text Generation", "abstract": "<p>Automatically constructed datasets for generating text from semi-structured\ndata (tables), such as WikiBio, often contain reference texts that diverge from\nthe information in the corresponding semi-structured data. We show that metrics\nwhich rely solely on the reference texts, such as BLEU and ROUGE, show poor\ncorrelation with human judgments when those references diverge. We propose a\nnew metric, PARENT, which aligns n-grams from the reference and generated texts\nto the semi-structured data before computing their precision and recall.\nThrough a large scale human evaluation study of table-to-text models for\nWikiBio, we show that PARENT correlates with human judgments better than\nexisting text generation metrics. We also adapt and evaluate the information\nextraction based evaluation proposed by Wiseman et al (2017), and show that\nPARENT has comparable correlation to it, while being easier to use. We show\nthat PARENT is also applicable when the reference texts are elicited from\nhumans using the data from the WebNLG challenge.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "dhingra2021time", "citations": "98", "year": "2022", "title":"Time-aware Language Models As Temporal Knowledge Bases", "abstract": "<p>Many facts come with an expiration date, from the name of the President to\nthe basketball team Lebron James plays for. But language models (LMs) are\ntrained on snapshots of data collected at a specific moment in time, and this\ncan limit their utility, especially in the closed-book setting where the\npretraining corpus must contain the facts the model should memorize. We\nintroduce a diagnostic dataset aimed at probing LMs for factual knowledge that\nchanges over time and highlight problems with LMs at either end of the spectrum\n– those trained on specific slices of temporal data, as well as those trained\non a wide range of temporal data. To mitigate these problems, we propose a\nsimple technique for jointly modeling text with its timestamp. This improves\nmemorization of seen facts from the training time period, as well as\ncalibration on predictions about unseen facts from future time periods. We also\nshow that models trained with temporal context can be efficiently “refreshed”\nas new data arrives, without the need for retraining from scratch.</p>\n", "tags": ["Datasets","TACL","Time Series","Training Techniques"] },
{"key": "dhoffschmidt2020fquad", "citations": "64", "year": "2020", "title":"Fquad: French Question Answering Dataset", "abstract": "<p>Recent advances in the field of language modeling have improved\nstate-of-the-art results on many Natural Language Processing tasks. Among them,\nReading Comprehension has made significant progress over the past few years.\nHowever, most results are reported in English since labeled resources available\nin other languages, such as French, remain scarce. In the present work, we\nintroduce the French Question Answering Dataset (FQuAD). FQuAD is a French\nNative Reading Comprehension dataset of questions and answers on a set of\nWikipedia articles that consists of 25,000+ samples for the 1.0 version and\n60,000+ samples for the 1.1 version. We train a baseline model which achieves\nan F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In order\nto track the progress of French Question Answering models we propose a\nleader-board and we have made the 1.0 version of our dataset freely available\nat https://illuin-tech.github.io/FQuAD-explorer/.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code"] },
{"key": "dhuliawala2023chain", "citations": "62", "year": "2024", "title":"Chain-of-verification Reduces Hallucination In Large Language Models", "abstract": "<p>Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.</p>\n", "tags": [] },
{"key": "dibia2018data2vis", "citations": "158", "year": "2019", "title":"Data2vis: Automatic Generation Of Data Visualizations Using Sequence To Sequence Recurrent Neural Networks", "abstract": "<p>Rapidly creating effective visualizations using expressive grammars is\nchallenging for users who have limited time and limited skills in statistics\nand data visualization. Even high-level, dedicated visualization tools often\nrequire users to manually select among data attributes, decide which\ntransformations to apply, and specify mappings between visual encoding\nvariables and raw or transformed attributes.\n  In this paper we introduce Data2Vis, a neural translation model for\nautomatically generating visualizations from given datasets. We formulate\nvisualization generation as a sequence to sequence translation problem where\ndata specifications are mapped to visualization specifications in a declarative\nlanguage (Vega-Lite). To this end, we train a multilayered attention-based\nrecurrent neural network (RNN) with long short-term memory (LSTM) units on a\ncorpus of visualization specifications.\n  Qualitative results show that our model learns the vocabulary and syntax for\na valid visualization specification, appropriate transformations (count, bins,\nmean) and how to use common data selection patterns that occur within data\nvisualizations. Data2Vis generates visualizations that are comparable to\nmanually-created visualizations in a fraction of the time, with potential to\nlearn more complex visualization strategies at scale.</p>\n", "tags": ["Applications","Datasets","Model Architecture","Tools"] },
{"key": "dimitrov2021semeval", "citations": "74", "year": "2021", "title":"Semeval-2021 Task 6: Detection Of Persuasion Techniques In Texts And Images", "abstract": "<p>We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in\nTexts and Images: the data, the annotation guidelines, the evaluation setup,\nthe results, and the participating systems. The task focused on memes and had\nthree subtasks: (i) detecting the techniques in the text, (ii) detecting the\ntext spans where the techniques are used, and (iii) detecting techniques in the\nentire meme, i.e., both in the text and in the image. It was a popular task,\nattracting 71 registrations, and 22 teams that eventually made an official\nsubmission on the test set. The evaluation results for the third subtask\nconfirmed the importance of both modalities, the text and the image. Moreover,\nsome teams reported benefits when not just combining the two modalities, e.g.,\nby using early or late fusion, but rather modeling the interaction between them\nin a joint model.</p>\n", "tags": ["Evaluation"] },
{"key": "dinan2018wizard", "citations": "501", "year": "2018", "title":"Wizard Of Wikipedia: Knowledge-powered Conversational Agents", "abstract": "<p>In open-domain dialogue intelligent agents should exhibit the use of\nknowledge, however there are few convincing demonstrations of this to date. The\nmost popular sequence to sequence models typically “generate and hope” generic\nutterances that can be memorized in the weights of the model when mapping from\ninput utterance(s) to output, rather than employing recalled knowledge as\ncontext. Use of knowledge has so far proved difficult, in part because of the\nlack of a supervised learning benchmark task which exhibits knowledgeable open\ndialogue with clear grounding. To that end we collect and release a large\ndataset with conversations directly grounded with knowledge retrieved from\nWikipedia. We then design architectures capable of retrieving knowledge,\nreading and conditioning on it, and finally generating natural responses. Our\nbest performing dialogue models are able to conduct knowledgeable discussions\non open-domain topics as evaluated by automatic metrics and human evaluations,\nwhile our new benchmark allows for measuring further improvements in this\nimportant research direction.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "dinan2019build", "citations": "157", "year": "2019", "title":"Build It Break It Fix It For Dialogue Safety: Robustness From Adversarial Human Attack", "abstract": "<p>The detection of offensive language in the context of a dialogue has become\nan increasingly important application of natural language processing. The\ndetection of trolls in public forums (Gal'an-Garc'ia et al., 2016), and the\ndeployment of chatbots in the public domain (Wolf et al., 2017) are two\nexamples that show the necessity of guarding against adversarially offensive\nbehavior on the part of humans. In this work, we develop a training scheme for\na model to become robust to such human attacks by an iterative build it, break\nit, fix it strategy with humans and models in the loop. In detailed experiments\nwe show this approach is considerably more robust than previous systems.\nFurther, we show that offensive language used within a conversation critically\ndepends on the dialogue context, and cannot be viewed as a single sentence\noffensive detection task as in most previous work. Our newly collected tasks\nand methods will be made open source and publicly available.</p>\n", "tags": ["EMNLP","Security","Training Techniques"] },
{"key": "dinan2019queens", "citations": "144", "year": "2020", "title":"Queens Are Powerful Too: Mitigating Gender Bias In Dialogue Generation", "abstract": "<p>Models often easily learn biases present in the training data, and their\npredictions directly reflect this bias. We analyze gender bias in dialogue\ndata, and examine how this bias is actually amplified in subsequent generative\nchit-chat dialogue models. We measure gender bias in six existing dialogue\ndatasets, and focus on the most biased one, the multi-player text-based fantasy\nadventure dataset LIGHT, as a testbed for our bias mitigation techniques. The\nLIGHT dataset is highly imbalanced with respect to gender, containing\npredominantly male characters, likely because it is entirely collected by\ncrowdworkers and reflects common biases that exist in fantasy or medieval\nsettings. We consider three techniques to mitigate gender bias: counterfactual\ndata augmentation, targeted data collection, and bias controlled training. We\nshow that our proposed techniques mitigate gender bias in LIGHT by balancing\nthe genderedness of generated dialogue utterances and are particularly\neffective in combination. We quantify performance using various evaluation\nmethods—such as quantity of gendered words, a dialogue safety classifier, and\nhuman studies—all of which show that our models generate less gendered, but\nequally engaging chit-chat responses.</p>\n", "tags": ["Datasets","EMNLP","Ethics & Fairness","Evaluation","Training Techniques"] },
{"key": "dinan2019second", "citations": "385", "year": "2019", "title":"The Second Conversational Intelligence Challenge (convai2)", "abstract": "<p>We describe the setting and results of the ConvAI2 NeurIPS competition that\naims to further the state-of-the-art in open-domain chatbots. Some key\ntakeaways from the competition are: (i) pretrained Transformer variants are\ncurrently the best performing models on this task, (ii) but to improve\nperformance on multi-turn conversations with humans, future systems must go\nbeyond single word metrics like perplexity to measure the performance across\nsequences of utterances (conversations) – in terms of repetition, consistency\nand balance of dialogue acts (e.g. how many questions asked vs. answered).</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "ding2019cognitive", "citations": "204", "year": "2019", "title":"Cognitive Graph For Multi-hop Reading Comprehension At Scale", "abstract": "<p>We propose a new CogQA framework for multi-hop question answering in\nweb-scale documents. Inspired by the dual process theory in cognitive science,\nthe framework gradually builds a \\textit{cognitive graph} in an iterative\nprocess by coordinating an implicit extraction module (System 1) and an\nexplicit reasoning module (System 2). While giving accurate answers, our\nframework further provides explainable reasoning paths. Specifically, our\nimplementation based on BERT and graph neural network efficiently handles\nmillions of documents for multi-hop reasoning questions in the HotpotQA\nfullwiki dataset, achieving a winning joint \\(F_1\\) score of 34.9 on the\nleaderboard, compared to 23.6 of the best competitor.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "ding2019saliency", "citations": "60", "year": "2019", "title":"Saliency-driven Word Alignment Interpretation For Neural Machine Translation", "abstract": "<p>Despite their original goal to jointly learn to align and translate, Neural\nMachine Translation (NMT) models, especially Transformer, are often perceived\nas not learning interpretable word alignments. In this paper, we show that NMT\nmodels do learn interpretable word alignments, which could only be revealed\nwith proper interpretation methods. We propose a series of such methods that\nare model-agnostic, are able to be applied either offline or online, and do not\nrequire parameter update or architectural change. We show that under the force\ndecoding setup, the alignments induced by our interpretation method are of\nbetter quality than fast-align for some systems, and when performing free\ndecoding, they agree well with the alignments induced by automatic alignment\ntools.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "ding2021openprompt", "citations": "136", "year": "2022", "title":"Openprompt: An Open-source Framework For Prompt-learning", "abstract": "<p>Prompt-learning has become a new paradigm in modern natural language\nprocessing, which directly adapts pre-trained language models (PLMs) to\n\\(cloze\\)-style prediction, autoregressive modeling, or sequence to sequence\ngeneration, resulting in promising performances on various tasks. However, no\nstandard implementation framework of prompt-learning is proposed yet, and most\nexisting prompt-learning codebases, often unregulated, only provide limited\nimplementations for specific scenarios. Since there are many details such as\ntemplating strategy, initializing strategy, and verbalizing strategy, etc. need\nto be considered in prompt-learning, practitioners face impediments to quickly\nadapting the desired prompt learning methods to their applications. In this\npaper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct\nprompt-learning over PLMs. OpenPrompt is a research-friendly framework that is\nequipped with efficiency, modularity, and extendibility, and its combinability\nallows the freedom to combine different PLMs, task formats, and prompting\nmodules in a unified paradigm. Users could expediently deploy prompt-learning\nframeworks and evaluate the generalization of them on different NLP tasks\nwithout constraints. OpenPrompt is publicly released at {\\url{\nhttps://github.com/thunlp/OpenPrompt}}.</p>\n", "tags": ["Applications","Has Code","Prompting","Tools"] },
{"key": "ding2021prompt", "citations": "78", "year": "2022", "title":"Prompt-learning For Fine-grained Entity Typing", "abstract": "<p>As an effective approach to tune pre-trained language models (PLMs) for\nspecific tasks, prompt-learning has recently attracted much attention from\nresearchers. By using \\textit{cloze}-style language prompts to stimulate the\nversatile knowledge of PLMs, prompt-learning can achieve promising results on a\nseries of NLP tasks, such as natural language inference, sentiment\nclassification, and knowledge probing. In this work, we investigate the\napplication of prompt-learning on fine-grained entity typing in fully\nsupervised, few-shot and zero-shot scenarios. We first develop a simple and\neffective prompt-learning pipeline by constructing entity-oriented verbalizers\nand templates and conducting masked language modeling. Further, to tackle the\nzero-shot regime, we propose a self-supervised strategy that carries out\ndistribution-level optimization in prompt-learning to automatically summarize\nthe information of entity types. Extensive experiments on three fine-grained\nentity typing benchmarks (with up to 86 classes) under fully supervised,\nfew-shot and zero-shot settings show that prompt-learning methods significantly\noutperform fine-tuning baselines, especially when the training data is\ninsufficient.</p>\n", "tags": ["EMNLP","Few-Shot","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "ding2022cogview2", "citations": "106", "year": "2022", "title":"Cogview2: Faster And Better Text-to-image Generation Via Hierarchical Transformers", "abstract": "<p>The development of the transformer-based text-to-image models are impeded by\nits slow generation and complexity for high-resolution images. In this work, we\nput forward a solution based on hierarchical transformers and local parallel\nauto-regressive generation. We pretrain a 6B-parameter transformer with a\nsimple and flexible self-supervised task, Cross-modal general language model\n(CogLM), and finetune it for fast super-resolution. The new text-to-image\nsystem, CogView2, shows very competitive generation compared to concurrent\nstate-of-the-art DALL-E-2, and naturally supports interactive text-guided\nediting on images.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "ding2022delta", "citations": "99", "year": "2022", "title":"Delta Tuning: A Comprehensive Study Of Parameter Efficient Methods For Pre-trained Language Models", "abstract": "<p>Despite the success, the process of fine-tuning large-scale PLMs brings\nprohibitive adaptation costs. In fact, fine-tuning all the parameters of a\ncolossal model and retaining separate instances for different tasks are\npractically infeasible. This necessitates a new branch of research focusing on\nthe parameter-efficient adaptation of PLMs, dubbed as delta tuning in this\npaper. In contrast with the standard fine-tuning, delta tuning only fine-tunes\na small portion of the model parameters while keeping the rest untouched,\nlargely reducing both the computation and storage costs. Recent studies have\ndemonstrated that a series of delta tuning methods with distinct tuned\nparameter selection could achieve performance on a par with full-parameter\nfine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In\nthis paper, we first formally describe the problem of delta tuning and then\ncomprehensively review recent delta tuning approaches. We also propose a\nunified categorization criterion that divide existing delta tuning methods into\nthree groups: addition-based, specification-based, and reparameterization-based\nmethods. Though initially proposed as an efficient method to steer large\nmodels, we believe that some of the fascinating evidence discovered along with\ndelta tuning could help further reveal the mechanisms of PLMs and even deep\nneural networks. To this end, we discuss the theoretical principles underlying\nthe effectiveness of delta tuning and propose frameworks to interpret delta\ntuning from the perspective of optimization and optimal control, respectively.\nFurthermore, we provide a holistic empirical study of representative methods,\nwhere results on over 100 NLP tasks demonstrate a comprehensive performance\ncomparison of different approaches. The experimental results also cover the\nanalysis of combinatorial, scaling and transferable properties of delta tuning.</p>\n", "tags": ["Efficiency","Fine-Tuning","Training Techniques"] },
{"key": "ding2022is", "citations": "93", "year": "2023", "title":"Is GPT-3 A Good Data Annotator?", "abstract": "<p>Data annotation is the process of labeling data that could be used to train\nmachine learning models. Having high-quality annotation is crucial, as it\nallows the model to learn the relationship between the input data and the\ndesired output. GPT-3, a large-scale language model developed by OpenAI, has\ndemonstrated impressive zero- and few-shot performance on a wide range of NLP\ntasks. It is therefore natural to wonder whether it can be used to effectively\nannotate data for NLP tasks. In this paper, we evaluate the performance of\nGPT-3 as a data annotator by comparing it with traditional data annotation\nmethods and analyzing its output on a range of tasks. Through this analysis, we\naim to provide insight into the potential of GPT-3 as a general-purpose data\nannotator in NLP.</p>\n", "tags": ["Datasets","Few-Shot","Model Architecture"] },
{"key": "ding2022mukea", "citations": "82", "year": "2022", "title":"Mukea: Multimodal Knowledge Extraction And Accumulation For Knowledge-based Visual Question Answering", "abstract": "<p>Knowledge-based visual question answering requires the ability of associating\nexternal knowledge for open-ended cross-modal scene understanding. One\nlimitation of existing solutions is that they capture relevant knowledge from\ntext-only knowledge bases, which merely contain facts expressed by first-order\npredicates or language descriptions while lacking complex but indispensable\nmultimodal knowledge for visual understanding. How to construct vision-relevant\nand explainable multimodal knowledge for the VQA scenario has been less\nstudied. In this paper, we propose MuKEA to represent multimodal knowledge by\nan explicit triplet to correlate visual objects and fact answers with implicit\nrelations. To bridge the heterogeneous gap, we propose three objective losses\nto learn the triplet representations from complementary views: embedding\nstructure, topological relation and semantic space. By adopting a pre-training\nand fine-tuning learning strategy, both basic and domain-specific multimodal\nknowledge are progressively accumulated for answer prediction. We outperform\nthe state-of-the-art by 3.35% and 6.08% respectively on two challenging\nknowledge-required datasets: OK-VQA and KRVQA. Experimental results prove the\ncomplementary benefits of the multimodal knowledge with existing knowledge\nbases and the advantages of our end-to-end framework over the existing pipeline\nmethods. The code is available at https://github.com/AndersonStra/MuKEA.</p>\n", "tags": ["CVPR","Datasets","Fine-Tuning","Has Code","Tools","Training Techniques"] },
{"key": "ding2022vlt", "citations": "83", "year": "2022", "title":"VLT: Vision-language Transformer And Query Generation For Referring Segmentation", "abstract": "<p>We propose a Vision-Language Transformer (VLT) framework for referring\nsegmentation to facilitate deep interactions among multi-modal information and\nenhance the holistic understanding to vision-language features. There are\ndifferent ways to understand the dynamic emphasis of a language expression,\nespecially when interacting with the image. However, the learned queries in\nexisting transformer works are fixed after training, which cannot cope with the\nrandomness and huge diversity of the language expressions. To address this\nissue, we propose a Query Generation Module, which dynamically produces\nmultiple sets of input-specific queries to represent the diverse comprehensions\nof language expression. To find the best among these diverse comprehensions, so\nas to generate a better mask, we propose a Query Balance Module to selectively\nfuse the corresponding responses of the set of queries. Furthermore, to enhance\nthe model’s ability in dealing with diverse language expressions, we consider\ninter-sample learning to explicitly endow the model with knowledge of\nunderstanding different language expressions to the same object. We introduce\nmasked contrastive learning to narrow down the features of different\nexpressions for the same target object while distinguishing the features of\ndifferent objects. The proposed approach is lightweight and achieves new\nstate-of-the-art referring segmentation results consistently on five datasets.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "djuric2016hierarchical", "citations": "78", "year": "2015", "title":"Hierarchical Neural Language Models For Joint Representation Of Streaming Documents And Their Content", "abstract": "<p>We consider the problem of learning distributed representations for documents\nin data streams. The documents are represented as low-dimensional vectors and\nare jointly learned with distributed vector representations of word tokens\nusing a hierarchical framework with two embedded neural language models. In\nparticular, we exploit the context of documents in streams and use one of the\nlanguage models to model the document sequences, and the other to model word\nsequences within them. The models learn continuous vector representations for\nboth word tokens and documents such that semantically similar documents and\nwords are close in a common vector space. We discuss extensions to our model,\nwhich can be applied to personalized recommendation and social relationship\nmining by adding further user layers to the hierarchy, thus learning\nuser-specific vectors to represent individual preferences. We validated the\nlearned representations on a public movie rating data set from MovieLens, as\nwell as on a large-scale Yahoo News data comprising three months of user\nactivity logs collected on Yahoo servers. The results indicate that the\nproposed model can learn useful representations of both documents and word\ntokens, outperforming the current state-of-the-art by a large margin.</p>\n", "tags": ["Tools"] },
{"key": "dmitry2020gshard", "citations": "294", "year": "2020", "title":"Gshard: Scaling Giant Models With Conditional Computation And Automatic Sharding", "abstract": "<p>Neural network scaling has been critical for improving the model quality in\nmany real-world machine learning applications with vast amounts of training\ndata and compute. Although this trend of scaling is affirmed to be a sure-fire\napproach for better model quality, there are challenges on the path such as the\ncomputation cost, ease of programming, and efficient implementation on parallel\ndevices. GShard is a module composed of a set of lightweight annotation APIs\nand an extension to the XLA compiler. It provides an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the\nexisting model code. GShard enabled us to scale up multilingual neural machine\ntranslation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600\nbillion parameters using automatic sharding. We demonstrate that such a giant\nmodel can efficiently be trained on 2048 TPU v3 accelerators in 4 days to\nachieve far superior quality for translation from 100 languages to English\ncompared to the prior art.</p>\n", "tags": ["Applications","Model Architecture","Training Techniques"] },
{"key": "do2019compact", "citations": "64", "year": "2019", "title":"Compact Trilinear Interaction For Visual Question Answering", "abstract": "<p>In Visual Question Answering (VQA), answers have a great correlation with\nquestion meaning and visual contents. Thus, to selectively utilize image,\nquestion and answer information, we propose a novel trilinear interaction model\nwhich simultaneously learns high level associations between these three inputs.\nIn addition, to overcome the interaction complexity, we introduce a multimodal\ntensor-based PARALIND decomposition which efficiently parameterizes trilinear\ninteraction between the three inputs. Moreover, knowledge distillation is first\ntime applied in Free-form Opened-ended VQA. It is not only for reducing the\ncomputational cost and required memory but also for transferring knowledge from\ntrilinear interaction model to bilinear interaction model. The extensive\nexperiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the\nproposed compact trilinear interaction model achieves state-of-the-art results\nwhen using a single model on all three datasets.</p>\n", "tags": ["Datasets","ICCV"] },
{"key": "do2021multiple", "citations": "84", "year": "2021", "title":"Multiple Meta-model Quantifying For Medical Visual Question Answering", "abstract": "<p>Transfer learning is an important step to extract meaningful features and\novercome the data limitation in the medical Visual Question Answering (VQA)\ntask. However, most of the existing medical VQA methods rely on external data\nfor transfer learning, while the meta-data within the dataset is not fully\nutilized. In this paper, we present a new multiple meta-model quantifying\nmethod that effectively learns meta-annotation and leverages meaningful\nfeatures to the medical VQA task. Our proposed method is designed to increase\nmeta-data by auto-annotation, deal with noisy labels, and output meta-models\nwhich provide robust features for medical VQA tasks. Extensively experimental\nresults on two public medical VQA datasets show that our approach achieves\nsuperior accuracy in comparison with other state-of-the-art methods, while does\nnot require external data to train meta-models.</p>\n", "tags": ["Datasets","Fine-Tuning"] },
{"key": "dodge2019explaining", "citations": "128", "year": "2019", "title":"Explaining Models: An Empirical Study Of How Explanations Impact Fairness Judgment", "abstract": "<p>Ensuring fairness of machine learning systems is a human-in-the-loop process.\nIt relies on developers, users, and the general public to identify fairness\nproblems and make improvements. To facilitate the process we need effective,\nunbiased, and user-friendly explanations that people can confidently rely on.\nTowards that end, we conducted an empirical study with four types of\nprogrammatically generated explanations to understand how they impact people’s\nfairness judgments of ML systems. With an experiment involving more than 160\nMechanical Turk workers, we show that: 1) Certain explanations are considered\ninherently less fair, while others can enhance people’s confidence in the\nfairness of the algorithm; 2) Different fairness problems–such as model-wide\nfairness issues versus case-specific fairness discrepancies–may be more\neffectively exposed through different styles of explanation; 3) Individual\ndifferences, including prior positions and judgment criteria of algorithmic\nfairness, impact how people react to different styles of explanation. We\nconclude with a discussion on providing personalized and adaptive explanations\nto support fairness judgments of ML systems.</p>\n", "tags": ["Ethics & Fairness"] },
{"key": "doetsch2016returnn", "citations": "71", "year": "2017", "title":"RETURNN: The RWTH Extensible Training Framework For Universal Recurrent Neural Networks", "abstract": "<p>In this work we release our extensible and easily configurable neural network\ntraining software. It provides a rich set of functional layers with a\nparticular focus on efficient training of recurrent neural network topologies\non multiple GPUs. The source of the software package is public and freely\navailable for academic research purposes and can be used as a framework or as a\nstandalone tool which supports a flexible configuration. The software allows to\ntrain state-of-the-art deep bidirectional long short-term memory (LSTM) models\non both one dimensional data like speech or two dimensional data like\nhandwritten text and was used to develop successful submission systems in\nseveral evaluation campaigns.</p>\n", "tags": ["Evaluation","ICASSP","Model Architecture","Tools","Training Techniques"] },
{"key": "domhan2020sockeye", "citations": "71", "year": "2020", "title":"The Sockeye 2 Neural Machine Translation Toolkit At AMTA 2020", "abstract": "<p>We present Sockeye 2, a modernized and streamlined version of the Sockeye\nneural machine translation (NMT) toolkit. New features include a simplified\ncode base through the use of MXNet’s Gluon API, a focus on state of the art\nmodel architectures, distributed mixed precision training, and efficient CPU\ndecoding with 8-bit quantization. These improvements result in faster training\nand inference, higher automatic metric scores, and a shorter path from research\nto production.</p>\n", "tags": ["Efficiency","Model Architecture","Tools","Training Techniques"] },
{"key": "donahue2020enabling", "citations": "134", "year": "2020", "title":"Enabling Language Models To Fill In The Blanks", "abstract": "<p>We present a simple approach for text infilling, the task of predicting\nmissing spans of text at any position in a document. While infilling could\nenable rich functionality especially for writing assistance tools, more\nattention has been devoted to language modeling—a special case of infilling\nwhere text is predicted at the end of a document. In this paper, we aim to\nextend the capabilities of language models (LMs) to the more general task of\ninfilling. To this end, we train (or fine-tune) off-the-shelf LMs on sequences\ncontaining the concatenation of artificially-masked text and the text which was\nmasked. We show that this approach, which we call infilling by language\nmodeling, can enable LMs to infill entire sentences effectively on three\ndifferent domains: short stories, scientific abstracts, and lyrics.\nFurthermore, we show that humans have difficulty identifying sentences infilled\nby our approach as machine-generated in the domain of short stories.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "donahue2020end", "citations": "70", "year": "2020", "title":"End-to-end Adversarial Text-to-speech", "abstract": "<p>Modern text-to-speech synthesis pipelines typically involve multiple\nprocessing stages, each of which is designed or learnt independently from the\nrest. In this work, we take on the challenging task of learning to synthesise\nspeech from normalised text or phonemes in an end-to-end manner, resulting in\nmodels which operate directly on character or phoneme input sequences and\nproduce raw speech audio outputs. Our proposed generator is feed-forward and\nthus efficient for both training and inference, using a differentiable\nalignment scheme based on token length prediction. It learns to produce high\nfidelity audio through a combination of adversarial feedback and prediction\nlosses constraining the generated audio to roughly match the ground truth in\nterms of its total duration and mel-spectrogram. To allow the model to capture\ntemporal variation in the generated audio, we employ soft dynamic time warping\nin the spectrogram-based prediction loss. The resulting model achieves a mean\nopinion score exceeding 4 on a 5 point scale, which is comparable to the\nstate-of-the-art models relying on multi-stage training and additional\nsupervision.</p>\n", "tags": ["Security","Training Techniques"] },
{"key": "dong2016language", "citations": "667", "year": "2016", "title":"Language To Logical Form With Neural Attention", "abstract": "<p>Semantic parsing aims at mapping natural language to machine interpretable\nmeaning representations. Traditional approaches rely on high-quality lexicons,\nmanually-built templates, and linguistic features which are either domain- or\nrepresentation-specific. In this paper we present a general method based on an\nattention-enhanced encoder-decoder model. We encode input utterances into\nvector representations, and generate their logical forms by conditioning the\noutput sequences or trees on the encoding vectors. Experimental results on four\ndatasets show that our approach performs competitively without using\nhand-engineered features and is easy to adapt across domains and meaning\nrepresentations.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "dong2017learning", "citations": "192", "year": "2017", "title":"Learning To Paraphrase For Question Answering", "abstract": "<p>Question answering (QA) systems are sensitive to the many different ways\nnatural language expresses the same information need. In this paper we turn to\nparaphrases as a means of capturing this knowledge and present a general\nframework which learns felicitous paraphrases for various QA tasks. Our method\nis trained end-to-end using question-answer pairs as a supervision signal. A\nquestion and its paraphrases serve as input to a neural scoring model which\nassigns higher weights to linguistic expressions most likely to yield correct\nanswers. We evaluate our approach on QA over Freebase and answer sentence\nselection. Experimental results on three datasets show that our framework\nconsistently improves performance, achieving competitive results despite the\nuse of simple QA models.</p>\n", "tags": ["Datasets","EMNLP","Tools"] },
{"key": "dong2018confidence", "citations": "68", "year": "2018", "title":"Confidence Modeling For Neural Semantic Parsing", "abstract": "<p>In this work we focus on confidence modeling for neural semantic parsers\nwhich are built upon sequence-to-sequence models. We outline three major causes\nof uncertainty, and design various metrics to quantify these factors. These\nmetrics are then used to estimate confidence scores that indicate whether model\npredictions are likely to be correct. Beyond confidence estimation, we identify\nwhich parts of the input contribute to uncertain predictions allowing users to\ninterpret their model, and verify or refine its input. Experimental results\nshow that our confidence model significantly outperforms a widely used method\nthat relies on posterior probability, and improves the quality of\ninterpretation compared to simply relying on attention scores.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "dong2019editnts", "citations": "149", "year": "2019", "title":"Editnts: An Neural Programmer-interpreter Model For Sentence Simplification Through Explicit Editing", "abstract": "<p>We present the first sentence simplification model that learns explicit edit\noperations (ADD, DELETE, and KEEP) via a neural programmer-interpreter\napproach. Most current neural sentence simplification systems are variants of\nsequence-to-sequence models adopted from machine translation. These methods\nlearn to simplify sentences as a byproduct of the fact that they are trained on\ncomplex-simple sentence pairs. By contrast, our neural programmer-interpreter\nis directly trained to predict explicit edit operations on targeted parts of\nthe input sentence, resembling the way that humans might perform simplification\nand revision. Our model outperforms previous state-of-the-art neural sentence\nsimplification models (without external knowledge) by large margins on three\nbenchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89\nWikiSmall, +1.41 Newsela), and is judged by humans to produce overall better\nand simpler output sentences.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "dong2019unified", "citations": "831", "year": "2019", "title":"Unified Language Model Pre-training For Natural Language Understanding And Generation", "abstract": "<p>This paper presents a new Unified pre-trained Language Model (UniLM) that can\nbe fine-tuned for both natural language understanding and generation tasks. The\nmodel is pre-trained using three types of language modeling tasks:\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\nmodeling is achieved by employing a shared Transformer network and utilizing\nspecific self-attention masks to control what context the prediction conditions\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\nand CoQA question answering tasks. Moreover, UniLM achieves new\nstate-of-the-art results on five natural language generation datasets,\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\nis 2.65). The code and pre-trained models are available at\nhttps://github.com/microsoft/unilm.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "dong2021survey", "citations": "101", "year": "2022", "title":"A Survey Of Natural Language Generation", "abstract": "<p>This paper offers a comprehensive review of the research on Natural Language\nGeneration (NLG) over the past two decades, especially in relation to\ndata-to-text generation and text-to-text generation deep learning methods, as\nwell as new applications of NLG technology. This survey aims to (a) give the\nlatest synthesis of deep learning research on the NLG core tasks, as well as\nthe architectures adopted in the field; (b) detail meticulously and\ncomprehensively various NLG tasks and datasets, and draw attention to the\nchallenges in NLG evaluation, focusing on different evaluation methods and\ntheir relationships; (c) highlight some future emphasis and relatively recent\nresearch issues that arise due to the increasing synergy between NLG and other\nartificial intelligence areas, such as computer vision, text and computational\ncreativity.</p>\n", "tags": ["Applications","Datasets","Evaluation","Model Architecture","Survey Paper"] },
{"key": "dong2022maskclip", "citations": "76", "year": "2023", "title":"Maskclip: Masked Self-distillation Advances Contrastive Language-image Pretraining", "abstract": "<p>This paper presents a simple yet effective framework MaskCLIP, which\nincorporates a newly proposed masked self-distillation into contrastive\nlanguage-image pretraining. The core idea of masked self-distillation is to\ndistill representation from a full image to the representation predicted from a\nmasked image. Such incorporation enjoys two vital benefits. First, masked\nself-distillation targets local patch representation learning, which is\ncomplementary to vision-language contrastive focusing on text-related\nrepresentation. Second, masked self-distillation is also consistent with\nvision-language contrastive from the perspective of training objective as both\nutilize the visual encoder for feature aligning, and thus is able to learn\nlocal semantics getting indirect supervision from the language. We provide\nspecially designed experiments with a comprehensive analysis to validate the\ntwo benefits. Symmetrically, we also introduce the local semantic supervision\ninto the text branch, which further improves the pretraining performance. With\nextensive experiments, we show that MaskCLIP, when applied to various\nchallenging downstream tasks, achieves superior results in linear probing,\nfinetuning, and zero-shot performance with the guidance of the language\nencoder. Code will be release at https://github.com/LightDXY/MaskCLIP.</p>\n", "tags": ["CVPR","Has Code","Tools","Training Techniques"] },
{"key": "dong2022survey", "citations": "220", "year": "2023", "title":"A Survey On In-context Learning", "abstract": "<p>With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.</p>\n", "tags": ["In Context Learning","Prompting","Survey Paper","Training Techniques"] },
{"key": "dou2018exploiting", "citations": "92", "year": "2018", "title":"Exploiting Deep Representations For Neural Machine Translation", "abstract": "<p>Advanced neural machine translation (NMT) models generally implement encoder\nand decoder as multiple layers, which allows systems to model complex functions\nand capture complicated linguistic structures. However, only the top layers of\nencoder and decoder are leveraged in the subsequent process, which misses the\nopportunity to exploit the useful information embedded in other layers. In this\nwork, we propose to simultaneously expose all of these signals with layer\naggregation and multi-layer attention mechanisms. In addition, we introduce an\nauxiliary regularization term to encourage different layers to capture diverse\ninformation. Experimental results on widely-used WMT14 English-German and WMT17\nChinese-English translation data demonstrate the effectiveness and universality\nof the proposed approach.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "dou2019investigating", "citations": "116", "year": "2019", "title":"Investigating Meta-learning Algorithms For Low-resource Natural Language Understanding Tasks", "abstract": "<p>Learning general representations of text is a fundamental problem for many\nnatural language understanding (NLU) tasks. Previously, researchers have\nproposed to use language model pre-training and multi-task learning to learn\nrobust representations. However, these methods can achieve sub-optimal\nperformance in low-resource scenarios. Inspired by the recent success of\noptimization-based meta-learning algorithms, in this paper, we explore the\nmodel-agnostic meta-learning algorithm (MAML) and its variants for low-resource\nNLU tasks. We validate our methods on the GLUE benchmark and show that our\nproposed models can outperform several strong baselines. We further empirically\ndemonstrate that the learned representations can be adapted to new tasks\nefficiently and effectively.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "dou2020gsum", "citations": "186", "year": "2021", "title":"Gsum: A General Framework For Guided Neural Abstractive Summarization", "abstract": "<p>Neural abstractive summarization models are flexible and can produce coherent\nsummaries, but they are sometimes unfaithful and can be difficult to control.\nWhile previous studies attempt to provide different types of guidance to\ncontrol the output and increase faithfulness, it is not clear how these\nstrategies compare and contrast to each other. In this paper, we propose a\ngeneral and extensible guided summarization framework (GSum) that can\neffectively take different kinds of external guidance as input, and we perform\nexperiments across several different varieties. Experiments demonstrate that\nthis model is effective, achieving state-of-the-art performance according to\nROUGE on 4 popular summarization datasets when using highlighted sentences as\nguidance. In addition, we show that our guided model can generate more faithful\nsummaries and demonstrate how different types of guidance generate\nqualitatively different summaries, lending a degree of controllability to the\nlearned models.</p>\n", "tags": ["Datasets","NAACL","Tools"] },
{"key": "dou2021empirical", "citations": "236", "year": "2022", "title":"An Empirical Study Of Training End-to-end Vision-and-language Transformers", "abstract": "<p>Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.</p>\n", "tags": ["CVPR","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "dou2021is", "citations": "61", "year": "2022", "title":"Is GPT-3 Text Indistinguishable From Human Text? Scarecrow: A Framework For Scrutinizing Machine Text", "abstract": "<p>Modern neural language models can produce remarkably fluent and grammatical\ntext. So much, in fact, that recent work by Clark et al. (2021) has reported\nthat conventional crowdsourcing can no longer reliably distinguish between\nmachine-authored (GPT-3) and human-authored writing. As errors in machine\ngenerations become ever subtler and harder to spot, it poses a new challenge to\nthe research community for robust machine text evaluation. We propose a new\nframework called Scarecrow for scrutinizing machine text via crowd annotation.\nTo support the broad range of real machine errors that can be identified by\nlaypeople, the ten error categories of Scarecrow – such as redundancy,\ncommonsense errors, and incoherence – are identified through several rounds of\ncrowd annotation experiments without a predefined ontology. We then use\nScarecrow to collect over 41k error spans in human-written and\nmachine-generated paragraphs of English language news text. We isolate factors\nfor detailed analysis, including parameter count, training data, and various\ndecoding-time configurations. Our approach successfully quantifies measurable\ngaps between human authored text and generations from models of several sizes,\nincluding fourteen configurations of GPT-3. In addition, our analysis unveils\nnew insights, with detailed rationales provided by laypeople, e.g., that the\ncommonsense capabilities have been improving with larger models while math\ncapabilities have not, and that the choices of simple decoding hyperparameters\ncan make remarkable differences on the perceived quality of machine text. We\nrelease our training material, annotation toolkit and dataset at\nhttps://yao-dou.github.io/scarecrow/.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "du2017learning", "citations": "634", "year": "2017", "title":"Learning To Ask: Neural Question Generation For Reading Comprehension", "abstract": "<p>We study automatic question generation for sentences from text passages in\nreading comprehension. We introduce an attention-based sequence learning model\nfor the task and investigate the effect of encoding sentence- vs.\nparagraph-level information. In contrast to all previous work, our model does\nnot rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead\ntrainable end-to-end via sequence-to-sequence learning. Automatic evaluation\nresults show that our system significantly outperforms the state-of-the-art\nrule-based system. In human evaluations, questions generated by our system are\nalso rated as being more natural (i.e., grammaticality, fluency) and as more\ndifficult to answer (in terms of syntactic and lexical divergence from the\noriginal text and reasoning needed to answer).</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "du2018harvesting", "citations": "176", "year": "2018", "title":"Harvesting Paragraph-level Question-answer Pairs From Wikipedia", "abstract": "<p>We study the task of generating from Wikipedia articles question-answer pairs\nthat cover content beyond a single sentence. We propose a neural network\napproach that incorporates coreference knowledge via a novel gating mechanism.\nCompared to models that only take into account sentence-level information\n(Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the\nlinguistic knowledge introduced by the coreference representation aids question\ngeneration significantly, producing models that outperform the current\nstate-of-the-art. We apply our system (composed of an answer span extraction\nsystem and the passage-level QG system) to the 10,000 top-ranking Wikipedia\narticles and create a corpus of over one million question-answer pairs. We also\nprovide a qualitative analysis for this large-scale generated corpus from\nWikipedia.</p>\n", "tags": ["Datasets"] },
{"key": "du2019sequential", "citations": "96", "year": "2019", "title":"Sequential Scenario-specific Meta Learner For Online Recommendation", "abstract": "<p>Cold-start problems are long-standing challenges for practical\nrecommendations. Most existing recommendation algorithms rely on extensive\nobserved data and are brittle to recommendation scenarios with few\ninteractions. This paper addresses such problems using few-shot learning and\nmeta learning. Our approach is based on the insight that having a good\ngeneralization from a few examples relies on both a generic model\ninitialization and an effective strategy for adapting this model to newly\narising tasks. To accomplish this, we combine the scenario-specific learning\nwith a model-agnostic sequential meta-learning and unify them into an\nintegrated end-to-end framework, namely Scenario-specific Sequential Meta\nlearner (or s^2 meta). By doing so, our meta-learner produces a generic initial\nmodel through aggregating contextual information from a variety of prediction\ntasks while effectively adapting to specific tasks by leveraging\nlearning-to-learn knowledge. Extensive experiments on various real-world\ndatasets demonstrate that our proposed model can achieve significant gains over\nthe state-of-the-arts for cold-start problems in online recommendation.\nDeployment is at the Guess You Like session, the front page of the Mobile\nTaobao.</p>\n", "tags": ["Datasets","Few-Shot","KDD","Tools"] },
{"key": "du2020document", "citations": "90", "year": "2020", "title":"Document-level Event Role Filler Extraction Using Multi-granularity Contextualized Encoding", "abstract": "<p>Few works in the literature of event extraction have gone beyond individual\nsentences to make extraction decisions. This is problematic when the\ninformation needed to recognize an event argument is spread across multiple\nsentences. We argue that document-level event extraction is a difficult task\nsince it requires a view of a larger context to determine which spans of text\ncorrespond to event role fillers. We first investigate how end-to-end neural\nsequence models (with pre-trained language model representations) perform on\ndocument-level role filler extraction, as well as how the length of context\ncaptured affects the models’ performance. To dynamically aggregate information\ncaptured by neural representations learned at different levels of granularity\n(e.g., the sentence- and paragraph-level), we propose a novel multi-granularity\nreader. We evaluate our models on the MUC-4 event extraction dataset, and show\nthat our best system performs substantially better than prior work. We also\nreport findings on the relationship between context length and neural model\nperformance on the task.</p>\n", "tags": ["Datasets","Memory & Context"] },
{"key": "du2020self", "citations": "110", "year": "2021", "title":"Self-training Improves Pre-training For Natural Language Understanding", "abstract": "<p>Unsupervised pre-training has led to much recent progress in natural language\nunderstanding. In this paper, we study self-training as another way to leverage\nunlabeled data through semi-supervised learning. To obtain additional data for\na specific task, we introduce SentAugment, a data augmentation method which\ncomputes task-specific query embeddings from labeled data to retrieve sentences\nfrom a bank of billions of unlabeled sentences crawled from the web. Unlike\nprevious semi-supervised methods, our approach does not require in-domain\nunlabeled data and is therefore more generally applicable. Experiments show\nthat self-training is complementary to strong RoBERTa baselines on a variety of\ntasks. Our augmentation approach leads to scalable and effective self-training\nwith improvements of up to 2.6% on standard text classification benchmarks.\nFinally, we also show strong gains on knowledge-distillation and few-shot\nlearning.</p>\n", "tags": ["Efficiency","Few-Shot","NAACL","Training Techniques"] },
{"key": "du2021glam", "citations": "97", "year": "2021", "title":"Glam: Efficient Scaling Of Language Models With Mixture-of-experts", "abstract": "<p>Scaling language models with more data, compute and parameters has driven\nsignificant progress in natural language processing. For example, thanks to\nscaling, GPT-3 was able to achieve strong results on in-context learning tasks.\nHowever, training these large dense models requires significant amounts of\ncomputing resources. In this paper, we propose and develop a family of language\nmodels named GLaM (Generalist Language Model), which uses a sparsely activated\nmixture-of-experts architecture to scale the model capacity while also\nincurring substantially less training cost compared to dense variants. The\nlargest GLaM has 1.2 trillion parameters, which is approximately 7x larger than\nGPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half\nof the computation flops for inference, while still achieving better overall\nzero-shot and one-shot performance across 29 NLP tasks.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "du2021glm", "citations": "561", "year": "2022", "title":"GLM: General Language Model Pretraining With Autoregressive Blank Infilling", "abstract": "<p>There have been various types of pretraining architectures including\nautoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and\nencoder-decoder models (e.g., T5). However, none of the pretraining frameworks\nperforms the best for all tasks of three main categories including natural\nlanguage understanding (NLU), unconditional generation, and conditional\ngeneration. We propose a General Language Model (GLM) based on autoregressive\nblank infilling to address this challenge. GLM improves blank filling\npretraining by adding 2D positional encodings and allowing an arbitrary order\nto predict spans, which results in performance gains over BERT and T5 on NLU\ntasks. Meanwhile, GLM can be pretrained for different types of tasks by varying\nthe number and lengths of blanks. On a wide range of tasks across NLU,\nconditional and unconditional generation, GLM outperforms BERT, T5, and GPT\ngiven the same model sizes and data, and achieves the best performance from a\nsingle pretrained model with 1.25x parameters of BERT Large , demonstrating its\ngeneralizability to different downstream tasks.</p>\n", "tags": ["Model Architecture"] },
{"key": "du2021towards", "citations": "60", "year": "2021", "title":"Towards Interpreting And Mitigating Shortcut Learning Behavior Of NLU Models", "abstract": "<p>Recent studies indicate that NLU models are prone to rely on shortcut\nfeatures for prediction, without achieving true language understanding. As a\nresult, these models fail to generalize to real-world out-of-distribution data.\nIn this work, we show that the words in the NLU training set can be modeled as\na long-tailed distribution. There are two findings: 1) NLU models have strong\npreference for features located at the head of the long-tailed distribution,\nand 2) Shortcut features are picked up during very early few iterations of the\nmodel training. These two observations are further employed to formulate a\nmeasurement which can quantify the shortcut degree of each training sample.\nBased on this shortcut measurement, we propose a shortcut mitigation framework\nLTGR, to suppress the model from making overconfident predictions for samples\nwith large shortcut degree. Experimental results on three NLU benchmarks\ndemonstrate that our long-tailed distribution explanation accurately reflects\nthe shortcut learning behavior of NLU models. Experimental analysis further\nindicates that LTGR can improve the generalization accuracy on OOD data, while\npreserving the accuracy on in-distribution data.</p>\n", "tags": ["NAACL","Training Techniques"] },
{"key": "du2022learning", "citations": "193", "year": "2022", "title":"Learning To Prompt For Open-vocabulary Object Detection With Vision-language Model", "abstract": "<p>Recently, vision-language pre-training shows great potential in\nopen-vocabulary object detection, where detectors trained on base classes are\ndevised for detecting new classes. The class text embedding is firstly\ngenerated by feeding prompts to the text encoder of a pre-trained\nvision-language model. It is then used as the region classifier to supervise\nthe training of a detector. The key element that leads to the success of this\nmodel is the proper prompt, which requires careful words tuning and ingenious\ndesign. To avoid laborious prompt engineering, there are some prompt\nrepresentation learning methods being proposed for the image classification\ntask, which however can only be sub-optimal solutions when applied to the\ndetection task. In this paper, we introduce a novel method, detection prompt\n(DetPro), to learn continuous prompt representations for open-vocabulary object\ndetection based on the pre-trained vision-language model. Different from the\nprevious classification-oriented methods, DetPro has two highlights: 1) a\nbackground interpretation scheme to include the proposals in image background\ninto the prompt training; 2) a context grading scheme to separate proposals in\nimage foreground for tailored prompt training. We assemble DetPro with ViLD, a\nrecent state-of-the-art open-world object detector, and conduct experiments on\nthe LVIS as well as transfer learning on the Pascal VOC, COCO, Objects365\ndatasets. Experimental results show that our DetPro outperforms the baseline\nViLD in all settings, e.g., +3.4 APbox and +3.0 APmask improvements on the\nnovel classes of LVIS. Code and models are available at\nhttps://github.com/dyabel/detpro.</p>\n", "tags": ["CVPR","Datasets","Fine-Tuning","Prompting","Training Techniques"] },
{"key": "du2022survey", "citations": "79", "year": "2022", "title":"A Survey Of Vision-language Pre-trained Models", "abstract": "<p>As transformer evolves, pre-trained models have advanced at a breakneck pace\nin recent years. They have dominated the mainstream techniques in natural\nlanguage processing (NLP) and computer vision (CV). How to adapt pre-training\nto the field of Vision-and-Language (V-L) learning and improve downstream task\nperformance becomes a focus of multimodal learning. In this paper, we review\nthe recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the\ncore content, we first briefly introduce several ways to encode raw images and\ntexts to single-modal embeddings before pre-training. Then, we dive into the\nmainstream architectures of VL-PTMs in modeling the interaction between text\nand image representations. We further present widely-used pre-training tasks,\nand then we introduce some common downstream tasks. We finally conclude this\npaper and present some promising research directions. Our survey aims to\nprovide researchers with synthesis and pointer to related research.</p>\n", "tags": ["IJCAI","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "dua2019drop", "citations": "210", "year": "2019", "title":"DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "abstract": "<p>Reading comprehension has recently seen rapid progress, with systems matching\nhumans on the most popular datasets for the task. However, a large body of work\nhas highlighted the brittleness of these systems, showing that there is much\nwork left to be done. We introduce a new English reading comprehension\nbenchmark, DROP, which requires Discrete Reasoning Over the content of\nParagraphs. In this crowdsourced, adversarially-created, 96k-question\nbenchmark, a system must resolve references in a question, perhaps to multiple\ninput positions, and perform discrete operations over them (such as addition,\ncounting, or sorting). These operations require a much more comprehensive\nunderstanding of the content of paragraphs than what was necessary for prior\ndatasets. We apply state-of-the-art methods from both the reading comprehension\nand semantic parsing literature on this dataset and show that the best systems\nonly achieve 32.7% F1 on our generalized accuracy metric, while expert human\nperformance is 96.0%. We additionally present a new model that combines reading\ncomprehension methods with simple numerical reasoning to achieve 47.0% F1.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "duan2021survey", "citations": "171", "year": "2022", "title":"A Survey Of Embodied AI: From Simulators To Research Tasks", "abstract": "<p>There has been an emerging paradigm shift from the era of “internet AI” to\n“embodied AI”, where AI algorithms and agents no longer learn from datasets of\nimages, videos or text curated primarily from the internet. Instead, they learn\nthrough interactions with their environments from an egocentric perception\nsimilar to humans. Consequently, there has been substantial growth in the\ndemand for embodied AI simulators to support various embodied AI research\ntasks. This growing interest in embodied AI is beneficial to the greater\npursuit of Artificial General Intelligence (AGI), but there has not been a\ncontemporary and comprehensive survey of this field. This paper aims to provide\nan encyclopedic survey for the field of embodied AI, from its simulators to its\nresearch. By evaluating nine current embodied AI simulators with our proposed\nseven features, this paper aims to understand the simulators in their provision\nfor use in embodied AI research and their limitations. Lastly, this paper\nsurveys the three main research tasks in embodied AI – visual exploration,\nvisual navigation and embodied question answering (QA), covering the\nstate-of-the-art approaches, evaluation metrics and datasets. Finally, with the\nnew insights revealed through surveying the field, the paper will provide\nsuggestions for simulator-for-task selections and recommendations for the\nfuture directions of the field.</p>\n", "tags": ["Evaluation","Survey Paper"] },
{"key": "dunn2017searchqa", "citations": "423", "year": "2017", "title":"Searchqa: A New Q&A Dataset Augmented With Context From A Search Engine", "abstract": "<p>We publicly release a new large-scale dataset, called SearchQA, for machine\ncomprehension, or question-answering. Unlike recently released datasets, such\nas DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to\nreflect a full pipeline of general question-answering. That is, we start not\nfrom an existing article and generate a question-answer pair, but start from an\nexisting question-answer pair, crawled from J! Archive, and augment it with\ntext snippets retrieved by Google. Following this approach, we built SearchQA,\nwhich consists of more than 140k question-answer pairs with each pair having\n49.6 snippets on average. Each question-answer-context tuple of the SearchQA\ncomes with additional meta-data such as the snippet’s URL, which we believe\nwill be valuable resources for future research. We conduct human evaluation as\nwell as test two baseline methods, one simple word selection and the other deep\nlearning based, on the SearchQA. We show that there is a meaningful gap between\nthe human and machine performances. This suggests that the proposed dataset\ncould well serve as a benchmark for question-answering.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "durmus2020feqa", "citations": "277", "year": "2020", "title":"FEQA: A Question Answering Evaluation Framework For Faithfulness Assessment In Abstractive Summarization", "abstract": "<p>Neural abstractive summarization models are prone to generate content\ninconsistent with the source document, i.e. unfaithful. Existing automatic\nmetrics do not capture such mistakes effectively. We tackle the problem of\nevaluating faithfulness of a generated summary given its source document. We\nfirst collected human annotations of faithfulness for outputs from numerous\nmodels on two datasets. We find that current models exhibit a trade-off between\nabstractiveness and faithfulness: outputs with less word overlap with the\nsource document are more likely to be unfaithful. Next, we propose an automatic\nquestion answering (QA) based metric for faithfulness, FEQA, which leverages\nrecent advances in reading comprehension. Given question-answer pairs generated\nfrom the summary, a QA model extracts answers from the document; non-matched\nanswers indicate unfaithful information in the summary. Among metrics based on\nword overlap, embedding similarity, and learned language understanding models,\nour QA-based metric has significantly higher correlation with human\nfaithfulness scores, especially on highly abstractive summaries.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Evaluation","Tools"] },
{"key": "dušek2016context", "citations": "66", "year": "2016", "title":"A Context-aware Natural Language Generator For Dialogue Systems", "abstract": "<p>We present a novel natural language generation system for spoken dialogue\nsystems capable of entraining (adapting) to users’ way of speaking, providing\ncontextually appropriate responses. The generator is based on recurrent neural\nnetworks and the sequence-to-sequence approach. It is fully trainable from data\nwhich include preceding context along with responses to be generated. We show\nthat the context-aware generator yields significant improvements over the\nbaseline in both automatic metrics and a human pairwise preference test.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation"] },
{"key": "dušek2016sequence", "citations": "68", "year": "2016", "title":"Sequence-to-sequence Generation For Spoken Dialogue Via Deep Syntax Trees And Strings", "abstract": "<p>We present a natural language generator based on the sequence-to-sequence\napproach that can be trained to produce natural language strings as well as\ndeep syntax dependency trees from input dialogue acts, and we use it to\ndirectly compare two-step generation with separate sentence planning and\nsurface realization stages to a joint, one-step approach. We were able to train\nboth setups successfully using very little training data. The joint setup\noffers better performance, surpassing state-of-the-art with regards to\nn-gram-based scores while providing more relevant outputs.</p>\n", "tags": ["Dialogue & Multi Turn","Training Techniques"] },
{"key": "dušek2018findings", "citations": "100", "year": "2018", "title":"Findings Of The E2E NLG Challenge", "abstract": "<p>This paper summarises the experimental setup and results of the first shared\ntask on end-to-end (E2E) natural language generation (NLG) in spoken dialogue\nsystems. Recent end-to-end generation systems are promising since they reduce\nthe need for data annotation. However, they are currently limited to small,\ndelexicalised datasets. The E2E NLG shared task aims to assess whether these\nnovel approaches can generate better-quality output by learning from a dataset\ncontaining higher lexical richness, syntactic complexity and diverse discourse\nphenomena. We compare 62 systems submitted by 17 institutions, covering a wide\nrange of approaches, including machine learning architectures – with the\nmajority implementing sequence-to-sequence models (seq2seq) – as well as\nsystems based on grammatical rules and templates.</p>\n", "tags": ["Datasets"] },
{"key": "dušek2019evaluating", "citations": "151", "year": "2019", "title":"Evaluating The State-of-the-art Of End-to-end Natural Language Generation: The E2E NLG Challenge", "abstract": "<p>This paper provides a comprehensive analysis of the first shared task on\nEnd-to-End Natural Language Generation (NLG) and identifies avenues for future\nresearch based on the results. This shared task aimed to assess whether recent\nend-to-end NLG systems can generate more complex output by learning from\ndatasets containing higher lexical richness, syntactic complexity and diverse\ndiscourse phenomena. Introducing novel automatic and human metrics, we compare\n62 systems submitted by 17 institutions, covering a wide range of approaches,\nincluding machine learning architectures – with the majority implementing\nsequence-to-sequence models (seq2seq) – as well as systems based on\ngrammatical rules and templates. Seq2seq-based systems have demonstrated a\ngreat potential for NLG in the challenge. We find that seq2seq systems\ngenerally score high in terms of word-overlap metrics and human evaluations of\nnaturalness – with the winning SLUG system (Juraska et al., 2018) being\nseq2seq-based. However, vanilla seq2seq models often fail to correctly express\na given meaning representation if they lack a strong semantic control mechanism\napplied during decoding. Moreover, seq2seq models can be outperformed by\nhand-engineered systems in terms of overall quality, as well as complexity,\nlength and diversity of outputs. This research has influenced, inspired and\nmotivated a number of recent studies outwith the original competition, which we\nalso summarise as part of this paper.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "dušek2019semantic", "citations": "86", "year": "2019", "title":"Semantic Noise Matters For Neural Natural Language Generation", "abstract": "<p>Neural natural language generation (NNLG) systems are known for their\npathological outputs, i.e. generating text which is unrelated to the input\nspecification. In this paper, we show the impact of semantic noise on\nstate-of-the-art NNLG models which implement different semantic control\nmechanisms. We find that cleaned data can improve semantic correctness by up to\n97%, while maintaining fluency. We also find that the most common error is\nomitting information, rather than hallucination.</p>\n", "tags": [] },
{"key": "dziri2018augmenting", "citations": "70", "year": "2019", "title":"Augmenting Neural Response Generation With Context-aware Topical Attention", "abstract": "<p>Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in\ngenerating natural conversational exchanges. Notwithstanding the syntactically\nwell-formed responses generated by these neural network models, they are prone\nto be acontextual, short and generic. In this work, we introduce a Topical\nHierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven,\nmulti-turn response generation system intended to produce contextual and\ntopic-aware responses. Our model is built upon the basic Seq2Seq model by\naugmenting it with a hierarchical joint attention mechanism that incorporates\ntopical concepts and previous interactions into the response generation. To\ntrain our model, we provide a clean and high-quality conversational dataset\nmined from Reddit comments. We evaluate THRED on two novel automated metrics,\ndubbed Semantic Similarity and Response Echo Index, as well as with human\nevaluation. Our experiments demonstrate that the proposed model is able to\ngenerate more diverse and contextually relevant responses compared to the\nstrong baselines.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "dziri2019evaluating", "citations": "75", "year": "2019", "title":"Evaluating Coherence In Dialogue Systems Using Entailment", "abstract": "<p>Evaluating open-domain dialogue systems is difficult due to the diversity of\npossible correct answers. Automatic metrics such as BLEU correlate weakly with\nhuman annotations, resulting in a significant bias across different models and\ndatasets. Some researchers resort to human judgment experimentation for\nassessing response quality, which is expensive, time consuming, and not\nscalable. Moreover, judges tend to evaluate a small number of dialogues,\nmeaning that minor differences in evaluation configuration may lead to\ndissimilar results. In this paper, we present interpretable metrics for\nevaluating topic coherence by making use of distributed sentence\nrepresentations. Furthermore, we introduce calculable approximations of human\njudgment based on conversational coherence by adopting state-of-the-art\nentailment techniques. Results show that our metrics can be used as a surrogate\nfor human judgment, making it easy to evaluate dialogue systems on large-scale\ndatasets and allowing an unbiased estimate for the quality of the responses.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Ethics & Fairness","Evaluation"] },
{"key": "dziri2022origin", "citations": "83", "year": "2022", "title":"On The Origin Of Hallucinations In Conversational Models: Is It The Datasets Or The Models?", "abstract": "<p>Knowledge-grounded conversational models are known to suffer from producing\nfactually invalid statements, a phenomenon commonly called hallucination. In\nthis work, we investigate the underlying causes of this phenomenon: is\nhallucination due to the training data, or to the models? We conduct a\ncomprehensive human study on both existing knowledge-grounded conversational\nbenchmarks and several state-of-the-art models. Our study reveals that the\nstandard benchmarks consist of &gt;60% hallucinated responses, leading to models\nthat not only hallucinate but even amplify hallucinations. Our findings raise\nimportant questions on the quality of existing datasets and models trained\nusing them. We make our annotations publicly available for future research.</p>\n", "tags": ["Datasets","NAACL","Training Techniques"] },
{"key": "ebner2019multi", "citations": "132", "year": "2020", "title":"Multi-sentence Argument Linking", "abstract": "<p>We present a novel document-level model for finding argument spans that fill\nan event’s roles, connecting related ideas in sentence-level semantic role\nlabeling and coreference resolution. Because existing datasets for\ncross-sentence linking are small, development of our neural model is supported\nthrough the creation of a new resource, Roles Across Multiple Sentences (RAMS),\nwhich contains 9,124 annotated events across 139 types. We demonstrate strong\nperformance of our model on RAMS and other event-related datasets.</p>\n", "tags": ["Datasets"] },
{"key": "ebrahimi2018adversarial", "citations": "155", "year": "2018", "title":"On Adversarial Examples For Character-level Neural Machine Translation", "abstract": "<p>Evaluating on adversarial examples has become a standard procedure to measure\nrobustness of deep learning models. Due to the difficulty of creating white-box\nadversarial examples for discrete text input, most analyses of the robustness\nof NLP models have been done through black-box adversarial examples. We\ninvestigate adversarial examples for character-level neural machine translation\n(NMT), and contrast black-box adversaries with a novel white-box adversary,\nwhich employs differentiable string-edit operations to rank adversarial\nchanges. We propose two novel types of attacks which aim to remove or change a\nword in a translation, rather than simply break the NMT. We demonstrate that\nwhite-box adversarial examples are significantly stronger than their black-box\ncounterparts in different attack scenarios, which show more serious\nvulnerabilities than previously known. In addition, after performing\nadversarial training, which takes only 3 times longer than regular training, we\ncan improve the model’s robustness significantly.</p>\n", "tags": ["COLING","Security","Training Techniques"] },
{"key": "ebrahimi2021americasnli", "citations": "62", "year": "2022", "title":"Americasnli: Evaluating Zero-shot Natural Language Understanding Of Pretrained Multilingual Models In Truly Low-resource Languages", "abstract": "<p>Pretrained multilingual models are able to perform cross-lingual transfer in\na zero-shot setting, even for languages unseen during pretraining. However,\nprior work evaluating performance on unseen languages has largely been limited\nto low-level, syntactic tasks, and it remains unclear if zero-shot learning of\nhigh-level, semantic tasks is possible for unseen languages. To explore this\nquestion, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018)\nto 10 indigenous languages of the Americas. We conduct experiments with XLM-R,\ntesting multiple zero-shot and translation-based approaches. Additionally, we\nexplore model adaptation via continued pretraining and provide an analysis of\nthe dataset by considering hypothesis-only models. We find that XLM-R’s\nzero-shot performance is poor for all 10 languages, with an average performance\nof 38.62%. Continued pretraining offers improvements, with an average accuracy\nof 44.05%. Surprisingly, training on poorly translated data by far outperforms\nall other methods with an accuracy of 48.72%.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "edunov2017classical", "citations": "184", "year": "2018", "title":"Classical Structured Prediction Losses For Sequence To Sequence Learning", "abstract": "<p>There has been much recent work on training neural attention models at the\nsequence-level using either reinforcement learning-style methods or by\noptimizing the beam. In this paper, we survey a range of classical objective\nfunctions that have been widely used to train linear models for structured\nprediction and apply them to neural sequence to sequence models. Our\nexperiments show that these losses can perform surprisingly well by slightly\noutperforming beam search optimization in a like for like setup. We also report\nnew state of the art results on both IWSLT’14 German-English translation as\nwell as Gigaword abstractive summarization. On the larger WMT’14 English-French\ntranslation task, sequence-level training achieves 41.5 BLEU which is on par\nwith the state of the art.</p>\n", "tags": ["Model Architecture","NAACL","Survey Paper","Training Techniques"] },
{"key": "edunov2018understanding", "citations": "1035", "year": "2018", "title":"Understanding Back-translation At Scale", "abstract": "<p>An effective method to improve neural machine translation with monolingual\ndata is to augment the parallel training corpus with back-translations of\ntarget language sentences. This work broadens the understanding of\nback-translation and investigates a number of methods to generate synthetic\nsource sentences. We find that in all but resource poor settings\nback-translations obtained via sampling or noised beam outputs are most\neffective. Our analysis shows that sampling or noisy synthetic data gives a\nmuch stronger training signal than data generated by beam or greedy search. We\nalso compare how synthetic data compares to genuine bitext and study various\ndomain effects. Finally, we scale to hundreds of millions of monolingual\nsentences and achieve a new state of the art of 35 BLEU on the WMT’14\nEnglish-German test set.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "edunov2019evaluation", "citations": "85", "year": "2020", "title":"On The Evaluation Of Machine Translation Systems Trained With Back-translation", "abstract": "<p>Back-translation is a widely used data augmentation technique which leverages\ntarget monolingual data. However, its effectiveness has been challenged since\nautomatic metrics such as BLEU only show significant improvements for test\nexamples where the source itself is a translation, or translationese. This is\nbelieved to be due to translationese inputs better matching the back-translated\ntraining data. In this work, we show that this conjecture is not empirically\nsupported and that back-translation improves translation quality of both\nnaturally occurring text as well as translationese according to professional\nhuman translators. We provide empirical evidence to support the view that\nback-translation is preferred by humans because it produces more fluent\noutputs. BLEU cannot capture human preferences because references are\ntranslationese when source sentences are natural text. We recommend\ncomplementing BLEU with a language model score to measure fluency.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "edunov2019pre", "citations": "124", "year": "2019", "title":"Pre-trained Language Model Representations For Language Generation", "abstract": "<p>Pre-trained language model representations have been successful in a wide\nrange of language understanding tasks. In this paper, we examine different\nstrategies to integrate pre-trained representations into sequence to sequence\nmodels and apply it to neural machine translation and abstractive\nsummarization. We find that pre-trained representations are most effective when\nadded to the encoder network which slows inference by only 14%. Our experiments\nin machine translation show gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish with more labeled data, we still\nobserve improvements when millions of sentence-pairs are available. Finally, on\nabstractive summarization we achieve a new state of the art on the full text\nversion of CNN/DailyMail.</p>\n", "tags": [] },
{"key": "edward2021lora", "citations": "1724", "year": "2021", "title":"Lora: Low-rank Adaptation Of Large Language Models", "abstract": "<p>An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example –\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.</p>\n", "tags": ["Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "edwards2022translation", "citations": "63", "year": "2022", "title":"Translation Between Molecules And Natural Language", "abstract": "<p>We present \\(\\textbf{MolT5}\\) \\(-\\) a self-supervised learning framework for\npretraining models on a vast amount of unlabeled natural language text and\nmolecule strings. \\(\\textbf{MolT5}\\) allows for new, useful, and challenging\nanalogs of traditional vision-language tasks, such as molecule captioning and\ntext-based de novo molecule generation (altogether: translation between\nmolecules and language), which we explore for the first time. Since\n\\(\\textbf{MolT5}\\) pretrains models on single-modal data, it helps overcome the\nchemistry domain shortcoming of data scarcity. Furthermore, we consider several\nmetrics, including a new cross-modal embedding-based metric, to evaluate the\ntasks of molecule captioning and text-based molecule generation. Our results\nshow that \\(\\textbf{MolT5}\\)-based models are able to generate outputs, both\nmolecules and captions, which in many cases are high quality.</p>\n", "tags": ["EMNLP","Evaluation","Tools"] },
{"key": "efimov2019sberquad", "citations": "68", "year": "2020", "title":"Sberquad -- Russian Reading Comprehension Dataset: Description And Analysis", "abstract": "<p>SberQuAD – a large scale analog of Stanford SQuAD in the Russian language -\nis a valuable resource that has not been properly presented to the scientific\ncommunity. We fill this gap by providing a description, a thorough analysis,\nand baseline experimental results.</p>\n", "tags": ["Datasets"] },
{"key": "eger2017neural", "citations": "168", "year": "2017", "title":"Neural End-to-end Learning For Computational Argumentation Mining", "abstract": "<p>We investigate neural techniques for end-to-end computational argumentation\nmining (AM). We frame AM both as a token-based dependency parsing and as a\ntoken-based sequence tagging problem, including a multi-task learning setup.\nContrary to models that operate on the argument component level, we find that\nframing AM as dependency parsing leads to subpar performance results. In\ncontrast, less complex (local) tagging models based on BiLSTMs perform robustly\nacross classification scenarios, being able to catch long-range dependencies\ninherent to the AM problem. Moreover, we find that jointly learning ‘natural’\nsubtasks, in a multi-task learning setup, improves performance.</p>\n", "tags": [] },
{"key": "ehsan2021expanding", "citations": "65", "year": "2021", "title":"Expanding Explainability: Towards Social Transparency In AI Systems", "abstract": "<p>As AI-powered systems increasingly mediate consequential decision-making,\ntheir explainability is critical for end-users to take informed and accountable\nactions. Explanations in human-human interactions are socially-situated. AI\nsystems are often socio-organizationally embedded. However, Explainable AI\n(XAI) approaches have been predominantly algorithm-centered. We take a\ndevelopmental step towards socially-situated XAI by introducing and exploring\nSocial Transparency (ST), a sociotechnically informed perspective that\nincorporates the socio-organizational context into explaining AI-mediated\ndecision-making. To explore ST conceptually, we conducted interviews with 29 AI\nusers and practitioners grounded in a speculative design scenario. We suggested\nconstitutive design elements of ST and developed a conceptual framework to\nunpack ST’s effect and implications at the technical, decision-making, and\norganizational level. The framework showcases how ST can potentially calibrate\ntrust in AI, improve decision-making, facilitate organizational collective\nactions, and cultivate holistic explainability. Our work contributes to the\ndiscourse of Human-Centered XAI by expanding the design space of XAI.</p>\n", "tags": ["Ethics & Fairness"] },
{"key": "eisenschlos2019multifit", "citations": "101", "year": "2019", "title":"Multifit: Efficient Multi-lingual Language Model Fine-tuning", "abstract": "<p>Pretrained language models are promising particularly for low-resource\nlanguages as they only require unlabelled data. However, training existing\nmodels requires huge amounts of compute, while pretrained cross-lingual models\noften underperform on low-resource languages. We propose Multi-lingual language\nmodel Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune\nlanguage models efficiently in their own language. In addition, we propose a\nzero-shot method using an existing pretrained cross-lingual model. We evaluate\nour methods on two widely used cross-lingual classification datasets where they\noutperform models pretrained on orders of magnitude more data and compute. We\nrelease all models and code.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "elazar2020amnesic", "citations": "127", "year": "2021", "title":"Amnesic Probing: Behavioral Explanation With Amnesic Counterfactuals", "abstract": "<p>A growing body of work makes use of probing to investigate the working of\nneural models, often considered black boxes. Recently, an ongoing debate\nemerged surrounding the limitations of the probing paradigm. In this work, we\npoint out the inability to infer behavioral conclusions from probing results\nand offer an alternative method that focuses on how the information is being\nused, rather than on what information is encoded. Our method, Amnesic Probing,\nfollows the intuition that the utility of a property for a given task can be\nassessed by measuring the influence of a causal intervention that removes it\nfrom the representation. Equipped with this new analysis tool, we can ask\nquestions that were not possible before, e.g. is part-of-speech information\nimportant for word prediction? We perform a series of analyses on BERT to\nanswer these types of questions. Our findings demonstrate that conventional\nprobing performance is not correlated to task importance, and we call for\nincreased scrutiny of claims that draw behavioral or causal conclusions from\nprobing results.</p>\n", "tags": ["TACL"] },
{"key": "elazar2021measuring", "citations": "142", "year": "2021", "title":"Measuring And Improving Consistency In Pretrained Language Models", "abstract": "<p>Consistency of a model – that is, the invariance of its behavior under\nmeaning-preserving alternations in its input – is a highly desirable property\nin natural language processing. In this paper we study the question: Are\nPretrained Language Models (PLMs) consistent with respect to factual knowledge?\nTo this end, we create ParaRel, a high-quality resource of cloze-style query\nEnglish paraphrases. It contains a total of 328 paraphrases for 38 relations.\nUsing ParaRel, we show that the consistency of all PLMs we experiment with is\npoor – though with high variance between relations. Our analysis of the\nrepresentational spaces of PLMs suggests that they have a poor structure and\nare currently not suitable for representing knowledge robustly. Finally, we\npropose a method for improving model consistency and experimentally demonstrate\nits effectiveness.</p>\n", "tags": ["TACL"] },
{"key": "elliott2016multi30k", "citations": "406", "year": "2016", "title":"Multi30k: Multilingual English-german Image Descriptions", "abstract": "<p>We introduce the Multi30K dataset to stimulate multilingual multimodal\nresearch. Recent advances in image description have been demonstrated on\nEnglish-language datasets almost exclusively, but image description should not\nbe limited to English. This dataset extends the Flickr30K dataset with i)\nGerman translations created by professional translators over a subset of the\nEnglish descriptions, and ii) descriptions crowdsourced independently of the\noriginal English descriptions. We outline how the data can be used for\nmultilingual image description and multimodal machine translation, but we\nanticipate the data will be useful for a broader range of tasks.</p>\n", "tags": ["Datasets"] },
{"key": "elliott2017imagination", "citations": "91", "year": "2017", "title":"Imagination Improves Multimodal Translation", "abstract": "<p>We decompose multimodal translation into two sub-tasks: learning to translate\nand learning visually grounded representations. In a multitask learning\nframework, translations are learned in an attention-based encoder-decoder, and\ngrounded representations are learned through image representation prediction.\nOur approach improves translation performance compared to the state of the art\non the Multi30K dataset. Furthermore, it is equally effective if we train the\nimage prediction task on the external MS COCO dataset, and we find improvements\nif we train the translation model on the external News Commentary parallel\ntext.</p>\n", "tags": ["Datasets","Model Architecture","Tools"] },
{"key": "eloundou2023gpts", "citations": "421", "year": "2023", "title":"Gpts Are Gpts: An Early Look At The Labor Market Impact Potential Of Large Language Models", "abstract": "<p>We investigate the potential implications of large language models (LLMs),\nsuch as Generative Pre-trained Transformers (GPTs), on the U.S. labor market,\nfocusing on the increased capabilities arising from LLM-powered software\ncompared to LLMs on their own. Using a new rubric, we assess occupations based\non their alignment with LLM capabilities, integrating both human expertise and\nGPT-4 classifications. Our findings reveal that around 80% of the U.S.\nworkforce could have at least 10% of their work tasks affected by the\nintroduction of LLMs, while approximately 19% of workers may see at least 50%\nof their tasks impacted. We do not make predictions about the development or\nadoption timeline of such LLMs. The projected effects span all wage levels,\nwith higher-income jobs potentially facing greater exposure to LLM capabilities\nand LLM-powered software. Significantly, these impacts are not restricted to\nindustries with higher recent productivity growth. Our analysis suggests that,\nwith access to an LLM, about 15% of all worker tasks in the US could be\ncompleted significantly faster at the same level of quality. When incorporating\nsoftware and tooling built on top of LLMs, this share increases to between 47\nand 56% of all tasks. This finding implies that LLM-powered software will have\na substantial effect on scaling the economic impacts of the underlying models.\nWe conclude that LLMs such as GPTs exhibit traits of general-purpose\ntechnologies, indicating that they could have considerable economic, social,\nand policy implications.</p>\n", "tags": ["Model Architecture"] },
{"key": "elsahar2018zero", "citations": "78", "year": "2018", "title":"Zero-shot Question Generation From Knowledge Graphs For Unseen Predicates And Entity Types", "abstract": "<p>We present a neural model for question generation from knowledge base triples\nin a “Zero-Shot” setup, that is generating questions for triples containing\npredicates, subject types or object types that were not seen at training time.\nOur model leverages triples occurrences in the natural language corpus in an\nencoder-decoder architecture, paired with an original part-of-speech copy\naction mechanism to generate questions. Benchmark and human evaluation show\nthat our model sets a new state-of-the-art for zero-shot QG.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","NAACL","Training Techniques"] },
{"key": "eric2017copy", "citations": "126", "year": "2017", "title":"A Copy-augmented Sequence-to-sequence Architecture Gives Good Performance On Task-oriented Dialogue", "abstract": "<p>Task-oriented dialogue focuses on conversational agents that participate in\nuser-initiated dialogues on domain-specific topics. In contrast to chatbots,\nwhich simply seek to sustain open-ended meaningful discourse, existing\ntask-oriented agents usually explicitly model user intent and belief states.\nThis paper examines bypassing such an explicit representation by depending on a\nlatent neural embedding of state and learning selective attention to dialogue\nhistory together with copying to incorporate relevant prior context. We\ncomplement recent work by showing the effectiveness of simple\nsequence-to-sequence neural architectures with a copy mechanism. Our model\noutperforms more complex memory-augmented models by 7% in per-response\ngeneration and is on par with the current state-of-the-art on DSTC2.</p>\n", "tags": ["Model Architecture","NAACL"] },
{"key": "eric2017key", "citations": "398", "year": "2017", "title":"Key-value Retrieval Networks For Task-oriented Dialogue", "abstract": "<p>Neural task-oriented dialogue systems often struggle to smoothly interface\nwith a knowledge base. In this work, we seek to address this problem by\nproposing a new neural dialogue agent that is able to effectively sustain\ngrounded, multi-domain discourse through a novel key-value retrieval mechanism.\nThe model is end-to-end differentiable and does not need to explicitly model\ndialogue state or belief trackers. We also release a new dataset of 3,031\ndialogues that are grounded through underlying knowledge bases and span three\ndistinct tasks in the in-car personal assistant space: calendar scheduling,\nweather information retrieval, and point-of-interest navigation. Our\narchitecture is simultaneously trained on data from all domains and\nsignificantly outperforms a competitive rule-based system and other existing\nneural dialogue architectures on the provided domains according to both\nautomatic and human evaluation metrics.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Model Architecture"] },
{"key": "eric2019multiwoz", "citations": "170", "year": "2019", "title":"Multiwoz 2.1: A Consolidated Multi-domain Dialogue Dataset With State Corrections And State Tracking Baselines", "abstract": "<p>MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain\ndialogue dataset spanning 7 distinct domains and containing over 10,000\ndialogues. Though immensely useful and one of the largest resources of its kind\nto-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there is substantial\nnoise in the dialogue state annotations and dialogue utterances which\nnegatively impact the performance of state-tracking models. Secondly, follow-up\nwork (Lee et al., 2019) has augmented the original dataset with user dialogue\nacts. This leads to multiple co-existent versions of the same dataset with\nminor modifications. In this work we tackle the aforementioned issues by\nintroducing MultiWOZ 2.1. To fix the noisy state annotations, we use\ncrowdsourced workers to re-annotate state and utterances based on the original\nutterances in the dataset. This correction process results in changes to over\n32% of state annotations across 40% of the dialogue turns. In addition, we fix\n146 dialogue utterances by canonicalizing slot values in the utterances to the\nvalues in the dataset ontology. To address the second problem, we combined the\ncontributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also\nincludes user dialogue acts as well as multiple slot descriptions per dialogue\nstate slot. We then benchmark a number of state-of-the-art dialogue state\ntracking models on the MultiWOZ 2.1 dataset and show the joint state tracking\nperformance on the corrected state annotations. We are publicly releasing\nMultiWOZ 2.1 to the community, hoping that this dataset resource will allow for\nmore effective models across various dialogue subproblems to be built in the\nfuture.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "eric2022star", "citations": "98", "year": "2022", "title":"Star: Bootstrapping Reasoning With Reasoning", "abstract": "<p>Generating step-by-step “chain-of-thought” rationales improves language model\nperformance on complex reasoning tasks like mathematics or commonsense\nquestion-answering. However, inducing language model rationale generation\ncurrently requires either constructing massive rationale datasets or\nsacrificing accuracy by using only few-shot inference. We propose a technique\nto iteratively leverage a small number of rationale examples and a large\ndataset without rationales, to bootstrap the ability to perform successively\nmore complex reasoning. This technique, the “Self-Taught Reasoner” (STaR),\nrelies on a simple loop: generate rationales to answer many questions, prompted\nwith a few rationale examples; if the generated answers are wrong, try again to\ngenerate a rationale given the correct answer; fine-tune on all the rationales\nthat ultimately yielded correct answers; repeat. We show that STaR\nsignificantly improves performance on multiple datasets compared to a model\nfine-tuned to directly predict final answers, and performs comparably to\nfine-tuning a 30\\(\\times\\) larger state-of-the-art language model on\nCommensenseQA. Thus, STaR lets a model improve itself by learning from its own\ngenerated reasoning.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Prompting","Training Techniques"] },
{"key": "eriguchi2016tree", "citations": "255", "year": "2016", "title":"Tree-to-sequence Attentional Neural Machine Translation", "abstract": "<p>Most of the existing Neural Machine Translation (NMT) models focus on the\nconversion of sequential data and do not directly use syntactic information. We\npropose a novel end-to-end syntactic NMT model, extending a\nsequence-to-sequence model with the source-side phrase structure. Our model has\nan attention mechanism that enables the decoder to generate a translated word\nwhile softly aligning it with phrases as well as words of the source sentence.\nExperimental results on the WAT’15 English-to-Japanese dataset demonstrate that\nour proposed model considerably outperforms sequence-to-sequence attentional\nNMT models and compares favorably with the state-of-the-art tree-to-string SMT\nsystem.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "eriguchi2017learning", "citations": "152", "year": "2017", "title":"Learning To Parse And Translate Improves Neural Machine Translation", "abstract": "<p>There has been relatively little attention to incorporating linguistic prior\nto neural machine translation. Much of the previous work was further\nconstrained to considering linguistic prior on the source side. In this paper,\nwe propose a hybrid model, called NMT+RNNG, that learns to parse and translate\nby combining the recurrent neural network grammar into the attention-based\nneural machine translation. Our approach encourages the neural machine\ntranslation model to incorporate linguistic prior during training, and lets it\ntranslate on its own afterward. Extensive experiments with four language pairs\nshow the effectiveness of the proposed NMT+RNNG.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "eriguchi2018zero", "citations": "80", "year": "2018", "title":"Zero-shot Cross-lingual Classification Using Multilingual Neural Machine Translation", "abstract": "<p>Transferring representations from large supervised tasks to downstream tasks\nhas shown promising results in AI fields such as Computer Vision and Natural\nLanguage Processing (NLP). In parallel, the recent progress in Machine\nTranslation (MT) has enabled one to train multilingual Neural MT (NMT) systems\nthat can translate between multiple languages and are also capable of\nperforming zero-shot translation. However, little attention has been paid to\nleveraging representations learned by a multilingual NMT system to enable\nzero-shot multilinguality in other NLP tasks. In this paper, we demonstrate a\nsimple framework, a multilingual Encoder-Classifier, for cross-lingual transfer\nlearning by reusing the encoder from a multilingual NMT system and stitching it\nwith a task-specific classifier component. Our proposed model achieves\nsignificant improvements in the English setup on three benchmark tasks - Amazon\nReviews, SST and SNLI. Further, our system can perform classification in a new\nlanguage for which no classification data was seen during training, showing\nthat zero-shot classification is possible and remarkably competitive. In order\nto understand the underlying factors contributing to this finding, we conducted\na series of analyses on the effect of the shared vocabulary, the training data\ntype for NMT, classifier complexity, encoder representation power, and model\ngeneralization on zero-shot performance. Our results provide strong evidence\nthat the representations learned from multilingual NMT systems are widely\napplicable across languages and tasks.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "erik2022codegen", "citations": "187", "year": "2022", "title":"Codegen: An Open Large Language Model For Code With Multi-turn Program Synthesis", "abstract": "<p>Program synthesis strives to generate a computer program as a solution to a\ngiven problem specification, expressed with input-output examples or natural\nlanguage descriptions. The prevalence of large language models advances the\nstate-of-the-art for program synthesis, though limited training resources and\ndata impede open access to such models. To democratize this, we train and\nrelease a family of large language models up to 16.1B parameters, called\nCODEGEN, on natural language and programming language data, and open source the\ntraining library JAXFORMER. We show the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-the-art on\nzero-shot Python code generation on HumanEval. We further investigate the\nmulti-step paradigm for program synthesis, where a single program is factorized\ninto multiple prompts specifying subproblems. To this end, we construct an open\nbenchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse\nproblem sets that are factorized into multi-turn prompts. Our analysis on MTPB\nshows that the same intent provided to CODEGEN in multi-turn fashion\nsignificantly improves program synthesis over that provided as a single turn.\nWe make the training library JAXFORMER and model checkpoints available as open\nsource contribution: https://github.com/salesforce/CodeGen.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Llm For Code","Tools","Training Techniques"] },
{"key": "españabonet2017empirical", "citations": "80", "year": "2017", "title":"An Empirical Analysis Of Nmt-derived Interlingual Embeddings And Their Use In Parallel Sentence Identification", "abstract": "<p>End-to-end neural machine translation has overtaken statistical machine\ntranslation in terms of translation quality for some language pairs, specially\nthose with large amounts of parallel data. Besides this palpable improvement,\nneural networks provide several new properties. A single system can be trained\nto translate between many languages at almost no additional cost other than\ntraining time. Furthermore, internal representations learned by the network\nserve as a new semantic representation of words -or sentences- which, unlike\nstandard word embeddings, are learned in an essentially bilingual or even\nmultilingual context. In view of these properties, the contribution of the\npresent work is two-fold. First, we systematically study the NMT context\nvectors, i.e. output of the encoder, and their power as an interlingua\nrepresentation of a sentence. We assess their quality and effectiveness by\nmeasuring similarities across translations, as well as semantically related and\nsemantically unrelated sentence pairs. Second, as extrinsic evaluation of the\nfirst point, we identify parallel sentences in comparable corpora, obtaining an\nF1=98.2% on data from a shared task when using only NMT context vectors. Using\ncontext vectors jointly with similarity measures F1 reaches 98.9%.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "ettinger2018assessing", "citations": "70", "year": "2018", "title":"Assessing Composition In Sentence Vector Representations", "abstract": "<p>An important component of achieving language understanding is mastering the\ncomposition of sentence meaning, but an immediate challenge to solving this\nproblem is the opacity of sentence vector representations produced by current\nneural sentence composition models. We present a method to address this\nchallenge, developing tasks that directly target compositional meaning\ninformation in sentence vector representations with a high degree of precision\nand control. To enable the creation of these controlled tasks, we introduce a\nspecialized sentence generation system that produces large, annotated sentence\nsets meeting specified syntactic, semantic and lexical constraints. We describe\nthe details of the method and generation system, and then present results of\nexperiments applying our method to probe for compositional information in\nembeddings from a number of existing sentence composition models. We find that\nthe method is able to extract useful information about the differing capacities\nof these models, and we discuss the implications of our results with respect to\nthese systems’ capturing of sentence information. We make available for public\nuse the datasets used for these experiments, as well as the generation system.</p>\n", "tags": ["COLING","Datasets"] },
{"key": "ettinger2019what", "citations": "553", "year": "2020", "title":"What BERT Is Not: Lessons From A New Suite Of Psycholinguistic Diagnostics For Language Models", "abstract": "<p>Pre-training by language modeling has become a popular and successful\napproach to NLP tasks, but we have yet to understand exactly what linguistic\ncapacities these pre-training processes confer upon models. In this paper we\nintroduce a suite of diagnostics drawn from human language experiments, which\nallow us to ask targeted questions about the information used by language\nmodels for generating predictions in context. As a case study, we apply these\ndiagnostics to the popular BERT model, finding that it can generally\ndistinguish good from bad completions involving shared category or role\nreversal, albeit with less sensitivity than humans, and it robustly retrieves\nnoun hypernyms, but it struggles with challenging inferences and role-based\nevent prediction – and in particular, it shows clear insensitivity to the\ncontextual impacts of negation.</p>\n", "tags": ["Model Architecture","TACL","Training Techniques"] },
{"key": "evans2018can", "citations": "63", "year": "2018", "title":"Can Neural Networks Understand Logical Entailment?", "abstract": "<p>We introduce a new dataset of logical entailments for the purpose of\nmeasuring models’ ability to capture and exploit the structure of logical\nexpressions against an entailment prediction task. We use this task to compare\na series of architectures which are ubiquitous in the sequence-processing\nliterature, in addition to a new model class—PossibleWorldNets—which\ncomputes entailment as a “convolution over possible worlds”. Results show that\nconvolutional networks present the wrong inductive bias for this class of\nproblems relative to LSTM RNNs, tree-structured neural networks outperform LSTM\nRNNs due to their enhanced ability to exploit the syntax of logic, and\nPossibleWorldNets outperform all benchmarks.</p>\n", "tags": ["Datasets","Ethics & Fairness","Model Architecture"] },
{"key": "eyal2019question", "citations": "102", "year": "2019", "title":"Question Answering As An Automatic Evaluation Metric For News Article Summarization", "abstract": "<p>Recent work in the field of automatic summarization and headline generation\nfocuses on maximizing ROUGE scores for various news datasets. We present an\nalternative, extrinsic, evaluation metric for this task, Answering Performance\nfor Evaluation of Summaries. APES utilizes recent progress in the field of\nreading-comprehension to quantify the ability of a summary to answer a set of\nmanually created questions regarding central entities in the source article. We\nfirst analyze the strength of this metric by comparing it to known manual\nevaluation metrics. We then present an end-to-end neural abstractive model that\nmaximizes APES, while increasing ROUGE scores to competitive results.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "ezencan2020comparison", "citations": "62", "year": "2020", "title":"A Comparison Of LSTM And BERT For Small Corpus", "abstract": "<p>Recent advancements in the NLP field showed that transfer learning helps with\nachieving state-of-the-art results for new tasks by tuning pre-trained models\ninstead of starting from scratch. Transformers have made a significant\nimprovement in creating new state-of-the-art results for many NLP tasks\nincluding but not limited to text classification, text generation, and sequence\nlabeling. Most of these success stories were based on large datasets. In this\npaper we focus on a real-life scenario that scientists in academia and industry\nface frequently: given a small dataset, can we use a large pre-trained model\nlike BERT and get better results than simple models? To answer this question,\nwe use a small dataset for intent classification collected for building\nchatbots and compare the performance of a simple bidirectional LSTM model with\na pre-trained BERT model. Our experimental results show that bidirectional LSTM\nmodels can achieve significantly higher results than a BERT model for a small\ndataset and these simple models get trained in much less time than tuning the\npre-trained counterparts. We conclude that the performance of a model is\ndependent on the task and the data, and therefore before making a model choice,\nthese factors should be taken into consideration instead of directly choosing\nthe most popular model.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture"] },
{"key": "fabbri2020summeval", "citations": "397", "year": "2021", "title":"Summeval: Re-evaluating Summarization Evaluation", "abstract": "<p>The scarcity of comprehensive up-to-date studies on evaluation metrics for\ntext summarization and the lack of consensus regarding evaluation protocols\ncontinue to inhibit progress. We address the existing shortcomings of\nsummarization evaluation methods along five dimensions: 1) we re-evaluate 14\nautomatic evaluation metrics in a comprehensive and consistent fashion using\nneural summarization model outputs along with expert and crowd-sourced human\nannotations, 2) we consistently benchmark 23 recent summarization models using\nthe aforementioned automatic evaluation metrics, 3) we assemble the largest\ncollection of summaries generated by models trained on the CNN/DailyMail news\ndataset and share it in a unified format, 4) we implement and share a toolkit\nthat provides an extensible and unified API for evaluating summarization models\nacross a broad range of automatic metrics, 5) we assemble and share the largest\nand most diverse, in terms of model types, collection of human judgments of\nmodel-generated summaries on the CNN/Daily Mail dataset annotated by both\nexpert judges and crowd-source workers. We hope that this work will help\npromote a more complete evaluation protocol for text summarization as well as\nadvance research in developing evaluation metrics that better correlate with\nhuman judgments.</p>\n", "tags": ["Datasets","Evaluation","TACL","Tools"] },
{"key": "fabbri2020template", "citations": "73", "year": "2020", "title":"Template-based Question Generation From Retrieved Sentences For Improved Unsupervised Question Answering", "abstract": "<p>Question Answering (QA) is in increasing demand as the amount of information\navailable online and the desire for quick access to this content grows. A\ncommon approach to QA has been to fine-tune a pretrained language model on a\ntask-specific labeled dataset. This paradigm, however, relies on scarce, and\ncostly to obtain, large-scale human-labeled data. We propose an unsupervised\napproach to training QA models with generated pseudo-training data. We show\nthat generating questions for QA training by applying a simple template on a\nrelated, retrieved sentence rather than the original context sentence improves\ndownstream QA performance by allowing the model to learn more complex\ncontext-question relationships. Training a QA model on this data gives a\nrelative improvement over a previous unsupervised model in F1 score on the\nSQuAD dataset by about 14%, and 20% when the answer is a named entity,\nachieving state-of-the-art performance on SQuAD for unsupervised QA.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "fabbri2021qafacteval", "citations": "63", "year": "2022", "title":"Qafacteval: Improved Qa-based Factual Consistency Evaluation For Summarization", "abstract": "<p>Factual consistency is an essential quality of text summarization models in\npractical settings. Existing work in evaluating this dimension can be broadly\ncategorized into two lines of research, entailment-based and question answering\n(QA)-based metrics, and different experimental setups often lead to contrasting\nconclusions as to which paradigm performs the best. In this work, we conduct an\nextensive comparison of entailment and QA-based metrics, demonstrating that\ncarefully choosing the components of a QA-based metric, especially question\ngeneration and answerability classification, is critical to performance.\nBuilding on those insights, we propose an optimized metric, which we call\nQAFactEval, that leads to a 14% average improvement over previous QA-based\nmetrics on the SummaC factual consistency benchmark, and also outperforms the\nbest-performing entailment-based metric. Moreover, we find that QA-based and\nentailment-based metrics can offer complementary signals and be combined into a\nsingle metric for a further performance boost.</p>\n", "tags": ["Datasets","Evaluation","NAACL"] },
{"key": "fabrizio2023chatgpt", "citations": "511", "year": "2023", "title":"Chatgpt Outperforms Crowd-workers For Text-annotation Tasks", "abstract": "<p>Many NLP applications require manual data annotations for a variety of tasks,\nnotably to train classifiers or evaluate the performance of unsupervised\nmodels. Depending on the size and degree of complexity, the tasks may be\nconducted by crowd-workers on platforms such as MTurk as well as trained\nannotators, such as research assistants. Using a sample of 2,382 tweets, we\ndemonstrate that ChatGPT outperforms crowd-workers for several annotation\ntasks, including relevance, stance, topics, and frames detection. Specifically,\nthe zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of\nfive tasks, while ChatGPT’s intercoder agreement exceeds that of both\ncrowd-workers and trained annotators for all tasks. Moreover, the\nper-annotation cost of ChatGPT is less than $0.003 – about twenty times\ncheaper than MTurk. These results show the potential of large language models\nto drastically increase the efficiency of text classification.</p>\n", "tags": ["Applications","Efficiency"] },
{"key": "fadaee2017data", "citations": "518", "year": "2017", "title":"Data Augmentation For Low-resource Neural Machine Translation", "abstract": "<p>The quality of a Neural Machine Translation system depends substantially on\nthe availability of sizable parallel corpora. For low-resource language pairs\nthis is not the case, resulting in poor translation quality. Inspired by work\nin computer vision, we propose a novel data augmentation approach that targets\nlow-frequency words by generating new sentence pairs containing rare words in\nnew, synthetically created contexts. Experimental results on simulated\nlow-resource settings show that our method improves translation quality by up\nto 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.</p>\n", "tags": [] },
{"key": "fadaee2018back", "citations": "82", "year": "2018", "title":"Back-translation Sampling By Targeting Difficult Words In Neural Machine Translation", "abstract": "<p>Neural Machine Translation has achieved state-of-the-art performance for\nseveral language pairs using a combination of parallel and synthetic data.\nSynthetic data is often generated by back-translating sentences randomly\nsampled from monolingual data using a reverse translation model. While\nback-translation has been shown to be very effective in many cases, it is not\nentirely clear why. In this work, we explore different aspects of\nback-translation, and show that words with high prediction loss during training\nbenefit most from the addition of synthetic data. We introduce several\nvariations of sampling strategies targeting difficult-to-predict words using\nprediction losses and frequencies of words. In addition, we also target the\ncontexts of difficult words and sample sentences that are similar in context.\nExperimental results for the WMT news translation task show that our method\nimproves translation quality by up to 1.7 and 1.2 Bleu points over\nback-translation using random sampling for German-English and English-German,\nrespectively.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "faggioli2023perspectives", "citations": "85", "year": "2023", "title":"Perspectives On Large Language Models For Relevance Judgment", "abstract": "<p>When asked, large language models (LLMs) like ChatGPT claim that they can\nassist with relevance judgments but it is not clear whether automated judgments\ncan reliably be used in evaluations of retrieval systems. In this perspectives\npaper, we discuss possible ways for LLMs to support relevance judgments along\nwith concerns and issues that arise. We devise a human–machine collaboration\nspectrum that allows to categorize different relevance judgment strategies,\nbased on how much humans rely on machines. For the extreme point of “fully\nautomated judgments”, we further include a pilot experiment on whether\nLLM-based relevance judgments correlate with judgments from trained human\nassessors. We conclude the paper by providing opposing perspectives for and\nagainst the use of~LLMs for automatic relevance judgments, and a compromise\nperspective, informed by our analyses of the literature, our preliminary\nexperimental evidence, and our experience as IR researchers.</p>\n", "tags": ["Retrieval Systems","SIGIR"] },
{"key": "fagni2020tweepfake", "citations": "129", "year": "2021", "title":"Tweepfake: About Detecting Deepfake Tweets", "abstract": "<p>The recent advances in language modeling significantly improved the\ngenerative capabilities of deep neural models: in 2019 OpenAI released GPT-2, a\npre-trained language model that can autonomously generate coherent, non-trivial\nand human-like text samples. Since then, ever more powerful text generative\nmodels have been developed. Adversaries can exploit these tremendous generative\ncapabilities to enhance social bots that will have the ability to write\nplausible deepfake messages, hoping to contaminate public debate. To prevent\nthis, it is crucial to develop deepfake social media messages detection\nsystems. However, to the best of our knowledge no one has ever addressed the\ndetection of machine-generated texts on social networks like Twitter or\nFacebook. With the aim of helping the research in this detection field, we\ncollected the first dataset of \\real deepfake tweets, TweepFake. It is real in\nthe sense that each deepfake tweet was actually posted on Twitter. We collected\ntweets from a total of 23 bots, imitating 17 human accounts. The bots are based\non various generation techniques, i.e., Markov Chains, RNN, RNN+Markov, LSTM,\nGPT-2. We also randomly selected tweets from the humans imitated by the bots to\nhave an overall balanced dataset of 25,572 tweets (half human and half bots\ngenerated). The dataset is publicly available on Kaggle. Lastly, we evaluated\n13 deepfake text detection methods (based on various state-of-the-art\napproaches) to both demonstrate the challenges that Tweepfake poses and create\na solid baseline of detection techniques. We hope that TweepFake can offer the\nopportunity to tackle the deepfake detection on social media messages as well.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "fan2018hierarchical", "citations": "1074", "year": "2018", "title":"Hierarchical Neural Story Generation", "abstract": "<p>We explore story generation: creative systems that can build coherent and\nfluent passages of text about a topic. We collect a large dataset of 300K\nhuman-written stories paired with writing prompts from an online forum. Our\ndataset enables hierarchical story generation, where the model first generates\na premise, and then transforms it into a passage of text. We gain further\nimprovements with a novel form of model fusion that improves the relevance of\nthe story to the prompt, and adding a new gated multi-scale self-attention\nmechanism to model long-range context. Experiments show large improvements over\nstrong baselines on both automated and human evaluations. Human judges prefer\nstories generated by our approach to those from a strong non-hierarchical model\nby a factor of two to one.</p>\n", "tags": ["Datasets","Model Architecture","Prompting"] },
{"key": "fan2018modeling", "citations": "68", "year": "2018", "title":"Modeling Diverse Relevance Patterns In Ad-hoc Retrieval", "abstract": "<p>Assessing relevance between a query and a document is challenging in ad-hoc\nretrieval due to its diverse patterns, i.e., a document could be relevant to a\nquery as a whole or partially as long as it provides sufficient information for\nusers’ need. Such diverse relevance patterns require an ideal retrieval model\nto be able to assess relevance in the right granularity adaptively.\nUnfortunately, most existing retrieval models compute relevance at a single\ngranularity, either document-wide or passage-level, or use fixed combination\nstrategy, restricting their ability in capturing diverse relevance patterns. In\nthis work, we propose a data-driven method to allow relevance signals at\ndifferent granularities to compete with each other for final relevance\nassessment. Specifically, we propose a HIerarchical Neural maTching model\n(HiNT) which consists of two stacked components, namely local matching layer\nand global decision layer. The local matching layer focuses on producing a set\nof local relevance signals by modeling the semantic matching between a query\nand each passage of a document. The global decision layer accumulates local\nsignals into different granularities and allows them to compete with each other\nto decide the final relevance score. Experimental results demonstrate that our\nHiNT model outperforms existing state-of-the-art retrieval models significantly\non benchmark ad-hoc retrieval datasets.</p>\n", "tags": ["Datasets","Evaluation","SIGIR"] },
{"key": "fan2019eli5", "citations": "273", "year": "2019", "title":"ELI5: Long Form Question Answering", "abstract": "<p>We introduce the first large-scale corpus for long-form question answering, a\ntask requiring elaborate and in-depth answers to open-ended questions. The\ndataset comprises 270K threads from the Reddit forum ``Explain Like I’m Five’’\n(ELI5) where an online community provides answers to questions which are\ncomprehensible by five year olds. Compared to existing datasets, ELI5 comprises\ndiverse questions requiring multi-sentence answers. We provide a large set of\nweb documents to help answer the question. Automatic and human evaluations show\nthat an abstractive model trained with a multi-task objective outperforms\nconventional Seq2Seq, language modeling, as well as a strong extractive\nbaseline. However, our best model is still far from human performance since\nraters prefer gold responses in over 86% of cases, leaving ample opportunity\nfor future improvement.</p>\n", "tags": ["Datasets"] },
{"key": "fan2019heterogeneous", "citations": "254", "year": "2019", "title":"Heterogeneous Memory Enhanced Multimodal Attention Model For Video Question Answering", "abstract": "<p>In this paper, we propose a novel end-to-end trainable Video Question\nAnswering (VideoQA) framework with three major components: 1) a new\nheterogeneous memory which can effectively learn global context information\nfrom appearance and motion features; 2) a redesigned question memory which\nhelps understand the complex semantics of question and highlights queried\nsubjects; and 3) a new multimodal fusion layer which performs multi-step\nreasoning by attending to relevant visual and textual hints with self-updated\nattention. Our VideoQA model firstly generates the global context-aware visual\nand textual features respectively by interacting current inputs with memory\ncontents. After that, it makes the attentional fusion of the multimodal visual\nand textual representations to infer the correct answer. Multiple cycles of\nreasoning can be made to iteratively refine attention weights of the multimodal\ndata and improve the final representation of the QA pair. Experimental results\ndemonstrate our approach achieves state-of-the-art performance on four VideoQA\nbenchmark datasets.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "fan2019reducing", "citations": "290", "year": "2019", "title":"Reducing Transformer Depth On Demand With Structured Dropout", "abstract": "<p>Overparameterized transformer networks have obtained state of the art results\nin various natural language processing tasks, such as machine translation,\nlanguage modeling, and question answering. These models contain hundreds of\nmillions of parameters, necessitating a large amount of computation and making\nthem prone to overfitting. In this work, we explore LayerDrop, a form of\nstructured dropout, which has a regularization effect during training and\nallows for efficient pruning at inference time. In particular, we show that it\nis possible to select sub-networks of any depth from one large network without\nhaving to finetune them and with limited impact on performance. We demonstrate\nthe effectiveness of our approach by improving the state of the art on machine\ntranslation, language modeling, summarization, question answering, and language\nunderstanding benchmarks. Moreover, we show that our approach leads to small\nBERT-like models of higher quality compared to training from scratch or using\ndistillation.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "fan2019strategies", "citations": "193", "year": "2019", "title":"Strategies For Structuring Story Generation", "abstract": "<p>Writers generally rely on plans or sketches to write long stories, but most\ncurrent language models generate word by word from left to right. We explore\ncoarse-to-fine models for creating narrative texts of several hundred words,\nand introduce new models which decompose stories by abstracting over actions\nand entities. The model first generates the predicate-argument structure of the\ntext, where different mentions of the same entity are marked with placeholder\ntokens. It then generates a surface realization of the predicate-argument\nstructure, and finally replaces the entity placeholders with context-sensitive\nnames and references. Human judges prefer the stories from our models to a wide\nrange of previous approaches to hierarchical text generation. Extensive\nanalysis shows that our methods can help improve the diversity and coherence of\nevents and entities in generated stories.</p>\n", "tags": [] },
{"key": "fan2019using", "citations": "100", "year": "2019", "title":"Using Local Knowledge Graph Construction To Scale Seq2seq Models To Multi-document Inputs", "abstract": "<p>Query-based open-domain NLP tasks require information synthesis from long and\ndiverse web results. Current approaches extractively select portions of web\ntext as input to Sequence-to-Sequence models using methods such as TF-IDF\nranking. We propose constructing a local graph structured knowledge base for\neach query, which compresses the web search information and reduces redundancy.\nWe show that by linearizing the graph into a structured input sequence, models\ncan encode the graph representations within a standard Sequence-to-Sequence\nsetting. For two generative tasks with very long text input, long-form question\nanswering and multi-document summarization, feeding graph representations as\ninput can achieve better performance than using retrieved text portions.</p>\n", "tags": ["EMNLP"] },
{"key": "fan2020beyond", "citations": "413", "year": "2020", "title":"Beyond English-centric Multilingual Machine Translation", "abstract": "<p>Existing work in translation demonstrated the potential of massively\nmultilingual machine translation by training a single model able to translate\nbetween any pair of languages. However, much of this work is English-Centric by\ntraining only on data which was translated from or to English. While this is\nsupported by large sources of training data, it does not reflect translation\nneeds worldwide. In this work, we create a true Many-to-Many multilingual\ntranslation model that can translate directly between any pair of 100\nlanguages. We build and open source a training dataset that covers thousands of\nlanguage directions with supervised data, created through large-scale mining.\nThen, we explore how to effectively increase model capacity through a\ncombination of dense scaling and language-specific sparse parameters to create\nhigh quality models. Our focus on non-English-Centric models brings gains of\nmore than 10 BLEU when directly translating between non-English directions\nwhile performing competitively to the best single systems of WMT. We\nopen-source our scripts so that others may reproduce the data, evaluation, and\nfinal M2M-100 model.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "fan2021continuous", "citations": "139", "year": "2021", "title":"Continuous-time Sequential Recommendation With Temporal Graph Collaborative Transformer", "abstract": "<p>In order to model the evolution of user preference, we should learn user/item\nembeddings based on time-ordered item purchasing sequences, which is defined as\nSequential Recommendation (SR) problem. Existing methods leverage sequential\npatterns to model item transitions. However, most of them ignore crucial\ntemporal collaborative signals, which are latent in evolving user-item\ninteractions and coexist with sequential patterns. Therefore, we propose to\nunify sequential patterns and temporal collaborative signals to improve the\nquality of recommendation, which is rather challenging. Firstly, it is hard to\nsimultaneously encode sequential patterns and collaborative signals. Secondly,\nit is non-trivial to express the temporal effects of collaborative signals.\n  Hence, we design a new framework Temporal Graph Sequential Recommender\n(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel\nTemporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the\nself-attention mechanism by adopting a novel collaborative attention. TCT layer\ncan simultaneously capture collaborative signals from both users and items, as\nwell as considering temporal dynamics inside sequential patterns. We propagate\nthe information learned fromTCTlayerover the temporal graph to unify sequential\npatterns and temporal collaborative signals. Empirical results on five datasets\nshow that TGSRec significantly outperforms other baselines, in average up to\n22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.</p>\n", "tags": ["CIKM","Datasets","Model Architecture","Tools"] },
{"key": "fan2021faceformer", "citations": "118", "year": "2022", "title":"Faceformer: Speech-driven 3D Facial Animation With Transformers", "abstract": "<p>Speech-driven 3D facial animation is challenging due to the complex geometry\nof human faces and the limited availability of 3D audio-visual data. Prior\nworks typically focus on learning phoneme-level features of short audio windows\nwith limited context, occasionally resulting in inaccurate lip movements. To\ntackle this limitation, we propose a Transformer-based autoregressive model,\nFaceFormer, which encodes the long-term audio context and autoregressively\npredicts a sequence of animated 3D face meshes. To cope with the data scarcity\nissue, we integrate the self-supervised pre-trained speech representations.\nAlso, we devise two biased attention mechanisms well suited to this specific\ntask, including the biased cross-modal multi-head (MH) attention and the biased\ncausal MH self-attention with a periodic positional encoding strategy. The\nformer effectively aligns the audio-motion modalities, whereas the latter\noffers abilities to generalize to longer audio sequences. Extensive experiments\nand a perceptual user study show that our approach outperforms the existing\nstate-of-the-arts. The code will be made available.</p>\n", "tags": ["CVPR","Model Architecture","Training Techniques"] },
{"key": "fan2022sequential", "citations": "119", "year": "2022", "title":"Sequential Recommendation With Auxiliary Item Relationships Via Multi-relational Transformer", "abstract": "<p>Sequential Recommendation (SR) models user dynamics and predicts the next\npreferred items based on the user history. Existing SR methods model the ‘was\ninteracted before’ item-item transitions observed in sequences, which can be\nviewed as an item relationship. However, there are multiple auxiliary item\nrelationships, e.g., items from similar brands and with similar contents in\nreal-world scenarios. Auxiliary item relationships describe item-item\naffinities in multiple different semantics and alleviate the long-lasting cold\nstart problem in the recommendation. However, it remains a significant\nchallenge to model auxiliary item relationships in SR.\n  To simultaneously model high-order item-item transitions in sequences and\nauxiliary item relationships, we propose a Multi-relational Transformer capable\nof modeling auxiliary item relationships for SR (MT4SR). Specifically, we\npropose a novel self-attention module, which incorporates arbitrary item\nrelationships and weights item relationships accordingly. Second, we regularize\nintra-sequence item relationships with a novel regularization module to\nsupervise attentions computations. Third, for inter-sequence item relationship\npairs, we introduce a novel inter-sequence related items modeling module.\nFinally, we conduct experiments on four benchmark datasets and demonstrate the\neffectiveness of MT4SR over state-of-the-art methods and the improvements on\nthe cold start problem. The code is available at\nhttps://github.com/zfan20/MT4SR.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "fan2023bibliometric", "citations": "82", "year": "2024", "title":"A Bibliometric Review Of Large Language Models Research From 2017 To 2023", "abstract": "<p>Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.</p>\n", "tags": ["Applications","Survey Paper"] },
{"key": "fan2023large", "citations": "107", "year": "2023", "title":"Large Language Models For Software Engineering: Survey And Open Problems", "abstract": "<p>This paper provides a survey of the emerging area of Large Language Models\n(LLMs) for Software Engineering (SE). It also sets out open research challenges\nfor the application of LLMs to technical problems faced by software engineers.\nLLMs’ emergent properties bring novelty and creativity with applications right\nacross the spectrum of Software Engineering activities including coding,\ndesign, requirements, repair, refactoring, performance improvement,\ndocumentation and analytics. However, these very same emergent properties also\npose significant technical challenges; we need techniques that can reliably\nweed out incorrect solutions, such as hallucinations. Our survey reveals the\npivotal role that hybrid techniques (traditional SE plus LLMs) have to play in\nthe development and deployment of reliable, efficient and effective LLM-based\nSE.</p>\n", "tags": ["Applications","Llm For Code","Survey Paper"] },
{"key": "fan2024survey", "citations": "92", "year": "2024", "title":"A Survey On RAG Meeting Llms: Towards Retrieval-augmented Large Language Models", "abstract": "<p>As one of the most advanced techniques in AI, Retrieval-Augmented Generation\n(RAG) can offer reliable and up-to-date external knowledge, providing huge\nconvenience for numerous tasks. Particularly in the era of AI-Generated Content\n(AIGC), the powerful capacity of retrieval in providing additional knowledge\nenables RAG to assist existing generative AI in producing high-quality outputs.\nRecently, Large Language Models (LLMs) have demonstrated revolutionary\nabilities in language understanding and generation, while still facing inherent\nlimitations, such as hallucinations and out-of-date internal knowledge. Given\nthe powerful abilities of RAG in providing the latest and helpful auxiliary\ninformation, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged\nto harness external and authoritative knowledge bases, rather than solely\nrelying on the model’s internal knowledge, to augment the generation quality of\nLLMs. In this survey, we comprehensively review existing research studies in\nRA-LLMs, covering three primary technical perspectives: architectures, training\nstrategies, and applications. As the preliminary knowledge, we briefly\nintroduce the foundations and recent advances of LLMs. Then, to illustrate the\npractical significance of RAG for LLMs, we systematically review mainstream\nrelevant work by their architectures, training strategies, and application\nareas, detailing specifically the challenges of each and the corresponding\ncapabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss\ncurrent limitations and several promising directions for future research.\nUpdated information about this survey can be found at\nhttps://advanced-recommender-systems.github.io/RAG-Meets-LLMs/</p>\n", "tags": ["Applications","KDD","RAG","Survey Paper","Training Techniques"] },
{"key": "fang2017generating", "citations": "110", "year": "2017", "title":"Generating Steganographic Text With Lstms", "abstract": "<p>Motivated by concerns for user privacy, we design a steganographic system\n(“stegosystem”) that enables two users to exchange encrypted messages without\nan adversary detecting that such an exchange is taking place. We propose a new\nlinguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network.\nWe demonstrate our approach on the Twitter and Enron email datasets and show\nthat it yields high-quality steganographic text while significantly improving\ncapacity (encrypted bits per word) relative to the state-of-the-art.</p>\n", "tags": ["Datasets","Model Architecture","Privacy"] },
{"key": "fang2018sounding", "citations": "62", "year": "2018", "title":"Sounding Board: A User-centric And Content-driven Social Chatbot", "abstract": "<p>We present Sounding Board, a social chatbot that won the 2017 Amazon Alexa\nPrize. The system architecture consists of several components including spoken\nlanguage processing, dialogue management, language generation, and content\nmanagement, with emphasis on user-centric and content-driven design. We also\nshare insights gained from large-scale online logs based on 160,000\nconversations with real-world users.</p>\n", "tags": ["Model Architecture","NAACL"] },
{"key": "fang2019hierarchical", "citations": "154", "year": "2020", "title":"Hierarchical Graph Network For Multi-hop Question Answering", "abstract": "<p>In this paper, we present Hierarchical Graph Network (HGN) for multi-hop\nquestion answering. To aggregate clues from scattered texts across multiple\nparagraphs, a hierarchical graph is created by constructing nodes on different\nlevels of granularity (questions, paragraphs, sentences, entities), the\nrepresentations of which are initialized with pre-trained contextual encoders.\nGiven this hierarchical graph, the initial node representations are updated\nthrough graph propagation, and multi-hop reasoning is performed via traversing\nthrough the graph edges for each subsequent sub-task (e.g., paragraph\nselection, supporting facts extraction, answer prediction). By weaving\nheterogeneous nodes into an integral unified graph, this hierarchical\ndifferentiation of node granularity enables HGN to support different question\nanswering sub-tasks simultaneously. Experiments on the HotpotQA benchmark\ndemonstrate that the proposed model achieves new state of the art,\noutperforming existing multi-hop QA approaches.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "fang2019scene", "citations": "169", "year": "2019", "title":"Scene Memory Transformer For Embodied Agents In Long-horizon Tasks", "abstract": "<p>Many robotic applications require the agent to perform long-horizon tasks in\npartially observable environments. In such applications, decision making at any\nstep can depend on observations received far in the past. Hence, being able to\nproperly memorize and utilize the long-term history is crucial. In this work,\nwe propose a novel memory-based policy, named Scene Memory Transformer (SMT).\nThe proposed policy embeds and adds each observation to a memory and uses the\nattention mechanism to exploit spatio-temporal dependencies. This model is\ngeneric and can be efficiently trained with reinforcement learning over long\nepisodes. On a range of visual navigation tasks, SMT demonstrates superior\nperformance to existing reactive and memory-based policies by a margin.</p>\n", "tags": ["Agentic","Applications","CVPR","Model Architecture","Reinforcement Learning"] },
{"key": "fang2020cert", "citations": "186", "year": "2020", "title":"CERT: Contrastive Self-supervised Learning For Language Understanding", "abstract": "<p>Pretrained language models such as BERT, GPT have shown great effectiveness\nin language understanding. The auxiliary predictive tasks in existing\npretraining approaches are mostly defined on tokens, thus may not be able to\ncapture sentence-level semantics very well. To address this issue, we propose\nCERT: Contrastive self-supervised Encoder Representations from Transformers,\nwhich pretrains language representation models using contrastive\nself-supervised learning at the sentence level. CERT creates augmentations of\noriginal sentences using back-translation. Then it finetunes a pretrained\nlanguage encoder (e.g., BERT) by predicting whether two augmented sentences\noriginate from the same sentence. CERT is simple to use and can be flexibly\nplugged into any pretraining-finetuning NLP pipeline. We evaluate CERT on 11\nnatural language understanding tasks in the GLUE benchmark where CERT\noutperforms BERT on 7 tasks, achieves the same performance as BERT on 2 tasks,\nand performs worse than BERT on 2 tasks. On the averaged score of the 11 tasks,\nCERT outperforms BERT. The data and code are available at\nhttps://github.com/UCSD-AI4H/CERT</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "fang2021injecting", "citations": "92", "year": "2022", "title":"Injecting Semantic Concepts Into End-to-end Image Captioning", "abstract": "<p>Tremendous progress has been made in recent years in developing better image\ncaptioning models, yet most of them rely on a separate object detector to\nextract regional features. Recent vision-language studies are shifting towards\nthe detector-free trend by leveraging grid representations for more flexible\nmodel training and faster inference speed. However, such development is\nprimarily focused on image understanding tasks, and remains less investigated\nfor the caption generation task. In this paper, we are concerned with a\nbetter-performing detector-free image captioning model, and propose a pure\nvision transformer-based image captioning model, dubbed as ViTCAP, in which\ngrid representations are used without extracting the regional features. For\nimproved performance, we introduce a novel Concept Token Network (CTN) to\npredict the semantic concepts and then incorporate them into the end-to-end\ncaptioning. In particular, the CTN is built on the basis of a vision\ntransformer and is designed to predict the concept tokens through a\nclassification task, from which the rich semantic information contained greatly\nbenefits the captioning task. Compared with the previous detector-based models,\nViTCAP drastically simplifies the architectures and at the same time achieves\ncompetitive performance on various challenging image captioning datasets. In\nparticular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split,\n93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets,\nrespectively.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "fang2022stemm", "citations": "66", "year": "2022", "title":"STEMM: Self-learning With Speech-text Manifold Mixup For Speech Translation", "abstract": "<p>How to learn a better speech representation for end-to-end speech-to-text\ntranslation (ST) with limited labeled data? Existing techniques often attempt\nto transfer powerful machine translation (MT) capabilities to ST, but neglect\nthe representation discrepancy across modalities. In this paper, we propose the\nSpeech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.\nSpecifically, we mix up the representation sequences of different modalities,\nand take both unimodal speech sequences and multimodal mixed sequences as input\nto the translation model in parallel, and regularize their output predictions\nwith a self-learning framework. Experiments on MuST-C speech translation\nbenchmark and further analysis show that our method effectively alleviates the\ncross-modal representation discrepancy, and achieves significant improvements\nover a strong baseline on eight translation directions.</p>\n", "tags": ["Datasets","Evaluation","Tools"] },
{"key": "fang2023bias", "citations": "65", "year": "2024", "title":"Bias Of Ai-generated Content: An Examination Of News Produced By Large Language Models", "abstract": "<p>Large language models (LLMs) have the potential to transform our lives and\nwork through the content they generate, known as AI-Generated Content (AIGC).\nTo harness this transformation, we need to understand the limitations of LLMs.\nHere, we investigate the bias of AIGC produced by seven representative LLMs,\nincluding ChatGPT and LLaMA. We collect news articles from The New York Times\nand Reuters, both known for their dedication to provide unbiased news. We then\napply each examined LLM to generate news content with headlines of these news\narticles as prompts, and evaluate the gender and racial biases of the AIGC\nproduced by the LLM by comparing the AIGC and the original news articles. We\nfurther analyze the gender bias of each LLM under biased prompts by adding\ngender-biased messages to prompts constructed from these news headlines. Our\nstudy reveals that the AIGC produced by each examined LLM demonstrates\nsubstantial gender and racial biases. Moreover, the AIGC generated by each LLM\nexhibits notable discrimination against females and individuals of the Black\nrace. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest\nlevel of bias, and ChatGPT is the sole model capable of declining content\ngeneration when provided with biased prompts.</p>\n", "tags": ["Ethics & Fairness"] },
{"key": "farag2018neural", "citations": "86", "year": "2018", "title":"Neural Automated Essay Scoring And Coherence Modeling For Adversarially Crafted Input", "abstract": "<p>We demonstrate that current state-of-the-art approaches to Automated Essay\nScoring (AES) are not well-suited to capturing adversarially crafted input of\ngrammatical but incoherent sequences of sentences. We develop a neural model of\nlocal coherence that can effectively learn connectedness features between\nsentences, and propose a framework for integrating and jointly training the\nlocal coherence model with a state-of-the-art AES model. We evaluate our\napproach against a number of baselines and experimentally demonstrate its\neffectiveness on both the AES task and the task of flagging adversarial input,\nfurther contributing to the development of an approach that strengthens the\nvalidity of neural essay scoring models.</p>\n", "tags": ["NAACL","Security","Tools","Training Techniques"] },
{"key": "fatemi2016policy", "citations": "70", "year": "2016", "title":"Policy Networks With Two-stage Training For Dialogue Systems", "abstract": "<p>In this paper, we propose to use deep policy networks which are trained with\nan advantage actor-critic method for statistically optimised dialogue systems.\nFirst, we show that, on summary state and action spaces, deep Reinforcement\nLearning (RL) outperforms Gaussian Processes methods. Summary state and action\nspaces lead to good performance but require pre-engineering effort, RL\nknowledge, and domain expertise. In order to remove the need to define such\nsummary spaces, we show that deep RL can also be trained efficiently on the\noriginal state and action spaces. Dialogue systems based on partially\nobservable Markov decision processes are known to require many dialogues to\ntrain, which makes them unappealing for practical deployment. We show that a\ndeep RL method based on an actor-critic architecture can exploit a small amount\nof data very efficiently. Indeed, with only a few hundred dialogues collected\nwith a handcrafted policy, the actor-critic deep learner is considerably\nbootstrapped from a combination of supervised and batch RL. In addition,\nconvergence to an optimal policy is significantly sped up compared to other\ndeep RL methods initialized on the data with batch RL. All experiments are\nperformed on a restaurant domain derived from the Dialogue State Tracking\nChallenge 2 (DSTC2) dataset.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "fedus2018maskgan", "citations": "276", "year": "2018", "title":"Maskgan: Better Text Generation Via Filling In The______", "abstract": "<p>Neural text generation models are often autoregressive language models or\nseq2seq models. These models generate text by sampling words sequentially, with\neach word conditioned on the previous word, and are state-of-the-art for\nseveral machine translation and summarization benchmarks. These benchmarks are\noften defined by validation perplexity even though this is not a direct measure\nof the quality of the generated text. Additionally, these models are typically\ntrained via maxi- mum likelihood and teacher forcing. These methods are\nwell-suited to optimizing perplexity but can result in poor sample quality\nsince generating text requires conditioning on sequences of words that may have\nnever been observed at training time. We propose to improve sample quality\nusing Generative Adversarial Networks (GANs), which explicitly train the\ngenerator to produce high quality samples and have shown a lot of success in\nimage generation. GANs were originally designed to output differentiable\nvalues, so discrete language generation is challenging for them. We claim that\nvalidation perplexity alone is not indicative of the quality of text generated\nby a model. We introduce an actor-critic conditional GAN that fills in missing\ntext conditioned on the surrounding context. We show qualitatively and\nquantitatively, evidence that this produces more realistic conditional and\nunconditional text samples compared to a maximum likelihood trained model.</p>\n", "tags": ["Reinforcement Learning","Security","Training Techniques"] },
{"key": "fedus2021switch", "citations": "647", "year": "2021", "title":"Switch Transformers: Scaling To Trillion Parameter Models With Simple And Efficient Sparsity", "abstract": "<p>In deep learning, models typically reuse the same parameters for all inputs.\nMixture of Experts (MoE) defies this and instead selects different parameters\nfor each incoming example. The result is a sparsely-activated model – with\noutrageous numbers of parameters – but a constant computational cost. However,\ndespite several notable successes of MoE, widespread adoption has been hindered\nby complexity, communication costs and training instability – we address these\nwith the Switch Transformer. We simplify the MoE routing algorithm and design\nintuitive improved models with reduced communication and computational costs.\nOur proposed training techniques help wrangle the instabilities and we show\nlarge sparse models may be trained, for the first time, with lower precision\n(bfloat16) formats. We design models based off T5-Base and T5-Large to obtain\nup to 7x increases in pre-training speed with the same computational resources.\nThese improvements extend into multilingual settings where we measure gains\nover the mT5-Base version across all 101 languages. Finally, we advance the\ncurrent scale of language models by pre-training up to trillion parameter\nmodels on the “Colossal Clean Crawled Corpus” and achieve a 4x speedup over the\nT5-XXL model.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "feldman2019multi", "citations": "94", "year": "2019", "title":"Multi-hop Paragraph Retrieval For Open-domain Question Answering", "abstract": "<p>This paper is concerned with the task of multi-hop open-domain Question\nAnswering (QA). This task is particularly challenging since it requires the\nsimultaneous performance of textual reasoning and efficient searching. We\npresent a method for retrieving multiple supporting paragraphs, nested amidst a\nlarge knowledge base, which contain the necessary evidence to answer a given\nquestion. Our method iteratively retrieves supporting paragraphs by forming a\njoint vector representation of both a question and a paragraph. The retrieval\nis performed by considering contextualized sentence-level representations of\nthe paragraphs in the knowledge source. Our method achieves state-of-the-art\nperformance over two well-known datasets, SQuAD-Open and HotpotQA, which serve\nas our single- and multi-hop open-domain QA benchmarks, respectively.</p>\n", "tags": ["Datasets"] },
{"key": "felix2016learning", "citations": "529", "year": "2016", "title":"Learning Distributed Representations Of Sentences From Unlabelled Data", "abstract": "<p>Unsupervised methods for learning distributed representations of words are\nubiquitous in today’s NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.</p>\n", "tags": ["Evaluation","NAACL","Training Techniques"] },
{"key": "feng2019deep", "citations": "237", "year": "2019", "title":"Deep Session Interest Network For Click-through Rate Prediction", "abstract": "<p>Click-Through Rate (CTR) prediction plays an important role in many\nindustrial applications, such as online advertising and recommender systems.\nHow to capture users’ dynamic and evolving interests from their behavior\nsequences remains a continuous research topic in the CTR prediction. However,\nmost existing studies overlook the intrinsic structure of the sequences: the\nsequences are composed of sessions, where sessions are user behaviors separated\nby their occurring time. We observe that user behaviors are highly homogeneous\nin each session, and heterogeneous cross sessions. Based on this observation,\nwe propose a novel CTR model named Deep Session Interest Network (DSIN) that\nleverages users’ multiple historical sessions in their behavior sequences. We\nfirst use self-attention mechanism with bias encoding to extract users’\ninterests in each session. Then we apply Bi-LSTM to model how users’ interests\nevolve and interact among sessions. Finally, we employ the local activation\nunit to adaptively learn the influences of various session interests on the\ntarget item. Experiments are conducted on both advertising and production\nrecommender datasets and DSIN outperforms other state-of-the-art models on both\ndatasets.</p>\n", "tags": ["Applications","Datasets","IJCAI","Model Architecture"] },
{"key": "feng2020codebert", "citations": "1545", "year": "2020", "title":"Codebert: A Pre-trained Model For Programming And Natural Languages", "abstract": "<p>We present CodeBERT, a bimodal pre-trained model for programming language\n(PL) and nat-ural language (NL). CodeBERT learns general-purpose\nrepresentations that support downstream NL-PL applications such as natural\nlanguage codesearch, code documentation generation, etc. We develop CodeBERT\nwith Transformer-based neural architecture, and train it with a hybrid\nobjective function that incorporates the pre-training task of replaced token\ndetection, which is to detect plausible alternatives sampled from generators.\nThis enables us to utilize both bimodal data of NL-PL pairs and unimodal data,\nwhere the former provides input tokens for model training while the latter\nhelps to learn better generators. We evaluate CodeBERT on two NL-PL\napplications by fine-tuning model parameters. Results show that CodeBERT\nachieves state-of-the-art performance on both natural language code search and\ncode documentation generation tasks. Furthermore, to investigate what type of\nknowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and\nevaluate in a zero-shot setting where parameters of pre-trained models are\nfixed. Results show that CodeBERT performs better than previous pre-trained\nmodels on NL-PL probing.</p>\n", "tags": ["Applications","EMNLP","Fine-Tuning","Llm For Code","Model Architecture","Training Techniques"] },
{"key": "feng2020doc2dial", "citations": "82", "year": "2020", "title":"Doc2dial: A Goal-oriented Document-grounded Dialogue Dataset", "abstract": "<p>We introduce doc2dial, a new dataset of goal-oriented dialogues that are\ngrounded in the associated documents. Inspired by how the authors compose\ndocuments for guiding end users, we first construct dialogue flows based on the\ncontent elements that corresponds to higher-level relations across text\nsections as well as lower-level relations between discourse units within a\nsection. Then we present these dialogue flows to crowd contributors to create\nconversational utterances. The dataset includes about 4800 annotated\nconversations with an average of 14 turns that are grounded in over 480\ndocuments from four domains. Compared to the prior document-grounded dialogue\ndatasets, this dataset covers a variety of dialogue scenes in\ninformation-seeking conversations. For evaluating the versatility of the\ndataset, we introduce multiple dialogue modeling tasks and present baseline\napproaches.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "feng2020language", "citations": "309", "year": "2022", "title":"Language-agnostic BERT Sentence Embedding", "abstract": "<p>While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning\n(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have\nyet to be explored. We systematically investigate methods for learning\nmultilingual sentence embeddings by combining the best methods for learning\nmonolingual and cross-lingual representations including: masked language\nmodeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),\ndual encoder translation ranking (Guo et al., 2018), and additive margin\nsoftmax (Yang et al., 2019a). We show that introducing a pre-trained\nmultilingual language model dramatically reduces the amount of parallel\ntraining data required to achieve good performance by 80%. Composing the best\nof these methods produces a model that achieves 83.7% bi-text retrieval\naccuracy over 112 languages on Tatoeba, well above the 65.5% achieved by\nArtetxe and Schwenk (2019b), while still performing competitively on\nmonolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "feng2020scalable", "citations": "188", "year": "2020", "title":"Scalable Multi-hop Relational Reasoning For Knowledge-aware Question Answering", "abstract": "<p>Existing work on augmenting question answering (QA) models with external\nknowledge (e.g., knowledge graphs) either struggle to model multi-hop relations\nefficiently, or lack transparency into the model’s prediction rationale. In\nthis paper, we propose a novel knowledge-aware approach that equips pre-trained\nlanguage models (PTLMs) with a multi-hop relational reasoning module, named\nmulti-hop graph relation network (MHGRN). It performs multi-hop,\nmulti-relational reasoning over subgraphs extracted from external knowledge\ngraphs. The proposed reasoning module unifies path-based reasoning methods and\ngraph neural networks to achieve better interpretability and scalability. We\nalso empirically show its effectiveness and scalability on CommonsenseQA and\nOpenbookQA datasets, and interpret its behaviors with case studies.</p>\n", "tags": ["Datasets","EMNLP","Ethics & Fairness"] },
{"key": "feng2021language", "citations": "63", "year": "2021", "title":"Language Model As An Annotator: Exploring Dialogpt For Dialogue Summarization", "abstract": "<p>Current dialogue summarization systems usually encode the text with a number\nof general semantic features (e.g., keywords and topics) to gain more powerful\ndialogue modeling capabilities. However, these features are obtained via\nopen-domain toolkits that are dialog-agnostic or heavily relied on human\nannotations. In this paper, we show how DialoGPT, a pre-trained model for\nconversational response generation, can be developed as an unsupervised\ndialogue annotator, which takes advantage of dialogue background knowledge\nencoded in DialoGPT. We apply DialoGPT to label three types of features on two\ndialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non\npre-trained models as our summarizes. Experimental results show that our\nproposed method can obtain remarkable improvements on both datasets and\nachieves new state-of-the-art performance on the SAMSum dataset.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn"] },
{"key": "feng2021survey", "citations": "406", "year": "2021", "title":"A Survey On Dialogue Summarization: Recent Advances And New Frontiers", "abstract": "<p>Dialogue summarization aims to condense the original dialogue into a shorter\nversion covering salient information, which is a crucial way to reduce dialogue\ndata overload. Recently, the promising achievements in both dialogue systems\nand natural language generation techniques drastically lead this task to a new\nlandscape, which results in significant research attentions. However, there\nstill remains a lack of a comprehensive survey for this task. To this end, we\ntake the first step and present a thorough review of this research field\ncarefully and widely. In detail, we systematically organize the current works\naccording to the characteristics of each domain, covering meeting, chat, email\nthread, customer service and medical dialogue. Additionally, we provide an\noverview of publicly available research datasets as well as organize two\nleaderboards under unified metrics. Furthermore, we discuss some future\ndirections, including faithfulness, multi-modal, multi-domain and multi-lingual\ndialogue summarization, and give our thoughts respectively. We hope that this\nfirst survey of dialogue summarization can provide the community with a quick\naccess and a general picture to this task and motivate future researches.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Survey Paper"] },
{"key": "feng2022promptdet", "citations": "89", "year": "2022", "title":"Promptdet: Towards Open-vocabulary Detection Using Uncurated Images", "abstract": "<p>The goal of this work is to establish a scalable pipeline for expanding an\nobject detector towards novel/unseen categories, using zero manual annotations.\nTo achieve that, we make the following four contributions: (i) in pursuit of\ngeneralisation, we propose a two-stage open-vocabulary object detector, where\nthe class-agnostic object proposals are classified with a text encoder from\npre-trained visual-language model; (ii) To pair the visual latent space (of RPN\nbox proposals) with that of the pre-trained text encoder, we propose the idea\nof regional prompt learning to align the textual embedding space with regional\nvisual object features; (iii) To scale up the learning procedure towards\ndetecting a wider spectrum of objects, we exploit the available online resource\nvia a novel self-training framework, which allows to train the proposed\ndetector on a large corpus of noisy uncurated web images. Lastly, (iv) to\nevaluate our proposed detector, termed as PromptDet, we conduct extensive\nexperiments on the challenging LVIS and MS-COCO dataset. PromptDet shows\nsuperior performance over existing approaches with fewer additional training\nimages and zero manual annotations whatsoever. Project page with code:\nhttps://fcjian.github.io/promptdet.</p>\n", "tags": ["Datasets","Has Code","Prompting","Tools","Training Techniques"] },
{"key": "ferrara2023genai", "citations": "72", "year": "2024", "title":"Genai Against Humanity: Nefarious Applications Of Generative Artificial Intelligence And Large Language Models", "abstract": "<p>Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\nare marvels of technology; celebrated for their prowess in natural language\nprocessing and multimodal content generation, they promise a transformative\nfuture. But as with all powerful tools, they come with their shadows. Picture\nliving in a world where deepfakes are indistinguishable from reality, where\nsynthetic identities orchestrate malicious campaigns, and where targeted\nmisinformation or scams are crafted with unparalleled precision. Welcome to the\ndarker side of GenAI applications. This article is not just a journey through\nthe meanders of potential misuse of GenAI and LLMs, but also a call to\nrecognize the urgency of the challenges ahead. As we navigate the seas of\nmisinformation campaigns, malicious content generation, and the eerie creation\nof sophisticated malware, we’ll uncover the societal implications that ripple\nthrough the GenAI revolution we are witnessing. From AI-powered botnets on\nsocial media platforms to the unnerving potential of AI to generate fabricated\nidentities, or alibis made of synthetic realities, the stakes have never been\nhigher. The lines between the virtual and the real worlds are blurring, and the\nconsequences of potential GenAI’s nefarious applications impact us all. This\narticle serves both as a synthesis of rigorous research presented on the risks\nof GenAI and misuse of LLMs and as a thought-provoking vision of the different\ntypes of harmful GenAI applications we might encounter in the near future, and\nsome ways we can prepare for them.</p>\n", "tags": ["Applications","Ethics & Fairness","Tools"] },
{"key": "ferrara2023should", "citations": "142", "year": "2023", "title":"Should Chatgpt Be Biased? Challenges And Risks Of Bias In Large Language Models", "abstract": "<p>As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.</p>\n", "tags": ["Applications","Ethics & Fairness","Model Architecture","Training Techniques"] },
{"key": "ferreira2019neural", "citations": "102", "year": "2019", "title":"Neural Data-to-text Generation: A Comparison Between Pipeline And End-to-end Architectures", "abstract": "<p>Traditionally, most data-to-text applications have been designed using a\nmodular pipeline architecture, in which non-linguistic input data is converted\ninto natural language through several intermediate transformations. In\ncontrast, recent neural models for data-to-text generation have been proposed\nas end-to-end approaches, where the non-linguistic input is rendered in natural\nlanguage with much less explicit intermediate representations in-between. This\nstudy introduces a systematic comparison between neural pipeline and end-to-end\ndata-to-text approaches for the generation of text from RDF triples. Both\narchitectures were implemented making use of state-of-the art deep learning\nmethods as the encoder-decoder Gated-Recurrent Units (GRU) and Transformer.\nAutomatic and human evaluations together with a qualitative analysis suggest\nthat having explicit intermediate steps in the generation process results in\nbetter texts than the ones generated by end-to-end approaches. Moreover, the\npipeline models generalize better to unseen inputs. Data and code are publicly\navailable.</p>\n", "tags": ["Applications","EMNLP","Model Architecture"] },
{"key": "ficler2017controlling", "citations": "273", "year": "2017", "title":"Controlling Linguistic Style Aspects In Neural Language Generation", "abstract": "<p>Most work on neural natural language generation (NNLG) focus on controlling\nthe content of the generated text. We experiment with controlling several\nstylistic aspects of the generated text, in addition to its content. The method\nis based on conditioned RNN language model, where the desired content as well\nas the stylistic parameters serve as conditioning contexts. We demonstrate the\napproach on the movie reviews domain and show that it is successful in\ngenerating coherent sentences corresponding to the required linguistic style\nand content.</p>\n", "tags": [] },
{"key": "firat2016zero", "citations": "274", "year": "2016", "title":"Zero-resource Translation With Multi-lingual Neural Machine Translation", "abstract": "<p>In this paper, we propose a novel finetuning algorithm for the recently\nintroduced multi-way, mulitlingual neural machine translate that enables\nzero-resource machine translation. When used together with novel many-to-one\ntranslation strategies, we empirically show that this finetuning algorithm\nallows the multi-way, multilingual model to translate a zero-resource language\npair (1) as well as a single-pair neural translation model trained with up to\n1M direct parallel sentences of the same language pair and (2) better than\npivot-based translation strategy, while keeping only one additional copy of\nattention-related parameters.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "fisch2019mrqa", "citations": "242", "year": "2019", "title":"MRQA 2019 Shared Task: Evaluating Generalization In Reading Comprehension", "abstract": "<p>We present the results of the Machine Reading for Question Answering (MRQA)\n2019 shared task on evaluating the generalization capabilities of reading\ncomprehension systems. In this task, we adapted and unified 18 distinct\nquestion answering datasets into the same format. Among them, six datasets were\nmade available for training, six datasets were made available for development,\nand the final six were hidden for final evaluation. Ten teams submitted\nsystems, which explored various ideas including data sampling, multi-task\nlearning, adversarial training and ensembling. The best system achieved an\naverage F1 score of 72.5 on the 12 held-out datasets, 10.7 absolute points\nhigher than our initial baseline based on BERT.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "fitzgerald2018large", "citations": "86", "year": "2018", "title":"Large-scale QA-SRL Parsing", "abstract": "<p>We present a new large-scale corpus of Question-Answer driven Semantic Role\nLabeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our\ncorpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for\nover 64,000 sentences across 3 domains and was gathered with a new\ncrowd-sourcing scheme that we show has high precision and good recall at modest\ncost. We also present neural models for two QA-SRL subtasks: detecting argument\nspans for a predicate and generating questions to label the semantic\nrelationship. The best models achieve question accuracy of 82.6% and span-level\naccuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL\nprediction task. They can also, as we show, be used to gather additional\nannotations at low cost.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "fitzgerald2022massive", "citations": "65", "year": "2023", "title":"MASSIVE: A 1m-example Multilingual Natural Language Understanding Dataset With 51 Typologically-diverse Languages", "abstract": "<p>We present the MASSIVE dataset–Multilingual Amazon Slu resource package\n(SLURP) for Slot-filling, Intent classification, and Virtual assistant\nEvaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant\nutterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE\nwas created by tasking professional translators to localize the English-only\nSLURP dataset into 50 typologically diverse languages from 29 genera. We also\npresent modeling results on XLM-R and mT5, including exact match accuracy,\nintent classification accuracy, and slot-filling F1 score. We have released our\ndataset, modeling code, and models publicly.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "font2019equalizing", "citations": "112", "year": "2019", "title":"Equalizing Gender Biases In Neural Machine Translation With Word Embeddings Techniques", "abstract": "<p>Neural machine translation has significantly pushed forward the quality of\nthe field. However, there are remaining big issues with the output translations\nand one of them is fairness. Neural models are trained on large text corpora\nwhich contain biases and stereotypes. As a consequence, models inherit these\nsocial biases. Recent methods have shown results in reducing gender bias in\nother natural language processing tools such as word embeddings. We take\nadvantage of the fact that word embeddings are used in neural machine\ntranslation to propose a method to equalize gender biases in neural machine\ntranslation using these representations. Specifically, we propose, experiment\nand analyze the integration of two debiasing techniques over GloVe embeddings\nin the Transformer translation architecture. We evaluate our proposed system on\nthe WMT English-Spanish benchmark task, showing gains up to one BLEU point. As\nfor the gender bias evaluation, we generate a test set of occupations and we\nshow that our proposed system learns to equalize existing biases from the\nbaseline system.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Model Architecture","Tools"] },
{"key": "frans2021clipdraw", "citations": "78", "year": "2021", "title":"Clipdraw: Exploring Text-to-drawing Synthesis Through Language-image Encoders", "abstract": "<p>This work presents CLIPDraw, an algorithm that synthesizes novel drawings\nbased on natural language input. CLIPDraw does not require any training; rather\na pre-trained CLIP language-image encoder is used as a metric for maximizing\nsimilarity between the given description and a generated drawing. Crucially,\nCLIPDraw operates over vector strokes rather than pixel images, a constraint\nthat biases drawings towards simpler human-recognizable shapes. Results compare\nbetween CLIPDraw and other synthesis-through-optimization methods, as well as\nhighlight various interesting behaviors of CLIPDraw, such as satisfying\nambiguous text in multiple ways, reliably producing drawings in diverse\nartistic styles, and scaling from simple to complex visual representations as\nstroke count is increased. Code for experimenting with the method is available\nat:\nhttps://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "frantar2022gptq", "citations": "96", "year": "2022", "title":"GPTQ: Accurate Post-training Quantization For Generative Pre-trained Transformers", "abstract": "<p>Generative Pre-trained Transformer models, known as GPT or OPT, set\nthemselves apart through breakthrough performance across complex language\nmodelling tasks, but also by their extremely high computational and storage\ncosts. Specifically, due to their massive size, even inference for large,\nhighly-accurate GPT models may require multiple performant GPUs, which limits\nthe usability of such models. While there is emerging work on relieving this\npressure via model compression, the applicability and performance of existing\ncompression techniques is limited by the scale and complexity of GPT models. In\nthis paper, we address this challenge, and propose GPTQ, a new one-shot weight\nquantization method based on approximate second-order information, that is both\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\nmodels with 175 billion parameters in approximately four GPU hours, reducing\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\ndegradation relative to the uncompressed baseline. Our method more than doubles\nthe compression gains relative to previously-proposed one-shot quantization\nmethods, preserving accuracy, allowing us for the first time to execute an 175\nbillion-parameter model inside a single GPU for generative inference. Moreover,\nwe also show that our method can still provide reasonable accuracy in the\nextreme quantization regime, in which weights are quantized to 2-bit or even\nternary quantization levels. We show experimentally that these improvements can\nbe leveraged for end-to-end inference speedups over FP16, of around 3.25x when\nusing high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones\n(NVIDIA A6000). The implementation is available at\nhttps://github.com/IST-DASLab/gptq.</p>\n", "tags": ["Efficiency","Has Code","Model Architecture","Training Techniques"] },
{"key": "freitag2016fast", "citations": "181", "year": "2016", "title":"Fast Domain Adaptation For Neural Machine Translation", "abstract": "<p>Neural Machine Translation (NMT) is a new approach for automatic translation\nof text from one human language into another. The basic concept in NMT is to\ntrain a large Neural Network that maximizes the translation performance on a\ngiven parallel corpus. NMT is gaining popularity in the research community\nbecause it outperformed traditional SMT approaches in several translation tasks\nat WMT and other evaluation tasks/benchmarks at least for some language pairs.\nHowever, many of the enhancements in SMT over the years have not been\nincorporated into the NMT framework. In this paper, we focus on one such\nenhancement namely domain adaptation. We propose an approach for adapting a NMT\nsystem to a new domain. The main idea behind domain adaptation is that the\navailability of large out-of-domain training data and a small in-domain\ntraining data. We report significant gains with our proposed method in both\nautomatic metrics and a human subjective evaluation metric on two language\npairs. With our adaptation method, we show large improvement on the new domain\nwhile the performance of our general domain only degrades slightly. In\naddition, our approach is fast enough to adapt an already trained system to a\nnew domain within few hours without the need to retrain the NMT model on the\ncombined data which usually takes several days/weeks depending on the volume of\nthe data.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Tools","Training Techniques"] },
{"key": "freitag2017ensemble", "citations": "67", "year": "2017", "title":"Ensemble Distillation For Neural Machine Translation", "abstract": "<p>Knowledge distillation describes a method for training a student network to\nperform better by learning from a stronger teacher network. Translating a\nsentence with an Neural Machine Translation (NMT) engine is time expensive and\nhaving a smaller model speeds up this process. We demonstrate how to transfer\nthe translation quality of an ensemble and an oracle BLEU teacher network into\na single NMT system. Further, we present translation improvements from a\nteacher network that has the same architecture and dimensions of the student\nnetwork. As the training of the student model is still expensive, we introduce\na data filtering method based on the knowledge of the teacher model that not\nonly speeds up the training, but also leads to better translation quality. Our\ntechniques need no code change and can be easily reproduced with any NMT\narchitecture to speed up the decoding process.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "fried2017unified", "citations": "74", "year": "2018", "title":"Unified Pragmatic Models For Generating And Following Instructions", "abstract": "<p>We show that explicit pragmatic inference aids in correctly generating and\nfollowing natural language instructions for complex, sequential tasks. Our\npragmatics-enabled models reason about why speakers produce certain\ninstructions, and about how listeners will react upon hearing them. Like\nprevious pragmatic models, we use learned base listener and speaker models to\nbuild a pragmatic speaker that uses the base listener to simulate the\ninterpretation of candidate descriptions, and a pragmatic listener that reasons\ncounterfactually about alternative descriptions. We extend these models to\ntasks with sequential structure. Evaluation of language generation and\ninterpretation shows that pragmatic inference improves state-of-the-art\nlistener models (at correctly interpreting human instructions) and speaker\nmodels (at producing instructions correctly interpreted by humans) in diverse\nsettings.</p>\n", "tags": ["Evaluation","Instruction Following","NAACL"] },
{"key": "fried2018speaker", "citations": "220", "year": "2018", "title":"Speaker-follower Models For Vision-and-language Navigation", "abstract": "<p>Navigation guided by natural language instructions presents a challenging\nreasoning problem for instruction followers. Natural language instructions\ntypically identify only a few high-level decisions and landmarks rather than\ncomplete low-level motor behaviors; much of the missing information must be\ninferred based on perceptual context. In machine learning settings, this is\ndoubly challenging: it is difficult to collect enough annotated data to enable\nlearning of this reasoning process from scratch, and also difficult to\nimplement the reasoning process using generic sequence models. Here we describe\nan approach to vision-and-language navigation that addresses both these issues\nwith an embedded speaker model. We use this speaker model to (1) synthesize new\ninstructions for data augmentation and to (2) implement pragmatic reasoning,\nwhich evaluates how well candidate action sequences explain an instruction.\nBoth steps are supported by a panoramic action space that reflects the\ngranularity of human-generated instructions. Experiments show that all three\ncomponents of this approach—speaker-driven data augmentation, pragmatic\nreasoning and panoramic action space—dramatically improve the performance of\na baseline instruction follower, more than doubling the success rate over the\nbest existing approach on a standard benchmark.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "fried2022incoder", "citations": "110", "year": "2022", "title":"Incoder: A Generative Model For Code Infilling And Synthesis", "abstract": "<p>Code is seldom written in a single left-to-right pass and is instead\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\nthat can perform program synthesis (via left-to-right generation) as well as\nediting (via infilling). InCoder is trained to generate code files from a large\ncorpus of permissively licensed code, where regions of code have been randomly\nmasked and moved to the end of each file, allowing code infilling with\nbidirectional context. Our model is the first generative model that is able to\ndirectly perform zero-shot code infilling, which we evaluate on challenging\ntasks such as type inference, comment generation, and variable re-naming. We\nfind that the ability to condition on bidirectional context substantially\nimproves performance on these tasks, while still performing comparably on\nstandard program synthesis benchmarks in comparison to left-to-right only\nmodels pretrained at similar scale. The InCoder models and code are publicly\nreleased. https://sites.google.com/view/incoder-code-models</p>\n", "tags": ["Datasets","Llm For Code"] },
{"key": "frolov2021adversarial", "citations": "159", "year": "2021", "title":"Adversarial Text-to-image Synthesis: A Review", "abstract": "<p>With the advent of generative adversarial networks, synthesizing images from\ntextual descriptions has recently become an active research area. It is a\nflexible and intuitive way for conditional image generation with significant\nprogress in the last years regarding visual realism, diversity, and semantic\nalignment. However, the field still faces several challenges that require\nfurther research efforts such as enabling the generation of high-resolution\nimages with multiple objects, and developing suitable and reliable evaluation\nmetrics that correlate with human judgement. In this review, we contextualize\nthe state of the art of adversarial text-to-image synthesis models, their\ndevelopment since their inception five years ago, and propose a taxonomy based\non the level of supervision. We critically examine current strategies to\nevaluate text-to-image synthesis models, highlight shortcomings, and identify\nnew areas of research, ranging from the development of better datasets and\nevaluation metrics to possible improvements in architectural design and model\ntraining. This review complements previous surveys on generative adversarial\nnetworks with a focus on text-to-image synthesis which we believe will help\nresearchers to further advance the field.</p>\n", "tags": ["Datasets","Evaluation","Security","Survey Paper","Training Techniques"] },
{"key": "fu2019language", "citations": "75", "year": "2019", "title":"From Language To Goals: Inverse Reinforcement Learning For Vision-based Instruction Following", "abstract": "<p>Reinforcement learning is a promising framework for solving control problems,\nbut its use in practical situations is hampered by the fact that reward\nfunctions are often difficult to engineer. Specifying goals and tasks for\nautonomous machines, such as robots, is a significant challenge:\nconventionally, reward functions and goal states have been used to communicate\nobjectives. But people can communicate objectives to each other simply by\ndescribing or demonstrating them. How can we build learning algorithms that\nwill allow us to tell machines what we want them to do? In this work, we\ninvestigate the problem of grounding language commands as reward functions\nusing inverse reinforcement learning, and argue that language-conditioned\nrewards are more transferable than language-conditioned policies to new\nenvironments. We propose language-conditioned reward learning (LC-RL), which\ngrounds language commands as a reward function represented by a deep neural\nnetwork. We demonstrate that our model learns rewards that transfer to novel\ntasks and environments on realistic, high-dimensional visual environments with\nnatural language commands, whereas directly learning a language-conditioned\npolicy leads to poor performance.</p>\n", "tags": ["Agentic","Instruction Following","Reinforcement Learning","Tools"] },
{"key": "fu2021violet", "citations": "88", "year": "2021", "title":"VIOLET : End-to-end Video-language Transformers With Masked Visual-token Modeling", "abstract": "<p>A great challenge in video-language (VidL) modeling lies in the disconnection\nbetween fixed video representations extracted from image/video understanding\nmodels and downstream VidL data. Recent studies try to mitigate this\ndisconnection via end-to-end training. To make it computationally feasible,\nprior works tend to “imagify” video inputs, i.e., a handful of sparsely sampled\nframes are fed into a 2D CNN, followed by a simple mean-pooling or\nconcatenation to obtain the overall video representations. Although achieving\npromising results, such simple approaches may lose temporal information that is\nessential for performing downstream VidL tasks. In this work, we present\nVIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video\ntransformer to explicitly model the temporal dynamics of video inputs. Further,\nunlike previous studies that found pre-training tasks on video inputs (e.g.,\nmasked frame modeling) not very effective, we design a new pre-training task,\nMasked Visual-token Modeling (MVM), for better video modeling. Specifically,\nthe original video frame patches are “tokenized” into discrete visual tokens,\nand the goal is to recover the original visual tokens based on the masked\npatches. Comprehensive analysis demonstrates the effectiveness of both explicit\ntemporal modeling via video transformer and MVM. As a result, VIOLET achieves\nnew state-of-the-art performance on 5 video question answering tasks and 4\ntext-to-video retrieval tasks.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "fu2022complexity", "citations": "64", "year": "2022", "title":"Complexity-based Prompting For Multi-step Reasoning", "abstract": "<p>We study the task of prompting large-scale language models to perform\nmulti-step reasoning. Existing work shows that when prompted with a chain of\nthoughts (CoT), sequences of short sentences describing intermediate reasoning\nsteps towards a final answer, large language models can generate new reasoning\nchains and predict answers for new inputs. A central question is which\nreasoning examples make the most effective prompts. In this work, we propose\ncomplexity-based prompting, a simple and effective example selection scheme for\nmulti-step reasoning. We show that prompts with higher reasoning complexity,\ni.e., chains with more reasoning steps, achieve substantially better\nperformance on multi-step reasoning tasks over strong baselines. We further\nextend our complexity-based criteria from prompting (selecting inputs) to\ndecoding (selecting outputs), where we sample multiple reasoning chains from\nthe model, then choose the majority of generated answers from complex reasoning\nchains (over simple chains). When used to prompt GPT-3 and Codex, our approach\nsubstantially improves multi-step reasoning accuracy and achieves new\nstate-of-the-art (SOTA) performance on three math benchmarks (GSM8K,\nMultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and\nPenguins), with an average +5.3 and up to +18 accuracy improvements. Compared\nwith existing example selection schemes like manual tuning or retrieval-based\nselection, selection based on reasoning complexity is intuitive, easy to\nimplement, and annotation-efficient. Further results demonstrate the robustness\nof performance gains from complex prompts under format perturbation and\ndistribution shift.</p>\n", "tags": ["Model Architecture","Prompting"] },
{"key": "fu2022effectiveness", "citations": "61", "year": "2023", "title":"On The Effectiveness Of Parameter-efficient Fine-tuning", "abstract": "<p>Fine-tuning pre-trained models has been ubiquitously proven to be effective\nin a wide range of NLP tasks. However, fine-tuning the whole model is parameter\ninefficient as it always yields an entirely new model for each task. Currently,\nmany research works propose to only fine-tune a small portion of the parameters\nwhile keeping most of the parameters shared across different tasks. These\nmethods achieve surprisingly good performance and are shown to be more stable\nthan their corresponding fully fine-tuned counterparts. However, such kind of\nmethods is still not well understood. Some natural questions arise: How does\nthe parameter sparsity lead to promising performance? Why is the model more\nstable than the fully fine-tuned models? How to choose the tunable parameters?\nIn this paper, we first categorize the existing methods into random approaches,\nrule-based approaches, and projection-based approaches based on how they choose\nwhich parameters to tune. Then, we show that all of the methods are actually\nsparse fine-tuned models and conduct a novel theoretical analysis of them. We\nindicate that the sparsity is actually imposing a regularization on the\noriginal model by controlling the upper bound of the stability. Such stability\nleads to better generalization capability which has been empirically observed\nin a lot of recent research works. Despite the effectiveness of sparsity\ngrounded by our theory, it still remains an open problem of how to choose the\ntunable parameters. To better choose the tunable parameters, we propose a novel\nSecond-order Approximation Method (SAM) which approximates the original problem\nwith an analytically solvable optimization function. The tunable parameters are\ndetermined by directly optimizing the approximation function. The experimental\nresults show that our proposed SAM model outperforms many strong baseline\nmodels and it also verifies our theoretical analysis.</p>\n", "tags": ["AAAI","Efficiency","Fine-Tuning","Training Techniques"] },
{"key": "fu2023gptscore", "citations": "74", "year": "2023", "title":"Gptscore: Evaluate As You Desire", "abstract": "<p>Generative Artificial Intelligence (AI) has enabled the development of\nsophisticated models that are capable of producing high-caliber text, images,\nand other outputs through the utilization of large pre-trained models.\nNevertheless, assessing the quality of the generation is an even more arduous\ntask than the generation itself, and this issue has not been given adequate\nconsideration recently. This paper proposes a novel evaluation framework,\nGPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)\nof generative pre-trained models to score generated texts. There are 19\npre-trained models explored in this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text\ngeneration tasks, 22 evaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively allow us to achieve what one\ndesires to evaluate for texts simply by natural language instructions. This\nnature helps us overcome several long-standing challenges in text\nevaluation–how to achieve customized, multi-faceted evaluation without the\nneed for annotated samples. We make our code publicly available at\nhttps://github.com/jinlanfu/GPTScore.</p>\n", "tags": ["Datasets","Emergent Abilities","Evaluation","Has Code","Tools"] },
{"key": "fu2023mme", "citations": "66", "year": "2023", "title":"MME: A Comprehensive Evaluation Benchmark For Multimodal Large Language Models", "abstract": "<p>Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform\nmultimodal tasks, showing amazing emergent abilities in recent studies, such as\nwriting poems based on an image. However, it is difficult for these case\nstudies to fully reflect the performance of MLLM, lacking a comprehensive\nevaluation. In this paper, we fill in this blank, presenting the first\ncomprehensive MLLM Evaluation benchmark MME. It measures both perception and\ncognition abilities on a total of 14 subtasks. In order to avoid data leakage\nthat may arise from direct use of public datasets for evaluation, the\nannotations of instruction-answer pairs are all manually designed. The concise\ninstruction design allows us to fairly compare MLLMs, instead of struggling in\nprompt engineering. Besides, with such an instruction, we can also easily carry\nout quantitative statistics. A total of 30 advanced MLLMs are comprehensively\nevaluated on our MME, which not only suggests that existing MLLMs still have a\nlarge room for improvement, but also reveals the potential directions for the\nsubsequent model optimization. The data application manner and online\nleaderboards are released at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.</p>\n", "tags": ["Datasets","Emergent Abilities","Evaluation","Has Code"] },
{"key": "fukui2016multimodal", "citations": "1350", "year": "2016", "title":"Multimodal Compact Bilinear Pooling For Visual Question Answering And Visual Grounding", "abstract": "<p>Modeling textual or visual information with vector representations trained\nfrom large language or visual datasets has been successfully explored in recent\nyears. However, tasks such as visual question answering require combining these\nvector representations with each other. Approaches to multimodal pooling\ninclude element-wise product or sum, as well as concatenation of the visual and\ntextual representations. We hypothesize that these methods are not as\nexpressive as an outer product of the visual and textual vectors. As the outer\nproduct is typically infeasible due to its high dimensionality, we instead\npropose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and\nexpressively combine multimodal features. We extensively evaluate MCB on the\nvisual question answering and grounding tasks. We consistently show the benefit\nof MCB over ablations without MCB. For visual question answering, we present an\narchitecture which uses MCB twice, once for predicting attention over spatial\nfeatures and again to combine the attended representation with the question\nrepresentation. This model outperforms the state-of-the-art on the Visual7W\ndataset and the VQA challenge.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "furrer2020compositional", "citations": "81", "year": "2020", "title":"Compositional Generalization In Semantic Parsing: Pre-training Vs. Specialized Architectures", "abstract": "<p>While mainstream machine learning methods are known to have limited ability\nto compositionally generalize, new architectures and techniques continue to be\nproposed to address this limitation. We investigate state-of-the-art techniques\nand architectures in order to assess their effectiveness in improving\ncompositional generalization in semantic parsing tasks based on the SCAN and\nCFQ datasets. We show that masked language model (MLM) pre-training rivals\nSCAN-inspired architectures on primitive holdout splits. On a more complex\ncompositional task, we show that pre-training leads to significant improvements\nin performance vs. comparable non-pre-trained models, whereas architectures\nproposed to encourage compositional generalization on SCAN or in the area of\nalgorithm learning fail to lead to significant improvements. We establish a new\nstate of the art on the CFQ compositional generalization benchmark using MLM\npre-training together with an intermediate representation.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "futrell2019neural", "citations": "172", "year": "2019", "title":"Neural Language Models As Psycholinguistic Subjects: Representations Of Syntactic State", "abstract": "<p>We deploy the methods of controlled psycholinguistic experimentation to shed\nlight on the extent to which the behavior of neural network language models\nreflects incremental representations of syntactic state. To do so, we examine\nmodel behavior on artificial sentences containing a variety of syntactically\ncomplex structures. We test four models: two publicly available LSTM sequence\nmodels of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on\nlarge datasets; an RNNG (Dyer et al., 2016) trained on a small, parsed dataset;\nand an LSTM trained on the same small corpus as the RNNG. We find evidence that\nthe LSTMs trained on large datasets represent syntactic state over large spans\nof text in a way that is comparable to the RNNG, while the LSTM trained on the\nsmall dataset does not or does so only weakly.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "gafni2022make", "citations": "211", "year": "2022", "title":"Make-a-scene: Scene-based Text-to-image Generation With Human Priors", "abstract": "<p>Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "gaido2020end", "citations": "64", "year": "2020", "title":"End-to-end Speech-translation With Knowledge Distillation: FBK@IWSLT2020", "abstract": "<p>This paper describes FBK’s participation in the IWSLT 2020 offline speech\ntranslation (ST) task. The task evaluates systems’ ability to translate English\nTED talks audio into German texts. The test talks are provided in two versions:\none contains the data already segmented with automatic tools and the other is\nthe raw data without any segmentation. Participants can decide whether to work\non custom segmentation or not. We used the provided segmentation. Our system is\nan end-to-end model based on an adaptation of the Transformer for speech data.\nIts training process is the main focus of this paper and it is based on: i)\ntransfer learning (ASR pretraining and knowledge distillation), ii) data\naugmentation (SpecAugment, time stretch and synthetic data), iii) combining\nsynthetic and real data marked as different domains, and iv) multi-task\nlearning using the CTC loss. Finally, after the training with word-level\nknowledge distillation is complete, our ST models are fine-tuned using label\nsmoothed cross entropy. Our best model scored 29 BLEU on the MuST-C En-De test\nset, which is an excellent result compared to recent papers, and 23.7 BLEU on\nthe same data segmented with VAD, showing the need for researching solutions\naddressing this specific data condition.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "gajos2022do", "citations": "68", "year": "2022", "title":"Do People Engage Cognitively With AI? Impact Of AI Assistance On Incidental Learning", "abstract": "<p>When people receive advice while making difficult decisions, they often make\nbetter decisions in the moment and also increase their knowledge in the\nprocess. However, such incidental learning can only occur when people\ncognitively engage with the information they receive and process this\ninformation thoughtfully. How do people process the information and advice they\nreceive from AI, and do they engage with it deeply enough to enable learning?\nTo answer these questions, we conducted three experiments in which individuals\nwere asked to make nutritional decisions and received simulated AI\nrecommendations and explanations. In the first experiment, we found that when\npeople were presented with both a recommendation and an explanation before\nmaking their choice, they made better decisions than they did when they\nreceived no such help, but they did not learn. In the second experiment,\nparticipants first made their own choice, and only then saw a recommendation\nand an explanation from AI; this condition also resulted in improved decisions,\nbut no learning. However, in our third experiment, participants were presented\nwith just an AI explanation but no recommendation and had to arrive at their\nown decision. This condition led to both more accurate decisions and learning\ngains. We hypothesize that learning gains in this condition were due to deeper\nengagement with explanations needed to arrive at the decisions. This work\nprovides some of the most direct evidence to date that it may not be sufficient\nto include explanations together with AI-generated recommendation to ensure\nthat people engage carefully with the AI-provided information. This work also\npresents one technique that enables incidental learning and, by implication,\ncan help people process AI recommendations and explanations more carefully.</p>\n", "tags": [] },
{"key": "gal2022image", "citations": "332", "year": "2022", "title":"An Image Is Worth One Word: Personalizing Text-to-image Generation Using Textual Inversion", "abstract": "<p>Text-to-image models offer unprecedented freedom to guide creation through\nnatural language. Yet, it is unclear how such freedom can be exercised to\ngenerate images of specific unique concepts, modify their appearance, or\ncompose them in new roles and novel scenes. In other words, we ask: how can we\nuse language-guided models to turn our cat into a painting, or imagine a new\nproduct based on our favorite toy? Here we present a simple approach that\nallows such creative freedom. Using only 3-5 images of a user-provided concept,\nlike an object or a style, we learn to represent it through new “words” in the\nembedding space of a frozen text-to-image model. These “words” can be composed\ninto natural language sentences, guiding personalized creation in an intuitive\nway. Notably, we find evidence that a single word embedding is sufficient for\ncapturing unique and varied concepts. We compare our approach to a wide range\nof baselines, and demonstrate that it can more faithfully portray the concepts\nacross a range of applications and tasks.\n  Our code, data and new words will be available at:\nhttps://textual-inversion.github.io</p>\n", "tags": ["Applications","Has Code"] },
{"key": "gal2023encoder", "citations": "92", "year": "2023", "title":"Encoder-based Domain Tuning For Fast Personalization Of Text-to-image Models", "abstract": "<p>Text-to-image personalization aims to teach a pre-trained diffusion model to\nreason about novel, user provided concepts, embedding them into new scenes\nguided by natural language prompts. However, current personalization approaches\nstruggle with lengthy training times, high storage requirements or loss of\nidentity. To overcome these limitations, we propose an encoder-based\ndomain-tuning approach. Our key insight is that by underfitting on a large set\nof concepts from a given domain, we can improve generalization and create a\nmodel that is more amenable to quickly adding novel concepts from the same\ndomain. Specifically, we employ two components: First, an encoder that takes as\nan input a single image of a target concept from a given domain, e.g. a\nspecific face, and learns to map it into a word-embedding representing the\nconcept. Second, a set of regularized weight-offsets for the text-to-image\nmodel that learn how to effectively ingest additional concepts. Together, these\ncomponents are used to guide the learning of unseen concepts, allowing us to\npersonalize a model using only a single image and as few as 5 training steps -\naccelerating personalization from dozens of minutes to seconds, while\npreserving quality.</p>\n", "tags": ["Training Techniques"] },
{"key": "gallegos2023bias", "citations": "143", "year": "2024", "title":"Bias And Fairness In Large Language Models: A Survey", "abstract": "<p>Rapid advancements of large language models (LLMs) have enabled the\nprocessing, understanding, and generation of human-like text, with increasing\nintegration into systems that touch our social sphere. Despite this success,\nthese models can learn, perpetuate, and amplify harmful social biases. In this\npaper, we present a comprehensive survey of bias evaluation and mitigation\ntechniques for LLMs. We first consolidate, formalize, and expand notions of\nsocial bias and fairness in natural language processing, defining distinct\nfacets of harm and introducing several desiderata to operationalize fairness\nfor LLMs. We then unify the literature by proposing three intuitive taxonomies,\ntwo for bias evaluation, namely metrics and datasets, and one for mitigation.\nOur first taxonomy of metrics for bias evaluation disambiguates the\nrelationship between metrics and evaluation datasets, and organizes metrics by\nthe different levels at which they operate in a model: embeddings,\nprobabilities, and generated text. Our second taxonomy of datasets for bias\nevaluation categorizes datasets by their structure as counterfactual inputs or\nprompts, and identifies the targeted harms and social groups; we also release a\nconsolidation of publicly-available datasets for improved access. Our third\ntaxonomy of techniques for bias mitigation classifies methods by their\nintervention during pre-processing, in-training, intra-processing, and\npost-processing, with granular subcategories that elucidate research trends.\nFinally, we identify open problems and challenges for future work. Synthesizing\na wide range of recent research, we aim to provide a clear guide of the\nexisting literature that empowers researchers and practitioners to better\nunderstand and prevent the propagation of bias in LLMs.</p>\n", "tags": ["Ethics & Fairness","Evaluation","Survey Paper","Training Techniques"] },
{"key": "gan2016learning", "citations": "95", "year": "2017", "title":"Learning Generic Sentence Representations Using Convolutional Neural Networks", "abstract": "<p>We propose a new encoder-decoder approach to learn distributed sentence\nrepresentations that are applicable to multiple purposes. The model is learned\nby using a convolutional neural network as an encoder to map an input sentence\ninto a continuous vector, and using a long short-term memory recurrent neural\nnetwork as a decoder. Several tasks are considered, including sentence\nreconstruction and future sentence prediction. Further, a hierarchical\nencoder-decoder model is proposed to encode a sentence to predict multiple\nfuture sentences. By training our models on a large collection of novels, we\nobtain a highly generic convolutional sentence encoder that performs well in\npractice. Experimental results on several benchmark datasets, and across a\nbroad range of applications, demonstrate the superiority of the proposed model\nover competing methods.</p>\n", "tags": ["Applications","Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "gan2017vqs", "citations": "106", "year": "2017", "title":"VQS: Linking Segmentations To Questions And Answers For Supervised Attention In VQA And Question-focused Semantic Segmentation", "abstract": "<p>Rich and dense human labeled datasets are among the main enabling factors for\nthe recent advance on vision-language understanding. Many seemingly distant\nannotations (e.g., semantic segmentation and visual question answering (VQA))\nare inherently connected in that they reveal different levels and perspectives\nof human understandings about the same visual scenes — and even the same set\nof images (e.g., of COCO). The popularity of COCO correlates those annotations\nand tasks. Explicitly linking them up may significantly benefit both individual\ntasks and the unified vision and language modeling. We present the preliminary\nwork of linking the instance segmentations provided by COCO to the questions\nand answers (QAs) in the VQA dataset, and name the collected links visual\nquestions and segmentation answers (VQS). They transfer human supervision\nbetween the previously separate tasks, offer more effective leverage to\nexisting problems, and also open the door for new research problems and models.\nWe study two applications of the VQS data in this paper: supervised attention\nfor VQA and a novel question-focused semantic segmentation task. For the\nformer, we obtain state-of-the-art results on the VQA real multiple-choice task\nby simply augmenting the multilayer perceptrons with some attention features\nthat are learned using the segmentation-QA links as explicit supervision. To\nput the latter in perspective, we study two plausible methods and compare them\nto an oracle method assuming that the instance segmentations are given at the\ntest stage.</p>\n", "tags": ["Applications","Datasets","ICCV","Model Architecture"] },
{"key": "gan2019multi", "citations": "107", "year": "2019", "title":"Multi-step Reasoning Via Recurrent Dual Attention For Visual Dialog", "abstract": "<p>This paper presents a new model for visual dialog, Recurrent Dual Attention\nNetwork (ReDAN), using multi-step reasoning to answer a series of questions\nabout an image. In each question-answering turn of a dialog, ReDAN infers the\nanswer progressively through multiple reasoning steps. In each step of the\nreasoning process, the semantic representation of the question is updated based\non the image and the previous dialog history, and the recurrently-refined\nrepresentation is used for further reasoning in the subsequent step. On the\nVisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art\nof 64.47% NDCG score. Visualization on the reasoning process further\ndemonstrates that ReDAN can locate context-relevant visual and textual clues\nvia iterative refinement, which can lead to the correct answer step-by-step.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "gan2020large", "citations": "270", "year": "2020", "title":"Large-scale Adversarial Training For Vision-and-language Representation Learning", "abstract": "<p>We present VILLA, the first known effort on large-scale adversarial training\nfor vision-and-language (V+L) representation learning. VILLA consists of two\ntraining stages: (i) task-agnostic adversarial pre-training; followed by (ii)\ntask-specific adversarial finetuning. Instead of adding adversarial\nperturbations on image pixels and textual tokens, we propose to perform\nadversarial training in the embedding space of each modality. To enable\nlarge-scale training, we adopt the “free” adversarial training strategy, and\ncombine it with KL-divergence-based regularization to promote higher invariance\nin the embedding space. We apply VILLA to current best-performing V+L models,\nand achieve new state of the art on a wide range of tasks, including Visual\nQuestion Answering, Visual Commonsense Reasoning, Image-Text Retrieval,\nReferring Expression Comprehension, Visual Entailment, and NLVR2.</p>\n", "tags": ["Training Techniques"] },
{"key": "ganesh2020compressing", "citations": "72", "year": "2021", "title":"Compressing Large-scale Transformer-based Models: A Case Study On BERT", "abstract": "<p>Pre-trained Transformer-based models have achieved state-of-the-art\nperformance for various Natural Language Processing (NLP) tasks. However, these\nmodels often have billions of parameters, and, thus, are too resource-hungry\nand computation-intensive to suit low-capability devices or applications with\nstrict latency requirements. One potential remedy for this is model\ncompression, which has attracted a lot of research attention. Here, we\nsummarize the research in compressing Transformers, focusing on the especially\npopular BERT model. In particular, we survey the state of the art in\ncompression for BERT, we clarify the current best practices for compressing\nlarge-scale Transformer models, and we provide insights into the workings of\nvarious methods. Our categorization and analysis also shed light on promising\nfuture research directions for achieving lightweight, accurate, and generic NLP\nmodels.</p>\n", "tags": ["Applications","Model Architecture","Survey Paper","TACL"] },
{"key": "gao2018neural", "citations": "121", "year": "2018", "title":"Neural Approaches To Conversational AI", "abstract": "<p>The present paper surveys neural approaches to conversational AI that have\nbeen developed in the last few years. We group conversational systems into\nthree categories: (1) question answering agents, (2) task-oriented dialogue\nagents, and (3) chatbots. For each category, we present a review of\nstate-of-the-art neural approaches, draw the connection between them and\ntraditional approaches, and discuss the progress that has been made and\nchallenges still being faced, using specific systems and models as case\nstudies.</p>\n", "tags": ["EMNLP"] },
{"key": "gao2018question", "citations": "77", "year": "2018", "title":"Question-guided Hybrid Convolution For Visual Question Answering", "abstract": "<p>In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC)\nnetwork for Visual Question Answering (VQA). Most state-of-the-art VQA methods\nfuse the high-level textual and visual features from the neural network and\nabandon the visual spatial information when learning multi-modal features.To\naddress these problems, question-guided kernels generated from the input\nquestion are designed to convolute with visual features for capturing the\ntextual and visual relationship in the early stage. The question-guided\nconvolution can tightly couple the textual and visual information but also\nintroduce more parameters when learning kernels. We apply the group\nconvolution, which consists of question-independent kernels and\nquestion-dependent kernels, to reduce the parameter size and alleviate\nover-fitting. The hybrid convolution can generate discriminative multi-modal\nfeatures with fewer parameters. The proposed approach is also complementary to\nexisting bilinear pooling fusion and attention based VQA methods. By\nintegrating with them, our method could further boost the performance.\nExtensive experiments on public VQA datasets validate the effectiveness of\nQGHC.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "gao2019dialog", "citations": "167", "year": "2019", "title":"Dialog State Tracking: A Neural Reading Comprehension Approach", "abstract": "<p>Dialog state tracking is used to estimate the current belief state of a\ndialog given all the preceding conversation. Machine reading comprehension, on\nthe other hand, focuses on building systems that read passages of text and\nanswer questions that require some understanding of passages. We formulate\ndialog state tracking as a reading comprehension task to answer the question\n\\(what\\ is\\ the\\ state\\ of\\ the\\ current\\ dialog?\\) after reading conversational\ncontext. In contrast to traditional state tracking methods where the dialog\nstate is often predicted as a distribution over a closed set of all the\npossible slot values within an ontology, our method uses a simple\nattention-based neural network to point to the slot values within the\nconversation. Experiments on MultiWOZ-2.0 cross-domain dialog dataset show that\nour simple system can obtain similar accuracies compared to the previous more\ncomplex methods. By exploiting recent advances in contextual word embeddings,\nadding a model that explicitly tracks whether a slot value should be carried\nover to the next turn, and combining our method with a traditional joint state\ntracking method that relies on closed set vocabulary, we can obtain a\njoint-goal accuracy of \\(47.33%\\) on the standard test split, exceeding current\nstate-of-the-art by \\(11.75%\\)**.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "gao2019jointly", "citations": "96", "year": "2019", "title":"Jointly Optimizing Diversity And Relevance In Neural Response Generation", "abstract": "<p>Although recent neural conversation models have shown great potential, they\noften generate bland and generic responses. While various approaches have been\nexplored to diversify the output of the conversation model, the improvement\noften comes at the cost of decreased relevance. In this paper, we propose a\nSpaceFusion model to jointly optimize diversity and relevance that essentially\nfuses the latent space of a sequence-to-sequence model and that of an\nautoencoder model by leveraging novel regularization terms. As a result, our\napproach induces a latent space in which the distance and direction from the\npredicted response vector roughly match the relevance and diversity,\nrespectively. This property also lends itself well to an intuitive\nvisualization of the latent space. Both automatic and human evaluation results\ndemonstrate that the proposed approach brings significant improvement compared\nto strong baselines in both diversity and relevance.</p>\n", "tags": ["Evaluation"] },
{"key": "gao2019multi", "citations": "69", "year": "2019", "title":"Multi-modality Latent Interaction Network For Visual Question Answering", "abstract": "<p>Exploiting relationships between visual regions and question words have\nachieved great success in learning multi-modality features for Visual Question\nAnswering (VQA). However, we argue that existing methods mostly model relations\nbetween individual visual regions and words, which are not enough to correctly\nanswer the question. From humans’ perspective, answering a visual question\nrequires understanding the summarizations of visual and language information.\nIn this paper, we proposed the Multi-modality Latent Interaction module (MLI)\nto tackle this problem. The proposed module learns the cross-modality\nrelationships between latent visual and language summarizations, which\nsummarize visual regions and question into a small number of latent\nrepresentations to avoid modeling uninformative individual region-word\nrelations. The cross-modality information between the latent summarizations are\npropagated to fuse valuable information from both modalities and are used to\nupdate the visual and word features. Such MLI modules can be stacked for\nseveral stages to model complex and latent relations between the two modalities\nand achieves highly competitive performance on public VQA benchmarks, VQA v2.0\nand TDIUC . In addition, we show that the performance of our methods could be\nsignificantly improved by combining with pre-trained language model BERT.</p>\n", "tags": ["ICCV","Model Architecture"] },
{"key": "gao2019product", "citations": "80", "year": "2019", "title":"Product-aware Answer Generation In E-commerce Question-answering", "abstract": "<p>In e-commerce portals, generating answers for product-related questions has\nbecome a crucial task. In this paper, we propose the task of product-aware\nanswer generation, which tends to generate an accurate and complete answer from\nlarge-scale unlabeled e-commerce reviews and product attributes. Unlike\nexisting question-answering problems, answer generation in e-commerce confronts\nthree main challenges: (1) Reviews are informal and noisy; (2) joint modeling\nof reviews and key-value product attributes is challenging; (3) traditional\nmethods easily generate meaningless answers. To tackle above challenges, we\npropose an adversarial learning based model, named PAAG, which is composed of\nthree components: a question-aware review representation module, a key-value\nmemory network encoding attributes, and a recurrent neural network as a\nsequence generator. Specifically, we employ a convolutional discriminator to\ndistinguish whether our generated answer matches the facts. To extract the\nsalience part of reviews, an attention-based review reader is proposed to\ncapture the most relevant words given the question. Conducted on a large-scale\nreal-world e-commerce dataset, our extensive experiments verify the\neffectiveness of each module in our proposed model. Moreover, our experiments\nshow that our model achieves the state-of-the-art performance in terms of both\nautomatic metrics and human evaluations.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "gao2020dialogue", "citations": "61", "year": "2020", "title":"Dialogue Generation On Infrequent Sentence Functions Via Structured Meta-learning", "abstract": "<p>Sentence function is an important linguistic feature indicating the\ncommunicative purpose in uttering a sentence. Incorporating sentence functions\ninto conversations has shown improvements in the quality of generated\nresponses. However, the number of utterances for different types of\nfine-grained sentence functions is extremely imbalanced. Besides a small number\nof high-resource sentence functions, a large portion of sentence functions is\ninfrequent. Consequently, dialogue generation conditioned on these infrequent\nsentence functions suffers from data deficiency. In this paper, we investigate\na structured meta-learning (SML) approach for dialogue generation on infrequent\nsentence functions. We treat dialogue generation conditioned on different\nsentence functions as separate tasks, and apply model-agnostic meta-learning to\nhigh-resource sentence functions data. Furthermore, SML enhances meta-learning\neffectiveness by promoting knowledge customization among different sentence\nfunctions but simultaneously preserving knowledge generalization for similar\nsentence functions. Experimental results demonstrate that SML not only improves\nthe informativeness and relevance of generated responses, but also can generate\nresponses consistent with the target sentence functions.</p>\n", "tags": ["EMNLP"] },
{"key": "gao2020making", "citations": "1076", "year": "2021", "title":"Making Pre-trained Language Models Better Few-shot Learners", "abstract": "<p>The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot\nperformance solely by leveraging a natural-language prompt and a few task\ndemonstrations as input context. Inspired by their findings, we study few-shot\nlearning in a more practical scenario, where we use smaller language models for\nwhich fine-tuning is computationally efficient. We present LM-BFF–better\nfew-shot fine-tuning of language models–a suite of simple and complementary\ntechniques for fine-tuning language models on a small number of annotated\nexamples. Our approach includes (1) prompt-based fine-tuning together with a\nnovel pipeline for automating prompt generation; and (2) a refined strategy for\ndynamically and selectively incorporating demonstrations into each context.\nFinally, we present a systematic evaluation for analyzing few-shot performance\non a range of NLP tasks, including classification and regression. Our\nexperiments demonstrate that our methods combine to dramatically outperform\nstandard fine-tuning procedures in this low resource setting, achieving up to\n30% absolute improvement, and 11% on average across all tasks. Our approach\nmakes minimal assumptions on task resources and domain expertise, and hence\nconstitutes a strong task-agnostic method for few-shot learning.</p>\n", "tags": ["Evaluation","Few-Shot","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "gao2020multi", "citations": "104", "year": "2020", "title":"Multi-modal Graph Neural Network For Joint Reasoning On Vision And Scene Text", "abstract": "<p>Answering questions that require reading texts in an image is challenging for\ncurrent models. One key difficulty of this task is that rare, polysemous, and\nambiguous words frequently appear in images, e.g., names of places, products,\nand sports teams. To overcome this difficulty, only resorting to pre-trained\nword embedding models is far from enough. A desired model should utilize the\nrich information in multiple modalities of the image to help understand the\nmeaning of scene texts, e.g., the prominent text on a bottle is most likely to\nbe the brand. Following this idea, we propose a novel VQA approach, Multi-Modal\nGraph Neural Network (MM-GNN). It first represents an image as a graph\nconsisting of three sub-graphs, depicting visual, semantic, and numeric\nmodalities respectively. Then, we introduce three aggregators which guide the\nmessage passing from one graph to another to utilize the contexts in various\nmodalities, so as to refine the features of nodes. The updated nodes have\nbetter features for the downstream question answering module. Experimental\nevaluations show that our MM-GNN represents the scene texts better and\nobviously facilitates the performances on two VQA tasks that require reading\nscene texts.</p>\n", "tags": ["CVPR"] },
{"key": "gao2020paraphrase", "citations": "77", "year": "2020", "title":"Paraphrase Augmented Task-oriented Dialog Generation", "abstract": "<p>Neural generative models have achieved promising performance on dialog\ngeneration tasks if given a huge data set. However, the lack of high-quality\ndialog data and the expensive data annotation process greatly limit their\napplication in real-world settings. We propose a paraphrase augmented response\ngeneration (PARG) framework that jointly trains a paraphrase model and a\nresponse generation model to improve the dialog generation performance. We also\ndesign a method to automatically construct paraphrase training data set based\non dialog state and dialog act labels. PARG is applicable to various dialog\ngeneration models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al.,\n2019). Experimental results show that the proposed framework improves these\nstate-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also\nsignificantly outperforms other data augmentation methods in dialog generation\ntasks, especially under low resource settings.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Tools","Training Techniques"] },
{"key": "gao2020pile", "citations": "459", "year": "2021", "title":"The Pile: An 800GB Dataset Of Diverse Text For Language Modeling", "abstract": "<p>Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets – both existing and newly constructed – many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "gao2020supert", "citations": "109", "year": "2020", "title":"SUPERT: Towards New Frontiers In Unsupervised Evaluation Metrics For Multi-document Summarization", "abstract": "<p>We study unsupervised multi-document summarization evaluation metrics, which\nrequire neither human-written reference summaries nor human annotations (e.g.\npreferences, ratings, etc.). We propose SUPERT, which rates the quality of a\nsummary by measuring its semantic similarity with a pseudo reference summary,\ni.e. selected salient sentences from the source documents, using contextualized\nembeddings and soft token alignment techniques. Compared to the\nstate-of-the-art unsupervised evaluation metrics, SUPERT correlates better with\nhuman ratings by 18-39%. Furthermore, we use SUPERT as rewards to guide a\nneural-based reinforcement learning summarizer, yielding favorable performance\ncompared to the state-of-the-art unsupervised summarizers. All source code is\navailable at https://github.com/yg211/acl20-ref-free-eval.</p>\n", "tags": ["Evaluation","Has Code"] },
{"key": "gao2021advances", "citations": "230", "year": "2021", "title":"Advances And Challenges In Conversational Recommender Systems: A Survey", "abstract": "<p>Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n  Considerable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.</p>\n", "tags": ["Applications","Evaluation","Survey Paper"] },
{"key": "gao2021clip", "citations": "376", "year": "2023", "title":"Clip-adapter: Better Vision-language Models With Feature Adapters", "abstract": "<p>Large-scale contrastive vision-language pre-training has shown significant\nprogress in visual representation learning. Unlike traditional visual systems\ntrained by a fixed set of discrete labels, a new paradigm was introduced in\n\\cite{radford2021learning} to directly learn to align images with raw texts in\nan open-vocabulary setting. On downstream tasks, a carefully chosen text prompt\nis employed to make zero-shot predictions.~To avoid non-trivial prompt\nengineering, context optimization \\cite{zhou2021coop} has been proposed to\nlearn continuous vectors as task-specific prompts with few-shot training\nexamples.~In this paper, we show that there is an alternative path to achieve\nbetter vision-language models other than prompt tuning.~While prompt tuning is\nfor the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with\nfeature adapters on either visual or language branch. Specifically,\nCLIP-Adapter adopts an additional bottleneck layer to learn new features and\nperforms residual-style feature blending with the original pre-trained\nfeatures.~As a consequence, CLIP-Adapter is able to outperform context\noptimization while maintains a simple design. Experiments and extensive\nablation studies on various visual classification tasks demonstrate the\neffectiveness of our approach. Code is released at t\nhttps://github.com/gaopengcuhk/CLIP-Adapter.</p>\n", "tags": ["Efficiency","Few-Shot","Fine-Tuning","Has Code","Prompting","Training Techniques"] },
{"key": "gao2021code", "citations": "72", "year": "2022", "title":"Code Structure Guided Transformer For Source Code Summarization", "abstract": "<p>Code summaries help developers comprehend programs and reduce their time to\ninfer the program functionalities during software maintenance. Recent efforts\nresort to deep learning techniques such as sequence-to-sequence models for\ngenerating accurate code summaries, among which Transformer-based approaches\nhave achieved promising performance. However, effectively integrating the code\nstructure information into the Transformer is under-explored in this task\ndomain. In this paper, we propose a novel approach named SG-Trans to\nincorporate code structural properties into Transformer. Specifically, we\ninject the local symbolic information (e.g., code tokens and statements) and\nglobal syntactic structure (e.g., data flow graph) into the self-attention\nmodule of Transformer as inductive bias. To further capture the hierarchical\ncharacteristics of code, the local information and global structure are\ndesigned to distribute in the attention heads of lower layers and high layers\nof Transformer. Extensive evaluation shows the superior performance of SG-Trans\nover the state-of-the-art approaches. Compared with the best-performing\nbaseline, SG-Trans still improves 1.4% and 2.0% in terms of METEOR score, a\nmetric widely used for measuring generation quality, respectively on two\nbenchmark datasets.</p>\n", "tags": ["Datasets","Evaluation","Llm For Code","Model Architecture"] },
{"key": "gao2021condenser", "citations": "131", "year": "2021", "title":"Condenser: A Pre-training Architecture For Dense Retrieval", "abstract": "<p>Pre-trained Transformer language models (LM) have become go-to text\nrepresentation encoders. Prior research fine-tunes deep LMs to encode text\nsequences such as sentences and passages into single dense vector\nrepresentations for efficient text comparison and retrieval. However, dense\nencoders require a lot of data and sophisticated techniques to effectively\ntrain and suffer in low data situations. This paper finds a key reason is that\nstandard LMs’ internal attention structure is not ready-to-use for dense\nencoders, which needs to aggregate text information into the dense\nrepresentation. We propose to pre-train towards dense encoder with a novel\nTransformer architecture, Condenser, where LM prediction CONditions on DENSE\nRepresentation. Our experiments show Condenser improves over standard LM by\nlarge margins on various text retrieval and similarity tasks.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "gao2021rethink", "citations": "64", "year": "2021", "title":"Rethink Training Of BERT Rerankers In Multi-stage Retrieval Pipeline", "abstract": "<p>Pre-trained deep language models~(LM) have advanced the state-of-the-art of\ntext retrieval. Rerankers fine-tuned from deep LM estimates candidate relevance\nbased on rich contextualized matching signals. Meanwhile, deep LMs can also be\nleveraged to improve search index, building retrievers with better recall. One\nwould expect a straightforward combination of both in a pipeline to have\nadditive performance gain. In this paper, we discover otherwise and that\npopular reranker cannot fully exploit the improved retrieval result. We,\ntherefore, propose a Localized Contrastive Estimation (LCE) for training\nrerankers and demonstrate it significantly improves deep two-stage models.</p>\n", "tags": ["Model Architecture","Retrieval Systems","Training Techniques"] },
{"key": "gao2021unsupervised", "citations": "109", "year": "2022", "title":"Unsupervised Corpus Aware Language Model Pre-training For Dense Passage Retrieval", "abstract": "<p>Recent research demonstrates the effectiveness of using fine-tuned language\nmodels~(LM) for dense retrieval. However, dense retrievers are hard to train,\ntypically requiring heavily engineered fine-tuning pipelines to realize their\nfull potential. In this paper, we identify and address two underlying problems\nof dense retrievers: i)~fragility to training data noise and ii)~requiring\nlarge batches to robustly learn the embedding space. We use the recently\nproposed Condenser pre-training architecture, which learns to condense\ninformation into the dense vector through LM pre-training. On top of it, we\npropose coCondenser, which adds an unsupervised corpus-level contrastive loss\nto warm up the passage embedding space. Retrieval experiments on MS-MARCO,\nNatural Question, and Trivia QA datasets show that coCondenser removes the need\nfor heavy data engineering such as augmentation, synthesis, or filtering, as\nwell as the need for large batch training. It shows comparable performance to\nRocketQA, a state-of-the-art, heavily engineered system, using simple small\nbatch fine-tuning.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "gao2022pal", "citations": "95", "year": "2022", "title":"PAL: Program-aided Language Models", "abstract": "<p>Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\nexamples at test time (“few-shot prompting”). Much of this success can be\nattributed to prompting methods such as “chain-of-thought’’, which employ LLMs\nfor both understanding the problem description by decomposing it into steps, as\nwell as solving each step of the problem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\nmistakes in the solution part, even when the problem is decomposed correctly.\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\nthat uses the LLM to read natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a runtime\nsuch as a Python interpreter. With PAL, decomposing the natural language\nproblem into runnable steps remains the only learning task for the LLM, while\nsolving is delegated to the interpreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\npublicly available at http://reasonwithpal.com/ .</p>\n", "tags": ["Datasets","Evaluation Frameworks","Evaluation","Few-Shot","In Context Learning","Prompting"] },
{"key": "gao2023llama", "citations": "101", "year": "2023", "title":"Llama-adapter V2: Parameter-efficient Visual Instruction Model", "abstract": "<p>How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.</p>\n", "tags": ["Datasets","Has Code","Instruction Following","Model Architecture","Tools","Training Techniques"] },
{"key": "gao2023retrieval", "citations": "288", "year": "2023", "title":"Retrieval-augmented Generation For Large Language Models: A Survey", "abstract": "<p>Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs’ intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.</p>\n", "tags": ["Datasets","Evaluation","RAG","Survey Paper","Tools"] },
{"key": "garcez2020neurosymbolic", "citations": "159", "year": "2023", "title":"Neurosymbolic AI: The 3rd Wave", "abstract": "<p>Current advances in Artificial Intelligence (AI) and Machine Learning (ML)\nhave achieved unprecedented impact across research communities and industry.\nNevertheless, concerns about trust, safety, interpretability and accountability\nof AI were raised by influential thinkers. Many have identified the need for\nwell-founded knowledge representation and reasoning to be integrated with deep\nlearning and for sound explainability. Neural-symbolic computing has been an\nactive area of research for many years seeking to bring together robust\nlearning in neural networks with reasoning and explainability via symbolic\nrepresentations for network models. In this paper, we relate recent and early\nresearch results in neurosymbolic AI with the objective of identifying the key\ningredients of the next wave of AI systems. We focus on research that\nintegrates in a principled way neural network-based learning with symbolic\nknowledge representation and logical reasoning. The insights provided by 20\nyears of neural-symbolic computing are shown to shed new light onto the\nincreasingly prominent role of trust, safety, interpretability and\naccountability of AI. We also identify promising directions and challenges for\nthe next decade of AI research from the perspective of neural-symbolic systems.</p>\n", "tags": ["Llm For Code","Nesy"] },
{"key": "garcia2019knowit", "citations": "68", "year": "2020", "title":"Knowit VQA: Answering Knowledge-based Questions About Videos", "abstract": "<p>We propose a novel video understanding task by fusing knowledge-based and\nvideo question answering. First, we introduce KnowIT VQA, a video dataset with\n24,282 human-generated question-answer pairs about a popular sitcom. The\ndataset combines visual, textual and temporal coherence reasoning together with\nknowledge-based questions, which need of the experience obtained from the\nviewing of the series to be answered. Second, we propose a video understanding\nmodel by combining the visual and textual video content with specific knowledge\nabout the show. Our main findings are: (i) the incorporation of knowledge\nproduces outstanding improvements for VQA in video, and (ii) the performance on\nKnowIT VQA still lags well behind human accuracy, indicating its usefulness for\nstudying current video modelling limitations.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "gardner2018allennlp", "citations": "1136", "year": "2018", "title":"Allennlp: A Deep Semantic Natural Language Processing Platform", "abstract": "<p>This paper describes AllenNLP, a platform for research on deep learning\nmethods in natural language understanding. AllenNLP is designed to support\nresearchers who want to build novel language understanding models quickly and\neasily. It is built on top of PyTorch, allowing for dynamic computation graphs,\nand provides (1) a flexible data API that handles intelligent batching and\npadding, (2) high-level abstractions for common operations in working with\ntext, and (3) a modular and extensible experiment framework that makes doing\ngood science easy. It also includes reference implementations of high quality\napproaches for both core semantic problems (e.g. semantic role labeling (Palmer\net al., 2005)) and language understanding applications (e.g. machine\ncomprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source\neffort maintained by engineers and researchers at the Allen Institute for\nArtificial Intelligence.</p>\n", "tags": ["Applications","Tools"] },
{"key": "gardner2020evaluating", "citations": "276", "year": "2020", "title":"Evaluating Models' Local Decision Boundaries Via Contrast Sets", "abstract": "<p>Standard test sets for supervised learning evaluate in-distribution\ngeneralization. Unfortunately, when a dataset has systematic gaps (e.g.,\nannotation artifacts), these evaluations are misleading: a model can learn\nsimple decision rules that perform well on the test set but do not capture a\ndataset’s intended capabilities. We propose a new annotation paradigm for NLP\nthat helps to close systematic gaps in the test data. In particular, after a\ndataset is constructed, we recommend that the dataset authors manually perturb\nthe test instances in small but meaningful ways that (typically) change the\ngold label, creating contrast sets. Contrast sets provide a local view of a\nmodel’s decision boundary, which can be used to more accurately evaluate a\nmodel’s true linguistic capabilities. We demonstrate the efficacy of contrast\nsets by creating them for 10 diverse NLP datasets (e.g., DROP reading\ncomprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets\nare not explicitly adversarial, model performance is significantly lower on\nthem than on the original test sets—up to 25% in some cases. We release our\ncontrast sets as new evaluation benchmarks and encourage future dataset\nconstruction efforts to follow similar annotation processes.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "garg2019jointly", "citations": "132", "year": "2019", "title":"Jointly Learning To Align And Translate With Transformer Models", "abstract": "<p>The state of the art in machine translation (MT) is governed by neural\napproaches, which typically provide superior translation accuracy over\nstatistical approaches. However, on the closely related task of word alignment,\ntraditional statistical word alignment models often remain the go-to solution.\nIn this paper, we present an approach to train a Transformer model to produce\nboth accurate translations and alignments. We extract discrete alignments from\nthe attention probabilities learnt during regular neural machine translation\nmodel training and leverage them in a multi-task framework to optimize towards\ntranslation and alignment objectives. We demonstrate that our approach produces\ncompetitive results compared to GIZA++ trained IBM alignment models without\nsacrificing translation accuracy and outperforms previous attempts on\nTransformer model based word alignment. Finally, by incorporating IBM model\nalignments into our multi-task training, we report significantly better\nalignment accuracies compared to GIZA++ on three publicly available data sets.</p>\n", "tags": ["EMNLP","Model Architecture","Tools","Training Techniques"] },
{"key": "garg2019tanda", "citations": "184", "year": "2020", "title":"TANDA: Transfer And Adapt Pre-trained Transformer Models For Answer Sentence Selection", "abstract": "<p>We propose TANDA, an effective technique for fine-tuning pre-trained\nTransformer models for natural language tasks. Specifically, we first transfer\na pre-trained model into a model for a general task by fine-tuning it with a\nlarge and high-quality dataset. We then perform a second fine-tuning step to\nadapt the transferred model to the target domain. We demonstrate the benefits\nof our approach for answer sentence selection, which is a well-known inference\ntask in Question Answering. We built a large scale dataset to enable the\ntransfer step, exploiting the Natural Questions dataset. Our approach\nestablishes the state of the art on two well-known benchmarks, WikiQA and\nTREC-QA, achieving MAP scores of 92% and 94.3%, respectively, which largely\noutperform the previous highest scores of 83.4% and 87.5%, obtained in very\nrecent work. We empirically show that TANDA generates more stable and robust\nmodels reducing the effort required for selecting optimal hyper-parameters.\nAdditionally, we show that the transfer step of TANDA makes the adaptation step\nmore robust to noise. This enables a more effective use of noisy datasets for\nfine-tuning. Finally, we also confirm the positive impact of TANDA in an\nindustrial setting, using domain specific datasets subject to different types\nof noise.</p>\n", "tags": ["AAAI","Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "garg2020bae", "citations": "390", "year": "2020", "title":"BAE: Bert-based Adversarial Examples For Text Classification", "abstract": "<p>Modern text classification models are susceptible to adversarial examples,\nperturbed versions of the original text indiscernible by humans which get\nmisclassified by the model. Recent works in NLP use rule-based synonym\nreplacement strategies to generate adversarial examples. These strategies can\nlead to out-of-context and unnaturally complex token replacements, which are\neasily identifiable by humans. We present BAE, a black box attack for\ngenerating adversarial examples using contextual perturbations from a BERT\nmasked language model. BAE replaces and inserts tokens in the original text by\nmasking a portion of the text and leveraging the BERT-MLM to generate\nalternatives for the masked tokens. Through automatic and human evaluations, we\nshow that BAE performs a stronger attack, in addition to generating adversarial\nexamples with improved grammaticality and semantic coherence as compared to\nprior work.</p>\n", "tags": ["EMNLP","Model Architecture","Security"] },
{"key": "gauthier2019linking", "citations": "80", "year": "2019", "title":"Linking Artificial And Human Neural Representations Of Language", "abstract": "<p>What information from an act of sentence understanding is robustly\nrepresented in the human brain? We investigate this question by comparing\nsentence encoding models on a brain decoding task, where the sentence that an\nexperimental participant has seen must be predicted from the fMRI signal evoked\nby the sentence. We take a pre-trained BERT architecture as a baseline sentence\nencoding model and fine-tune it on a variety of natural language understanding\n(NLU) tasks, asking which lead to improvements in brain-decoding performance.\n  We find that none of the sentence encoding tasks tested yield significant\nincreases in brain decoding performance. Through further task ablations and\nrepresentational analyses, we find that tasks which produce syntax-light\nrepresentations yield significant improvements in brain decoding performance.\nOur results constrain the space of NLU models that could best account for human\nneural representations of language, but also suggest limits on the possibility\nof decoding fine-grained syntactic information from fMRI human neuroimaging.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "ge2018reaching", "citations": "100", "year": "2018", "title":"Reaching Human-level Performance In Automatic Grammatical Error Correction: An Empirical Study", "abstract": "<p>Neural sequence-to-sequence (seq2seq) approaches have proven to be successful\nin grammatical error correction (GEC). Based on the seq2seq framework, we\npropose a novel fluency boost learning and inference mechanism. Fluency\nboosting learning generates diverse error-corrected sentence pairs during\ntraining, enabling the error correction model to learn how to improve a\nsentence’s fluency from more instances, while fluency boosting inference allows\nthe model to correct a sentence incrementally with multiple inference steps.\nCombining fluency boost learning and inference with convolutional seq2seq\nmodels, our approach achieves the state-of-the-art performance: 75.72 (F_{0.5})\non CoNLL-2014 10 annotation dataset and 62.42 (GLEU) on JFLEG test set\nrespectively, becoming the first GEC system that reaches human-level\nperformance (72.58 for CoNLL and 62.37 for JFLEG) on both of the benchmarks.</p>\n", "tags": ["Datasets","Evaluation","Tools","Training Techniques"] },
{"key": "ge2023openagi", "citations": "63", "year": "2023", "title":"Openagi: When LLM Meets Domain Experts", "abstract": "<p>Human Intelligence (HI) excels at combining basic skills to solve complex\ntasks. This capability is vital for Artificial Intelligence (AI) and should be\nembedded in comprehensive AI Agents, enabling them to harness expert models for\ncomplex task-solving towards Artificial General Intelligence (AGI). Large\nLanguage Models (LLMs) show promising learning and reasoning abilities, and can\neffectively use external models, tools, plugins, or APIs to tackle complex\nproblems. In this work, we introduce OpenAGI, an open-source AGI research and\ndevelopment platform designed for solving multi-step, real-world tasks.\nSpecifically, OpenAGI uses a dual strategy, integrating standard benchmark\ntasks for benchmarking and evaluation, and open-ended tasks including more\nexpandable models, tools, plugins, or APIs for creative problem-solving. Tasks\nare presented as natural language queries to the LLM, which then selects and\nexecutes appropriate models. We also propose a Reinforcement Learning from Task\nFeedback (RLTF) mechanism that uses task results to improve the LLM’s\ntask-solving ability, which creates a self-improving AI feedback loop. While we\nacknowledge that AGI is a broad and multifaceted research challenge with no\nsingularly defined solution path, the integration of LLMs with domain-specific\nexpert models, inspired by mirroring the blend of general and specialized\nintelligence in humans, offers a promising approach towards AGI. We are\nopen-sourcing the OpenAGI project’s code, dataset, benchmarks, evaluation\nmethods, and the UI demo to foster community involvement in AGI advancement:\nhttps://github.com/agiresearch/OpenAGI.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Has Code","Reinforcement Learning","Tools"] },
{"key": "gehman2020realtoxicityprompts", "citations": "430", "year": "2020", "title":"Realtoxicityprompts: Evaluating Neural Toxic Degeneration In Language Models", "abstract": "<p>Pretrained neural language models (LMs) are prone to generating racist,\nsexist, or otherwise toxic language which hinders their safe deployment. We\ninvestigate the extent to which pretrained LMs can be prompted to generate\ntoxic language, and the effectiveness of controllable text generation\nalgorithms at preventing such toxic degeneration. We create and release\nRealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level\nprompts derived from a large corpus of English web text, paired with toxicity\nscores from a widely-used toxicity classifier. Using RealToxicityPrompts, we\nfind that pretrained LMs can degenerate into toxic text even from seemingly\ninnocuous prompts. We empirically assess several controllable generation\nmethods, and find that while data- or compute-intensive methods (e.g., adaptive\npretraining on non-toxic data) are more effective at steering away from\ntoxicity than simpler solutions (e.g., banning “bad” words), no current method\nis failsafe against neural toxic degeneration. To pinpoint the potential cause\nof such persistent toxic degeneration, we analyze two web text corpora used to\npretrain several LMs (including GPT-2; Radford et. al, 2019), and find a\nsignificant amount of offensive, factually unreliable, and otherwise toxic\ncontent. Our work provides a test bed for evaluating toxic generations by LMs\nand stresses the need for better data selection processes for pretraining.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "gehring2016convolutional", "citations": "434", "year": "2017", "title":"A Convolutional Encoder Model For Neural Machine Translation", "abstract": "<p>The prevalent approach to neural machine translation relies on bi-directional\nLSTMs to encode the source sentence. In this paper we present a faster and\nsimpler architecture based on a succession of convolutional layers. This allows\nto encode the entire source sentence simultaneously compared to recurrent\nnetworks for which computation is constrained by temporal dependencies. On\nWMT’16 English-Romanian translation we achieve competitive accuracy to the\nstate-of-the-art and we outperform several recently published results on the\nWMT’15 English-German task. Our models obtain almost the same accuracy as a\nvery deep LSTM setup on WMT’14 English-French translation. Our convolutional\nencoder speeds up CPU decoding by more than two times at the same or higher\naccuracy as a strong bi-directional LSTM baseline.</p>\n", "tags": ["Model Architecture"] },
{"key": "gehring2017convolutional", "citations": "1994", "year": "2017", "title":"Convolutional Sequence To Sequence Learning", "abstract": "<p>The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT’14 English-German and WMT’14 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "gehrmann2018end", "citations": "81", "year": "2018", "title":"End-to-end Content And Plan Selection For Data-to-text Generation", "abstract": "<p>Learning to generate fluent natural language from structured data with neural\nnetworks has become an common approach for NLG. This problem can be challenging\nwhen the form of the structured data varies between examples. This paper\npresents a survey of several extensions to sequence-to-sequence models to\naccount for the latent content selection process, particularly variants of copy\nattention and coverage decoding. We further propose a training method based on\ndiverse ensembling to encourage models to learn distinct sentence templates\nduring training. An empirical evaluation of these techniques shows an increase\nin the quality of generated text across five automated metrics, as well as\nhuman evaluation.</p>\n", "tags": ["Evaluation","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "gehrmann2019gltr", "citations": "240", "year": "2019", "title":"GLTR: Statistical Detection And Visualization Of Generated Text", "abstract": "<p>The rapid improvement of language models has raised the specter of abuse of\ntext generation systems. This progress motivates the development of simple\nmethods for detecting generated text that can be used by and explained to\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\ntext was generated by a model. GLTR applies a suite of baseline statistical\nmethods that can detect generation artifacts across common sampling schemes. In\na human-subjects study, we show that the annotation scheme provided by GLTR\nimproves the human detection-rate of fake text from 54% to 72% without any\nprior training. GLTR is open-source and publicly deployed, and has already been\nwidely used to detect generated outputs</p>\n", "tags": ["Training Techniques"] },
{"key": "geminiteam2023gemini", "citations": "462", "year": "2023", "title":"Gemini: A Family Of Highly Capable Multimodal Models", "abstract": "<p>This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.</p>\n", "tags": ["Applications","Datasets","Evaluation Frameworks","Evaluation","Training Techniques"] },
{"key": "geminiteam2024gemini", "citations": "109", "year": "2024", "title":"Gemini 1.5: Unlocking Multimodal Understanding Across Millions Of Tokens Of Context", "abstract": "<p>In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra’s state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5’s long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(&gt;99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.</p>\n", "tags": ["Applications","Efficiency","Model Architecture"] },
{"key": "gemmateam2024gemma", "citations": "105", "year": "2024", "title":"Gemma 2: Improving Open Language Models At A Practical Size", "abstract": "<p>In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "gero2021sparks", "citations": "115", "year": "2022", "title":"Sparks: Inspiration For Science Writing Using Language Models", "abstract": "<p>Large-scale language models are rapidly improving, performing well on a wide\nvariety of tasks with little to no customization. In this work we investigate\nhow language models can support science writing, a challenging writing task\nthat is both open-ended and highly constrained. We present a system for\ngenerating “sparks”, sentences related to a scientific concept intended to\ninspire writers. We find that our sparks are more coherent and diverse than a\ncompetitive language model baseline, and approach a human-created gold\nstandard. In a study with 13 PhD students writing on topics of their own\nselection, we find three main use cases of sparks: aiding with crafting\ndetailed sentences, providing interesting angles to engage readers, and\ndemonstrating common reader perspectives. We also report on the various reasons\nsparks were considered unhelpful, and discuss how we might improve language\nmodels as writing support tools.</p>\n", "tags": ["Applications","Tools"] },
{"key": "geva2020injecting", "citations": "161", "year": "2020", "title":"Injecting Numerical Reasoning Skills Into Language Models", "abstract": "<p>Large pre-trained language models (LMs) are known to encode substantial\namounts of linguistic information. However, high-level reasoning skills, such\nas numerical reasoning, are difficult to learn from a language-modeling\nobjective only. Consequently, existing models for numerical reasoning have used\nspecialized architectures with limited flexibility. In this work, we show that\nnumerical reasoning is amenable to automatic data generation, and thus one can\ninject this skill into pre-trained LMs, by generating large amounts of data,\nand training in a multi-task setup. We show that pre-training our model,\nGenBERT, on this data, dramatically improves performance on DROP (49.3\n\\(\\rightarrow\\) 72.3 F1), reaching performance that matches state-of-the-art\nmodels of comparable size, while using a simple and general-purpose\nencoder-decoder architecture. Moreover, GenBERT generalizes well to math word\nproblem datasets, while maintaining high performance on standard RC tasks. Our\napproach provides a general recipe for injecting skills into large pre-trained\nLMs, whenever the skill is amenable to automatic data augmentation.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "geva2020transformer", "citations": "174", "year": "2021", "title":"Transformer Feed-forward Layers Are Key-value Memories", "abstract": "<p>Feed-forward layers constitute two-thirds of a transformer model’s\nparameters, yet their role in the network remains under-explored. We show that\nfeed-forward layers in transformer-based language models operate as key-value\nmemories, where each key correlates with textual patterns in the training\nexamples, and each value induces a distribution over the output vocabulary. Our\nexperiments show that the learned patterns are human-interpretable, and that\nlower layers tend to capture shallow patterns, while upper layers learn more\nsemantic ones. The values complement the keys’ input patterns by inducing\noutput distributions that concentrate probability mass on tokens likely to\nappear immediately after each pattern, particularly in the upper layers.\nFinally, we demonstrate that the output of a feed-forward layer is a\ncomposition of its memories, which is subsequently refined throughout the\nmodel’s layers via residual connections to produce the final output\ndistribution.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "geva2021did", "citations": "63", "year": "2021", "title":"Did Aristotle Use A Laptop? A Question Answering Benchmark With Implicit Reasoning Strategies", "abstract": "<p>A key limitation in current datasets for multi-hop reasoning is that the\nrequired steps for answering the question are mentioned in it explicitly. In\nthis work, we introduce StrategyQA, a question answering (QA) benchmark where\nthe required reasoning steps are implicit in the question, and should be\ninferred using a strategy. A fundamental challenge in this setup is how to\nelicit such creative questions from crowdsourcing workers, while covering a\nbroad range of potential strategies. We propose a data collection procedure\nthat combines term-based priming to inspire annotators, careful control over\nthe annotator population, and adversarial filtering for eliminating reasoning\nshortcuts. Moreover, we annotate each question with (1) a decomposition into\nreasoning steps for answering it, and (2) Wikipedia paragraphs that contain the\nanswers to each step. Overall, StrategyQA includes 2,780 examples, each\nconsisting of a strategy question, its decomposition, and evidence paragraphs.\nAnalysis shows that questions in StrategyQA are short, topic-diverse, and cover\na wide range of strategies. Empirically, we show that humans perform well (87%)\non this task, while our best baseline reaches an accuracy of \\(\\sim\\)66%.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "geva2022transformer", "citations": "70", "year": "2022", "title":"Transformer Feed-forward Layers Build Predictions By Promoting Concepts In The Vocabulary Space", "abstract": "<p>Transformer-based language models (LMs) are at the core of modern NLP, but\ntheir internal prediction construction process is opaque and largely not\nunderstood. In this work, we make a substantial step towards unveiling this\nunderlying prediction process, by reverse-engineering the operation of the\nfeed-forward network (FFN) layers, one of the building blocks of transformer\nmodels. We view the token representation as a changing distribution over the\nvocabulary, and the output from each FFN layer as an additive update to that\ndistribution. Then, we analyze the FFN updates in the vocabulary space, showing\nthat each update can be decomposed to sub-updates corresponding to single FFN\nparameter vectors, each promoting concepts that are often human-interpretable.\nWe then leverage these findings for controlling LM predictions, where we reduce\nthe toxicity of GPT2 by almost 50%, and for improving computation efficiency\nwith a simple early exit rule, saving 20% of computation on average.</p>\n", "tags": ["EMNLP","Efficiency","Model Architecture"] },
{"key": "ghaeini2018dr", "citations": "94", "year": "2018", "title":"Dr-bilstm: Dependent Reading Bidirectional LSTM For Natural Language Inference", "abstract": "<p>We present a novel deep learning architecture to address the natural language\ninference (NLI) task. Existing approaches mostly rely on simple reading\nmechanisms for independent encoding of the premise and hypothesis. Instead, we\npropose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to\nefficiently model the relationship between a premise and a hypothesis during\nencoding and inference. We also introduce a sophisticated ensemble strategy to\ncombine our proposed models, which noticeably improves final predictions.\nFinally, we demonstrate how the results can be improved further with an\nadditional preprocessing step. Our evaluation shows that DR-BiLSTM obtains the\nbest single model and ensemble model results achieving the new state-of-the-art\nscores on the Stanford NLI dataset.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","NAACL"] },
{"key": "ghaeini2018interpreting", "citations": "90", "year": "2018", "title":"Interpreting Recurrent And Attention-based Neural Models: A Case Study On Natural Language Inference", "abstract": "<p>Deep learning models have achieved remarkable success in natural language\ninference (NLI) tasks. While these models are widely explored, they are hard to\ninterpret and it is often unclear how and why they actually work. In this\npaper, we take a step toward explaining such deep learning based models through\na case study on a popular neural model for NLI. In particular, we propose to\ninterpret the intermediate layers of NLI models by visualizing the saliency of\nattention and LSTM gating signals. We present several examples for which our\nmethods are able to reveal interesting insights and identify the critical\ninformation contributing to the model decisions.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "ghandeharioun2018emma", "citations": "99", "year": "2019", "title":"EMMA: An Emotion-aware Wellbeing Chatbot", "abstract": "<p>The delivery of mental health interventions via ubiquitous devices has shown\nmuch promise. A conversational chatbot is a promising oracle for delivering\nappropriate just-in-time interventions. However, designing emotionally-aware\nagents, specially in this context, is under-explored. Furthermore, the\nfeasibility of automating the delivery of just-in-time mHealth interventions\nvia such an agent has not been fully studied. In this paper, we present the\ndesign and evaluation of EMMA (EMotion-Aware mHealth Agent) through a two-week\nlong human-subject experiment with N=39 participants. EMMA provides emotionally\nappropriate micro-activities in an empathetic manner. We show that the system\ncan be extended to detect a user’s mood purely from smartphone sensor data. Our\nresults show that our personalized machine learning model was perceived as\nlikable via self-reports of emotion from users. Finally, we provide a set of\nguidelines for the design of emotion-aware bots for mHealth.</p>\n", "tags": ["Agentic","Evaluation"] },
{"key": "ghazarian2019better", "citations": "85", "year": "2019", "title":"Better Automatic Evaluation Of Open-domain Dialogue Systems With Contextualized Embeddings", "abstract": "<p>Despite advances in open-domain dialogue systems, automatic evaluation of\nsuch systems is still a challenging problem. Traditional reference-based\nmetrics such as BLEU are ineffective because there could be many valid\nresponses for a given context that share no common words with reference\nresponses. A recent work proposed Referenced metric and Unreferenced metric\nBlended Evaluation Routine (RUBER) to combine a learning-based metric, which\npredicts relatedness between a generated response and a given query, with\nreference-based metric; it showed high correlation with human judgments. In\nthis paper, we explore using contextualized word embeddings to compute more\naccurate relatedness scores, thus better evaluation metrics. Experiments show\nthat our evaluation metrics outperform RUBER, which is trained on static\nembeddings.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation"] },
{"key": "ghazvininejad2017knowledge", "citations": "440", "year": "2018", "title":"A Knowledge-grounded Neural Conversation Model", "abstract": "<p>Neural network models are capable of generating extremely natural sounding\nconversational interactions. Nevertheless, these models have yet to demonstrate\nthat they can incorporate content in the form of factual information or\nentity-grounded opinion that would enable them to serve in more task-oriented\nconversational applications. This paper presents a novel, fully data-driven,\nand knowledge-grounded neural conversation model aimed at producing more\ncontentful responses without slot filling. We generalize the widely-used\nSeq2Seq approach by conditioning responses on both conversation history and\nexternal “facts”, allowing the model to be versatile and applicable in an\nopen-domain setting. Our approach yields significant improvements over a\ncompetitive Seq2Seq baseline. Human judges found that our outputs are\nsignificantly more informative.</p>\n", "tags": ["AAAI","Applications"] },
{"key": "ghazvininejad2019mask", "citations": "443", "year": "2019", "title":"Mask-predict: Parallel Decoding Of Conditional Masked Language Models", "abstract": "<p>Most machine translation systems generate text autoregressively from left to\nright. We, instead, use a masked language modeling objective to train a model\nto predict any subset of the target words, conditioned on both the input text\nand a partially masked target translation. This approach allows for efficient\niterative decoding, where we first predict all of the target words\nnon-autoregressively, and then repeatedly mask out and regenerate the subset of\nwords that the model is least confident about. By applying this strategy for a\nconstant number of iterations, our model improves state-of-the-art performance\nlevels for non-autoregressive and parallel decoding translation models by over\n4 BLEU on average. It is also able to reach within about 1 BLEU point of a\ntypical left-to-right transformer model, while decoding significantly faster.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "ghazvininejad2020aligned", "citations": "67", "year": "2020", "title":"Aligned Cross Entropy For Non-autoregressive Machine Translation", "abstract": "<p>Non-autoregressive machine translation models significantly speed up decoding\nby allowing for parallel prediction of the entire target sequence. However,\nmodeling word order is more challenging due to the lack of autoregressive\nfactors in the model. This difficultly is compounded during training with cross\nentropy loss, which can highly penalize small shifts in word order. In this\npaper, we propose aligned cross entropy (AXE) as an alternative loss function\nfor training of non-autoregressive models. AXE uses a differentiable dynamic\nprogram to assign loss based on the best possible monotonic alignment between\ntarget tokens and model predictions. AXE-based training of conditional masked\nlanguage models (CMLMs) substantially improves performance on major WMT\nbenchmarks, while setting a new state of the art for non-autoregressive models.</p>\n", "tags": ["Training Techniques"] },
{"key": "gheini2021cross", "citations": "63", "year": "2021", "title":"Cross-attention Is All You Need: Adapting Pretrained Transformers For Machine Translation", "abstract": "<p>We study the power of cross-attention in the Transformer architecture within\nthe context of transfer learning for machine translation, and extend the\nfindings of studies into cross-attention when training from scratch. We conduct\na series of experiments through fine-tuning a translation model on data where\neither the source or target language has changed. These experiments reveal that\nfine-tuning only the cross-attention parameters is nearly as effective as\nfine-tuning all parameters (i.e., the entire translation model). We provide\ninsights into why this is the case and observe that limiting fine-tuning in\nthis manner yields cross-lingually aligned embeddings. The implications of this\nfinding for researchers and practitioners include a mitigation of catastrophic\nforgetting, the potential for zero-shot translation, and the ability to extend\nmachine translation models to several new language pairs with reduced parameter\nstorage overhead.</p>\n", "tags": ["EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "ghosh2016contextual", "citations": "191", "year": "2016", "title":"Contextual LSTM (CLSTM) Models For Large Scale NLP Tasks", "abstract": "<p>Documents exhibit sequential structure at multiple levels of abstraction\n(e.g., sentences, paragraphs, sections). These abstractions constitute a\nnatural hierarchy for representing the context in which to infer the meaning of\nwords and larger fragments of text. In this paper, we present CLSTM (Contextual\nLSTM), an extension of the recurrent neural network LSTM (Long-Short Term\nMemory) model, where we incorporate contextual features (e.g., topics) into the\nmodel. We evaluate CLSTM on three specific NLP tasks: word prediction, next\nsentence selection, and sentence topic prediction. Results from experiments run\non two corpora, English documents in Wikipedia and a subset of articles from a\nrecent snapshot of English Google News, indicate that using both words and\ntopics as features improves performance of the CLSTM models over baseline LSTM\nmodels for these tasks. For example on the next sentence selection task, we get\nrelative accuracy improvements of 21% for the Wikipedia dataset and 18% for the\nGoogle News dataset. This clearly demonstrates the significant benefit of using\ncontext appropriately in natural language (NL) tasks. This has implications for\na wide variety of NL applications like question answering, sentence completion,\nparaphrase generation, and next utterance prediction in dialog systems.</p>\n", "tags": ["Applications","Datasets","Model Architecture"] },
{"key": "ghosh2017affect", "citations": "158", "year": "2017", "title":"Affect-lm: A Neural Language Model For Customizable Affective Text Generation", "abstract": "<p>Human verbal communication includes affective messages which are conveyed\nthrough use of emotionally colored words. There has been a lot of research in\nthis direction but the problem of integrating state-of-the-art neural language\nmodels with affective information remains an area ripe for exploration. In this\npaper, we propose an extension to an LSTM (Long Short-Term Memory) language\nmodel for generating conversational text, conditioned on affect categories. Our\nproposed model, Affect-LM enables us to customize the degree of emotional\ncontent in generated sentences through an additional design parameter.\nPerception studies conducted using Amazon Mechanical Turk show that Affect-LM\ngenerates naturally looking emotional sentences without sacrificing grammatical\ncorrectness. Affect-LM also learns affect-discriminative word representations,\nand perplexity experiments show that additional affective information in\nconversational text can improve language model prediction.</p>\n", "tags": ["Model Architecture"] },
{"key": "ghosh2021synthesis", "citations": "110", "year": "2021", "title":"Synthesis Of Compositional Animations From Textual Descriptions", "abstract": "<p>“How can we animate 3D-characters from a movie script or move robots by\nsimply telling them what we would like them to do?” “How unstructured and\ncomplex can we make a sentence and still generate plausible movements from it?”\nThese are questions that need to be answered in the long-run, as the field is\nstill in its infancy. Inspired by these problems, we present a new technique\nfor generating compositional actions, which handles complex input sentences.\nOur output is a 3D pose sequence depicting the actions in the input sentence.\nWe propose a hierarchical two-stream sequential model to explore a finer\njoint-level mapping between natural language sentences and 3D pose sequences\ncorresponding to the given motion. We learn two manifold representations of the\nmotion – one each for the upper body and the lower body movements. Our model\ncan generate plausible pose sequences for short sentences describing single\nactions as well as long compositional sentences describing multiple sequential\nand superimposed actions. We evaluate our proposed model on the publicly\navailable KIT Motion-Language Dataset containing 3D pose data with\nhuman-annotated sentences. Experimental results show that our model advances\nthe state-of-the-art on text-based motion synthesis in objective evaluations by\na margin of 50%. Qualitative evaluations based on a user study indicate that\nour synthesized motions are perceived to be the closest to the ground-truth\nmotion captures for both short and compositional sentences.</p>\n", "tags": ["Datasets","ICCV"] },
{"key": "ghosh2023chatgpt", "citations": "74", "year": "2023", "title":"Chatgpt Perpetuates Gender Bias In Machine Translation And Ignores Non-gendered Pronouns: Findings Across Bengali And Five Other Low-resource Languages", "abstract": "<p>In this multicultural age, language translation is one of the most performed\ntasks, and it is becoming increasingly AI-moderated and automated. As a novel\nAI system, ChatGPT claims to be proficient in such translation tasks and in\nthis paper, we put that claim to the test. Specifically, we examine ChatGPT’s\naccuracy in translating between English and languages that exclusively use\ngender-neutral pronouns. We center this study around Bengali, the 7\\(^{th}\\) most\nspoken language globally, but also generalize our findings across five other\nlanguages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT\nperpetuates gender defaults and stereotypes assigned to certain occupations\n(e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to\nwork), as it converts gender-neutral pronouns in languages to <code class=\"language-plaintext highlighter-rouge\">he' or </code>she’. We\nalso observe ChatGPT completely failing to translate the English gender-neutral\npronoun `they’ into equivalent gender-neutral pronouns in other languages, as\nit produces translations that are incoherent and incorrect. While it does\nrespect and provide appropriately gender-marked versions of Bengali words when\nprompted with gender information in English, ChatGPT appears to confer a higher\nrespect to men than to women in the same occupation. We conclude that ChatGPT\nexhibits the same gender biases which have been demonstrated for tools like\nGoogle Translate or MS Translator, as we provide recommendations for a human\ncentered approach for future designers of AIs that perform language translation\nto better accommodate such low-resource languages.</p>\n", "tags": ["AAAI","Ethics & Fairness"] },
{"key": "gill2023transformative", "citations": "311", "year": "2023", "title":"Transformative Effects Of Chatgpt On Modern Education: Emerging Era Of AI Chatbots", "abstract": "<p>ChatGPT, an AI-based chatbot, was released to provide coherent and useful\nreplies based on analysis of large volumes of data. In this article, leading\nscientists, researchers and engineers discuss the transformative effects of\nChatGPT on modern education. This research seeks to improve our knowledge of\nChatGPT capabilities and its use in the education sector, identifying potential\nconcerns and challenges. Our preliminary evaluation concludes that ChatGPT\nperformed differently in each subject area including finance, coding and maths.\nWhile ChatGPT has the ability to help educators by creating instructional\ncontent, offering suggestions and acting as an online educator to learners by\nanswering questions and promoting group work, there are clear drawbacks in its\nuse, such as the possibility of producing inaccurate or false data and\ncircumventing duplicate content (plagiarism) detectors where originality is\nessential. The often reported hallucinations within Generative AI in general,\nand also relevant for ChatGPT, can render its use of limited benefit where\naccuracy is essential. What ChatGPT lacks is a stochastic measure to help\nprovide sincere and sensitive communication with its users. Academic\nregulations and evaluation practices used in educational institutions need to\nbe updated, should ChatGPT be used as a tool in education. To address the\ntransformative effects of ChatGPT on the learning environment, educating\nteachers and students alike about its capabilities and limitations will be\ncrucial.</p>\n", "tags": ["Evaluation"] },
{"key": "girdhar2023imagebind", "citations": "332", "year": "2023", "title":"Imagebind: One Embedding Space To Bind Them All", "abstract": "<p>We present ImageBind, an approach to learn a joint embedding across six\ndifferent modalities - images, text, audio, depth, thermal, and IMU data. We\nshow that all combinations of paired data are not necessary to train such a\njoint embedding, and only image-paired data is sufficient to bind the\nmodalities together. ImageBind can leverage recent large scale vision-language\nmodels, and extends their zero-shot capabilities to new modalities just by\nusing their natural pairing with images. It enables novel emergent applications\n‘out-of-the-box’ including cross-modal retrieval, composing modalities with\narithmetic, cross-modal detection and generation. The emergent capabilities\nimprove with the strength of the image encoder and we set a new\nstate-of-the-art on emergent zero-shot recognition tasks across modalities,\noutperforming specialist supervised models. Finally, we show strong few-shot\nrecognition results outperforming prior work, and that ImageBind serves as a\nnew way to evaluate vision models for visual and non-visual tasks.</p>\n", "tags": ["Applications","CVPR","Few-Shot"] },
{"key": "gliwa2019samsum", "citations": "149", "year": "2019", "title":"Samsum Corpus: A Human-annotated Dialogue Dataset For Abstractive Summarization", "abstract": "<p>This paper introduces the SAMSum Corpus, a new dataset with abstractive\ndialogue summaries. We investigate the challenges it poses for automated\nsummarization by testing several models and comparing their results with those\nobtained on a corpus of news articles. We show that model-generated summaries\nof dialogues achieve higher ROUGE scores than the model-generated summaries of\nnews – in contrast with human evaluators’ judgement. This suggests that a\nchallenging task of abstractive dialogue summarization requires dedicated\nmodels and non-standard quality measures. To our knowledge, our study is the\nfirst attempt to introduce a high-quality chat-dialogues corpus, manually\nannotated with abstractive summarizations, which can be used by the research\ncommunity for further studies.</p>\n", "tags": ["Datasets"] },
{"key": "glm2024chatglm", "citations": "77", "year": "2024", "title":"Chatglm: A Family Of Large Language Models From GLM-130B To GLM-4 All Tools", "abstract": "<p>We introduce ChatGLM, an evolving family of large language models that we\nhave been developing over time. This report primarily focuses on the GLM-4\nlanguage series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent\nour most capable models that are trained with all the insights and lessons\ngained from the preceding three generations of ChatGLM. To date, the GLM-4\nmodels are pre-trained on ten trillions of tokens mostly in Chinese and\nEnglish, along with a small set of corpus from 24 languages, and aligned\nprimarily for Chinese and English usage. The high-quality alignment is achieved\nvia a multi-stage post-training process, which involves supervised fine-tuning\nand learning from human feedback. Evaluations show that GLM-4 1) closely rivals\nor outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH,\nBBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following\nas measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long\ncontext tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by\nAlignBench. The GLM-4 All Tools model is further aligned to understand user\nintent and autonomously decide when and which tool(s) touse – including web\nbrowser, Python interpreter, text-to-image model, and user-defined functions –\nto effectively complete complex tasks. In practical applications, it matches\nand even surpasses GPT-4 All Tools in tasks like accessing online information\nvia web browsing and solving math problems using Python interpreter. Over the\ncourse, we have open-sourced a series of models, including ChatGLM-6B (three\ngenerations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting\nover 10 million downloads on Hugging face in the year 2023 alone. The open\nmodels can be accessed through https://github.com/THUDM and\nhttps://huggingface.co/THUDM.</p>\n", "tags": ["Applications","Datasets","Evaluation Frameworks","Evaluation","Fine-Tuning","Has Code","Instruction Following","Model Architecture","Tools","Training Techniques"] },
{"key": "glockner2018breaking", "citations": "379", "year": "2018", "title":"Breaking NLI Systems With Sentences That Require Simple Lexical Inferences", "abstract": "<p>We create a new NLI test set that shows the deficiency of state-of-the-art\nmodels in inferences that require lexical and world knowledge. The new examples\nare simpler than the SNLI test set, containing sentences that differ by at most\none word from sentences in the training set. Yet, the performance on the new\ntest set is substantially worse across systems trained on SNLI, demonstrating\nthat these systems are limited in their generalization ability, failing to\ncapture many simple inferences.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "goel2019hyst", "citations": "84", "year": "2019", "title":"Hyst: A Hybrid Approach For Flexible And Accurate Dialogue State Tracking", "abstract": "<p>Recent works on end-to-end trainable neural network based approaches have\ndemonstrated state-of-the-art results on dialogue state tracking. The best\nperforming approaches estimate a probability distribution over all possible\nslot values. However, these approaches do not scale for large value sets\ncommonly present in real-life applications and are not ideal for tracking slot\nvalues that were not observed in the training set. To tackle these issues,\ncandidate-generation-based approaches have been proposed. These approaches\nestimate a set of values that are possible at each turn based on the\nconversation history and/or language understanding outputs, and hence enable\nstate tracking over unseen values and large value sets however, they fall short\nin terms of performance in comparison to the first group. In this work, we\nanalyze the performance of these two alternative dialogue state tracking\nmethods, and present a hybrid approach (HyST) which learns the appropriate\nmethod for each slot type. To demonstrate the effectiveness of HyST on a\nrich-set of slot types, we experiment with the recently released MultiWOZ-2.0\nmulti-domain, task-oriented dialogue-dataset. Our experiments show that HyST\nscales to multi-domain applications. Our best performing model results in a\nrelative improvement of 24% and 10% over the previous SOTA and our best\nbaseline respectively.</p>\n", "tags": ["Applications","INTERSPEECH","Training Techniques"] },
{"key": "gokhale2020mutant", "citations": "120", "year": "2020", "title":"MUTANT: A Training Paradigm For Out-of-distribution Generalization In Visual Question Answering", "abstract": "<p>While progress has been made on the visual question answering leaderboards,\nmodels often utilize spurious correlations and priors in datasets under the\ni.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples\nhas emerged as a proxy for generalization. In this paper, we present MUTANT, a\ntraining paradigm that exposes the model to perceptually similar, yet\nsemantically distinct mutations of the input, to improve OOD generalization,\nsuch as the VQA-CP challenge. Under this paradigm, models utilize a\nconsistency-constrained training objective to understand the effect of semantic\nchanges in input (question-image pair) on the output (answer). Unlike existing\nmethods on VQA-CP, MUTANT does not rely on the knowledge about the nature of\ntrain and test answer distributions. MUTANT establishes a new state-of-the-art\naccuracy on VQA-CP with a \\(10.57%\\) improvement. Our work opens up avenues for\nthe use of semantic input mutations for OOD generalization in question\nanswering.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "gokhale2020vqa", "citations": "73", "year": "2020", "title":"VQA-LOL: Visual Question Answering Under The Lens Of Logic", "abstract": "<p>Logical connectives and their implications on the meaning of a natural\nlanguage sentence are a fundamental aspect of understanding. In this paper, we\ninvestigate whether visual question answering (VQA) systems trained to answer a\nquestion about an image, are able to answer the logical composition of multiple\nsuch questions. When put under this \\textit{Lens of Logic}, state-of-the-art\nVQA models have difficulty in correctly answering these logically composed\nquestions. We construct an augmentation of the VQA dataset as a benchmark, with\nquestions containing logical compositions and linguistic transformations\n(negation, disjunction, conjunction, and antonyms). We propose our {Lens of\nLogic (LOL)} model which uses question-attention and logic-attention to\nunderstand logical connectives in the question, and a novel\nFr'echet-Compatibility Loss, which ensures that the answers of the component\nquestions and the composed question are consistent with the inferred logical\noperation. Our model shows substantial improvement in learning logical\ncompositions while retaining performance on VQA. We suggest this work as a move\ntowards robustness by embedding logical connectives in visual understanding.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "goldfarbtarrant2020content", "citations": "105", "year": "2020", "title":"Content Planning For Neural Story Generation With Aristotelian Rescoring", "abstract": "<p>Long-form narrative text generated from large language models manages a\nfluent impersonation of human writing, but only at the local sentence level,\nand lacks structure or global cohesion. We posit that many of the problems of\nstory generation can be addressed via high-quality content planning, and\npresent a system that focuses on how to learn good plot structures to guide\nstory generation. We utilize a plot-generation language model along with an\nensemble of rescoring models that each implement an aspect of good\nstory-writing as detailed in Aristotle’s Poetics. We find that stories written\nwith our more principled plot-structure are both more relevant to a given\nprompt and higher quality than baselines that do not content plan, or that plan\nin an unprincipled way.</p>\n", "tags": ["EMNLP","Prompting"] },
{"key": "golub2016character", "citations": "110", "year": "2016", "title":"Character-level Question Answering With Attention", "abstract": "<p>We show that a character-level encoder-decoder framework can be successfully\napplied to question answering with a structured knowledge base. We use our\nmodel for single-relation question answering and demonstrate the effectiveness\nof our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we\nimprove state-of-the-art accuracy from 63.9% to 70.9%, without use of\nensembles. Importantly, our character-level model has 16x fewer parameters than\nan equivalent word-level model, can be learned with significantly less data\ncompared to previous work, which relies on data augmentation, and is robust to\nnew entities in testing.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Tools"] },
{"key": "gonen2022demystifying", "citations": "64", "year": "2023", "title":"Demystifying Prompts In Language Models Via Perplexity Estimation", "abstract": "<p>Language models can be prompted to perform a wide variety of zero- and\nfew-shot learning problems. However, performance varies significantly with the\nchoice of prompt, and we do not yet understand why this happens or how to pick\nthe best prompts. In this work, we analyze the factors that contribute to this\nvariance and establish a new empirical hypothesis: the performance of a prompt\nis coupled with the extent to which the model is familiar with the language it\ncontains. Over a wide range of tasks, we show that the lower the perplexity of\nthe prompt is, the better the prompt is able to perform the task. As a result,\nwe devise a method for creating prompts: (1) automatically extend a small seed\nset of manually written prompts by paraphrasing using GPT3 and backtranslation\nand (2) choose the lowest perplexity prompts to get significant gains in\nperformance.</p>\n", "tags": ["EMNLP","Few-Shot","Prompting"] },
{"key": "gong2017natural", "citations": "197", "year": "2017", "title":"Natural Language Inference Over Interaction Space", "abstract": "<p>Natural Language Inference (NLI) task requires an agent to determine the\nlogical relationship between a natural language premise and a natural language\nhypothesis. We introduce Interactive Inference Network (IIN), a novel class of\nneural network architectures that is able to achieve high-level understanding\nof the sentence pair by hierarchically extracting semantic features from\ninteraction space. We show that an interaction tensor (attention weight)\ncontains semantic information to solve natural language inference, and a denser\ninteraction tensor contains richer semantic information. One instance of such\narchitecture, Densely Interactive Inference Network (DIIN), demonstrates the\nstate-of-the-art performance on large scale NLI copora and large-scale NLI\nalike corpus. It’s noteworthy that DIIN achieve a greater than 20% error\nreduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to\nthe strongest published system.</p>\n", "tags": ["Agentic","Datasets","Model Architecture"] },
{"key": "gong2022diffuseq", "citations": "72", "year": "2022", "title":"Diffuseq: Sequence To Sequence Text Generation With Diffusion Models", "abstract": "<p>Recently, diffusion models have emerged as a new paradigm for generative\nmodels. Despite the success in domains using continuous signals such as vision\nand audio, adapting diffusion models to natural language is under-explored due\nto the discrete nature of texts, especially for conditional generation. We\ntackle this challenge by proposing DiffuSeq: a diffusion model designed for\nsequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation\nover a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or\neven better performance than six established baselines, including a\nstate-of-the-art model that is based on pre-trained language models. Apart from\nquality, an intriguing property of DiffuSeq is its high diversity during\ngeneration, which is desired in many Seq2Seq tasks. We further include a\ntheoretical analysis revealing the connection between DiffuSeq and\nautoregressive/non-autoregressive models. Bringing together theoretical\nanalysis and empirical evidence, we demonstrate the great potential of\ndiffusion models in complex conditional language generation tasks. Code is\navailable at https://github.com/Shark-NLP/DiffuSeq</p>\n", "tags": ["Evaluation","Has Code"] },
{"key": "goo2018abstractive", "citations": "109", "year": "2018", "title":"Abstractive Dialogue Summarization With Sentence-gated Modeling Optimized By Dialogue Acts", "abstract": "<p>Neural abstractive summarization has been increasingly studied, where the\nprior work mainly focused on summarizing single-speaker documents (news,\nscientific publications, etc). In dialogues, there are different interactions\nbetween speakers, which are usually defined as dialogue acts. The interactive\nsignals may provide informative cues for better summarizing dialogues. This\npaper proposes to explicitly leverage dialogue acts in a neural summarization\nmodel, where a sentence-gated mechanism is designed for modeling the\nrelationship between dialogue acts and the summary. The experiments show that\nour proposed model significantly improves the abstractive summarization\nperformance compared to the state-of-the-art baselines on AMI meeting corpus,\ndemonstrating the usefulness of the interactive signal provided by dialogue\nacts.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","SLT"] },
{"key": "gopalakrishnan2023topical", "citations": "206", "year": "2019", "title":"Topical-chat: Towards Knowledge-grounded Open-domain Conversations", "abstract": "<p>Building socialbots that can have deep, engaging open-domain conversations\nwith humans is one of the grand challenges of artificial intelligence (AI). To\nthis end, bots need to be able to leverage world knowledge spanning several\ndomains effectively when conversing with humans who have their own world\nknowledge. Existing knowledge-grounded conversation datasets are primarily\nstylized with explicit roles for conversation partners. These datasets also do\nnot explore depth or breadth of topical coverage with transitions in\nconversations. We introduce Topical-Chat, a knowledge-grounded human-human\nconversation dataset where the underlying knowledge spans 8 broad topics and\nconversation partners don’t have explicitly defined roles, to help further\nresearch in open-domain conversational AI. We also train several\nstate-of-the-art encoder-decoder conversational models on Topical-Chat and\nperform automated and human evaluation for benchmarking.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH"] },
{"key": "gordon2017iqa", "citations": "348", "year": "2018", "title":"IQA: Visual Question Answering In Interactive Environments", "abstract": "<p>We introduce Interactive Question Answering (IQA), the task of answering\nquestions that require an autonomous agent to interact with a dynamic visual\nenvironment. IQA presents the agent with a scene and a question, like: “Are\nthere any apples in the fridge?” The agent must navigate around the scene,\nacquire visual understanding of scene elements, interact with objects (e.g.\nopen refrigerators) and plan for a series of actions conditioned on the\nquestion. Popular reinforcement learning approaches with a single controller\nperform poorly on IQA owing to the large and diverse state space. We propose\nthe Hierarchical Interactive Memory Network (HIMN), consisting of a factorized\nset of controllers, allowing the system to operate at multiple levels of\ntemporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset\nbuilt upon AI2-THOR, a simulated photo-realistic environment of configurable\nindoor scenes with interactive objects (code and dataset available at\nhttps://github.com/danielgordon10/thor-iqa-cvpr-2018). IQUAD V1 has 75,000\nquestions, each paired with a unique scene configuration. Our experiments show\nthat our proposed model outperforms popular single controller based methods on\nIQUAD V1. For sample questions and results, please view our video:\nhttps://youtu.be/pXd3C-1jr98</p>\n", "tags": ["CVPR","Datasets","Has Code","Reinforcement Learning"] },
{"key": "gordon2020compressing", "citations": "233", "year": "2020", "title":"Compressing BERT: Studying The Effects Of Weight Pruning On Transfer Learning", "abstract": "<p>Pre-trained universal feature extractors, such as BERT for natural language\nprocessing and VGG for computer vision, have become effective methods for\nimproving deep learning models without requiring more labeled data. While\neffective, feature extractors like BERT may be prohibitively large for some\ndeployment scenarios. We explore weight pruning for BERT and ask: how does\ncompression during pre-training affect transfer learning? We find that pruning\naffects transfer learning in three broad regimes. Low levels of pruning\n(30-40%) do not affect pre-training loss or transfer to downstream tasks at\nall. Medium levels of pruning increase the pre-training loss and prevent useful\npre-training information from being transferred to downstream tasks. High\nlevels of pruning additionally prevent models from fitting downstream datasets,\nleading to further degradation. Finally, we observe that fine-tuning BERT on a\nspecific task does not improve its prunability. We conclude that BERT can be\npruned once during pre-training rather than separately for each task without\naffecting performance.</p>\n", "tags": ["Datasets","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "gorti2022x", "citations": "125", "year": "2022", "title":"X-pool: Cross-modal Language-video Attention For Text-video Retrieval", "abstract": "<p>In text-video retrieval, the objective is to learn a cross-modal similarity\nfunction between a text and a video that ranks relevant text-video pairs higher\nthan irrelevant pairs. However, videos inherently express a much wider gamut of\ninformation than texts. Instead, texts often capture sub-regions of entire\nvideos and are most semantically similar to certain frames within videos.\nTherefore, for a given text, a retrieval model should focus on the text’s most\nsemantically similar video sub-regions to make a more relevant comparison. Yet,\nmost existing works aggregate entire videos without directly considering text.\nCommon text-agnostic aggregations schemes include mean-pooling or\nself-attention over the frames, but these are likely to encode misleading\nvisual information not described in the given text. To address this, we propose\na cross-modal attention model called X-Pool that reasons between a text and the\nframes of a video. Our core mechanism is a scaled dot product attention for a\ntext to attend to its most semantically similar frames. We then generate an\naggregated video representation conditioned on the text’s attention weights\nover the frames. We evaluate our method on three benchmark datasets of MSR-VTT,\nMSVD and LSMDC, achieving new state-of-the-art results by up to 12% in relative\nimprovement in Recall@1. Our findings thereby highlight the importance of joint\ntext-video reasoning to extract important visual cues according to text. Full\ncode and demo can be found at: https://layer6ai-labs.github.io/xpool/</p>\n", "tags": ["CVPR","Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "goyal2019using", "citations": "61", "year": "2019", "title":"Using Natural Language For Reward Shaping In Reinforcement Learning", "abstract": "<p>Recent reinforcement learning (RL) approaches have shown strong performance\nin complex domains such as Atari games, but are often highly sample\ninefficient. A common approach to reduce interaction time with the environment\nis to use reward shaping, which involves carefully designing reward functions\nthat provide the agent intermediate rewards for progress towards the goal.\nHowever, designing appropriate shaping rewards is known to be difficult as well\nas time-consuming. In this work, we address this problem by using natural\nlanguage instructions to perform reward shaping. We propose the LanguagE-Action\nReward Network (LEARN), a framework that maps free-form natural language\ninstructions to intermediate rewards based on actions taken by the agent. These\nintermediate language-based rewards can seamlessly be integrated into any\nstandard reinforcement learning algorithm. We experiment with Montezuma’s\nRevenge from the Atari Learning Environment, a popular benchmark in RL. Our\nexperiments on a diverse set of 15 tasks demonstrate that, for the same number\nof interactions with the environment, language-based rewards lead to successful\ncompletion of the task 60% more often on average, compared to learning without\nlanguage.</p>\n", "tags": ["Agentic","Datasets","Evaluation","IJCAI","Reinforcement Learning","Tools"] },
{"key": "goyal2020evaluating", "citations": "87", "year": "2020", "title":"Evaluating Factuality In Generation With Dependency-level Entailment", "abstract": "<p>Despite significant progress in text generation models, a serious limitation\nis their tendency to produce text that is factually inconsistent with\ninformation in the input. Recent work has studied whether textual entailment\nsystems can be used to identify factual errors; however, these sentence-level\nentailment models are trained to solve a different problem than generation\nfiltering and they do not localize which part of a generation is non-factual.\nIn this paper, we propose a new formulation of entailment that decomposes it at\nthe level of dependency arcs. Rather than focusing on aggregate decisions, we\ninstead ask whether the semantic relationship manifested by individual\ndependency arcs in the generated output is supported by the input. Human\njudgments on this task are difficult to obtain; we therefore propose a method\nto automatically create data based on existing entailment or paraphrase\ncorpora. Experiments show that our dependency arc entailment model trained on\nthis data can identify factual inconsistencies in paraphrasing and\nsummarization better than sentence-level methods or those based on question\ngeneration, while additionally localizing the erroneous parts of the\ngeneration.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "goyal2020neural", "citations": "101", "year": "2020", "title":"Neural Syntactic Preordering For Controlled Paraphrase Generation", "abstract": "<p>Paraphrasing natural language sentences is a multifaceted process: it might\ninvolve replacing individual words or short phrases, local rearrangement of\ncontent, or high-level restructuring like topicalization or passivization. Past\napproaches struggle to cover this space of paraphrase possibilities in an\ninterpretable manner. Our work, inspired by pre-ordering literature in machine\ntranslation, uses syntactic transformations to softly “reorder’’ the source\nsentence and guide our neural paraphrasing model. First, given an input\nsentence, we derive a set of feasible syntactic rearrangements using an\nencoder-decoder model. This model operates over a partially lexical, partially\nsyntactic view of the sentence and can reorder big chunks. Next, we use each\nproposed rearrangement to produce a sequence of position embeddings, which\nencourages our final encoder-decoder paraphrase model to attend to the source\nwords in a particular order. Our evaluation, both automatic and human, shows\nthat the proposed system retains the quality of the baseline approaches while\ngiving a substantial increase in the diversity of the generated paraphrases</p>\n", "tags": ["Evaluation"] },
{"key": "goyal2021flores", "citations": "83", "year": "2021", "title":"The FLORES-101 Evaluation Benchmark For Low-resource And Multilingual Machine Translation", "abstract": "<p>One of the biggest challenges hindering progress in low-resource and\nmultilingual machine translation is the lack of good evaluation benchmarks.\nCurrent evaluation benchmarks either lack good coverage of low-resource\nlanguages, consider only restricted domains, or are low quality because they\nare constructed using semi-automatic procedures. In this work, we introduce the\nFLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from\nEnglish Wikipedia and covering a variety of different topics and domains. These\nsentences have been translated in 101 languages by professional translators\nthrough a carefully controlled process. The resulting dataset enables better\nassessment of model quality on the long tail of low-resource languages,\nincluding the evaluation of many-to-many multilingual translation systems, as\nall translations are multilingually aligned. By publicly releasing such a\nhigh-quality and high-coverage dataset, we hope to foster progress in the\nmachine translation community and beyond.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "goyal2021larger", "citations": "65", "year": "2021", "title":"Larger-scale Transformers For Multilingual Masked Language Modeling", "abstract": "<p>Recent work has demonstrated the effectiveness of cross-lingual language\nmodel pretraining for cross-lingual understanding. In this study, we present\nthe results of two larger multilingual masked language models, with 3.5B and\n10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform\nXLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the\nRoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on\naverage while handling 99 more languages. This suggests pretrained models with\nlarger capacity may obtain both strong performance on high-resource languages\nwhile greatly improving low-resource languages. We make our code and models\npublicly available.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "goyal2022news", "citations": "172", "year": "2022", "title":"News Summarization And Evaluation In The Era Of GPT-3", "abstract": "<p>The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "goyle2023neural", "citations": "171", "year": "2022", "title":"Neural Machine Translation For Low Resource Languages", "abstract": "<p>Neural Machine translation is a challenging task due to the inherent complex\nnature and the fluidity that natural languages bring. Nonetheless, in recent\nyears, it has achieved state-of-the-art performance in several language pairs.\nAlthough, a lot of traction can be seen in the areas of multilingual neural\nmachine translation (MNMT) in the recent years, there are no comprehensive\nsurvey done to identify what approaches work well. The goal of this paper is to\ninvestigate the realm of low resource languages and build a Neural Machine\nTranslation model to achieve state-of-the-art results. The paper looks to build\nupon the mBART language model and explore strategies to augment it with various\nNLP and Deep Learning techniques like back translation and transfer learning.\nThis implementation tries to unpack the architecture of the NMT application and\ndetermine the different components which offers us opportunities to amend the\nsaid application within the purview of the low resource languages problem\nspace.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Survey Paper"] },
{"key": "gozalobrizuela2023chatgpt", "citations": "175", "year": "2023", "title":"Chatgpt Is Not All You Need. A State Of The Art Review Of Large Generative AI Models", "abstract": "<p>During the last two years there has been a plethora of large generative\nmodels such as ChatGPT or Stable Diffusion that have been published.\nConcretely, these models are able to perform tasks such as being a general\nquestion and answering system or automatically creating artistic images that\nare revolutionizing several sectors. Consequently, the implications that these\ngenerative models have in the industry and society are enormous, as several job\npositions may be transformed. For example, Generative AI is capable of\ntransforming effectively and creatively texts to images, like the DALLE-2\nmodel; text to 3D images, like the Dreamfusion model; images to text, like the\nFlamingo model; texts to video, like the Phenaki model; texts to audio, like\nthe AudioLM model; texts to other texts, like ChatGPT; texts to code, like the\nCodex model; texts to scientific texts, like the Galactica model or even create\nalgorithms like AlphaTensor. This work consists on an attempt to describe in a\nconcise way the main models are sectors that are affected by generative AI and\nto provide a taxonomy of the main generative models published recently.</p>\n", "tags": [] },
{"key": "grace2017when", "citations": "468", "year": "2018", "title":"When Will AI Exceed Human Performance? Evidence From AI Experts", "abstract": "<p>Advances in artificial intelligence (AI) will transform modern life by\nreshaping transportation, health, science, finance, and the military. To adapt\npublic policy, we need to better anticipate these advances. Here we report the\nresults from a large survey of machine learning researchers on their beliefs\nabout progress in AI. Researchers predict AI will outperform humans in many\nactivities in the next ten years, such as translating languages (by 2024),\nwriting high-school essays (by 2026), driving a truck (by 2027), working in\nretail (by 2031), writing a bestselling book (by 2049), and working as a\nsurgeon (by 2053). Researchers believe there is a 50% chance of AI\noutperforming humans in all tasks in 45 years and of automating all human jobs\nin 120 years, with Asian respondents expecting these dates much sooner than\nNorth Americans. These results will inform discussion amongst researchers and\npolicymakers about anticipating and managing trends in AI.</p>\n", "tags": ["Survey Paper"] },
{"key": "grave2016improving", "citations": "206", "year": "2016", "title":"Improving Neural Language Models With A Continuous Cache", "abstract": "<p>We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.</p>\n", "tags": ["Datasets","Memory & Context"] },
{"key": "greshake2023not", "citations": "109", "year": "2023", "title":"Not What You've Signed Up For: Compromising Real-world Llm-integrated Applications With Indirect Prompt Injection", "abstract": "<p>Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks’ practical viability against\nboth real-world systems, such as Bing’s GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication’s functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.</p>\n", "tags": ["Applications","Model Architecture","Prompting","Security"] },
{"key": "grégoire2023augmented", "citations": "122", "year": "2023", "title":"Augmented Language Models: A Survey", "abstract": "<p>This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.</p>\n", "tags": ["Survey Paper","Tools"] },
{"key": "grönroos2018memad", "citations": "63", "year": "2018", "title":"The Memad Submission To The WMT18 Multimodal Translation Task", "abstract": "<p>This paper describes the MeMAD project entry to the WMT Multimodal Machine\nTranslation Shared Task.\n  We propose adapting the Transformer neural machine translation (NMT)\narchitecture to a multi-modal setting. In this paper, we also describe the\npreliminary experiments with text-only translation systems leading us up to\nthis choice.\n  We have the top scoring system for both English-to-German and\nEnglish-to-French, according to the automatic metrics for flickr18.\n  Our experiments show that the effect of the visual features in our system is\nsmall. Our largest gains come from the quality of the underlying text-only NMT\nsystem. We find that appropriate use of additional data is effective.</p>\n", "tags": ["Evaluation"] },
{"key": "gu2016deep", "citations": "488", "year": "2016", "title":"Deep API Learning", "abstract": "<p>Developers often wonder how to implement a certain functionality (e.g., how\nto parse XML files) using APIs. Obtaining an API usage sequence based on an\nAPI-related natural language query is very helpful in this regard. Given a\nquery, existing approaches utilize information retrieval models to search for\nmatching API sequences. These approaches treat queries and APIs as bag-of-words\n(i.e., keyword matching or word-to-word alignment) and lack a deep\nunderstanding of the semantics of the query.\n  We propose DeepAPI, a deep learning based approach to generate API usage\nsequences for a given natural language query. Instead of a bags-of-words\nassumption, it learns the sequence of words in a query and the sequence of\nassociated APIs. DeepAPI adapts a neural language model named RNN\nEncoder-Decoder. It encodes a word sequence (user query) into a fixed-length\ncontext vector, and generates an API sequence based on the context vector. We\nalso augment the RNN Encoder-Decoder by considering the importance of\nindividual APIs. We empirically evaluate our approach with more than 7 million\nannotated code snippets collected from GitHub. The results show that our\napproach generates largely accurate API sequences and outperforms the related\napproaches.</p>\n", "tags": ["Has Code","Llm For Code","Tools"] },
{"key": "gu2016empirical", "citations": "145", "year": "2017", "title":"An Empirical Study Of Language CNN For Image Captioning", "abstract": "<p>Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.</p>\n", "tags": ["Datasets","ICCV"] },
{"key": "gu2016incorporating", "citations": "1457", "year": "2016", "title":"Incorporating Copying Mechanism In Sequence-to-sequence Learning", "abstract": "<p>We address an important problem in sequence-to-sequence (Seq2Seq) learning\nreferred to as copying, in which certain segments in the input sequence are\nselectively replicated in the output sequence. A similar phenomenon is\nobservable in human language communication. For example, humans tend to repeat\nentity names or even long phrases in conversation. The challenge with regard to\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\nthe operation. In this paper, we incorporate copying into neural network-based\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\nstructure. CopyNet can nicely integrate the regular way of word generation in\nthe decoder with the new copying mechanism which can choose sub-sequences in\nthe input sequence and put them at proper places in the output sequence. Our\nempirical study on both synthetic data sets and real world data sets\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\nregular RNN-based model with remarkable margins on text summarization tasks.</p>\n", "tags": [] },
{"key": "gu2016learning", "citations": "200", "year": "2017", "title":"Learning To Translate In Real-time With Neural Machine Translation", "abstract": "<p>Translating in real-time, a.k.a. simultaneous translation, outputs\ntranslation words before the input sentence ends, which is a challenging\nproblem for conventional machine translation methods. We propose a neural\nmachine translation (NMT) framework for simultaneous translation in which an\nagent learns to make decisions on when to translate from the interaction with a\npre-trained NMT environment. To trade off quality and delay, we extensively\nexplore various targets for delay and design a method for beam-search\napplicable in the simultaneous MT setting. Experiments against state-of-the-art\nbaselines on two language pairs demonstrate the efficacy of the proposed\nframework both quantitatively and qualitatively.</p>\n", "tags": ["NAACL","Tools"] },
{"key": "gu2017non", "citations": "471", "year": "2017", "title":"Non-autoregressive Neural Machine Translation", "abstract": "<p>Existing approaches to neural machine translation condition each output word\non previously generated outputs. We introduce a model that avoids this\nautoregressive property and produces its outputs in parallel, allowing an order\nof magnitude lower latency during inference. Through knowledge distillation,\nthe use of input token fertilities as a latent variable, and policy gradient\nfine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative\nto the autoregressive Transformer network used as a teacher. We demonstrate\nsubstantial cumulative improvements associated with each of the three aspects\nof our training strategy, and validate our approach on IWSLT 2016\nEnglish-German and two WMT language pairs. By sampling fertilities in parallel\nat inference time, our non-autoregressive model achieves near-state-of-the-art\nperformance of 29.8 BLEU on WMT 2016 English-Romanian.</p>\n", "tags": ["Efficiency","Fine-Tuning","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "gu2017stack", "citations": "174", "year": "2018", "title":"Stack-captioning: Coarse-to-fine Learning For Image Captioning", "abstract": "<p>The existing image captioning approaches typically train a one-stage sentence\ndecoder, which is difficult to generate rich fine-grained descriptions. On the\nother hand, multi-stage image caption model is hard to train due to the\nvanishing gradient problem. In this paper, we propose a coarse-to-fine\nmulti-stage prediction framework for image captioning, composed of multiple\ndecoders each of which operates on the output of the previous stage, producing\nincreasingly refined image descriptions. Our proposed learning approach\naddresses the difficulty of vanishing gradients during training by providing a\nlearning objective function that enforces intermediate supervisions.\nParticularly, we optimize our model with a reinforcement learning approach\nwhich utilizes the output of each intermediate decoder’s test-time inference\nalgorithm as well as the output of its preceding decoder to normalize the\nrewards, which simultaneously solves the well-known exposure bias problem and\nthe loss-evaluation mismatch problem. We extensively evaluate the proposed\napproach on MSCOCO and show that our approach can achieve the state-of-the-art\nperformance.</p>\n", "tags": ["AAAI","Evaluation","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "gu2018dialogwae", "citations": "103", "year": "2018", "title":"Dialogwae: Multimodal Response Generation With Conditional Wasserstein Auto-encoder", "abstract": "<p>Variational autoencoders~(VAEs) have shown a promise in data-driven\nconversation modeling. However, most VAE conversation models match the\napproximate posterior distribution over the latent variables to a simple prior\nsuch as standard normal distribution, thereby restricting the generated\nresponses to a relatively simple (e.g., unimodal) scope. In this paper, we\npropose DialogWAE, a conditional Wasserstein autoencoder~(WAE) specially\ndesigned for dialogue modeling. Unlike VAEs that impose a simple distribution\nover the latent variables, DialogWAE models the distribution of data by\ntraining a GAN within the latent variable space. Specifically, our model\nsamples from the prior and posterior distributions over the latent variables by\ntransforming context-dependent random noise using neural networks and minimizes\nthe Wasserstein distance between the two distributions. We further develop a\nGaussian mixture prior network to enrich the latent space. Experiments on two\npopular datasets show that DialogWAE outperforms the state-of-the-art\napproaches in generating more coherent, informative and diverse responses.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Training Techniques"] },
{"key": "gu2018meta", "citations": "322", "year": "2018", "title":"Meta-learning For Low-resource Neural Machine Translation", "abstract": "<p>In this paper, we propose to extend the recently introduced model-agnostic\nmeta-learning algorithm (MAML) for low-resource neural machine translation\n(NMT). We frame low-resource translation as a meta-learning problem, and we\nlearn to adapt to low-resource languages based on multilingual high-resource\nlanguage tasks. We use the universal lexical\nrepresentation~\\citep{gu2018universal} to overcome the input-output mismatch\nacross different languages. We evaluate the proposed meta-learning strategy\nusing eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt,\nNl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,\nLv, Fi, Tr and Ko) as target tasks. We show that the proposed approach\nsignificantly outperforms the multilingual, transfer learning based\napproach~\\citep{zoph2016transfer} and enables us to train a competitive NMT\nsystem with only a fraction of training examples. For instance, the proposed\napproach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing\nonly 16,000 translated words (~600 parallel sentences).</p>\n", "tags": ["EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "gu2018universal", "citations": "249", "year": "2018", "title":"Universal Neural Machine Translation For Extremely Low Resource Languages", "abstract": "<p>In this paper, we propose a new universal machine translation approach\nfocusing on languages with a limited amount of parallel data. Our proposed\napproach utilizes a transfer-learning approach to share lexical and sentence\nlevel representations across multiple source languages into one target\nlanguage. The lexical part is shared through a Universal Lexical Representation\nto support multilingual word-level sharing. The sentence-level sharing is\nrepresented by a model of experts from all source languages that share the\nsource encoders with all other languages. This enables the low-resource\nlanguage to utilize the lexical and sentence representations of the higher\nresource languages. Our approach is able to achieve 23 BLEU on Romanian-English\nWMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU\nof strong baseline system which uses multilingual training and\nback-translation. Furthermore, we show that the proposed approach can achieve\nalmost 20 BLEU on the same dataset through fine-tuning a pre-trained\nmulti-lingual system in a zero-shot setting.</p>\n", "tags": ["Datasets","Fine-Tuning","NAACL","Training Techniques"] },
{"key": "gu2018unpaired", "citations": "93", "year": "2018", "title":"Unpaired Image Captioning By Language Pivoting", "abstract": "<p>Image captioning is a multimodal task involving computer vision and natural\nlanguage processing, where the goal is to learn a mapping from the image to its\nnatural language description. In general, the mapping function is learned from\na training set of image-caption pairs. However, for some language, large scale\nimage-caption paired corpus might not be available. We present an approach to\nthis unpaired image captioning problem by language pivoting. Our method can\neffectively capture the characteristics of an image captioner from the pivot\nlanguage (Chinese) and align it to the target language (English) using another\npivot-target (Chinese-English) sentence parallel corpus. We evaluate our method\non two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative\ncomparisons against several baseline approaches demonstrate the effectiveness\nof our method.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "gu2019improved", "citations": "88", "year": "2019", "title":"Improved Zero-shot Neural Machine Translation Via Ignoring Spurious Correlations", "abstract": "<p>Zero-shot translation, translating between language pairs on which a Neural\nMachine Translation (NMT) system has never been trained, is an emergent\nproperty when training the system in multilingual settings. However, naive\ntraining for zero-shot NMT easily fails, and is sensitive to hyper-parameter\nsetting. The performance typically lags far behind the more conventional\npivot-based approach which translates twice using a third language as a pivot.\nIn this work, we address the degeneracy problem due to capturing spurious\ncorrelations by quantitatively analyzing the mutual information between\nlanguage IDs of the source and decoded sentences. Inspired by this analysis, we\npropose to use two simple but effective approaches: (1) decoder pre-training;\n(2) back-translation. These methods show significant improvement (4~22 BLEU\npoints) over the vanilla zero-shot translation on three challenging\nmultilingual datasets, and achieve similar or better results than the\npivot-based approach.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "gu2019insertion", "citations": "93", "year": "2019", "title":"Insertion-based Decoding With Automatically Inferred Generation Order", "abstract": "<p>Conventional neural autoregressive decoding commonly assumes a fixed\nleft-to-right generation order, which may be sub-optimal. In this work, we\npropose a novel decoding algorithm – InDIGO – which supports flexible\nsequence generation in arbitrary orders through insertion operations. We extend\nTransformer, a state-of-the-art sequence generation model, to efficiently\nimplement the proposed approach, enabling it to be trained with either a\npre-defined generation order or adaptive orders obtained from beam-search.\nExperiments on four real-world tasks, including word order recovery, machine\ntranslation, image caption and code generation, demonstrate that our algorithm\ncan generate sequences following arbitrary orders, while achieving competitive\nor even better performance compared to the conventional left-to-right\ngeneration. The generated sequences show that InDIGO adopts adaptive generation\norders based on input information.</p>\n", "tags": ["Llm For Code","Model Architecture","TACL"] },
{"key": "gu2019interactive", "citations": "71", "year": "2019", "title":"Interactive Matching Network For Multi-turn Response Selection In Retrieval-based Chatbots", "abstract": "<p>In this paper, we propose an interactive matching network (IMN) for the\nmulti-turn response selection task. First, IMN constructs word representations\nfrom three aspects to address the challenge of out-of-vocabulary (OOV) words.\nSecond, an attentive hierarchical recurrent encoder (AHRE), which is capable of\nencoding sentences hierarchically and generating more descriptive\nrepresentations by aggregating with an attention mechanism, is designed.\nFinally, the bidirectional interactions between whole multi-turn contexts and\nresponse candidates are calculated to derive the matching information between\nthem. Experiments on four public datasets show that IMN outperforms the\nbaseline models on all metrics, achieving a new state-of-the-art performance\nand demonstrating compatibility across domains for multi-turn response\nselection.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Model Architecture","Retrieval Systems"] },
{"key": "gu2019levenshtein", "citations": "204", "year": "2019", "title":"Levenshtein Transformer", "abstract": "<p>Modern neural sequence generation models are built to either generate tokens\nstep-by-step from scratch or (iteratively) modify a sequence of tokens bounded\nby a fixed length. In this work, we develop Levenshtein Transformer, a new\npartially autoregressive model devised for more flexible and amenable sequence\ngeneration. Unlike previous approaches, the atomic operations of our model are\ninsertion and deletion. The combination of them facilitates not only generation\nbut also sequence refinement allowing dynamic length changes. We also propose a\nset of new training techniques dedicated at them, effectively exploiting one as\nthe other’s learning signal thanks to their complementary nature. Experiments\napplying the proposed model achieve comparable performance but much-improved\nefficiency on both generation (e.g. machine translation, text summarization)\nand refinement tasks (e.g. automatic post-editing). We further confirm the\nflexibility of our model by showing a Levenshtein Transformer trained by\nmachine translation can straightforwardly be used for automatic post-editing.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "gu2020beyond", "citations": "122", "year": "2021", "title":"Beyond I.I.D.: Three Levels Of Generalization For Question Answering On Knowledge Bases", "abstract": "<p>Existing studies on question answering on knowledge bases (KBQA) mainly\noperate with the standard i.i.d assumption, i.e., training distribution over\nquestions is the same as the test distribution. However, i.i.d may be neither\nreasonably achievable nor desirable on large-scale KBs because 1) true user\ndistribution is hard to capture and 2) randomly sample training examples from\nthe enormous space would be highly data-inefficient. Instead, we suggest that\nKBQA models should have three levels of built-in generalization: i.i.d,\ncompositional, and zero-shot. To facilitate the development of KBQA models with\nstronger generalization, we construct and release a new large-scale,\nhigh-quality dataset with 64,331 questions, GrailQA, and provide evaluation\nsettings for all three levels of generalization. In addition, we propose a\nnovel BERT-based KBQA model. The combination of our dataset and model enables\nus to thoroughly examine and demonstrate, for the first time, the key role of\npre-trained contextual embeddings like BERT in the generalization of KBQA.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "gu2020dialogbert", "citations": "60", "year": "2021", "title":"Dialogbert: Discourse-aware Response Generation Via Learning To Recover And Rank Utterances", "abstract": "<p>Recent advances in pre-trained language models have significantly improved\nneural response generation. However, existing methods usually view the dialogue\ncontext as a linear sequence of tokens and learn to generate the next word\nthrough token-level self-attention. Such token-level encoding hinders the\nexploration of discourse-level coherence among utterances. This paper presents\nDialogBERT, a novel conversational response generation model that enhances\nprevious PLM-based dialogue models. DialogBERT employs a hierarchical\nTransformer architecture. To efficiently capture the discourse-level coherence\namong utterances, we propose two training objectives, including masked\nutterance regression and distributed utterance order ranking in analogy to the\noriginal BERT training. Experiments on three multi-turn conversation datasets\nshow that our approach remarkably outperforms the baselines, such as BART and\nDialoGPT, in terms of quantitative evaluation. The human evaluation suggests\nthat DialogBERT generates more coherent, informative, and human-like responses\nthan the baselines with significant margins.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "gu2020domain", "citations": "1355", "year": "2021", "title":"Domain-specific Language Model Pretraining For Biomedical Natural Language Processing", "abstract": "<p>Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning\nBenchmark) at https://aka.ms/BLURB.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "gu2020speaker", "citations": "147", "year": "2020", "title":"Speaker-aware BERT For Multi-turn Response Selection In Retrieval-based Chatbots", "abstract": "<p>In this paper, we study the problem of employing pre-trained language models\nfor multi-turn response selection in retrieval-based chatbots. A new model,\nnamed Speaker-Aware BERT (SA-BERT), is proposed in order to make the model\naware of the speaker change information, which is an important and intrinsic\nproperty of multi-turn dialogues. Furthermore, a speaker-aware disentanglement\nstrategy is proposed to tackle the entangled dialogues. This strategy selects a\nsmall number of most important utterances as the filtered context according to\nthe speakers’ information in them. Finally, domain adaptation is performed to\nincorporate the in-domain knowledge into pre-trained language models.\nExperiments on five public datasets show that our proposed model outperforms\nthe present models on all metrics by large margins and achieves new\nstate-of-the-art performances for multi-turn response selection.</p>\n", "tags": ["CIKM","Datasets","Dialogue & Multi Turn","Evaluation","Fine-Tuning","Model Architecture"] },
{"key": "gu2021open", "citations": "244", "year": "2021", "title":"Open-vocabulary Object Detection Via Vision And Language Knowledge Distillation", "abstract": "<p>We aim at advancing open-vocabulary object detection, which detects objects\ndescribed by arbitrary text inputs. The fundamental challenge is the\navailability of training data. It is costly to further scale up the number of\nclasses contained in existing object detection datasets. To overcome this\nchallenge, we propose ViLD, a training method via Vision and Language knowledge\nDistillation. Our method distills the knowledge from a pretrained\nopen-vocabulary image classification model (teacher) into a two-stage detector\n(student). Specifically, we use the teacher model to encode category texts and\nimage regions of object proposals. Then we train a student detector, whose\nregion embeddings of detected boxes are aligned with the text and image\nembeddings inferred by the teacher. We benchmark on LVIS by holding out all\nrare categories as novel categories that are not seen during training. ViLD\nobtains 16.1 mask AP\\(<em>r\\) with a ResNet-50 backbone, even outperforming the\nsupervised counterpart by 3.8. When trained with a stronger teacher model\nALIGN, ViLD achieves 26.3 AP\\(_r\\). The model can directly transfer to other\ndatasets without finetuning, achieving 72.2 AP\\(</em>{50}\\) on PASCAL VOC, 36.6 AP on\nCOCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous\nstate-of-the-art by 4.8 on novel AP and 11.4 on overall AP. Code and demo are\nopen-sourced at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.</p>\n", "tags": ["Datasets","Evaluation","Has Code","ICLR","Training Techniques"] },
{"key": "gu2021ppt", "citations": "185", "year": "2022", "title":"PPT: Pre-trained Prompt Tuning For Few-shot Learning", "abstract": "<p>Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework “PPT”. To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.</p>\n", "tags": ["Few-Shot","Fine-Tuning","Prompting","Tools","Training Techniques"] },
{"key": "gu2022xylayoutlm", "citations": "63", "year": "2022", "title":"Xylayoutlm: Towards Layout-aware Multimodal Networks For Visually-rich Document Understanding", "abstract": "<p>Recently, various multimodal networks for Visually-Rich Document\nUnderstanding(VRDU) have been proposed, showing the promotion of transformers\nby integrating visual and layout information with the text embeddings. However,\nmost existing approaches utilize the position embeddings to incorporate the\nsequence information, neglecting the noisy improper reading order obtained by\nOCR tools. In this paper, we propose a robust layout-aware multimodal network\nnamed XYLayoutLM to capture and leverage rich layout information from proper\nreading orders produced by our Augmented XY Cut. Moreover, a Dilated\nConditional Position Encoding module is proposed to deal with the input\nsequence of variable lengths, and it additionally extracts local layout\ninformation from both textual and visual modalities while generating position\nembeddings. Experiment results show that our XYLayoutLM achieves competitive\nresults on document understanding tasks.</p>\n", "tags": ["CVPR","Tools"] },
{"key": "guan2018story", "citations": "170", "year": "2019", "title":"Story Ending Generation With Incremental Encoding And Commonsense Knowledge", "abstract": "<p>Generating a reasonable ending for a given story context, i.e., story ending\ngeneration, is a strong indication of story comprehension. This task requires\nnot only to understand the context clues which play an important role in\nplanning the plot but also to handle implicit knowledge to make a reasonable,\ncoherent story.\n  In this paper, we devise a novel model for story ending generation. The model\nadopts an incremental encoding scheme to represent context clues which are\nspanning in the story context. In addition, commonsense knowledge is applied\nthrough multi-source attention to facilitate story comprehension, and thus to\nhelp generate coherent and reasonable endings. Through building context clues\nand using implicit knowledge, the model is able to produce reasonable story\nendings. context clues implied in the post and make the inference based on it.\n  Automatic and manual evaluation shows that our model can generate more\nreasonable story endings than state-of-the-art baselines.</p>\n", "tags": ["AAAI","Evaluation","Model Architecture"] },
{"key": "guan2020knowledge", "citations": "235", "year": "2020", "title":"A Knowledge-enhanced Pretraining Model For Commonsense Story Generation", "abstract": "<p>Story generation, namely generating a reasonable story from a leading\ncontext, is an important but challenging task. In spite of the success in\nmodeling fluency and local coherence, existing neural language generation\nmodels (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of\nlong-range coherence in generated stories. We conjecture that this is because\nof the difficulty of associating relevant commonsense knowledge, understanding\nthe causal relationships, and planning entities and events with proper temporal\norder. In this paper, we devise a knowledge-enhanced pretraining model for\ncommonsense story generation. We propose to utilize commonsense knowledge from\nexternal knowledge bases to generate reasonable stories. To further capture the\ncausal and temporal dependencies between the sentences in a reasonable story,\nwe employ multi-task learning which combines a discriminative objective to\ndistinguish true and fake stories during fine-tuning. Automatic and manual\nevaluation shows that our model can generate more reasonable stories than\nstate-of-the-art baselines, particularly in terms of logic and global\ncoherence.</p>\n", "tags": ["Evaluation","Fine-Tuning","Model Architecture","TACL","Training Techniques"] },
{"key": "guanzhi2023voyager", "citations": "131", "year": "2023", "title":"Voyager: An Open-ended Embodied Agent With Large Language Models", "abstract": "<p>We introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent’s abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https://voyager.minedojo.org/.</p>\n", "tags": ["Agentic","Fine-Tuning","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "guha2023legalbench", "citations": "64", "year": "2023", "title":"Legalbench: A Collaboratively Built Benchmark For Measuring Legal Reasoning In Large Language Models", "abstract": "<p>The advent of large language models (LLMs) and their adoption by the legal\ncommunity has given rise to the question: what types of legal reasoning can\nLLMs perform? To enable greater study of this question, we present LegalBench:\na collaboratively constructed legal reasoning benchmark consisting of 162 tasks\ncovering six different types of legal reasoning. LegalBench was built through\nan interdisciplinary process, in which we collected tasks designed and\nhand-crafted by legal professionals. Because these subject matter experts took\na leading role in construction, tasks either measure legal reasoning\ncapabilities that are practically useful, or measure reasoning skills that\nlawyers find interesting. To enable cross-disciplinary conversations about LLMs\nin the law, we additionally show how popular legal frameworks for describing\nlegal reasoning – which distinguish between its many forms – correspond to\nLegalBench tasks, thus giving lawyers and LLM developers a common vocabulary.\nThis paper describes LegalBench, presents an empirical evaluation of 20\nopen-source and commercial LLMs, and illustrates the types of research\nexplorations LegalBench enables.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "guhur2021airbert", "citations": "87", "year": "2021", "title":"Airbert: In-domain Pretraining For Vision-and-language Navigation", "abstract": "<p>Vision-and-language navigation (VLN) aims to enable embodied agents to\nnavigate in realistic environments using natural language instructions. Given\nthe scarcity of domain-specific training data and the high diversity of image\nand language inputs, the generalization of VLN agents to unseen environments\nremains challenging. Recent methods explore pretraining to improve\ngeneralization, however, the use of generic image-caption datasets or existing\nsmall-scale VLN environments is suboptimal and results in limited improvements.\nIn this work, we introduce BnB, a large-scale and diverse in-domain VLN\ndataset. We first collect image-caption (IC) pairs from hundreds of thousands\nof listings from online rental marketplaces. Using IC pairs we next propose\nautomatic strategies to generate millions of VLN path-instruction (PI) pairs.\nWe further propose a shuffling loss that improves the learning of temporal\norder inside PI pairs. We use BnB pretrain our Airbert model that can be\nadapted to discriminative and generative settings and show that it outperforms\nstate of the art for Room-to-Room (R2R) navigation and Remote Referring\nExpression (REVERIE) benchmarks. Moreover, our in-domain pretraining\nsignificantly increases performance on a challenging few-shot VLN evaluation,\nwhere we train the model only on VLN instructions from a few houses.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","ICCV","Training Techniques"] },
{"key": "gui2017question", "citations": "183", "year": "2017", "title":"A Question Answering Approach To Emotion Cause Extraction", "abstract": "<p>Emotion cause extraction aims to identify the reasons behind a certain\nemotion expressed in text. It is a much more difficult task compared to emotion\nclassification. Inspired by recent advances in using deep memory networks for\nquestion answering (QA), we propose a new approach which considers emotion\ncause identification as a reading comprehension task in QA. Inspired by\nconvolutional neural networks, we propose a new mechanism to store relevant\ncontext in different memory slots to model context information. Our proposed\napproach can extract both word level sequence features and lexical features.\nPerformance evaluation shows that our method achieves the state-of-the-art\nperformance on a recently released emotion cause dataset, outperforming a\nnumber of competitive baselines by at least 3.01% in F-measure.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "gui2021kat", "citations": "73", "year": "2022", "title":"KAT: A Knowledge Augmented Transformer For Vision-and-language", "abstract": "<p>The primary focus of recent work with largescale transformers has been on\noptimizing the amount of information packed into the model’s parameters. In\nthis work, we ask a different question: Can multimodal transformers leverage\nexplicit knowledge in their reasoning? Existing, primarily unimodal, methods\nhave explored approaches under the paradigm of knowledge retrieval followed by\nanswer prediction, but leave open questions about the quality and relevance of\nthe retrieved knowledge used, and how the reasoning processes over implicit and\nexplicit knowledge should be integrated. To address these challenges, we\npropose a novel model - Knowledge Augmented Transformer (KAT) - which achieves\na strong state-of-the-art result (+6 points absolute) on the open-domain\nmultimodal task of OK-VQA. Our approach integrates implicit and explicit\nknowledge in an end to end encoder-decoder architecture, while still jointly\nreasoning over both knowledge sources during answer generation. An additional\nbenefit of explicit knowledge integration is seen in improved interpretability\nof model predictions in our analysis.</p>\n", "tags": ["Model Architecture","NAACL","RAG"] },
{"key": "gui2021textflint", "citations": "75", "year": "2021", "title":"Textflint: Unified Multilingual Robustness Evaluation Toolkit For Natural Language Processing", "abstract": "<p>Various robustness evaluation methodologies from different perspectives have\nbeen proposed for different natural language processing (NLP) tasks. These\nmethods have often focused on either universal or task-specific generalization\ncapabilities. In this work, we propose a multilingual robustness evaluation\nplatform for NLP tasks (TextFlint) that incorporates universal text\ntransformation, task-specific transformation, adversarial attack,\nsubpopulation, and their combinations to provide comprehensive robustness\nanalysis. TextFlint enables practitioners to automatically evaluate their\nmodels from all aspects or to customize their evaluations as desired with just\na few lines of code. To guarantee user acceptability, all the text\ntransformations are linguistically based, and we provide a human evaluation for\neach one. TextFlint generates complete analytical reports as well as targeted\naugmented data to address the shortcomings of the model’s robustness. To\nvalidate TextFlint’s utility, we performed large-scale empirical evaluations\n(over 67,000 evaluations) on state-of-the-art deep learning models, classic\nsupervised methods, and real-world systems. Almost all models showed\nsignificant performance degradation, including a decline of more than 50% of\nBERT’s prediction accuracy on tasks such as aspect-level sentiment\nclassification, named entity recognition, and natural language inference.\nTherefore, we call for the robustness to be included in the model evaluation,\nso as to promote the healthy development of NLP technology.</p>\n", "tags": ["Evaluation","Tools"] },
{"key": "gulordava2018colorless", "citations": "559", "year": "2018", "title":"Colorless Green Recurrent Networks Dream Hierarchically", "abstract": "<p>Recurrent neural networks (RNNs) have achieved impressive results in a\nvariety of linguistic processing tasks, suggesting that they can induce\nnon-trivial properties of language. We investigate here to what extent RNNs\nlearn to track abstract hierarchical syntactic structure. We test whether RNNs\ntrained with a generic language modeling objective in four languages (Italian,\nEnglish, Hebrew, Russian) can predict long-distance number agreement in various\nconstructions. We include in our evaluation nonsensical sentences where RNNs\ncannot rely on semantic or lexical cues (“The colorless green ideas I ate with\nthe chair sleep furiously”), and, for Italian, we compare model performance to\nhuman intuitions. Our language-model-trained RNNs make reliable predictions\nabout long-distance agreement, and do not lag much behind human performance. We\nthus bring support to the hypothesis that RNNs are not just shallow-pattern\nextractors, but they also acquire deeper grammatical competence.</p>\n", "tags": ["Evaluation","NAACL"] },
{"key": "gunasekar2023textbooks", "citations": "82", "year": "2023", "title":"Textbooks Are All You Need", "abstract": "<p>We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality” data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.</p>\n", "tags": ["Datasets","Llm For Code","Model Architecture"] },
{"key": "gunel2020supervised", "citations": "220", "year": "2020", "title":"Supervised Contrastive Learning For Pre-trained Language Model Fine-tuning", "abstract": "<p>State-of-the-art natural language understanding classification models follow\ntwo-stages: pre-training a large language model on an auxiliary task, and then\nfine-tuning the model on a task-specific labeled dataset using cross-entropy\nloss. However, the cross-entropy loss has several shortcomings that can lead to\nsub-optimal generalization and instability. Driven by the intuition that good\ngeneralization requires capturing the similarity between examples in one class\nand contrasting them with examples in other classes, we propose a supervised\ncontrastive learning (SCL) objective for the fine-tuning stage. Combined with\ncross-entropy, our proposed SCL loss obtains significant improvements over a\nstrong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in\nfew-shot learning settings, without requiring specialized architecture, data\naugmentations, memory banks, or additional unsupervised data. Our proposed\nfine-tuning objective leads to models that are more robust to different levels\nof noise in the fine-tuning training data, and can generalize better to related\ntasks with limited labeled data.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "guo2017deepfm", "citations": "1894", "year": "2017", "title":"Deepfm: A Factorization-machine Based Neural Network For CTR Prediction", "abstract": "<p>Learning sophisticated feature interactions behind user behaviors is critical\nin maximizing CTR for recommender systems. Despite great progress, existing\nmethods seem to have a strong bias towards low- or high-order interactions, or\nrequire expertise feature engineering. In this paper, we show that it is\npossible to derive an end-to-end learning model that emphasizes both low- and\nhigh-order feature interactions. The proposed model, DeepFM, combines the power\nof factorization machines for recommendation and deep learning for feature\nlearning in a new neural network architecture. Compared to the latest Wide \\&amp;\nDeep model from Google, DeepFM has a shared input to its “wide” and “deep”\nparts, with no need of feature engineering besides raw features. Comprehensive\nexperiments are conducted to demonstrate the effectiveness and efficiency of\nDeepFM over the existing models for CTR prediction, on both benchmark data and\ncommercial data.</p>\n", "tags": ["Datasets","Evaluation","IJCAI","Model Architecture"] },
{"key": "guo2017long", "citations": "419", "year": "2018", "title":"Long Text Generation Via Adversarial Training With Leaked Information", "abstract": "<p>Automatically generating coherent and semantically meaningful text has many\napplications in machine translation, dialogue systems, image captioning, etc.\nRecently, by combining with policy gradient, Generative Adversarial Nets (GAN)\nthat use a discriminative model to guide the training of the generative model\nas a reinforcement learning policy has shown promising results in text\ngeneration. However, the scalar guiding signal is only available after the\nentire text has been generated and lacks intermediate information about text\nstructure during the generative process. As such, it limits its success when\nthe length of the generated text samples is long (more than 20 words). In this\npaper, we propose a new framework, called LeakGAN, to address the problem for\nlong text generation. We allow the discriminative net to leak its own\nhigh-level extracted features to the generative net to further help the\nguidance. The generator incorporates such informative signals into all\ngeneration steps through an additional Manager module, which takes the\nextracted features of current generated words and outputs a latent vector to\nguide the Worker module for next-word generation. Our extensive experiments on\nsynthetic data and various real-world tasks with Turing test demonstrate that\nLeakGAN is highly effective in long text generation and also improves the\nperformance in short text generation scenarios. More importantly, without any\nsupervision, LeakGAN would be able to implicitly learn sentence structures only\nthrough the interaction between Manager and Worker.</p>\n", "tags": ["AAAI","Applications","Dialogue & Multi Turn","Reinforcement Learning","Security","Tools","Training Techniques"] },
{"key": "guo2018dialog", "citations": "91", "year": "2018", "title":"Dialog-based Interactive Image Retrieval", "abstract": "<p>Existing methods for interactive image retrieval have demonstrated the merit\nof integrating user feedback, improving retrieval results. However, most\ncurrent systems rely on restricted forms of user feedback, such as binary\nrelevance responses, or feedback based on a fixed set of relative attributes,\nwhich limits their impact. In this paper, we introduce a new approach to\ninteractive image search that enables users to provide feedback via natural\nlanguage, allowing for more natural and effective interaction. We formulate the\ntask of dialog-based interactive image retrieval as a reinforcement learning\nproblem, and reward the dialog system for improving the rank of the target\nimage during each dialog turn. To mitigate the cumbersome and costly process of\ncollecting human-machine conversations as the dialog system learns, we train\nour system with a user simulator, which is itself trained to describe the\ndifferences between target and candidate images. The efficacy of our approach\nis demonstrated in a footwear retrieval application. Experiments on both\nsimulated and real-world data show that 1) our proposed learning framework\nachieves better accuracy than other supervised and reinforcement learning\nbaselines and 2) user feedback based on natural language rather than\npre-specified attributes leads to more effective retrieval results, and a more\nnatural and expressive communication interface.</p>\n", "tags": ["Dialogue & Multi Turn","Reinforcement Learning","Retrieval Systems","Tools"] },
{"key": "guo2018non", "citations": "121", "year": "2019", "title":"Non-autoregressive Neural Machine Translation With Enhanced Decoder Input", "abstract": "<p>Non-autoregressive translation (NAT) models, which remove the dependence on\nprevious target tokens from the inputs of the decoder, achieve significantly\ninference speedup but at the cost of inferior accuracy compared to\nautoregressive translation (AT) models. Previous work shows that the quality of\nthe inputs of the decoder is important and largely impacts the model accuracy.\nIn this paper, we propose two methods to enhance the decoder inputs so as to\nimprove NAT models. The first one directly leverages a phrase table generated\nby conventional SMT approaches to translate source tokens to target tokens,\nwhich are then fed into the decoder as inputs. The second one transforms\nsource-side word embeddings to target-side word embeddings through\nsentence-level alignment and word-level adversary learning, and then feeds the\ntransformed word embeddings into the decoder as inputs. Experimental results\nshow our method largely outperforms the NAT baseline~\\citep{gu2017non} by\n\\(5.11\\) BLEU scores on WMT14 English-German task and \\(4.72\\) BLEU scores on WMT16\nEnglish-Romanian task.</p>\n", "tags": ["AAAI"] },
{"key": "guo2018soft", "citations": "143", "year": "2018", "title":"Soft Layer-specific Multi-task Summarization With Entailment And Question Generation", "abstract": "<p>An accurate abstractive summary of a document should contain all its salient\ninformation and should be logically entailed by the input document. We improve\nthese important aspects of abstractive summarization via multi-task learning\nwith the auxiliary tasks of question generation and entailment generation,\nwhere the former teaches the summarization model how to look for salient\nquestioning-worthy details, and the latter teaches the model how to rewrite a\nsummary which is a directed-logical subset of the input document. We also\npropose novel multi-task architectures with high-level (semantic)\nlayer-specific sharing across multiple encoder and decoder layers of the three\ntasks, as well as soft-sharing mechanisms (and show performance ablations and\nanalysis examples of each contribution). Overall, we achieve statistically\nsignificant improvements over the state-of-the-art on both the CNN/DailyMail\nand Gigaword datasets, as well as on the DUC-2002 transfer setup. We also\npresent several quantitative and qualitative analysis studies of our model’s\nlearned saliency and entailment skills.</p>\n", "tags": ["Datasets"] },
{"key": "guo2019fine", "citations": "68", "year": "2020", "title":"Fine-tuning By Curriculum Learning For Non-autoregressive Neural Machine Translation", "abstract": "<p>Non-autoregressive translation (NAT) models remove the dependence on previous\ntarget tokens and generate all target tokens in parallel, resulting in\nsignificant inference speedup but at the cost of inferior translation accuracy\ncompared to autoregressive translation (AT) models. Considering that AT models\nhave higher accuracy and are easier to train than NAT models, and both of them\nshare the same model configurations, a natural idea to improve the accuracy of\nNAT models is to transfer a well-trained AT model to an NAT model through\nfine-tuning. However, since AT and NAT models differ greatly in training\nstrategy, straightforward fine-tuning does not work well. In this work, we\nintroduce curriculum learning into fine-tuning for NAT. Specifically, we design\na curriculum in the fine-tuning process to progressively switch the training\nfrom autoregressive generation to non-autoregressive generation. Experiments on\nfour benchmark translation datasets show that the proposed method achieves good\nimprovement (more than \\(1\\) BLEU score) over previous NAT baselines in terms of\ntranslation accuracy, and greatly speed up (more than \\(10\\) times) the inference\nprocess over AT baselines.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "guo2019gluoncv", "citations": "167", "year": "2020", "title":"Gluoncv And Gluonnlp: Deep Learning In Computer Vision And Natural Language Processing", "abstract": "<p>We present GluonCV and GluonNLP, the deep learning toolkits for computer\nvision and natural language processing based on Apache MXNet (incubating).\nThese toolkits provide state-of-the-art pre-trained models, training scripts,\nand training logs, to facilitate rapid prototyping and promote reproducible\nresearch. We also provide modular APIs with flexible building blocks to enable\nefficient customization. Leveraging the MXNet ecosystem, the deep learning\nmodels in GluonCV and GluonNLP can be deployed onto a variety of platforms with\ndifferent programming languages. The Apache 2.0 license has been adopted by\nGluonCV and GluonNLP to allow for software distribution, modification, and\nusage.</p>\n", "tags": ["Training Techniques"] },
{"key": "guo2019image", "citations": "80", "year": "2019", "title":"Image-question-answer Synergistic Network For Visual Dialog", "abstract": "<p>The image, question (combined with the history for de-referencing), and the\ncorresponding answer are three vital components of visual dialog. Classical\nvisual dialog systems integrate the image, question, and history to search for\nor generate the best matched answer, and so, this approach significantly\nignores the role of the answer. In this paper, we devise a novel\nimage-question-answer synergistic network to value the role of the answer for\nprecise visual dialog. We extend the traditional one-stage solution to a\ntwo-stage solution. In the first stage, candidate answers are coarsely scored\naccording to their relevance to the image and question pair. Afterward, in the\nsecond stage, answers with high probability of being correct are re-ranked by\nsynergizing with image and question. On the Visual Dialog v1.0 dataset, the\nproposed synergistic network boosts the discriminative visual dialog model to\nachieve a new state-of-the-art of 57.88% normalized discounted cumulative\ngain. A generative visual dialog model equipped with the proposed technique\nalso shows promising improvements.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "guo2019star", "citations": "137", "year": "2019", "title":"Star-transformer", "abstract": "<p>Although Transformer has achieved great successes on many NLP tasks, its\nheavy structure with fully-connected attention connections leads to\ndependencies on large training data. In this paper, we present\nStar-Transformer, a lightweight alternative by careful sparsification. To\nreduce model complexity, we replace the fully-connected structure with a\nstar-shaped topology, in which every two non-adjacent nodes are connected\nthrough a shared relay node. Thus, complexity is reduced from quadratic to\nlinear, while preserving capacity to capture both local composition and\nlong-range dependency. The experiments on four tasks (22 datasets) show that\nStar-Transformer achieved significant improvements against the standard\nTransformer for the modestly sized datasets.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "guo2020graphcodebert", "citations": "326", "year": "2021", "title":"Graphcodebert: Pre-training Code Representations With Data Flow", "abstract": "<p>Pre-trained models for programming language have achieved dramatic empirical\nimprovements on a variety of code-related tasks such as code search, code\ncompletion, code summarization, etc. However, existing pre-trained models\nregard a code snippet as a sequence of tokens, while ignoring the inherent\nstructure of code, which provides crucial code semantics and would enhance the\ncode understanding process. We present GraphCodeBERT, a pre-trained model for\nprogramming language that considers the inherent structure of code. Instead of\ntaking syntactic-level structure of code like abstract syntax tree (AST), we\nuse data flow in the pre-training stage, which is a semantic-level structure of\ncode that encodes the relation of “where-the-value-comes-from” between\nvariables. Such a semantic-level structure is neat and does not bring an\nunnecessarily deep hierarchy of AST, the property of which makes the model more\nefficient. We develop GraphCodeBERT based on Transformer. In addition to using\nthe task of masked language modeling, we introduce two structure-aware\npre-training tasks. One is to predict code structure edges, and the other is to\nalign representations between source code and code structure. We implement the\nmodel in an efficient way with a graph-guided masked attention function to\nincorporate the code structure. We evaluate our model on four tasks, including\ncode search, clone detection, code translation, and code refinement. Results\nshow that code structure and newly introduced pre-training tasks can improve\nGraphCodeBERT and achieves state-of-the-art performance on the four downstream\ntasks. We further show that the model prefers structure-level attentions over\ntoken-level attentions in the task of code search.</p>\n", "tags": ["Llm For Code","Model Architecture","Training Techniques"] },
{"key": "guo2020parameter", "citations": "158", "year": "2021", "title":"Parameter-efficient Transfer Learning With Diff Pruning", "abstract": "<p>While task-specific finetuning of pretrained networks has led to significant\nempirical advances in NLP, the large size of networks makes finetuning\ndifficult to deploy in multi-task, memory-constrained settings. We propose diff\npruning as a simple approach to enable parameter-efficient transfer learning\nwithin the pretrain-finetune framework. This approach views finetuning as\nlearning a task-specific diff vector that is applied on top of the pretrained\nparameter vector, which remains fixed and is shared across different tasks. The\ndiff vector is adaptively pruned during training with a differentiable\napproximation to the L0-norm penalty to encourage sparsity. Diff pruning\nbecomes parameter-efficient as the number of tasks increases, as it requires\nstoring only the nonzero positions and weights of the diff vector for each\ntask, while the cost of storing the shared pretrained model remains constant.\nIt further does not require access to all tasks during training, which makes it\nattractive in settings where tasks arrive in stream or the set of tasks is\nunknown. We find that models finetuned with diff pruning can match the\nperformance of fully finetuned baselines on the GLUE benchmark while only\nmodifying 0.5% of the pretrained model’s parameters per task.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Fine-Tuning","Tools","Training Techniques"] },
{"key": "guo2020sequence", "citations": "71", "year": "2020", "title":"Sequence-level Mixed Sample Data Augmentation", "abstract": "<p>Despite their empirical success, neural networks still have difficulty\ncapturing compositional aspects of natural language. This work proposes a\nsimple data augmentation approach to encourage compositional behavior in neural\nmodels for sequence-to-sequence problems. Our approach, SeqMix, creates new\nsynthetic examples by softly combining input/output sequences from the training\nset. We connect this approach to existing techniques such as SwitchOut and word\ndropout, and show that these techniques are all approximating variants of a\nsingle objective. SeqMix consistently yields approximately 1.0 BLEU improvement\non five different translation datasets over strong Transformer baselines. On\ntasks that require strong compositional generalization such as SCAN and\nsemantic parsing, SeqMix also offers further improvements.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "guo2021longt5", "citations": "151", "year": "2022", "title":"Longt5: Efficient Text-to-text Transformer For Long Sequences", "abstract": "<p>Recent work has shown that either (1) increasing the input length or (2)\nincreasing model size can improve the performance of Transformer-based neural\nmodels. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same\ntime. Specifically, we integrated attention ideas from long-input transformers\n(ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention\nmechanism we call {\\em Transient Global} (TGlobal), which mimics ETC’s\nlocal/global attention mechanism, but without requiring additional side-inputs.\nWe are able to achieve state-of-the-art results on several summarization tasks\nand outperform the original T5 models on question answering tasks.</p>\n", "tags": ["Efficiency","Model Architecture","NAACL","Training Techniques"] },
{"key": "guo2022tm2t", "citations": "87", "year": "2022", "title":"TM2T: Stochastic And Tokenized Modeling For The Reciprocal Generation Of 3D Human Motions And Texts", "abstract": "<p>Inspired by the strong ties between vision and language, the two intimate\nhuman sensing and communication modalities, our paper aims to explore the\ngeneration of 3D human full-body motions from texts, as well as its reciprocal\ntask, shorthanded for text2motion and motion2text, respectively. To tackle the\nexisting challenges, especially to enable the generation of multiple distinct\nmotions from the same text, and to avoid the undesirable production of trivial\nmotionless pose sequences, we propose the use of motion token, a discrete and\ncompact motion representation. This provides one level playing ground when\nconsidering both motions and text signals, as the motion and text tokens,\nrespectively. Moreover, our motion2text module is integrated into the inverse\nalignment process of our text2motion training pipeline, where a significant\ndeviation of synthesized text from the input text would be penalized by a large\ntraining loss; empirically this is shown to effectively improve performance.\nFinally, the mappings in-between the two modalities of motions and texts are\nfacilitated by adapting the neural model for machine translation (NMT) to our\ncontext. This autoregressive modeling of the distribution over discrete motion\ntokens further enables non-deterministic production of pose sequences, of\nvariable lengths, from an input text. Our approach is flexible, could be used\nfor both text2motion and motion2text tasks. Empirical evaluations on two\nbenchmark datasets demonstrate the superior performance of our approach on both\ntasks over a variety of state-of-the-art methods. Project page:\nhttps://ericguo5513.github.io/TM2T/</p>\n", "tags": ["Datasets","Evaluation","Has Code","Training Techniques"] },
{"key": "guo2022unixcoder", "citations": "288", "year": "2022", "title":"Unixcoder: Unified Cross-modal Pre-training For Code Representation", "abstract": "<p>Pre-trained models for programming languages have recently demonstrated great\nsuccess on code intelligence. To support both code-related understanding and\ngeneration tasks, recent works attempt to pre-train unified encoder-decoder\nmodels. However, such encoder-decoder framework is sub-optimal for\nauto-regressive tasks, especially code completion that requires a decoder-only\nmanner for efficient inference. In this paper, we present UniXcoder, a unified\ncross-modal pre-trained model for programming language. The model utilizes mask\nattention matrices with prefix adapters to control the behavior of the model\nand leverages cross-modal contents like AST and code comment to enhance code\nrepresentation. To encode AST that is represented as a tree in parallel, we\npropose a one-to-one mapping method to transform AST in a sequence structure\nthat retains all structural information from the tree. Furthermore, we propose\nto utilize multi-modal contents to learn representation of code fragment with\ncontrastive learning, and then align representations among programming\nlanguages using a cross-modal generation task. We evaluate UniXcoder on five\ncode-related tasks over nine datasets. To further evaluate the performance of\ncode fragment representation, we also construct a dataset for a new task,\ncalled zero-shot code-to-code search. Results show that our model achieves\nstate-of-the-art performance on most tasks and analysis reveals that comment\nand AST can both enhance UniXcoder.</p>\n", "tags": ["Datasets","Llm For Code","Model Architecture","Tools","Training Techniques"] },
{"key": "guo2023what", "citations": "61", "year": "2023", "title":"What Can Large Language Models Do In Chemistry? A Comprehensive Benchmark On Eight Tasks", "abstract": "<p>Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs’ performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Has Code","In Context Learning","Llm For Code","Model Architecture"] },
{"key": "guo2024large", "citations": "76", "year": "2024", "title":"Large Language Model For Mental Health: A Systematic Review", "abstract": "<p>Large language models (LLMs) have attracted significant attention for\npotential applications in digital health, while their application in mental\nhealth is subject to ongoing debate. This systematic review aims to evaluate\nthe usage of LLMs in mental health, focusing on their strengths and limitations\nin early screening, digital interventions, and clinical applications. Adhering\nto PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM\nusing keywords: ‘mental health OR mental illness OR mental disorder OR\npsychiatry’ AND ‘large language models’. We included articles published between\nJanuary 1, 2017, and April 30, 2024, excluding non-English articles. 30\narticles were evaluated, which included research on mental health conditions\nand suicidal ideation detection through text (n=15), usage of LLMs for mental\nhealth conversational agents (CAs) (n=7), and other applications and\nevaluations of LLMs in mental health (n=18). LLMs exhibit substantial\neffectiveness in detecting mental health issues and providing accessible,\nde-stigmatized eHealth services. However, the current risks associated with the\nclinical use might surpass their benefits. The study identifies several\nsignificant issues: the lack of multilingual datasets annotated by experts,\nconcerns about the accuracy and reliability of the content generated,\nchallenges in interpretability due to the ‘black box’ nature of LLMs, and\npersistent ethical dilemmas. These include the lack of a clear ethical\nframework, concerns about data privacy, and the potential for over-reliance on\nLLMs by both therapists and patients, which could compromise traditional\nmedical practice. Despite these issues, the rapid development of LLMs\nunderscores their potential as new clinical aids, emphasizing the need for\ncontinued research and development in this area.</p>\n", "tags": ["Applications","Datasets","IJCAI","Model Architecture","Survey Paper","Tools"] },
{"key": "gupta2017deep", "citations": "185", "year": "2018", "title":"A Deep Generative Framework For Paraphrase Generation", "abstract": "<p>Paraphrase generation is an important problem in NLP, especially in question\nanswering, information retrieval, information extraction, conversation systems,\nto name a few. In this paper, we address the problem of generating paraphrases\nautomatically. Our proposed method is based on a combination of deep generative\nmodels (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases,\ngiven an input sentence. Traditional VAEs when combined with recurrent neural\nnetworks can generate free text but they are not suitable for paraphrase\ngeneration for a given sentence. We address this problem by conditioning the\nboth, encoder and decoder sides of VAE, on the original sentence, so that it\ncan generate the given sentence’s paraphrases. Unlike most existing models, our\nmodel is simple, modular and can generate multiple paraphrases, for a given\nsentence. Quantitative evaluation of the proposed method on a benchmark\nparaphrase dataset demonstrates its efficacy, and its performance improvement\nover the state-of-the-art methods by a significant margin, whereas qualitative\nhuman evaluation indicate that the generated paraphrases are well-formed,\ngrammatically correct, and are relevant to the input sentence. Furthermore, we\nevaluate our method on a newly released question paraphrase dataset, and\nestablish a new baseline for future research.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "gupta2018imagine", "citations": "64", "year": "2018", "title":"Imagine This! Scripts To Compositions To Videos", "abstract": "<p>Imagining a scene described in natural language with realistic layout and\nappearance of entities is the ultimate test of spatial, visual, and semantic\nworld knowledge. Towards this goal, we present the Composition, Retrieval, and\nFusion Network (CRAFT), a model capable of learning this knowledge from\nvideo-caption data and applying it while generating videos from novel captions.\nCRAFT explicitly predicts a temporal-layout of mentioned entities (characters\nand objects), retrieves spatio-temporal entity segments from a video database\nand fuses them to generate scene videos. Our contributions include sequential\ntraining of components of CRAFT while jointly modeling layout and appearances,\nand losses that encourage learning compositional representations for retrieval.\nWe evaluate CRAFT on semantic fidelity to caption, composition consistency, and\nvisual quality. CRAFT outperforms direct pixel generation approaches and\ngeneralizes well to unseen captions and to unseen video databases with no text\nannotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated\nvideo-caption dataset with over 25000 videos. For a glimpse of videos generated\nby CRAFT, see https://youtu.be/688Vv86n0z8.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "gupta2018semantic", "citations": "189", "year": "2018", "title":"Semantic Parsing For Task Oriented Dialog Using Hierarchical Representations", "abstract": "<p>Task oriented dialog systems typically first parse user utterances to\nsemantic frames comprised of intents and slots. Previous work on task oriented\nintent and slot-filling work has been restricted to one intent per query and\none slot label per token, and thus cannot model complex compositional requests.\nAlternative semantic parsing systems have represented queries as logical forms,\nbut these are challenging to annotate and parse. We propose a hierarchical\nannotation scheme for semantic parsing that allows the representation of\ncompositional queries, and can be efficiently and accurately parsed by standard\nconstituency parsing models. We release a dataset of 44k annotated queries\n(fb.me/semanticparsingdialog), and show that parsing models outperform\nsequence-to-sequence approaches on this dataset.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","EMNLP"] },
{"key": "gupta2019neural", "citations": "73", "year": "2019", "title":"Neural Module Networks For Reasoning Over Text", "abstract": "<p>Answering compositional questions that require multiple steps of reasoning\nagainst text is challenging, especially when they involve discrete, symbolic\noperations. Neural module networks (NMNs) learn to parse such questions as\nexecutable programs composed of learnable modules, performing well on synthetic\nvisual QA domains. However, we find that it is challenging to learn these\nmodels for non-synthetic questions on open-domain text, where a model needs to\ndeal with the diversity of natural language and perform a broader range of\nreasoning. We extend NMNs by: (a) introducing modules that reason over a\nparagraph of text, performing symbolic reasoning (such as arithmetic, sorting,\ncounting) over numbers and dates in a probabilistic and differentiable manner;\nand (b) proposing an unsupervised auxiliary loss to help extract arguments\nassociated with the events in text. Additionally, we show that a limited amount\nof heuristically-obtained question program and intermediate module output\nsupervision provides sufficient inductive bias for accurate learning. Our\nproposed model significantly outperforms state-of-the-art models on a subset of\nthe DROP dataset that poses a variety of reasoning challenges that are covered\nby our modules.</p>\n", "tags": ["Datasets","Ethics & Fairness"] },
{"key": "gupta2020contrastive", "citations": "114", "year": "2020", "title":"Contrastive Learning For Weakly Supervised Phrase Grounding", "abstract": "<p>Phrase grounding, the problem of associating image regions to caption words,\nis a crucial component of vision-language tasks. We show that phrase grounding\ncan be learned by optimizing word-region attention to maximize a lower bound on\nmutual information between images and caption words. Given pairs of images and\ncaptions, we maximize compatibility of the attention-weighted regions and the\nwords in the corresponding caption, compared to non-corresponding pairs of\nimages and captions. A key idea is to construct effective negative captions for\nlearning through language model guided word substitutions. Training with our\nnegatives yields a \\(\\sim10%\\) absolute gain in accuracy over randomly-sampled\nnegatives from the training data. Our weakly supervised phrase grounding model\ntrained on COCO-Captions shows a healthy gain of \\(5.7%\\) to achieve \\(76.7%\\)\naccuracy on Flickr30K Entities benchmark.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "gupta2020infotabs", "citations": "78", "year": "2020", "title":"INFOTABS: Inference On Tables As Semi-structured Data", "abstract": "<p>In this paper, we observe that semi-structured tabulated text is ubiquitous;\nunderstanding them requires not only comprehending the meaning of text\nfragments, but also implicit relationships between them. We argue that such\ndata can prove as a testing ground for understanding how we reason about\ninformation. To study this, we introduce a new dataset called INFOTABS,\ncomprising of human-written textual hypotheses based on premises that are\ntables extracted from Wikipedia info-boxes. Our analysis shows that the\nsemi-structured, multi-domain and heterogeneous nature of the premises admits\ncomplex, multi-faceted reasoning. Experiments reveal that, while human\nannotators agree on the relationships between a table-hypothesis pair, several\nstandard modeling strategies are unsuccessful at the task, suggesting that\nreasoning about tables can pose a difficult modeling challenge.</p>\n", "tags": ["Datasets"] },
{"key": "gupta2022visual", "citations": "102", "year": "2023", "title":"Visual Programming: Compositional Visual Reasoning Without Training", "abstract": "<p>We present VISPROG, a neuro-symbolic approach to solving complex and\ncompositional visual tasks given natural language instructions. VISPROG avoids\nthe need for any task-specific training. Instead, it uses the in-context\nlearning ability of large language models to generate python-like modular\nprograms, which are then executed to get both the solution and a comprehensive\nand interpretable rationale. Each line of the generated program may invoke one\nof several off-the-shelf computer vision models, image processing routines, or\npython functions to produce intermediate outputs that may be consumed by\nsubsequent parts of the program. We demonstrate the flexibility of VISPROG on 4\ndiverse tasks - compositional visual question answering, zero-shot reasoning on\nimage pairs, factual knowledge object tagging, and language-guided image\nediting. We believe neuro-symbolic approaches like VISPROG are an exciting\navenue to easily and effectively expand the scope of AI systems to serve the\nlong tail of complex tasks that people may wish to perform.</p>\n", "tags": ["CVPR","Training Techniques"] },
{"key": "gutiérrez2022thinking", "citations": "80", "year": "2022", "title":"Thinking About GPT-3 In-context Learning For Biomedical IE? Think Again", "abstract": "<p>The strong few-shot in-context learning capability of large pre-trained\nlanguage models (PLMs) such as GPT-3 is highly appealing for application\ndomains such as biomedicine, which feature high and diverse demands of language\ntechnologies but also high data annotation costs. In this paper, we present the\nfirst systematic and comprehensive study to compare the few-shot performance of\nGPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on\ntwo highly representative biomedical information extraction tasks, named entity\nrecognition and relation extraction. We follow the true few-shot setting to\navoid overestimating models’ few-shot performance by model selection over a\nlarge validation set. We also optimize GPT-3’s performance with known\ntechniques such as contextual calibration and dynamic in-context example\nretrieval. However, our results show that GPT-3 still significantly\nunderperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3\nin-context learning also yields smaller gains in accuracy when more training\ndata becomes available. Our in-depth analyses further reveal issues of the\nin-context learning setting that may be detrimental to information extraction\ntasks in general. Given the high cost of experimenting with GPT-3, we hope our\nstudy provides guidance for biomedical researchers and practitioners towards\nmore promising directions such as fine-tuning small PLMs.</p>\n", "tags": ["Datasets","EMNLP","Few-Shot","Fine-Tuning","In Context Learning","Model Architecture","Training Techniques"] },
{"key": "guu2017generating", "citations": "306", "year": "2018", "title":"Generating Sentences By Editing Prototypes", "abstract": "<p>We propose a new generative model of sentences that first samples a prototype\nsentence from the training corpus and then edits it into a new sentence.\nCompared to traditional models that generate from scratch either left-to-right\nor by first sampling a latent sentence vector, our prototype-then-edit model\nimproves perplexity on language modeling and generates higher quality outputs\naccording to human evaluation. Furthermore, the model gives rise to a latent\nedit vector that captures interpretable semantics such as sentence similarity\nand sentence-level analogies.</p>\n", "tags": ["Datasets","Evaluation","TACL","Training Techniques"] },
{"key": "guu2017language", "citations": "159", "year": "2017", "title":"From Language To Programs: Bridging Reinforcement Learning And Maximum Marginal Likelihood", "abstract": "<p>Our goal is to learn a semantic parser that maps natural language utterances\ninto executable programs when only indirect supervision is available: examples\nare labeled with the correct execution result, but not the program itself.\nConsequently, we must search the space of programs for those that output the\ncorrect result, while not being misled by spurious programs: incorrect programs\nthat coincidentally output the correct result. We connect two common learning\nparadigms, reinforcement learning (RL) and maximum marginal likelihood (MML),\nand then present a new learning algorithm that combines the strengths of both.\nThe new algorithm guards against spurious programs by combining the systematic\nsearch traditionally employed in MML with the randomized exploration of RL, and\nby updating parameters such that probability is spread more evenly across\nconsistent programs. We apply our learning algorithm to a new neural semantic\nparser and show significant gains over existing state-of-the-art results on a\nrecent context-dependent semantic parsing task.</p>\n", "tags": ["Agentic","Reinforcement Learning"] },
{"key": "guu2020realm", "citations": "516", "year": "2020", "title":"REALM: Retrieval-augmented Language Model Pre-training", "abstract": "<p>Language model pre-training has been shown to capture a surprising amount of\nworld knowledge, crucial for NLP tasks such as question answering. However,\nthis knowledge is stored implicitly in the parameters of a neural network,\nrequiring ever-larger networks to cover more facts.\n  To capture knowledge in a more modular and interpretable way, we augment\nlanguage model pre-training with a latent knowledge retriever, which allows the\nmodel to retrieve and attend over documents from a large corpus such as\nWikipedia, used during pre-training, fine-tuning and inference. For the first\ntime, we show how to pre-train such a knowledge retriever in an unsupervised\nmanner, using masked language modeling as the learning signal and\nbackpropagating through a retrieval step that considers millions of documents.\n  We demonstrate the effectiveness of Retrieval-Augmented Language Model\npre-training (REALM) by fine-tuning on the challenging task of Open-domain\nQuestion Answering (Open-QA). We compare against state-of-the-art models for\nboth explicit and implicit knowledge storage on three popular Open-QA\nbenchmarks, and find that we outperform all previous methods by a significant\nmargin (4-16% absolute accuracy), while also providing qualitative benefits\nsuch as interpretability and modularity.</p>\n", "tags": ["Datasets","Fine-Tuning","RAG","Retrieval Systems","Training Techniques"] },
{"key": "haase2023artificial", "citations": "103", "year": "2023", "title":"Artificial Muses: Generative Artificial Intelligence Chatbots Have Risen To Human-level Creativity", "abstract": "<p>A widespread view is that Artificial Intelligence cannot be creative. We\ntested this assumption by comparing human-generated ideas with those generated\nby six Generative Artificial Intelligence (GAI) chatbots: \\(alpa.!ai\\),\n\\(Copy.!ai\\), ChatGPT (versions 3 and 4), \\(Studio.!ai\\), and YouChat. Humans and\na specifically trained AI independently assessed the quality and quantity of\nideas. We found no qualitative difference between AI and human-generated\ncreativity, although there are differences in how ideas are generated.\nInterestingly, 9.4 percent of humans were more creative than the most creative\nGAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the\ncreative process. Continued research and development of GAI in creative tasks\nis crucial to fully understand this technology’s potential benefits and\ndrawbacks in shaping the future of creativity. Finally, we discuss the question\nof whether GAIs are capable of being truly creative.</p>\n", "tags": ["Model Architecture"] },
{"key": "haber2019photobook", "citations": "73", "year": "2019", "title":"The Photobook Dataset: Building Common Ground Through Visually-grounded Dialogue", "abstract": "<p>This paper introduces the PhotoBook dataset, a large-scale collection of\nvisually-grounded, task-oriented dialogues in English designed to investigate\nshared dialogue history accumulating during conversation. Taking inspiration\nfrom seminal work on dialogue analysis, we propose a data-collection task\nformulated as a collaborative game prompting two online participants to refer\nto images utilising both their visual context as well as previously established\nreferring expressions. We provide a detailed description of the task setup and\na thorough analysis of the 2,500 dialogues collected. To further illustrate the\nnovel features of the dataset, we propose a baseline model for reference\nresolution which uses a simple method to take into account shared information\naccumulated in a reference chain. Our results show that this information is\nparticularly important to resolve later descriptions and underline the need to\ndevelop more sophisticated models of common ground in dialogue interaction.</p>\n", "tags": ["Datasets","Prompting"] },
{"key": "habernal2017argument", "citations": "127", "year": "2018", "title":"The Argument Reasoning Comprehension Task: Identification And Reconstruction Of Implicit Warrants", "abstract": "<p>Reasoning is a crucial part of natural language argumentation. To comprehend\nan argument, one must analyze its warrant, which explains why its claim follows\nfrom its premises. As arguments are highly contextualized, warrants are usually\npresupposed and left implicit. Thus, the comprehension does not only require\nlanguage understanding and logic skills, but also depends on common sense. In\nthis paper we develop a methodology for reconstructing warrants systematically.\nWe operationalize it in a scalable crowdsourcing process, resulting in a freely\nlicensed dataset with warrants for 2k authentic arguments from news comments.\nOn this basis, we present a new challenging task, the argument reasoning\ncomprehension task. Given an argument with a claim and a premise, the goal is\nto choose the correct implicit warrant from two options. Both warrants are\nplausible and lexically close, but lead to contradicting claims. A solution to\nthis task will define a substantial step towards automatic warrant\nreconstruction. However, experiments with several neural attention and language\nmodels reveal that current approaches do not suffice.</p>\n", "tags": ["Datasets","Model Architecture","NAACL"] },
{"key": "hacker2023regulating", "citations": "295", "year": "2023", "title":"Regulating Chatgpt And Other Large Generative AI Models", "abstract": "<p>Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable\nDiffusion, are rapidly transforming the way we communicate, illustrate, and\ncreate. However, AI regulation, in the EU and beyond, has primarily focused on\nconventional AI models, not LGAIMs. This paper will situate these new\ngenerative models in the current debate on trustworthy AI regulation, and ask\nhow the law can be tailored to their capabilities. After laying technical\nfoundations, the legal part of the paper proceeds in four steps, covering (1)\ndirect regulation, (2) data protection, (3) content moderation, and (4) policy\nproposals. It suggests a novel terminology to capture the AI value chain in\nLGAIM settings by differentiating between LGAIM developers, deployers,\nprofessional and non-professional users, as well as recipients of LGAIM output.\nWe tailor regulatory duties to these different actors along the value chain and\nsuggest strategies to ensure that LGAIMs are trustworthy and deployed for the\nbenefit of society at large. Rules in the AI Act and other direct regulation\nmust match the specificities of pre-trained models. The paper argues for three\nlayers of obligations concerning LGAIMs (minimum standards for all LGAIMs;\nhigh-risk obligations for high-risk use cases; collaborations along the AI\nvalue chain). In general, regulation should focus on concrete high-risk\napplications, and not the pre-trained model itself, and should include (i)\nobligations regarding transparency and (ii) risk management. Non-discrimination\nprovisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core\nof the DSA content moderation rules should be expanded to cover LGAIMs. This\nincludes notice and action mechanisms, and trusted flaggers. In all areas,\nregulators and lawmakers need to act fast to keep track with the dynamics of\nChatGPT et al.</p>\n", "tags": ["Applications","Ethics & Fairness"] },
{"key": "haddow2021survey", "citations": "74", "year": "2022", "title":"Survey Of Low-resource Machine Translation", "abstract": "<p>We present a survey covering the state of the art in low-resource machine\ntranslation research. There are currently around 7000 languages spoken in the\nworld and almost all language pairs lack significant resources for training\nmachine translation models. There has been increasing interest in research\naddressing the challenge of producing useful translation models when very\nlittle translated training data is available. We present a summary of this\ntopical research field and provide a description of the techniques evaluated by\nresearchers in several recent shared tasks in low-resource MT.</p>\n", "tags": ["Survey Paper","Training Techniques"] },
{"key": "hahn2019theoretical", "citations": "60", "year": "2020", "title":"Theoretical Limitations Of Self-attention In Neural Sequence Models", "abstract": "<p>Transformers are emerging as the new workhorse of NLP, showing great success\nacross tasks. Unlike LSTMs, transformers process input sequences entirely\nthrough self-attention. Previous work has suggested that the computational\ncapabilities of self-attention to process hierarchical structures are limited.\nIn this work, we mathematically investigate the computational power of\nself-attention to model formal languages. Across both soft and hard attention,\nwe show strong theoretical limitations of the computational abilities of\nself-attention, finding that it cannot model periodic finite-state languages,\nnor hierarchical structure, unless the number of layers or heads increases with\ninput length. These limitations seem surprising given the practical success of\nself-attention and the prominent role assigned to hierarchical structure in\nlinguistics, suggesting that natural language can be approximated well with\nmodels that are too weak for the formal languages typically assumed in\ntheoretical linguistics.</p>\n", "tags": ["Model Architecture","TACL"] },
{"key": "hambardzumyan2021warp", "citations": "219", "year": "2021", "title":"WARP: Word-level Adversarial Reprogramming", "abstract": "<p>Transfer learning from pretrained language models recently became the\ndominant approach for solving many NLP tasks. A common approach to transfer\nlearning for multiple tasks that maximize parameter sharing trains one or more\ntask-specific layers on top of the language model. In this paper, we present an\nalternative approach based on adversarial reprogramming, which extends earlier\nwork on automatic prompt generation. Adversarial reprogramming attempts to\nlearn task-specific word embeddings that, when concatenated to the input text,\ninstruct the language model to solve the specified task. Using up to 25K\ntrainable parameters per task, this approach outperforms all existing methods\nwith up to 25M trainable parameters on the public leaderboard of the GLUE\nbenchmark. Our method, initialized with task-specific human-readable prompts,\nalso works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks\nwith just 32 training samples.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Model Architecture","Security","Training Techniques"] },
{"key": "han2020explaining", "citations": "92", "year": "2020", "title":"Explaining Black Box Predictions And Unveiling Data Artifacts Through Influence Functions", "abstract": "<p>Modern deep learning models for NLP are notoriously opaque. This has\nmotivated the development of methods for interpreting such models, e.g., via\ngradient-based saliency maps or the visualization of attention weights. Such\napproaches aim to provide explanations for a particular model prediction by\nhighlighting important words in the corresponding input text. While this might\nbe useful for tasks where decisions are explicitly influenced by individual\ntokens in the input, we suspect that such highlighting is not suitable for\ntasks where model decisions should be driven by more complex reasoning. In this\nwork, we investigate the use of influence functions for NLP, providing an\nalternative approach to interpreting neural text classifiers. Influence\nfunctions explain the decisions of a model by identifying influential training\nexamples. Despite the promise of this approach, influence functions have not\nyet been extensively evaluated in the context of NLP, a gap addressed by this\nwork. We conduct a comparison between influence functions and common\nword-saliency methods on representative tasks. As suspected, we find that\ninfluence functions are particularly useful for natural language inference, a\ntask in which ‘saliency maps’ may not have clear interpretation. Furthermore,\nwe develop a new quantitative measure based on influence functions that can\nreveal artifacts in training data.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "han2020learning", "citations": "69", "year": "2020", "title":"Learning-to-rank With BERT In Tf-ranking", "abstract": "<p>This paper describes a machine learning algorithm for document (re)ranking,\nin which queries and documents are firstly encoded using BERT [1], and on top\nof that a learning-to-rank (LTR) model constructed with TF-Ranking (TFR) [2] is\napplied to further optimize the ranking performance. This approach is proved to\nbe effective in a public MS MARCO benchmark [3]. Our first two submissions\nachieve the best performance for the passage re-ranking task [4], and the\nsecond best performance for the passage full-ranking task as of April 10, 2020\n[5]. To leverage the lately development of pre-trained language models, we\nrecently integrate RoBERTa [6] and ELECTRA [7]. Our latest submissions improve\nour previously state-of-the-art re-ranking performance by 4.3% [8], and achieve\nthe third best performance for the full-ranking task [9] as of June 8, 2020.\nBoth of them demonstrate the effectiveness of combining ranking losses with\nBERT representations for document ranking.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "han2021greedy", "citations": "64", "year": "2021", "title":"Greedy Gradient Ensemble For Robust Visual Question Answering", "abstract": "<p>Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.</p>\n", "tags": ["Datasets","ICCV","Model Architecture","Tools"] },
{"key": "han2021ptr", "citations": "377", "year": "2022", "title":"PTR: Prompt Tuning With Rules For Text Classification", "abstract": "<p>Fine-tuned pre-trained language models (PLMs) have achieved awesome\nperformance on almost all NLP tasks. By using additional prompts to fine-tune\nPLMs, we can further stimulate the rich knowledge distributed in PLMs to better\nserve downstream tasks. Prompt tuning has achieved promising results on some\nfew-class classification tasks such as sentiment classification and natural\nlanguage inference. However, manually designing lots of language prompts is\ncumbersome and fallible. For those auto-generated prompts, it is also expensive\nand time-consuming to verify their effectiveness in non-few-shot scenarios.\nHence, it is still challenging for prompt tuning to address many-class\nclassification tasks. To this end, we propose prompt tuning with rules (PTR)\nfor many-class text classification and apply logic rules to construct prompts\nwith several sub-prompts. In this way, PTR is able to encode prior knowledge of\neach class into prompt tuning. We conduct experiments on relation\nclassification, a typical and complicated many-class classification task, and\nthe results show that PTR can significantly and consistently outperform\nexisting state-of-the-art baselines. This indicates that PTR is a promising\napproach to take advantage of both human prior knowledge and PLMs for those\ncomplicated classification tasks.</p>\n", "tags": ["Few-Shot","Prompting"] },
{"key": "han2023medalpaca", "citations": "71", "year": "2023", "title":"Medalpaca -- An Open-source Collection Of Medical Conversational AI Models And Training Data", "abstract": "<p>As large language models (LLMs) like OpenAI’s GPT series continue to make\nstrides, we witness the emergence of artificial intelligence applications in an\never-expanding range of fields. In medicine, these LLMs hold considerable\npromise for improving medical workflows, diagnostics, patient care, and\neducation. Yet, there is an urgent need for open-source models that can be\ndeployed on-premises to safeguard patient privacy. In our work, we present an\ninnovative dataset consisting of over 160,000 entries, specifically crafted to\nfine-tune LLMs for effective medical applications. We investigate the impact of\nfine-tuning these datasets on publicly accessible pre-trained LLMs, and\nsubsequently, we juxtapose the performance of pre-trained-only models against\nthe fine-tuned models concerning the examinations that future medical doctors\nmust pass to achieve certification.</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Model Architecture","Privacy","Training Techniques"] },
{"key": "han2023svdiff", "citations": "81", "year": "2023", "title":"Svdiff: Compact Parameter Space For Diffusion Fine-tuning", "abstract": "<p>Diffusion models have achieved remarkable success in text-to-image\ngeneration, enabling the creation of high-quality images from text prompts or\nother modalities. However, existing methods for customizing these models are\nlimited by handling multiple personalized subjects and the risk of overfitting.\nMoreover, their large number of parameters is inefficient for model storage. In\nthis paper, we propose a novel approach to address these limitations in\nexisting text-to-image diffusion models for personalization. Our method\ninvolves fine-tuning the singular values of the weight matrices, leading to a\ncompact and efficient parameter space that reduces the risk of overfitting and\nlanguage drifting. We also propose a Cut-Mix-Unmix data-augmentation technique\nto enhance the quality of multi-subject image generation and a simple\ntext-based image editing framework. Our proposed SVDiff method has a\nsignificantly smaller model size compared to existing methods (approximately\n2,200 times fewer parameters compared with vanilla DreamBooth), making it more\npractical for real-world applications.</p>\n", "tags": ["Applications","Fine-Tuning","ICCV","Tools","Training Techniques"] },
{"key": "hao2019modeling", "citations": "92", "year": "2019", "title":"Modeling Recurrence For Transformer", "abstract": "<p>Recently, the Transformer model that is based solely on attention mechanisms,\nhas advanced the state-of-the-art on various machine translation tasks.\nHowever, recent studies reveal that the lack of recurrence hinders its further\nimprovement of translation capacity. In response to this problem, we propose to\ndirectly model recurrence for Transformer with an additional recurrence\nencoder. In addition to the standard recurrent neural network, we introduce a\nnovel attentive recurrent network to leverage the strengths of both attention\nand recurrent networks. Experimental results on the widely-used WMT14\nEnglish-German and WMT17 Chinese-English translation tasks demonstrate the\neffectiveness of the proposed approach. Our studies also reveal that the\nproposed model benefits from a short-cut that bridges the source and target\nsequences with a single recurrent layer, which outperforms its deep\ncounterpart.</p>\n", "tags": ["Model Architecture"] },
{"key": "hao2019visualizing", "citations": "168", "year": "2019", "title":"Visualizing And Understanding The Effectiveness Of BERT", "abstract": "<p>Language model pre-training, such as BERT, has achieved remarkable results in\nmany NLP tasks. However, it is unclear why the pre-training-then-fine-tuning\nparadigm can improve performance and generalization capability across different\ntasks. In this paper, we propose to visualize loss landscapes and optimization\ntrajectories of fine-tuning BERT on specific datasets. First, we find that\npre-training reaches a good initial point across downstream tasks, which leads\nto wider optima and easier optimization compared with training from scratch. We\nalso demonstrate that the fine-tuning procedure is robust to overfitting, even\nthough BERT is highly over-parameterized for downstream tasks. Second, the\nvisualization results indicate that fine-tuning BERT tends to generalize better\nbecause of the flat and wide optima, and the consistency between the training\nloss surface and the generalization error surface. Third, the lower layers of\nBERT are more invariant during fine-tuning, which suggests that the layers that\nare close to input learn more transferable representations of language.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "hao2020pre", "citations": "81", "year": "2021", "title":"Pre-training Graph Neural Networks For Cold-start Users And Items Representation", "abstract": "<p>Cold-start problem is a fundamental challenge for recommendation tasks.\nDespite the recent advances on Graph Neural Networks (GNNs) incorporate the\nhigh-order collaborative signal to alleviate the problem, the embeddings of the\ncold-start users and items aren’t explicitly optimized, and the cold-start\nneighbors are not dealt with during the graph convolution in GNNs. This paper\nproposes to pre-train a GNN model before applying it for recommendation. Unlike\nthe goal of recommendation, the pre-training GNN simulates the cold-start\nscenarios from the users/items with sufficient interactions and takes the\nembedding reconstruction as the pretext task, such that it can directly improve\nthe embedding quality and can be easily adapted to the new cold-start\nusers/items. To further reduce the impact from the cold-start neighbors, we\nincorporate a self-attention-based meta aggregator to enhance the aggregation\nability of each graph convolution step, and an adaptive neighbor sampler to\nselect the effective neighbors according to the feedbacks from the pre-training\nGNN model. Experiments on three public recommendation datasets show the\nsuperiority of our pre-training GNN model against the original GNN models on\nuser/item embedding inference and the recommendation task.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "hao2020self", "citations": "123", "year": "2021", "title":"Self-attention Attribution: Interpreting Information Interactions Inside Transformer", "abstract": "<p>The great success of Transformer-based models benefits from the powerful\nmulti-head self-attention mechanism, which learns token dependencies and\nencodes contextual information from the input. Prior work strives to attribute\nmodel decisions to individual input features with different saliency measures,\nbut they fail to explain how these input features interact with each other to\nreach predictions. In this paper, we propose a self-attention attribution\nmethod to interpret the information interactions inside Transformer. We take\nBERT as an example to conduct extensive studies. Firstly, we apply\nself-attention attribution to identify the important attention heads, while\nothers can be pruned with marginal performance degradation. Furthermore, we\nextract the most salient dependencies in each layer to construct an attribution\ntree, which reveals the hierarchical interactions inside Transformer. Finally,\nwe show that the attribution results can be used as adversarial patterns to\nimplement non-targeted attacks towards BERT.</p>\n", "tags": ["AAAI","Model Architecture"] },
{"key": "hao2020towards", "citations": "194", "year": "2020", "title":"Towards Learning A Generic Agent For Vision-and-language Navigation Via Pre-training", "abstract": "<p>Learning to navigate in a visual environment following natural-language\ninstructions is a challenging task, because the multimodal inputs to the agent\nare highly variable, and the training data on a new task is often limited. In\nthis paper, we present the first pre-training and fine-tuning paradigm for\nvision-and-language navigation (VLN) tasks. By training on a large amount of\nimage-text-action triplets in a self-supervised learning manner, the\npre-trained model provides generic representations of visual environments and\nlanguage instructions. It can be easily used as a drop-in for existing VLN\nframeworks, leading to the proposed agent called Prevalent. It learns more\neffectively in new tasks and generalizes better in a previously unseen\nenvironment. The performance is validated on three VLN tasks. On the\nRoom-to-Room benchmark, our model improves the state-of-the-art from 47% to 51%\non success rate weighted by path length. Further, the learned representation is\ntransferable to other VLN tasks. On two recent tasks, vision-and-dialog\nnavigation and “Help, Anna!” the proposed Prevalent leads to significant\nimprovement over existing methods, achieving a new state of the art.</p>\n", "tags": ["Agentic","CVPR","Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "hao2022mixgen", "citations": "60", "year": "2023", "title":"Mixgen: A New Multi-modal Data Augmentation", "abstract": "<p>Data augmentation is a necessity to enhance data efficiency in deep learning.\nFor vision-language pre-training, data is only augmented either for images or\nfor text in previous works. In this paper, we present MixGen: a joint data\naugmentation for vision-language representation learning to further improve\ndata efficiency. It generates new image-text pairs with semantic relationships\npreserved by interpolating images and concatenating text. It’s simple, and can\nbe plug-and-played into existing pipelines. We evaluate MixGen on four\narchitectures, including CLIP, ViLT, ALBEF and TCL, across five downstream\nvision-language tasks to show its versatility and effectiveness. For example,\nadding MixGen in ALBEF pre-training leads to absolute performance improvements\non downstream tasks: image-text retrieval (+6.2% on COCO fine-tuned and +5.3%\non Flicker30K zero-shot), visual grounding (+0.9% on RefCOCO+), visual\nreasoning (+$0.9% on NLVR2), visual question answering (+0.3% on VQA2.0), and\nvisual entailment (+0.4% on SNLI-VE).</p>\n", "tags": ["Applications","Training Techniques"] },
{"key": "haotian2023visual", "citations": "537", "year": "2023", "title":"Visual Instruction Tuning", "abstract": "<p>Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.</p>\n", "tags": ["Datasets","Instruction Following","Model Architecture"] },
{"key": "haque2022i", "citations": "166", "year": "2022", "title":"\"I Think This Is The Most Disruptive Technology\": Exploring Sentiments Of Chatgpt Early Adopters Using Twitter Data", "abstract": "<p>Large language models have recently attracted significant attention due to\ntheir impressive performance on a variety of tasks. ChatGPT developed by OpenAI\nis one such implementation of a large, pre-trained language model that has\ngained immense popularity among early adopters, where certain users go to the\nextent of characterizing it as a disruptive technology in many domains.\nUnderstanding such early adopters’ sentiments is important because it can\nprovide insights into the potential success or failure of the technology, as\nwell as its strengths and weaknesses. In this paper, we conduct a mixed-method\nstudy using 10,732 tweets from early ChatGPT users. We first use topic\nmodelling to identify the main topics and then perform an in-depth qualitative\nsentiment analysis of each topic. Our results show that the majority of the\nearly adopters have expressed overwhelmingly positive sentiments related to\ntopics such as Disruptions to software development, Entertainment and\nexercising creativity. Only a limited percentage of users expressed concerns\nabout issues such as the potential for misuse of ChatGPT, especially regarding\ntopics such as Impact on educational aspects. We discuss these findings by\nproviding specific examples for each topic and then detail implications related\nto addressing these concerns for both researchers and users.</p>\n", "tags": ["Model Architecture"] },
{"key": "hardy2018guided", "citations": "70", "year": "2018", "title":"Guided Neural Language Generation For Abstractive Summarization Using Abstract Meaning Representation", "abstract": "<p>Recent work on abstractive summarization has made progress with neural\nencoder-decoder architectures. However, such models are often challenged due to\ntheir lack of explicit semantic modeling of the source document and its\nsummary. In this paper, we extend previous work on abstractive summarization\nusing Abstract Meaning Representation (AMR) with a neural language generation\nstage which we guide using the source document. We demonstrate that this\nguidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using\ngold standard AMR parses and parses obtained from an off-the-shelf parser\nrespectively. We also find that the summarization performance using the latter\nis 2 ROUGE-2 points higher than that of a well-established neural\nencoder-decoder approach trained on a larger dataset. Code is available at\nhttps://github.com/sheffieldnlp/AMR2Text-summ</p>\n", "tags": ["Datasets","EMNLP","Has Code"] },
{"key": "harkous2020have", "citations": "66", "year": "2020", "title":"Have Your Text And Use It Too! End-to-end Neural Data-to-text Generation With Semantic Fidelity", "abstract": "<p>End-to-end neural data-to-text (D2T) generation has recently emerged as an\nalternative to pipeline-based architectures. However, it has faced challenges\nin generalizing to new domains and generating semantically consistent text. In\nthis work, we present DataTuner, a neural, end-to-end data-to-text generation\nsystem that makes minimal assumptions about the data representation and the\ntarget domain. We take a two-stage generation-reranking approach, combining a\nfine-tuned language model with a semantic fidelity classifier. Each of our\ncomponents is learnt end-to-end without the need for dataset-specific\nheuristics, entity delexicalization, or post-processing. We show that DataTuner\nachieves state of the art results on the automated metrics across four major\nD2T datasets (LDC2017T10, WebNLG, ViGGO, and Cleaned E2E), with a fluency\nassessed by human annotators nearing or exceeding the human-written reference\ntexts. We further demonstrate that the model-based semantic fidelity scorer in\nDataTuner is a better assessment tool compared to traditional, heuristic-based\nmeasures. Our generated text has a significantly better semantic fidelity than\nthe state of the art across all four datasets</p>\n", "tags": ["COLING","Datasets","Evaluation"] },
{"key": "hartvigsen2022toxigen", "citations": "127", "year": "2022", "title":"Toxigen: A Large-scale Machine-generated Dataset For Adversarial And Implicit Hate Speech Detection", "abstract": "<p>Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset. Our code and data can be found at\nhttps://github.com/microsoft/ToxiGen.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Security","Tools"] },
{"key": "hasan2021xl", "citations": "156", "year": "2021", "title":"Xl-sum: Large-scale Multilingual Abstractive Summarization For 44 Languages", "abstract": "<p>Contemporary works on abstractive text summarization have focused primarily\non high-resource languages like English, mostly due to the limited availability\nof datasets for low/mid-resource ones. In this work, we present XL-Sum, a\ncomprehensive and diverse dataset comprising 1 million professionally annotated\narticle-summary pairs from BBC, extracted using a set of carefully designed\nheuristics. The dataset covers 44 languages ranging from low to high-resource,\nfor many of which no public dataset is currently available. XL-Sum is highly\nabstractive, concise, and of high quality, as indicated by human and intrinsic\nevaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model,\nwith XL-Sum and experiment on multilingual and low-resource summarization\ntasks. XL-Sum induces competitive results compared to the ones obtained using\nsimilar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10\nlanguages we benchmark on, with some of them exceeding 15, as obtained by\nmultilingual training. Additionally, training on low-resource languages\nindividually also provides competitive performance. To the best of our\nknowledge, XL-Sum is the largest abstractive summarization dataset in terms of\nthe number of samples collected from a single source and the number of\nlanguages covered. We are releasing our dataset and models to encourage future\nresearch on multilingual abstractive summarization. The resources can be found\nat https://github.com/csebuetnlp/xl-sum.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Training Techniques"] },
{"key": "hase2020evaluating", "citations": "197", "year": "2020", "title":"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?", "abstract": "<p>Algorithmic approaches to interpreting machine learning models have\nproliferated in recent years. We carry out human subject tests that are the\nfirst of their kind to isolate the effect of algorithmic explanations on a key\naspect of model interpretability, simulatability, while avoiding important\nconfounding experimental factors. A model is simulatable when a person can\npredict its behavior on new inputs. Through two kinds of simulation tests\ninvolving text and tabular data, we evaluate five explanations methods: (1)\nLIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a\nComposite approach that combines explanations from each method. Clear evidence\nof method effectiveness is found in very few cases: LIME improves\nsimulatability in tabular classification, and our Prototype method is effective\nin counterfactual simulation tests. We also collect subjective ratings of\nexplanations, but we do not find that ratings are predictive of how helpful\nexplanations are. Our results provide the first reliable and comprehensive\nestimates of how explanations influence simulatability across a variety of\nexplanation methods and data domains. We show that (1) we need to be careful\nabout the metrics we use to evaluate explanation methods, and (2) there is\nsignificant room for improvement in current methods. All our supporting code,\ndata, and models are publicly available at:\nhttps://github.com/peterbhase/InterpretableNLP-ACL2020</p>\n", "tags": ["Evaluation"] },
{"key": "hashemi2019antique", "citations": "64", "year": "2020", "title":"ANTIQUE: A Non-factoid Question Answering Benchmark", "abstract": "<p>Considering the widespread use of mobile and voice search, answer passage\nretrieval for non-factoid questions plays a critical role in modern information\nretrieval systems. Despite the importance of the task, the community still\nfeels the significant lack of large-scale non-factoid question answering\ncollections with real questions and comprehensive relevance judgments. In this\npaper, we develop and release a collection of 2,626 open-domain non-factoid\nquestions from a diverse set of categories. The dataset, called ANTIQUE,\ncontains 34,011 manual relevance annotations. The questions were asked by real\nusers in a community question answering service, i.e., Yahoo! Answers.\nRelevance judgments for all the answers to each question were collected through\ncrowdsourcing. To facilitate further research, we also include a brief analysis\nof the data as well as baseline results on both classical and recently\ndeveloped neural IR models.</p>\n", "tags": ["Datasets","Evaluation","Retrieval Systems"] },
{"key": "hashimoto2018retrieve", "citations": "96", "year": "2018", "title":"A Retrieve-and-edit Framework For Predicting Structured Outputs", "abstract": "<p>For the task of generating complex outputs such as source code, editing\nexisting outputs can be easier than generating complex outputs from scratch.\nWith this motivation, we propose an approach that first retrieves a training\nexample based on the input (e.g., natural language description) and then edits\nit to the desired output (e.g., code). Our contribution is a computationally\nefficient method for learning a retrieval model that embeds the input in a\ntask-dependent way without relying on a hand-crafted metric or incurring the\nexpense of jointly training the retriever with the editor. Our\nretrieve-and-edit framework can be applied on top of any base model. We show\nthat on a new autocomplete task for GitHub Python code and the Hearthstone\ncards benchmark, retrieve-and-edit significantly boosts the performance of a\nvanilla sequence-to-sequence model on both tasks.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Retrieval Systems","Tools","Training Techniques"] },
{"key": "hashimoto2019unifying", "citations": "180", "year": "2019", "title":"Unifying Human And Statistical Evaluation For Natural Language Generation", "abstract": "<p>How can we measure whether a natural language generation system produces both\nhigh quality and diverse outputs? Human evaluation captures quality but not\ndiversity, as it does not catch models that simply plagiarize from the training\nset. On the other hand, statistical evaluation (i.e., perplexity) captures\ndiversity but not quality, as models that occasionally emit low quality samples\nwould be insufficiently penalized. In this paper, we propose a unified\nframework which evaluates both diversity and quality, based on the optimal\nerror rate of predicting whether a sentence is human- or machine-generated. We\ndemonstrate that this error rate can be efficiently estimated by combining\nhuman and statistical evaluation, using an evaluation metric which we call\nHUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects\ndiversity defects which fool pure human evaluation and that (ii) techniques\nsuch as annealing for improving quality actually decrease HUSE due to decreased\ndiversity.</p>\n", "tags": ["Evaluation","Tools","Training Techniques"] },
{"key": "hasler2018neural", "citations": "93", "year": "2018", "title":"Neural Machine Translation Decoding With Terminology Constraints", "abstract": "<p>Despite the impressive quality improvements yielded by neural machine\ntranslation (NMT) systems, controlling their translation output to adhere to\nuser-provided terminology constraints remains an open problem. We describe our\napproach to constrained neural decoding based on finite-state machines and\nmulti-stack decoding which supports target-side constraints as well as\nconstraints with corresponding aligned input text spans. We demonstrate the\nperformance of our framework on multiple translation tasks and motivate the\nneed for constrained decoding with attentions as a means of reducing\nmisplacement and duplication when translating user constraints.</p>\n", "tags": ["NAACL","Tools"] },
{"key": "hassan2018achieving", "citations": "573", "year": "2018", "title":"Achieving Human Parity On Automatic Chinese To English News Translation", "abstract": "<p>Machine translation has made rapid advances in recent years. Millions of\npeople are using it today in online translation systems and mobile applications\nin order to communicate across language barriers. The question naturally arises\nwhether such systems can approach or achieve parity with human translations. In\nthis paper, we first address the problem of how to define and accurately\nmeasure human parity in translation. We then describe Microsoft’s machine\ntranslation system and measure the quality of its translations on the widely\nused WMT 2017 news translation task from Chinese to English. We find that our\nlatest neural machine translation system has reached a new state-of-the-art,\nand that the translation quality is at human parity when compared to\nprofessional human translations. We also find that it significantly exceeds the\nquality of crowd-sourced non-professional translations.</p>\n", "tags": ["Applications"] },
{"key": "hassani2021escaping", "citations": "270", "year": "2021", "title":"Escaping The Big Data Paradigm With Compact Transformers", "abstract": "<p>With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, there has been a corresponding growth in\nparameter size and amounts of training data. Many have come to believe that\nbecause of this, transformers are not suitable for small sets of data. This\ntrend leads to concerns such as: limited availability of data in certain\nscientific domains and the exclusion of those with limited resource from\nresearch in the field. In this paper, we aim to present an approach for\nsmall-scale learning by introducing Compact Transformers. We show for the first\ntime that with the right size, convolutional tokenization, transformers can\navoid overfitting and outperform state-of-the-art CNNs on small datasets. Our\nmodels are flexible in terms of model size, and can have as little as 0.28M\nparameters while achieving competitive results. Our best model can reach 98%\naccuracy when training from scratch on CIFAR-10 with only 3.7M parameters,\nwhich is a significant improvement in data-efficiency over previous Transformer\nbased models being over 10x smaller than other transformers and is 15% the size\nof ResNet50 while achieving similar performance. CCT also outperforms many\nmodern CNN based approaches, and even some recent NAS-based approaches.\nAdditionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1\naccuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy\nwith 29% as many parameters as ViT), as well as NLP tasks. Our simple and\ncompact design for transformers makes them more feasible to study for those\nwith limited computing resources and/or dealing with small datasets, while\nextending existing research efforts in data efficient transformers. Our code\nand pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Compact-Transformers.</p>\n", "tags": ["Datasets","Efficiency","Has Code","Model Architecture","Training Techniques"] },
{"key": "hausknecht2019interactive", "citations": "101", "year": "2020", "title":"Interactive Fiction Games: A Colossal Adventure", "abstract": "<p>A hallmark of human intelligence is the ability to understand and communicate\nwith language. Interactive Fiction games are fully text-based simulation\nenvironments where a player issues text commands to effect change in the\nenvironment and progress through the story. We argue that IF games are an\nexcellent testbed for studying language-based autonomous agents. In particular,\nIF games combine challenges of combinatorial action spaces, language\nunderstanding, and commonsense reasoning. To facilitate rapid development of\nlanguage-based agents, we introduce Jericho, a learning environment for\nman-made IF games and conduct a comprehensive study of text-agents across a\nrich set of games, highlighting directions in which agents can improve.</p>\n", "tags": ["AAAI","Agentic"] },
{"key": "haviv2021bertese", "citations": "66", "year": "2021", "title":"Bertese: Learning To Speak To BERT", "abstract": "<p>Large pre-trained language models have been shown to encode large amounts of\nworld and commonsense knowledge in their parameters, leading to substantial\ninterest in methods for extracting that knowledge. In past work, knowledge was\nextracted by taking manually-authored queries and gathering paraphrases for\nthem using a separate pipeline. In this work, we propose a method for\nautomatically rewriting queries into “BERTese”, a paraphrase query that is\ndirectly optimized towards better knowledge extraction. To encourage meaningful\nrewrites, we add auxiliary loss functions that encourage the query to\ncorrespond to actual language tokens. We empirically show our approach\noutperforms competing baselines, obviating the need for complex pipelines.\nMoreover, BERTese provides some insight into the type of language that helps\nlanguage models perform knowledge extraction.</p>\n", "tags": ["EACL","Model Architecture","NAACL"] },
{"key": "hayati2018retrieval", "citations": "82", "year": "2018", "title":"Retrieval-based Neural Code Generation", "abstract": "<p>In models to generate program source code from natural language, representing\nthis code in a tree structure has been a common approach. However, existing\nmethods often fail to generate complex code correctly due to a lack of ability\nto memorize large and complex structures. We introduce ReCode, a method based\non subtree retrieval that makes it possible to explicitly reference existing\ncode examples within a neural code generation model. First, we retrieve\nsentences that are similar to input sentences using a dynamic-programming-based\nsentence similarity scoring method. Next, we extract n-grams of action\nsequences that build the associated abstract syntax tree. Finally, we increase\nthe probability of actions that cause the retrieved n-gram action subtree to be\nin the predicted code. We show that our approach improves the performance on\ntwo code generation tasks by up to +2.6 BLEU.</p>\n", "tags": ["EMNLP","Llm For Code"] },
{"key": "hayati2020inspired", "citations": "84", "year": "2020", "title":"INSPIRED: Toward Sociable Recommendation Dialog Systems", "abstract": "<p>In recommendation dialogs, humans commonly disclose their preference and make\nrecommendations in a friendly manner. However, this is a challenge when\ndeveloping a sociable recommendation dialog system, due to the lack of dialog\ndataset annotated with such sociable strategies. Therefore, we present\nINSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation\nwith measures for successful recommendations. To better understand how humans\nmake recommendations in communication, we design an annotation scheme related\nto recommendation strategies based on social science theories and annotate\nthese dialogs. Our analysis shows that sociable recommendation strategies, such\nas sharing personal opinions or communicating with encouragement, more\nfrequently lead to successful recommendations. Based on our dataset, we train\nend-to-end recommendation dialog systems with and without our strategy labels.\nIn both automatic and human evaluation, our model with strategy incorporation\noutperforms the baseline model. This work is a first step for building sociable\nrecommendation dialog systems with a basis of social science theories.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "hazarika2019conversational", "citations": "87", "year": "2020", "title":"Conversational Transfer Learning For Emotion Recognition", "abstract": "<p>Recognizing emotions in conversations is a challenging task due to the\npresence of contextual dependencies governed by self- and inter-personal\ninfluences. Recent approaches have focused on modeling these dependencies\nprimarily via supervised learning. However, purely supervised strategies demand\nlarge amounts of annotated data, which is lacking in most of the available\ncorpora in this task. To tackle this challenge, we look at transfer learning\napproaches as a viable alternative. Given the large amount of available\nconversational data, we investigate whether generative conversational models\ncan be leveraged to transfer affective knowledge for detecting emotions in\ncontext. We propose an approach, TL-ERC, where we pre-train a hierarchical\ndialogue model on multi-turn conversations (source) and then transfer its\nparameters to a conversational emotion classifier (target). In addition to the\npopular practice of using pre-trained sentence encoders, our approach also\nincorporates recurrent parameters that model inter-sentential context across\nthe whole conversation. Based on this idea, we perform several experiments\nacross multiple datasets and find improvement in performance and robustness\nagainst limited training data. TL-ERC also achieves better validation\nperformances in significantly fewer epochs. Overall, we infer that knowledge\nacquired from dialogue generators can indeed help recognize emotions in\nconversations.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "he2017dureader", "citations": "238", "year": "2018", "title":"Dureader: A Chinese Machine Reading Comprehension Dataset From Real-world Applications", "abstract": "<p>This paper introduces DuReader, a new large-scale, open-domain Chinese ma-\nchine reading comprehension (MRC) dataset, designed to address real-world MRC.\nDuReader has three advantages over previous MRC datasets: (1) data sources:\nquestions and documents are based on Baidu Search and Baidu Zhidao; answers are\nmanually generated. (2) question types: it provides rich annotations for more\nquestion types, especially yes-no and opinion questions, that leaves more\nopportunity for the research community. (3) scale: it contains 200K questions,\n420K answers and 1M documents; it is the largest Chinese MRC dataset so far.\nExperiments show that human performance is well above current state-of-the-art\nbaseline systems, leaving plenty of room for the community to make\nimprovements. To help the community make these improvements, both DuReader and\nbaseline systems have been posted online. We also organize a shared competition\nto encourage the exploration of more models. Since the release of the task,\nthere are significant improvements over the baselines.</p>\n", "tags": ["Applications","Datasets"] },
{"key": "he2017translation", "citations": "390", "year": "2017", "title":"Translation-based Recommendation", "abstract": "<p>Modeling the complex interactions between users and items as well as amongst\nitems themselves is at the core of designing successful recommender systems.\nOne classical setting is predicting users’ personalized sequential behavior (or\n<code class=\"language-plaintext highlighter-rouge\">next-item' recommendation), where the challenges mainly lie in modeling\n</code>third-order’ interactions between a user, her previously visited item(s), and\nthe next item to consume. Existing methods typically decompose these\nhigher-order interactions into a combination of pairwise relationships, by way\nof which user preferences (user-item interactions) and sequential patterns\n(item-item interactions) are captured by separate components. In this paper, we\npropose a unified method, TransRec, to model such third-order relationships for\nlarge-scale sequential prediction. Methodologically, we embed items into a\n`transition space’ where users are modeled as translation vectors operating on\nitem sequences. Empirically, this approach outperforms the state-of-the-art on\na wide spectrum of real-world datasets. Data and code are available at\nhttps://sites.google.com/a/eng.ucsd.edu/ruining-he/.</p>\n", "tags": ["Datasets"] },
{"key": "he2019revisiting", "citations": "152", "year": "2019", "title":"Revisiting Self-training For Neural Sequence Generation", "abstract": "<p>Self-training is one of the earliest and simplest semi-supervised methods.\nThe key idea is to augment the original labeled dataset with unlabeled data\npaired with the model’s prediction (i.e. the pseudo-parallel data). While\nself-training has been extensively studied on classification problems, in\ncomplex sequence generation tasks (e.g. machine translation) it is still\nunclear how self-training works due to the compositionality of the target\nspace. In this work, we first empirically show that self-training is able to\ndecently improve the supervised baseline on neural sequence generation tasks.\nThrough careful examination of the performance gains, we find that the\nperturbation on the hidden states (i.e. dropout) is critical for self-training\nto benefit from the pseudo-parallel data, which acts as a regularizer and\nforces the model to yield close predictions for similar unlabeled inputs. Such\neffect helps the model correct some incorrect predictions on unlabeled data. To\nfurther encourage this mechanism, we propose to inject noise to the input\nspace, resulting in a “noisy” version of self-training. Empirical study on\nstandard machine translation and text summarization benchmarks shows that noisy\nself-training is able to effectively utilize unlabeled data and improve the\nperformance of the supervised baseline by a large margin.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "he2019robust", "citations": "82", "year": "2019", "title":"Robust Sequence-to-sequence Acoustic Modeling With Stepwise Monotonic Attention For Neural TTS", "abstract": "<p>Neural TTS has demonstrated strong capabilities to generate human-like speech\nwith high quality and naturalness, while its generalization to out-of-domain\ntexts is still a challenging task, with regard to the design of attention-based\nsequence-to-sequence acoustic modeling. Various errors occur in those inputs\nwith unseen context, including attention collapse, skipping, repeating, etc.,\nwhich limits the broader applications. In this paper, we propose a novel\nstepwise monotonic attention method in sequence-to-sequence acoustic modeling\nto improve the robustness on out-of-domain inputs. The method utilizes the\nstrict monotonic property in TTS with constraints on monotonic hard attention\nthat the alignments between inputs and outputs sequence must be not only\nmonotonic but allowing no skipping on inputs. Soft attention could be used to\nevade mismatch between training and inference. The experimental results show\nthat the proposed method could achieve significant improvements in robustness\non out-of-domain scenarios for phoneme-based models, without any regression on\nthe in-domain naturalness test.</p>\n", "tags": ["Applications","INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "he2019structure", "citations": "79", "year": "2020", "title":"Structure-invariant Testing For Machine Translation", "abstract": "<p>In recent years, machine translation software has increasingly been\nintegrated into our daily lives. People routinely use machine translation for\nvarious applications, such as describing symptoms to a foreign doctor and\nreading political news in a foreign language. However, the complexity and\nintractability of neural machine translation (NMT) models that power modern\nmachine translation make the robustness of these systems difficult to even\nassess, much less guarantee. Machine translation systems can return inferior\nresults that lead to misunderstanding, medical misdiagnoses, threats to\npersonal safety, or political conflicts. Despite its apparent importance,\nvalidating the robustness of machine translation systems is very difficult and\nhas, therefore, been much under-explored.\n  To tackle this challenge, we introduce structure-invariant testing (SIT), a\nnovel metamorphic testing approach for validating machine translation software.\nOur key insight is that the translation results of “similar” source sentences\nshould typically exhibit similar sentence structures. Specifically, SIT (1)\ngenerates similar source sentences by substituting one word in a given sentence\nwith semantically similar, syntactically equivalent words; (2) represents\nsentence structure by syntax parse trees (obtained via constituency or\ndependency parsing); (3) reports sentence pairs whose structures differ\nquantitatively by more than some threshold. To evaluate SIT, we use it to test\nGoogle Translate and Bing Microsoft Translator with 200 source sentences as\ninput, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy,\nrespectively. The translation errors are diverse, including under-translation,\nover-translation, incorrect modification, word/phrase mistranslation, and\nunclear logic.</p>\n", "tags": ["Applications"] },
{"key": "he2020deberta", "citations": "915", "year": "2021", "title":"Deberta: Decoding-enhanced BERT With Disentangled Attention", "abstract": "<p>Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models’ generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "he2020image", "citations": "85", "year": "2021", "title":"Image Captioning Through Image Transformer", "abstract": "<p>Automatic captioning of images is a task that combines the challenges of\nimage analysis and text generation. One important aspect in captioning is the\nnotion of attention: How to decide what to describe and in which order.\nInspired by the successes in text analysis and translation, previous work have\nproposed the \\textit{transformer} architecture for image captioning. However,\nthe structure between the \\textit{semantic units} in images (usually the\ndetected regions from object detection model) and sentences (each single word)\nis different. Limited work has been done to adapt the transformer’s internal\narchitecture to images. In this work, we introduce the \\textbf{\\textit{image\ntransformer}}, which consists of a modified encoding transformer and an\nimplicit decoding transformer, motivated by the relative spatial relationship\nbetween image regions. Our design widen the original transformer layer’s inner\narchitecture to adapt to the structure of images. With only regions feature as\ninputs, our model achieves new state-of-the-art performance on both MSCOCO\noffline and online testing benchmarks.</p>\n", "tags": ["Model Architecture"] },
{"key": "he2020realformer", "citations": "64", "year": "2021", "title":"Realformer: Transformer Likes Residual Attention", "abstract": "<p>Transformer is the backbone of modern NLP models. In this paper, we propose\nRealFormer, a simple and generic technique to create Residual Attention Layer\nTransformer networks that significantly outperform the canonical Transformer\nand its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked\nLanguage Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA,\nNatural Questions, and OpenKP. We also observe empirically that RealFormer\nstabilizes training and leads to models with sparser attention. Source code and\npre-trained checkpoints for RealFormer can be found at\nhttps://github.com/google-research/google-research/tree/master/realformer.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "he2021debertav3", "citations": "346", "year": "2021", "title":"Debertav3: Improving Deberta Using Electra-style Pre-training With Gradient-disentangled Embedding Sharing", "abstract": "<p>This paper presents a new pre-trained language model, DeBERTaV3, which\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\nefficiency and model performance. This is because the training losses of the\ndiscriminator and the generator pull token embeddings in different directions,\ncreating the “tug-of-war” dynamics. We thus propose a new gradient-disentangled\nembedding sharing method that avoids the tug-of-war dynamics, improving both\ntraining efficiency and the quality of the pre-trained model. We have\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\nexceptional performance on a wide range of downstream natural language\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\n(SOTA) among the models with a similar structure. Furthermore, we have\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\nover strong baselines compared to English models. For example, the mDeBERTa\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\nmade our pre-trained models and inference code publicly available at\nhttps://github.com/microsoft/DeBERTa.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Training Techniques"] },
{"key": "he2021effectiveness", "citations": "98", "year": "2021", "title":"On The Effectiveness Of Adapter-based Tuning For Pretrained Language Model Adaptation", "abstract": "<p>Adapter-based tuning has recently arisen as an alternative to fine-tuning. It\nworks by adding light-weight adapter modules to a pretrained language model\n(PrLM) and only updating the parameters of adapter modules when learning on a\ndownstream task. As such, it adds only a few trainable parameters per new task,\nallowing a high degree of parameter sharing. Prior studies have shown that\nadapter-based tuning often achieves comparable results to fine-tuning. However,\nexisting work only focuses on the parameter-efficient aspect of adapter-based\ntuning while lacking further investigation on its effectiveness. In this paper,\nwe study the latter. We first show that adapter-based tuning better mitigates\nforgetting issues than fine-tuning since it yields representations with less\ndeviation from those generated by the initial PrLM. We then empirically compare\nthe two tuning methods on several downstream NLP tasks and settings. We\ndemonstrate that 1) adapter-based tuning outperforms fine-tuning on\nlow-resource and cross-lingual tasks; 2) it is more robust to overfitting and\nless sensitive to changes in learning rates.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "he2021stem", "citations": "66", "year": "2021", "title":"The Stem Cell Hypothesis: Dilemma Behind Multi-task Learning With Transformer Encoders", "abstract": "<p>Multi-task learning with transformer encoders (MTL) has emerged as a powerful\ntechnique to improve performance on closely-related tasks for both accuracy and\nefficiency while a question still remains whether or not it would perform as\nwell on tasks that are distinct in nature. We first present MTL results on five\nNLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over\nsingle-task learning. We then conduct an extensive pruning analysis to show\nthat a certain set of attention heads get claimed by most tasks during MTL, who\ninterfere with one another to fine-tune those heads for their own objectives.\nBased on this finding, we propose the Stem Cell Hypothesis to reveal the\nexistence of attention heads naturally talented for many tasks that cannot be\njointly trained to create adequate embeddings for all of those tasks. Finally,\nwe design novel parameter-free probes to justify our hypothesis and demonstrate\nhow attention heads are transformed across the five tasks during MTL through\nlabel analysis.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "he2021towards", "citations": "254", "year": "2021", "title":"Towards A Unified View Of Parameter-efficient Transfer Learning", "abstract": "<p>Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.</p>\n", "tags": ["Fine-Tuning","Tools","Training Techniques"] },
{"key": "heck2020trippy", "citations": "191", "year": "2020", "title":"Trippy: A Triple Copy Strategy For Value Independent Neural Dialog State Tracking", "abstract": "<p>Task-oriented dialog systems rely on dialog state tracking (DST) to monitor\nthe user’s goal during the course of an interaction. Multi-domain and\nopen-vocabulary settings complicate the task considerably and demand scalable\nsolutions. In this paper we present a new approach to DST which makes use of\nvarious copy mechanisms to fill slots with values. Our model has no need to\nmaintain a list of candidate values. Instead, all values are extracted from the\ndialog context on-the-fly. A slot is filled by one of three copy mechanisms:\n(1) Span prediction may extract values directly from the user input; (2) a\nvalue may be copied from a system inform memory that keeps track of the\nsystem’s inform operations; (3) a value may be copied over from a different\nslot that is already contained in the dialog state to resolve coreferences\nwithin and across domains. Our approach combines the advantages of span-based\nslot filling methods with memory methods to avoid the use of value picklists\naltogether. We argue that our strategy simplifies the DST task while at the\nsame time achieving state of the art performance on various popular evaluation\nsets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55%.</p>\n", "tags": ["Evaluation"] },
{"key": "heinzerling2017bpemb", "citations": "129", "year": "2017", "title":"Bpemb: Tokenization-free Pre-trained Subword Embeddings In 275 Languages", "abstract": "<p>We present BPEmb, a collection of pre-trained subword unit embeddings in 275\nlanguages, based on Byte-Pair Encoding (BPE). In an evaluation using\nfine-grained entity typing as testbed, BPEmb performs competitively, and for\nsome languages bet- ter than alternative subword approaches, while requiring\nvastly fewer resources and no tokenization. BPEmb is available at\nhttps://github.com/bheinzerling/bpemb</p>\n", "tags": ["Evaluation","Has Code"] },
{"key": "helcl2018cuni", "citations": "60", "year": "2018", "title":"CUNI System For The WMT18 Multimodal Translation Task", "abstract": "<p>We present our submission to the WMT18 Multimodal Translation Task. The main\nfeature of our submission is applying a self-attentive network instead of a\nrecurrent neural network. We evaluate two methods of incorporating the visual\nfeatures in the model: first, we include the image representation as another\ninput to the network; second, we train the model to predict the visual features\nand use it as an auxiliary objective. For our submission, we acquired both\ntextual and multimodal additional data. Both of the proposed methods yield\nsignificant improvements over recurrent networks and self-attentive textual\nbaselines.</p>\n", "tags": [] },
{"key": "hellas2023exploring", "citations": "83", "year": "2023", "title":"Exploring The Responses Of Large Language Models To Beginner Programmers' Help Requests", "abstract": "<p>Background and Context: Over the past year, large language models (LLMs) have\ntaken the world by storm. In computing education, like in other walks of life,\nmany opportunities and threats have emerged as a consequence.\n  Objectives: In this article, we explore such opportunities and threats in a\nspecific area: responding to student programmers’ help requests. More\nspecifically, we assess how good LLMs are at identifying issues in problematic\ncode that students request help on.\n  Method: We collected a sample of help requests and code from an online\nprogramming course. We then prompted two different LLMs (OpenAI Codex and\nGPT-3.5) to identify and explain the issues in the students’ code and assessed\nthe LLM-generated answers both quantitatively and qualitatively.\n  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently\nfind at least one actual issue in each student program (GPT-3.5 in 90% of the\ncases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%\nof the time). False positives are common (40% chance for GPT-3.5). The advice\nthat the LLMs provide on the issues is often sensible. The LLMs perform better\non issues involving program logic rather than on output formatting. Model\nsolutions are frequently provided even when the LLM is prompted not to. LLM\nresponses to prompts in a non-English language are only slightly worse than\nresponses to English prompts.\n  Implications: Our results continue to highlight the utility of LLMs in\nprogramming education. At the same time, the results highlight the\nunreliability of LLMs: LLMs make some of the same mistakes that students do,\nperhaps especially when formatting output as required by automated assessment\nsystems. Our study informs teachers interested in using LLMs as well as future\nefforts to customize LLMs for the needs of programming education.</p>\n", "tags": ["Llm For Code","Model Architecture"] },
{"key": "henderson2017efficient", "citations": "218", "year": "2017", "title":"Efficient Natural Language Response Suggestion For Smart Reply", "abstract": "<p>This paper presents a computationally efficient machine-learned method for\nnatural language response suggestion. Feed-forward neural networks using n-gram\nembedding features encode messages into vectors which are optimized to give\nmessage-response pairs a high dot-product value. An optimized search finds\nresponse suggestions. The method is evaluated in a large-scale commercial\ne-mail application, Inbox by Gmail. Compared to a sequence-to-sequence\napproach, the new system achieves the same quality at a small fraction of the\ncomputational requirements and latency.</p>\n", "tags": [] },
{"key": "henderson2017ethical", "citations": "157", "year": "2018", "title":"Ethical Challenges In Data-driven Dialogue Systems", "abstract": "<p>The use of dialogue systems as a medium for human-machine interaction is an\nincreasingly prevalent paradigm. A growing number of dialogue systems use\nconversation strategies that are learned from large datasets. There are well\ndocumented instances where interactions with these system have resulted in\nbiased or even offensive conversations due to the data-driven training process.\nHere, we highlight potential ethical issues that arise in dialogue systems\nresearch, including: implicit biases in data-driven systems, the rise of\nadversarial examples, potential sources of privacy violations, safety concerns,\nspecial considerations for reinforcement learning systems, and reproducibility\nconcerns. We also suggest areas stemming from these issues that deserve further\ninvestigation. Through this initial survey, we hope to spur research leading to\nrobust, safe, and ethically sound dialogue systems.</p>\n", "tags": ["AAAI","Dialogue & Multi Turn","Ethics & Fairness","Survey Paper"] },
{"key": "henderson2019convert", "citations": "164", "year": "2020", "title":"Convert: Efficient And Accurate Conversational Representations From Transformers", "abstract": "<p>General-purpose pretrained sentence encoders such as BERT are not ideal for\nreal-world conversational AI applications; they are computationally heavy,\nslow, and expensive to train. We propose ConveRT (Conversational\nRepresentations from Transformers), a pretraining framework for conversational\ntasks satisfying all the following requirements: it is effective, affordable,\nand quick to train. We pretrain using a retrieval-based response selection\ntask, effectively leveraging quantization and subword-level parameterization in\nthe dual encoder to build a lightweight memory- and energy-efficient model. We\nshow that ConveRT achieves state-of-the-art performance across widely\nestablished response selection tasks. We also demonstrate that the use of\nextended dialog history as context yields further performance gains. Finally,\nwe show that pretrained representations from the proposed encoder can be\ntransferred to the intent classification task, yielding strong results across\nthree diverse data sets. ConveRT trains substantially faster than standard\nsentence encoders or previous state-of-the-art dual encoders. With its reduced\nsize and superior performance, we believe this model promises wider portability\nand scalability for Conversational AI applications.</p>\n", "tags": ["Applications","EMNLP","Model Architecture","Tools"] },
{"key": "henderson2019repository", "citations": "81", "year": "2019", "title":"A Repository Of Conversational Datasets", "abstract": "<p>Progress in Machine Learning is often driven by the availability of large\ndatasets, and consistent evaluation metrics for comparing modeling approaches.\nTo this end, we present a repository of conversational datasets consisting of\nhundreds of millions of examples, and a standardised evaluation procedure for\nconversational response selection models using ‘1-of-100 accuracy’. The\nrepository contains scripts that allow researchers to reproduce the standard\ndatasets, or to adapt the pre-processing and data filtering steps to their\nneeds. We introduce and evaluate several competitive baselines for\nconversational response selection, whose implementations are shared in the\nrepository, as well as a neural encoder model that is trained on the entire\ntraining set.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Training Techniques"] },
{"key": "henderson2019training", "citations": "86", "year": "2019", "title":"Training Neural Response Selection For Task-oriented Dialogue Systems", "abstract": "<p>Despite their popularity in the chatbot literature, retrieval-based models\nhave had modest impact on task-oriented dialogue systems, with the main\nobstacle to their application being the low-data regime of most task-oriented\ndialogue tasks. Inspired by the recent success of pretraining in language\nmodelling, we propose an effective method for deploying response selection in\ntask-oriented dialogue. To train response selection models for task-oriented\ndialogue tasks, we propose a novel method which: 1) pretrains the response\nselection model on large general-domain conversational corpora; and then 2)\nfine-tunes the pretrained model for the target dialogue domain, relying only on\nthe small in-domain dataset to capture the nuances of the given dialogue\ndomain. Our evaluation on six diverse application domains, ranging from\ne-commerce to banking, demonstrates the effectiveness of the proposed training\nmethod.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Training Techniques"] },
{"key": "hendricks2016generating", "citations": "498", "year": "2016", "title":"Generating Visual Explanations", "abstract": "<p>Clearly explaining a rationale for a classification decision to an end-user\ncan be as important as the decision itself. Existing approaches for deep visual\nrecognition are generally opaque and do not output any justification text;\ncontemporary vision-language models can describe image content but fail to take\ninto account class-discriminative image aspects which justify visual\npredictions. We propose a new model that focuses on the discriminating\nproperties of the visible object, jointly predicts a class label, and explains\nwhy the predicted label is appropriate for the image. We propose a novel loss\nfunction based on sampling and reinforcement learning that learns to generate\nsentences that realize a global sentence property, such as class specificity.\nOur results on a fine-grained bird species classification dataset show that our\nmodel is able to generate explanations which are not only consistent with an\nimage but also more discriminative than descriptions produced by existing\ncaptioning methods.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning"] },
{"key": "hendricks2018grounding", "citations": "204", "year": "2018", "title":"Grounding Visual Explanations", "abstract": "<p>Existing visual explanation generating agents learn to fluently justify a\nclass prediction. However, they may mention visual attributes which reflect a\nstrong class prior, although the evidence may not actually be in the image.\nThis is particularly concerning as ultimately such agents fail in building\ntrust with human users. To overcome this limitation, we propose a phrase-critic\nmodel to refine generated candidate explanations augmented with flipped phrases\nwhich we use as negative examples while training. At inference time, our\nphrase-critic model takes an image and a candidate explanation as input and\noutputs a score indicating how well the candidate explanation is grounded in\nthe image. Our explainable AI agent is capable of providing counter arguments\nfor an alternative prediction, i.e. counterfactuals, along with explanations\nthat justify the correct classification decisions. Our model improves the\ntextual explanation quality of fine-grained classification decisions on the CUB\ndataset by mentioning phrases that are grounded in the image. Moreover, on the\nFOIL tasks, our agent detects when there is a mistake in the sentence, grounds\nthe incorrect phrase and corrects it significantly better than other models.</p>\n", "tags": ["Training Techniques"] },
{"key": "hendrycks2019using", "citations": "415", "year": "2019", "title":"Using Pre-training Can Improve Model Robustness And Uncertainty", "abstract": "<p>He et al. (2018) have called into question the utility of pre-training by\nshowing that training from scratch can often yield similar performance to\npre-training. We show that although pre-training may not improve performance on\ntraditional classification metrics, it improves model robustness and\nuncertainty estimates. Through extensive experiments on adversarial examples,\nlabel corruption, class imbalance, out-of-distribution detection, and\nconfidence calibration, we demonstrate large gains from pre-training and\ncomplementary effects with task-specific methods. We introduce adversarial\npre-training and show approximately a 10% absolute improvement over the\nprevious state-of-the-art in adversarial robustness. In some cases, using\npre-training without task-specific methods also surpasses the state-of-the-art,\nhighlighting the need for pre-training when evaluating future methods on\nrobustness and uncertainty tasks.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "hendrycks2020aligning", "citations": "99", "year": "2020", "title":"Aligning AI With Shared Human Values", "abstract": "<p>We show how to assess a language model’s knowledge of basic concepts of\nmorality. We introduce the ETHICS dataset, a new benchmark that spans concepts\nin justice, well-being, duties, virtues, and commonsense morality. Models\npredict widespread moral judgments about diverse text scenarios. This requires\nconnecting physical and social world knowledge to value judgements, a\ncapability that may enable us to steer chatbot outputs or eventually regularize\nopen-ended reinforcement learning agents. With the ETHICS dataset, we find that\ncurrent language models have a promising but incomplete ability to predict\nbasic human ethical judgements. Our work shows that progress can be made on\nmachine ethics today, and it provides a steppingstone toward AI that is aligned\nwith human values.</p>\n", "tags": ["Ethics & Fairness","Evaluation"] },
{"key": "hendrycks2020measuring", "citations": "312", "year": "2021", "title":"Measuring Massive Multitask Language Understanding", "abstract": "<p>We propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel’s academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.</p>\n", "tags": ["Model Architecture"] },
{"key": "hendrycks2020pretrained", "citations": "277", "year": "2020", "title":"Pretrained Transformers Improve Out-of-distribution Robustness", "abstract": "<p>Although pretrained Transformers such as BERT achieve high accuracy on\nin-distribution examples, do they generalize to new distributions? We\nsystematically measure out-of-distribution (OOD) generalization for seven NLP\ndatasets by constructing a new robustness benchmark with realistic distribution\nshifts. We measure the generalization of previous models including bag-of-words\nmodels, ConvNets, and LSTMs, and we show that pretrained Transformers’\nperformance declines are substantially smaller. Pretrained transformers are\nalso more effective at detecting anomalous or OOD examples, while many previous\nmodels are frequently worse than chance. We examine which factors affect\nrobustness, finding that larger models are not necessarily more robust,\ndistillation can be harmful, and more diverse pretraining data can enhance\nrobustness. Finally, we show where future work can improve OOD robustness.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Model Architecture"] },
{"key": "hendrycks2021cuad", "citations": "90", "year": "2021", "title":"CUAD: An Expert-annotated NLP Dataset For Legal Contract Review", "abstract": "<p>Many specialized domains remain untouched by deep learning, as large labeled\ndatasets require expensive expert annotators. We address this bottleneck within\nthe legal domain by introducing the Contract Understanding Atticus Dataset\n(CUAD), a new dataset for legal contract review. CUAD was created with dozens\nof legal experts from The Atticus Project and consists of over 13,000\nannotations. The task is to highlight salient portions of a contract that are\nimportant for a human to review. We find that Transformer models have nascent\nperformance, but that this performance is strongly influenced by model design\nand training dataset size. Despite these promising results, there is still\nsubstantial room for improvement. As one of the only large, specialized NLP\nbenchmarks annotated by experts, CUAD can serve as a challenging research\nbenchmark for the broader NLP community.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "hendy2023how", "citations": "143", "year": "2023", "title":"How Good Are GPT Models At Machine Translation? A Comprehensive Evaluation", "abstract": "<p>Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.</p>\n", "tags": ["Evaluation","Model Architecture","Prompting"] },
{"key": "heo2019fooling", "citations": "67", "year": "2019", "title":"Fooling Neural Network Interpretations Via Adversarial Model Manipulation", "abstract": "<p>We ask whether the neural network interpretation methods can be fooled via\nadversarial model manipulation, which is defined as a model fine-tuning step\nthat aims to radically alter the explanations without hurting the accuracy of\nthe original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating\nthe interpretation results directly in the penalty term of the objective\nfunction for fine-tuning, we show that the state-of-the-art saliency map based\ninterpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with\nour model manipulation. We propose two types of fooling, Passive and Active,\nand demonstrate such foolings generalize well to the entire validation set as\nwell as transfer to other interpretation methods. Our results are validated by\nboth visually showing the fooled explanations and reporting quantitative\nmetrics that measure the deviations from the original explanations. We claim\nthat the stability of neural network interpretation method with respect to our\nadversarial model manipulation is an important criterion to check for\ndeveloping robust and reliable neural network interpretation method.</p>\n", "tags": ["Evaluation","Fine-Tuning","ICCV","NEURIPS","Security","Training Techniques"] },
{"key": "hermann2017grounded", "citations": "163", "year": "2017", "title":"Grounded Language Learning In A Simulated 3D World", "abstract": "<p>We are increasingly surrounded by artificially intelligent technology that\ntakes decisions and executes actions on our behalf. This creates a pressing\nneed for general means to communicate with, instruct and guide artificial\nagents, with human language the most compelling means for such communication.\nTo achieve this in a scalable fashion, agents must be able to relate language\nto the world and to actions; that is, their understanding of language must be\ngrounded and embodied. However, learning grounded language is a notoriously\nchallenging problem in artificial intelligence research. Here we present an\nagent that learns to interpret language in a simulated 3D environment where it\nis rewarded for the successful execution of written instructions. Trained via a\ncombination of reinforcement and unsupervised learning, and beginning with\nminimal prior knowledge, the agent learns to relate linguistic symbols to\nemergent perceptual representations of its physical surroundings and to\npertinent sequences of actions. The agent’s comprehension of language extends\nbeyond its prior experience, enabling it to apply familiar language to\nunfamiliar situations and to interpret entirely novel instructions. Moreover,\nthe speed with which this agent learns new words increases as its semantic\nknowledge grows. This facility for generalising and bootstrapping semantic\nknowledge indicates the potential of the present approach for reconciling\nambiguous natural language with the complexity of the physical world.</p>\n", "tags": ["Agentic"] },
{"key": "hertz2022prompt", "citations": "317", "year": "2022", "title":"Prompt-to-prompt Image Editing With Cross Attention Control", "abstract": "<p>Recent large-scale text-driven synthesis models have attracted much attention\nthanks to their remarkable capabilities of generating highly diverse images\nthat follow given text prompts. Such text-based synthesis methods are\nparticularly appealing to humans who are used to verbally describe their\nintent. Therefore, it is only natural to extend the text-driven image synthesis\nto text-driven image editing. Editing is challenging for these generative\nmodels, since an innate property of an editing technique is to preserve most of\nthe original image, while in the text-based models, even a small modification\nof the text prompt often leads to a completely different outcome.\nState-of-the-art methods mitigate this by requiring the users to provide a\nspatial mask to localize the edit, hence, ignoring the original structure and\ncontent within the masked region. In this paper, we pursue an intuitive\nprompt-to-prompt editing framework, where the edits are controlled by text\nonly. To this end, we analyze a text-conditioned model in depth and observe\nthat the cross-attention layers are the key to controlling the relation between\nthe spatial layout of the image to each word in the prompt. With this\nobservation, we present several applications which monitor the image synthesis\nby editing the textual prompt only. This includes localized editing by\nreplacing a word, global editing by adding a specification, and even delicately\ncontrolling the extent to which a word is reflected in the image. We present\nour results over diverse images and prompts, demonstrating high-quality\nsynthesis and fidelity to the edited prompts.</p>\n", "tags": ["Applications","Model Architecture","Prompting","Tools"] },
{"key": "herzig2020span", "citations": "64", "year": "2021", "title":"Span-based Semantic Parsing For Compositional Generalization", "abstract": "<p>Despite the success of sequence-to-sequence (seq2seq) models in semantic\nparsing, recent work has shown that they fail in compositional generalization,\ni.e., the ability to generalize to new structures built of components observed\nduring training. In this work, we posit that a span-based parser should lead to\nbetter compositional generalization. we propose SpanBasedSP, a parser that\npredicts a span tree over an input utterance, explicitly encoding how partial\nprograms compose over spans in the input. SpanBasedSP extends Pasupat et al.\n(2019) to be comparable to seq2seq models by (i) training from programs,\nwithout access to gold trees, treating trees as latent variables, (ii) parsing\na class of non-projective trees through an extension to standard CKY. On\nGeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong\nseq2seq baselines on random splits, but dramatically improves performance\ncompared to baselines on splits that require compositional generalization: from\n\\(61.0 \\rightarrow 88.9\\) average accuracy.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "hessel2021clipscore", "citations": "488", "year": "2021", "title":"Clipscore: A Reference-free Evaluation Metric For Image Captioning", "abstract": "<p>Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "hewlett2016wikireading", "citations": "149", "year": "2016", "title":"Wikireading: A Novel Large-scale Language Understanding Task Over Wikipedia", "abstract": "<p>We present WikiReading, a large-scale natural language understanding task and\npublicly-available dataset with 18 million instances. The task is to predict\ntextual values from the structured knowledge base Wikidata by reading the text\nof the corresponding Wikipedia articles. The task contains a rich variety of\nchallenging classification and extraction sub-tasks, making it well-suited for\nend-to-end models such as deep neural networks (DNNs). We compare various\nstate-of-the-art DNN-based architectures for document classification,\ninformation extraction, and question answering. We find that models supporting\na rich answer space, such as word or character sequences, perform best. Our\nbest-performing model, a word-level sequence to sequence model with a mechanism\nto copy out-of-vocabulary words, obtains an accuracy of 71.8%.</p>\n", "tags": ["Datasets"] },
{"key": "hieber2017sockeye", "citations": "205", "year": "2017", "title":"Sockeye: A Toolkit For Neural Machine Translation", "abstract": "<p>We describe Sockeye (version 1.12), an open-source sequence-to-sequence\ntoolkit for Neural Machine Translation (NMT). Sockeye is a production-ready\nframework for training and applying models as well as an experimental platform\nfor researchers. Written in Python and built on MXNet, the toolkit offers\nscalable training and inference for the three most prominent encoder-decoder\narchitectures: attentional recurrent neural networks, self-attentional\ntransformers, and fully convolutional networks. Sockeye also supports a wide\nrange of optimizers, normalization and regularization techniques, and inference\nimprovements from current NMT literature. Users can easily run standard\ntraining recipes, explore different model settings, and incorporate new ideas.\nIn this paper, we highlight Sockeye’s features and benchmark it against other\nNMT toolkits on two language arcs from the 2017 Conference on Machine\nTranslation (WMT): English-German and Latvian-English. We report competitive\nBLEU scores across all three architectures, including an overall best score for\nSockeye’s transformer implementation. To facilitate further comparison, we\nrelease all system outputs and training scripts used in our experiments. The\nSockeye toolkit is free software released under the Apache 2.0 license.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "hitschler2016multimodal", "citations": "92", "year": "2016", "title":"Multimodal Pivots For Image Caption Translation", "abstract": "<p>We present an approach to improve statistical machine translation of image\ndescriptions by multimodal pivots defined in visual space. The key idea is to\nperform image retrieval over a database of images that are captioned in the\ntarget language, and use the captions of the most similar images for\ncrosslingual reranking of translation outputs. Our approach does not depend on\nthe availability of large amounts of in-domain parallel data, but only relies\non available large datasets of monolingually captioned images, and on\nstate-of-the-art convolutional neural networks to compute image similarities.\nOur experimental evaluation shows improvements of 1 BLEU point over strong\nbaselines.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "ho2022large", "citations": "62", "year": "2023", "title":"Large Language Models Are Reasoning Teachers", "abstract": "<p>Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model’s ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "hoffman2018metrics", "citations": "350", "year": "2018", "title":"Metrics For Explainable AI: Challenges And Prospects", "abstract": "<p>The question addressed in this paper is: If we present to a user an AI system\nthat explains how it works, how do we know whether the explanation works and\nthe user has achieved a pragmatic understanding of the AI? In other words, how\ndo we know that an explanainable AI system (XAI) is any good? Our focus is on\nthe key concepts of measurement. We discuss specific methods for evaluating:\n(1) the goodness of explanations, (2) whether users are satisfied by\nexplanations, (3) how well users understand the AI systems, (4) how curiosity\nmotivates the search for explanations, (5) whether the user’s trust and\nreliance on the AI are appropriate, and finally, (6) how the human-XAI work\nsystem performs. The recommendations we present derive from our integration of\nextensive research literatures and our own psychometric evaluations.</p>\n", "tags": ["Evaluation"] },
{"key": "hoffmann2022training", "citations": "461", "year": "2022", "title":"Training Compute-optimal Large Language Models", "abstract": "<p>We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4\\(\\times\\) more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "hofstätter2020improving", "citations": "65", "year": "2020", "title":"Improving Efficient Neural Ranking Models With Cross-architecture Knowledge Distillation", "abstract": "<p>Retrieval and ranking models are the backbone of many applications such as\nweb search, open domain QA, or text-based recommender systems. The latency of\nneural ranking models at query time is largely dependent on the architecture\nand deliberate choices by their designers to trade-off effectiveness for higher\nefficiency. This focus on low query latency of a rising number of efficient\nranking architectures make them feasible for production deployment. In machine\nlearning an increasingly common approach to close the effectiveness gap of more\nefficient models is to apply knowledge distillation from a large teacher model\nto a smaller student model. We find that different ranking architectures tend\nto produce output scores in different magnitudes. Based on this finding, we\npropose a cross-architecture training procedure with a margin focused loss\n(Margin-MSE), that adapts knowledge distillation to the varying score output\ndistributions of different BERT and non-BERT passage ranking architectures. We\napply the teachable information as additional fine-grained labels to existing\ntraining triples of the MSMARCO-Passage collection. We evaluate our procedure\nof distilling knowledge from state-of-the-art concatenated BERT models to four\ndifferent efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot\nproduct model). We show that across our evaluated architectures our Margin-MSE\nknowledge distillation significantly improves re-ranking effectiveness without\ncompromising their efficiency. Additionally, we show our general distillation\nmethod to improve nearest neighbor based index retrieval with the BERT dot\nproduct model, offering competitive results with specialized and much more\ncostly training methods. To benefit the community, we publish the teacher-score\ntraining files in a ready-to-use package.</p>\n", "tags": ["Applications","Efficiency","Model Architecture","Training Techniques"] },
{"key": "hofstätter2021efficiently", "citations": "222", "year": "2021", "title":"Efficiently Teaching An Effective Dense Retriever With Balanced Topic Aware Sampling", "abstract": "<p>A vital step towards the widespread adoption of neural retrieval models is\ntheir resource efficiency throughout the training, indexing and query\nworkflows. The neural IR community made great advancements in training\neffective dual-encoder dense retrieval (DR) models recently. A dense text\nretrieval model uses a single vector representation per query and passage to\nscore a match, which enables low-latency first stage retrieval with a nearest\nneighbor search. Increasingly common, training approaches require enormous\ncompute power, as they either conduct negative passage sampling out of a\ncontinuously updating refreshing index or require very large batch sizes for\nin-batch negative sampling. Instead of relying on more compute capability, we\nintroduce an efficient topic-aware query and balanced margin sampling\ntechnique, called TAS-Balanced. We cluster queries once before training and\nsample queries out of a cluster per batch. We train our lightweight 6-layer DR\nmodel with a novel dual-teacher supervision that combines pairwise and in-batch\nnegative teachers. Our method is trainable on a single consumer-grade GPU in\nunder 48 hours (as opposed to a common configuration of 8x V100s). We show that\nour TAS-Balanced training method achieves state-of-the-art low-latency (64ms\nper query) results on two TREC Deep Learning Track query sets. Evaluated on\nNDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by\n11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces\nthe first dense retriever that outperforms every other method on recall at any\ncutoff on TREC-DL and allows more resource intensive re-ranking models to\noperate on fewer passages to improve results further.</p>\n", "tags": ["Efficiency","Retrieval Systems","SIGIR","Training Techniques"] },
{"key": "hohenstein2021artificial", "citations": "94", "year": "2023", "title":"Artificial Intelligence In Communication Impacts Language And Social Relationships", "abstract": "<p>Artificial intelligence (AI) is now widely used to facilitate social\ninteraction, but its impact on social relationships and communication is not\nwell understood. We study the social consequences of one of the most pervasive\nAI applications: algorithmic response suggestions (“smart replies”). Two\nrandomized experiments (n = 1036) provide evidence that a commercially-deployed\nAI changes how people interact with and perceive one another in pro-social and\nanti-social ways. We find that using algorithmic responses increases\ncommunication efficiency, use of positive emotional language, and positive\nevaluations by communication partners. However, consistent with common\nassumptions about the negative implications of AI, people are evaluated more\nnegatively if they are suspected to be using algorithmic responses. Thus, even\nthough AI can increase communication efficiency and improve interpersonal\nperceptions, it risks changing users’ language production and continues to be\nviewed negatively.</p>\n", "tags": ["Applications","Has Code"] },
{"key": "hokamp2017lexically", "citations": "290", "year": "2017", "title":"Lexically Constrained Decoding For Sequence Generation Using Grid Beam Search", "abstract": "<p>We present Grid Beam Search (GBS), an algorithm which extends beam search to\nallow the inclusion of pre-specified lexical constraints. The algorithm can be\nused with any model that generates a sequence \\( \\mathbf{\\hat{y}} =\n\\{y_{0}\\ldots y_{T}\\} \\), by maximizing \\( p(\\mathbf{y} | \\mathbf{x}) =\n\\prod\\limits_{t}p(y_{t} | \\mathbf{x}; \\{y_{0} \\ldots y_{t-1}\\}) \\). Lexical\nconstraints take the form of phrases or words that must be present in the\noutput sequence. This is a very general way to incorporate additional knowledge\ninto a model’s output without requiring any modification of the model\nparameters or training data. We demonstrate the feasibility and flexibility of\nLexically Constrained Decoding by conducting experiments on Neural\nInteractive-Predictive Translation, as well as Domain Adaptation for Neural\nMachine Translation. Experiments show that GBS can provide large improvements\nin translation quality in interactive scenarios, and that, even without any\nuser input, GBS can be used to achieve significant gains in performance in\ndomain adaptation scenarios.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "holtzman2019curious", "citations": "1194", "year": "2019", "title":"The Curious Case Of Neural Text Degeneration", "abstract": "<p>Despite considerable advancements with deep neural language models, the\nenigma of neural text degeneration persists when these models are tested as\ntext generators. The counter-intuitive empirical observation is that even\nthough the use of likelihood as training objective leads to high quality models\nfor a broad range of language understanding tasks, using likelihood as a\ndecoding objective leads to text that is bland and strangely repetitive.\n  In this paper, we reveal surprising distributional differences between human\ntext and machine text. In addition, we find that decoding strategies alone can\ndramatically effect the quality of machine text, even when generated from\nexactly the same neural language model. Our findings motivate Nucleus Sampling,\na simple but effective method to draw the best out of neural generation. By\nsampling text from the dynamic nucleus of the probability distribution, which\nallows for diversity while effectively truncating the less reliable tail of the\ndistribution, the resulting text better demonstrates the quality of human text,\nyielding enhanced diversity without sacrificing fluency and coherence.</p>\n", "tags": ["Training Techniques"] },
{"key": "hong2019learning", "citations": "144", "year": "2019", "title":"Learning To Compose And Reason With Language Tree Structures For Visual Grounding", "abstract": "<p>Grounding natural language in images, such as localizing “the black dog on\nthe left of the tree”, is one of the core problems in artificial intelligence,\nas it needs to comprehend the fine-grained and compositional language space.\nHowever, existing solutions rely on the association between the holistic\nlanguage features and visual features, while neglect the nature of\ncompositional reasoning implied in the language. In this paper, we propose a\nnatural language grounding model that can automatically compose a binary tree\nstructure for parsing the language and then perform visual reasoning along the\ntree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding\nTree, which is inspired by the intuition that any language expression can be\nrecursively decomposed into two constituent parts, and the grounding confidence\nscore can be recursively accumulated by calculating their grounding scores\nreturned by sub-trees. RVG-TREE can be trained end-to-end by using the\nStraight-Through Gumbel-Softmax estimator that allows the gradients from the\ncontinuous score functions passing through the discrete tree construction.\nExperiments on several benchmarks show that our model achieves the\nstate-of-the-art performance with more explainable reasoning.</p>\n", "tags": ["Llm For Code"] },
{"key": "hong2020language", "citations": "70", "year": "2020", "title":"Language And Visual Entity Relationship Graph For Agent Navigation", "abstract": "<p>Vision-and-Language Navigation (VLN) requires an agent to navigate in a\nreal-world environment following natural language instructions. From both the\ntextual and visual perspectives, we find that the relationships among the\nscene, its objects,and directional clues are essential for the agent to\ninterpret complex instructions and correctly perceive the environment. To\ncapture and utilize the relationships, we propose a novel Language and Visual\nEntity Relationship Graph for modelling the inter-modal relationships between\ntext and vision, and the intra-modal relationships among visual entities. We\npropose a message passing algorithm for propagating information between\nlanguage elements and visual entities in the graph, which we then combine to\ndetermine the next action to take. Experiments show that by taking advantage of\nthe relationships we are able to improve over state-of-the-art. On the\nRoom-to-Room (R2R) benchmark, our method achieves the new best performance on\nthe test unseen split with success rate weighted by path length (SPL) of 52%.\nOn the Room-for-Room (R4R) dataset, our method significantly improves the\nprevious best from 13% to 34% on the success weighted by normalized dynamic\ntime warping (SDTW). Code is available at:\nhttps://github.com/YicongHong/Entity-Graph-VLN.</p>\n", "tags": ["Agentic","Evaluation","Has Code"] },
{"key": "hong2020recurrent", "citations": "143", "year": "2021", "title":"A Recurrent Vision-and-language BERT For Navigation", "abstract": "<p>Accuracy of many visiolinguistic tasks has benefited significantly from the\napplication of vision-and-language(V&amp;L) BERT. However, its application for the\ntask of vision-and-language navigation (VLN) remains limited. One reason for\nthis is the difficulty adapting the BERT architecture to the partially\nobservable Markov decision process present in VLN, requiring history-dependent\nattention and decision making. In this paper we propose a recurrent BERT model\nthat is time-aware for use in VLN. Specifically, we equip the BERT model with a\nrecurrent function that maintains cross-modal state information for the agent.\nThrough extensive experiments on R2R and REVERIE we demonstrate that our model\ncan replace more complex encoder-decoder models to achieve state-of-the-art\nresults. Moreover, our approach can be generalised to other transformer-based\narchitectures, supports pre-training, and is capable of solving navigation and\nreferring expression tasks simultaneously.</p>\n", "tags": ["Agentic","CVPR","Model Architecture","Training Techniques"] },
{"key": "hong2022cogvideo", "citations": "100", "year": "2022", "title":"Cogvideo: Large-scale Pretraining For Text-to-video Generation Via Transformers", "abstract": "<p>Large-scale pretrained transformers have created milestones in text (GPT-3)\nand text-to-image (DALL-E and CogView) generation. Its application to video\ngeneration is still facing many challenges: The potential huge computation cost\nmakes the training from scratch unaffordable; The scarcity and weak relevance\nof text-video datasets hinder the model understanding complex movement\nsemantics. In this work, we present 9B-parameter transformer CogVideo, trained\nby inheriting a pretrained text-to-image model, CogView2. We also propose\nmulti-frame-rate hierarchical training strategy to better align text and video\nclips. As (probably) the first open-source large-scale pretrained text-to-video\nmodel, CogVideo outperforms all publicly available models at a large margin in\nmachine and human evaluations.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "hong2023metagpt", "citations": "62", "year": "2023", "title":"Metagpt: Meta Programming For A Multi-agent Collaborative Framework", "abstract": "<p>Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT</p>\n", "tags": ["Agentic","Has Code","Llm For Code","Prompting","Tools"] },
{"key": "hoover2019exbert", "citations": "141", "year": "2020", "title":"Exbert: A Visual Analysis Tool To Explore Learned Representations In Transformers Models", "abstract": "<p>Large language models can produce powerful contextual representations that\nlead to improvements across many NLP tasks. Since these models are typically\nguided by a sequence of learned self attention mechanisms and may comprise\nundesired inductive biases, it is paramount to be able to explore what the\nattention has learned. While static analyses of these models lead to targeted\ninsights, interactive tools are more dynamic and can help humans better gain an\nintuition for the model-internal reasoning process. We present exBERT, an\ninteractive tool named after the popular BERT language model, that provides\ninsights into the meaning of the contextual representations by matching a\nhuman-specified input to similar contexts in a large annotated dataset. By\naggregating the annotations of the matching similar contexts, exBERT helps\nintuitively explain what each attention-head has learned.</p>\n", "tags": ["Datasets","Model Architecture","Tools"] },
{"key": "hori2017advances", "citations": "318", "year": "2017", "title":"Advances In Joint Ctc-attention Based End-to-end Speech Recognition With A Deep CNN Encoder And RNN-LM", "abstract": "<p>We present a state-of-the-art end-to-end Automatic Speech Recognition (ASR)\nmodel. We learn to listen and write characters with a joint Connectionist\nTemporal Classification (CTC) and attention-based encoder-decoder network. The\nencoder is a deep Convolutional Neural Network (CNN) based on the VGG network.\nThe CTC network sits on top of the encoder and is jointly trained with the\nattention-based decoder. During the beam search process, we combine the CTC\npredictions, the attention-based decoder predictions and a separately trained\nLSTM language model. We achieve a 5-10% error reduction compared to prior\nsystems on spontaneous Japanese and Chinese speech, and our end-to-end model\nbeats out traditional hybrid ASR systems.</p>\n", "tags": ["INTERSPEECH","Model Architecture"] },
{"key": "hori2018end", "citations": "115", "year": "2019", "title":"End-to-end Audio Visual Scene-aware Dialog Using Multimodal Attention-based Video Features", "abstract": "<p>Dialog systems need to understand dynamic visual scenes in order to have\nconversations with users about the objects and events around them. Scene-aware\ndialog systems for real-world applications could be developed by integrating\nstate-of-the-art technologies from multiple research areas, including:\nend-to-end dialog technologies, which generate system responses using models\ntrained from dialog data; visual question answering (VQA) technologies, which\nanswer questions about images using learned image features; and video\ndescription technologies, in which descriptions/captions are generated from\nvideos using multimodal information. We introduce a new dataset of dialogs\nabout videos of human behaviors. Each dialog is a typed conversation that\nconsists of a sequence of 10 question-and-answer(QA) pairs between two Amazon\nMechanical Turk (AMT) workers. In total, we collected dialogs on roughly 9,000\nvideos. Using this new dataset for Audio Visual Scene-aware dialog (AVSD), we\ntrained an end-to-end conversation model that generates responses in a dialog\nabout a video. Our experiments demonstrate that using multimodal features that\nwere developed for multimodal attention-based video description enhances the\nquality of generated dialog about dynamic scenes (videos). Our dataset, model\ncode and pretrained models will be publicly available for a new Video\nScene-Aware Dialog challenge.</p>\n", "tags": ["Applications","Datasets","ICASSP","Model Architecture"] },
{"key": "hori2019end", "citations": "115", "year": "2019", "title":"End-to-end Speech Recognition With Word-based RNN Language Models", "abstract": "<p>This paper investigates the impact of word-based RNN language models\n(RNN-LMs) on the performance of end-to-end automatic speech recognition (ASR).\nIn our prior work, we have proposed a multi-level LM, in which character-based\nand word-based RNN-LMs are combined in hybrid CTC/attention-based ASR. Although\nthis multi-level approach achieves significant error reduction in the Wall\nStreet Journal (WSJ) task, two different LMs need to be trained and used for\ndecoding, which increase the computational cost and memory usage. In this\npaper, we further propose a novel word-based RNN-LM, which allows us to decode\nwith only the word-based LM, where it provides look-ahead word probabilities to\npredict next characters instead of the character-based LM, leading competitive\naccuracy with less computation compared to the multi-level LM. We demonstrate\nthe efficacy of the word-based RNN-LMs using a larger corpus, LibriSpeech, in\naddition to WSJ we used in the prior work. Furthermore, we show that the\nproposed model achieves 5.1 %WER for WSJ Eval’92 test set when the vocabulary\nsize is increased, which is the best WER reported for end-to-end ASR systems on\nthis benchmark.</p>\n", "tags": ["Datasets","Evaluation","ICASSP","Model Architecture"] },
{"key": "hosseiniasl2020simple", "citations": "263", "year": "2020", "title":"A Simple Language Model For Task-oriented Dialogue", "abstract": "<p>Task-oriented dialogue is often decomposed into three tasks: understanding\nuser input, deciding actions, and generating a response. While such\ndecomposition might suggest a dedicated model for each sub-task, we find a\nsimple, unified approach leads to state-of-the-art performance on the MultiWOZ\ndataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a\nsingle, causal language model trained on all sub-tasks recast as a single\nsequence prediction problem. This allows SimpleTOD to fully leverage transfer\nlearning from pre-trained, open domain, causal language models such as GPT-2.\nSimpleTOD improves over the prior state-of-the-art in joint goal accuracy for\ndialogue state tracking, and our analysis reveals robustness to noisy\nannotations in this setting. SimpleTOD also improves the main metrics used to\nevaluate action decisions and response generation in an end-to-end setting:\ninform rate by 8.1 points, success rate by 9.7 points, and combined score by\n7.2 points.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Model Architecture"] },
{"key": "hou2018sequence", "citations": "121", "year": "2018", "title":"Sequence-to-sequence Data Augmentation For Dialogue Language Understanding", "abstract": "<p>In this paper, we study the problem of data augmentation for language\nunderstanding in task-oriented dialogue system. In contrast to previous work\nwhich augments an utterance without considering its relation with other\nutterances, we propose a sequence-to-sequence generation based data\naugmentation framework that leverages one utterance’s same semantic\nalternatives in the training data. A novel diversity rank is incorporated into\nthe utterance representation to make the model produce diverse utterances and\nthese diversely augmented utterances help to improve the language understanding\nmodule. Experimental results on the Airline Travel Information System dataset\nand a newly created semantic frame annotation on Stanford Multi-turn,\nMultidomain Dialogue Dataset show that our framework achieves significant\nimprovements of 6.38 and 10.04 F-scores respectively when only a training set\nof hundreds utterances is represented. Case studies also confirm that our\nmethod generates diverse utterances.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Tools","Training Techniques"] },
{"key": "hou2020dynabert", "citations": "122", "year": "2020", "title":"Dynabert: Dynamic BERT With Adaptive Width And Depth", "abstract": "<p>The pre-trained language models like BERT, though powerful in many natural\nlanguage processing tasks, are both computation and memory expensive. To\nalleviate this problem, one approach is to compress them for specific tasks\nbefore deployment. However, recent works on BERT compression usually compress\nthe large BERT model to a fixed smaller size. They can not fully satisfy the\nrequirements of different edge devices with various hardware performances. In\nthis paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT),\nwhich can flexibly adjust the size and latency by selecting adaptive width and\ndepth. The training process of DynaBERT includes first training a\nwidth-adaptive BERT and then allowing both adaptive width and depth, by\ndistilling knowledge from the full-sized model to small sub-networks. Network\nrewiring is also used to keep the more important attention heads and neurons\nshared by more sub-networks. Comprehensive experiments under various efficiency\nconstraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its\nlargest size has comparable performance as BERT-base (or RoBERTa-base), while\nat smaller widths and depths consistently outperforms existing BERT compression\nmethods. Code is available at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.</p>\n", "tags": ["Efficiency","Has Code","Model Architecture","Training Techniques"] },
{"key": "hou2022learning", "citations": "75", "year": "2023", "title":"Learning Vector-quantized Item Representation For Transferable Sequential Recommenders", "abstract": "<p>Recently, the generality of natural language text has been leveraged to\ndevelop transferable recommender systems. The basic idea is to employ\npre-trained language models~(PLM) to encode item text into item\nrepresentations. Despite the promising transferability, the binding between\nitem text and item representations might be too tight, leading to potential\nproblems such as over-emphasizing the effect of text features and exaggerating\nthe negative impact of domain gap. To address this issue, this paper proposes\nVQ-Rec, a novel approach to learning Vector-Quantized item representations for\ntransferable sequential Recommenders. The main novelty of our approach lies in\nthe new item representation scheme: it first maps item text into a vector of\ndiscrete indices (called item code), and then employs these indices to lookup\nthe code embedding table for deriving item representations. Such a scheme can\nbe denoted as “text \\(\\Longrightarrow\\) code \\(\\Longrightarrow\\) representation”.\nBased on this representation scheme, we further propose an enhanced contrastive\npre-training approach, using semi-synthetic and mixed-domain code\nrepresentations as hard negatives. Furthermore, we design a new cross-domain\nfine-tuning method based on a differentiable permutation-based network.\nExtensive experiments conducted on six public benchmarks demonstrate the\neffectiveness of the proposed approach, in both cross-domain and cross-platform\nsettings. Code and pre-trained model are available at:\nhttps://github.com/RUCAIBox/VQ-Rec.</p>\n", "tags": ["Fine-Tuning","Has Code","Tools","Training Techniques"] },
{"key": "hou2022towards", "citations": "138", "year": "2022", "title":"Towards Universal Sequence Representation Learning For Recommender Systems", "abstract": "<p>In order to develop effective sequential recommenders, a series of sequence\nrepresentation learning (SRL) methods are proposed to model historical user\nbehaviors. Most existing SRL methods rely on explicit item IDs for developing\nthe sequence models to better capture user preference. Though effective to some\nextent, these methods are difficult to be transferred to new recommendation\nscenarios, due to the limitation by explicitly modeling item IDs. To tackle\nthis issue, we present a novel universal sequence representation learning\napproach, named UniSRec. The proposed approach utilizes the associated\ndescription text of items to learn transferable representations across\ndifferent recommendation scenarios. For learning universal item\nrepresentations, we design a lightweight item encoding architecture based on\nparametric whitening and mixture-of-experts enhanced adaptor. For learning\nuniversal sequence representations, we introduce two contrastive pre-training\ntasks by sampling multi-domain negatives. With the pre-trained universal\nsequence representation model, our approach can be effectively transferred to\nnew recommendation domains or platforms in a parameter-efficient way, under\neither inductive or transductive settings. Extensive experiments conducted on\nreal-world datasets demonstrate the effectiveness of the proposed approach.\nEspecially, our approach also leads to a performance improvement in a\ncross-platform setting, showing the strong transferability of the proposed\nuniversal SRL method. The code and pre-trained model are available at:\nhttps://github.com/RUCAIBox/UniSRec.</p>\n", "tags": ["Datasets","Has Code","KDD","Model Architecture","Tools","Training Techniques"] },
{"key": "hou2023large", "citations": "110", "year": "2024", "title":"Large Language Models Are Zero-shot Rankers For Recommender Systems", "abstract": "<p>Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated\nimpressive general-purpose task-solving abilities, including the potential to\napproach recommendation tasks. Along this line of research, this work aims to\ninvestigate the capacity of LLMs that act as the ranking model for recommender\nsystems. We first formalize the recommendation problem as a conditional ranking\ntask, considering sequential interaction histories as conditions and the items\nretrieved by other candidate generation models as candidates. To solve the\nranking task by LLMs, we carefully design the prompting template and conduct\nextensive experiments on two widely-used datasets. We show that LLMs have\npromising zero-shot ranking abilities but (1) struggle to perceive the order of\nhistorical interactions, and (2) can be biased by popularity or item positions\nin the prompts. We demonstrate that these issues can be alleviated using\nspecially designed prompting and bootstrapping strategies. Equipped with these\ninsights, zero-shot LLMs can even challenge conventional recommendation models\nwhen ranking candidates are retrieved by multiple candidate generators. The\ncode and processed datasets are available at\nhttps://github.com/RUCAIBox/LLMRank.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Prompting"] },
{"key": "hou2024large", "citations": "110", "year": "2024", "title":"Large Language Models As Misleading Assistants In Conversation", "abstract": "<p>Large Language Models (LLMs) are able to provide assistance on a wide range\nof information-seeking tasks. However, model outputs may be misleading, whether\nunintentionally or in cases of intentional deception. We investigate the\nability of LLMs to be deceptive in the context of providing assistance on a\nreading comprehension task, using LLMs as proxies for human users. We compare\noutcomes of (1) when the model is prompted to provide truthful assistance, (2)\nwhen it is prompted to be subtly misleading, and (3) when it is prompted to\nargue for an incorrect answer. Our experiments show that GPT-4 can effectively\nmislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up\nto a 23% drop in accuracy on the task compared to when a truthful assistant is\nused. We also find that providing the user model with additional context from\nthe passage partially mitigates the influence of the deceptive model. This work\nhighlights the ability of LLMs to produce misleading information and the\neffects this may have in real-world situations.</p>\n", "tags": ["Model Architecture"] },
{"key": "houlsby2019parameter", "citations": "1102", "year": "2019", "title":"Parameter-efficient Transfer Learning For NLP", "abstract": "<p>Fine-tuning large pre-trained models is an effective transfer mechanism in\nNLP. However, in the presence of many downstream tasks, fine-tuning is\nparameter inefficient: an entire new model is required for every task. As an\nalternative, we propose transfer with adapter modules. Adapter modules yield a\ncompact and extensible model; they add only a few trainable parameters per\ntask, and new tasks can be added without revisiting previous ones. The\nparameters of the original network remain fixed, yielding a high degree of\nparameter sharing. To demonstrate adapter’s effectiveness, we transfer the\nrecently proposed BERT Transformer model to 26 diverse text classification\ntasks, including the GLUE benchmark. Adapters attain near state-of-the-art\nperformance, whilst adding only a few parameters per task. On GLUE, we attain\nwithin 0.4% of the performance of full fine-tuning, adding only 3.6% parameters\nper task. By contrast, fine-tuning trains 100% of the parameters per task.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "howard2018universal", "citations": "3464", "year": "2018", "title":"Universal Language Model Fine-tuning For Text Classification", "abstract": "<p>Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "hrinchuk2019correction", "citations": "78", "year": "2020", "title":"Correction Of Automatic Speech Recognition With Transformer Sequence-to-sequence Model", "abstract": "<p>In this work, we introduce a simple yet efficient post-processing model for\nautomatic speech recognition (ASR). Our model has Transformer-based\nencoder-decoder architecture which “translates” ASR model output into\ngrammatically and semantically correct text. We investigate different\nstrategies for regularizing and optimizing the model and show that extensive\ndata augmentation and the initialization with pre-trained weights are required\nto achieve good performance. On the LibriSpeech benchmark, our method\ndemonstrates significant improvement in word error rate over the baseline\nacoustic model with greedy decoding, especially on much noisier dev-other and\ntest-other portions of the evaluation dataset. Our model also outperforms\nbaseline with 6-gram language model re-scoring and approaches the performance\nof re-scoring with Transformer-XL neural language model.</p>\n", "tags": ["Datasets","Evaluation","ICASSP","Model Architecture"] },
{"key": "hsiao2021compound", "citations": "108", "year": "2021", "title":"Compound Word Transformer: Learning To Compose Full-song Music Over Dynamic Directed Hypergraphs", "abstract": "<p>To apply neural sequence models such as the Transformers to music generation\ntasks, one has to represent a piece of music by a sequence of tokens drawn from\na finite set of pre-defined vocabulary. Such a vocabulary usually involves\ntokens of various types. For example, to describe a musical note, one needs\nseparate tokens to indicate the note’s pitch, duration, velocity (dynamics),\nand placement (onset time) along the time grid. While different types of tokens\nmay possess different properties, existing models usually treat them equally,\nin the same way as modeling words in natural languages. In this paper, we\npresent a conceptually different approach that explicitly takes into account\nthe type of the tokens, such as note types and metric types. And, we propose a\nnew Transformer decoder architecture that uses different feed-forward heads to\nmodel tokens of different types. With an expansion-compression trick, we\nconvert a piece of music to a sequence of compound words by grouping\nneighboring tokens, greatly reducing the length of the token sequences. We show\nthat the resulting model can be viewed as a learner over dynamic directed\nhypergraphs. And, we employ it to learn to compose expressive Pop piano music\nof full-song length (involving up to 10K individual tokens per song), both\nconditionally and unconditionally. Our experiment shows that, compared to\nstate-of-the-art models, the proposed model converges 5–10 times faster at\ntraining (i.e., within a day on a single GPU with 11 GB memory), and with\ncomparable quality in the generated music.</p>\n", "tags": ["AAAI","Model Architecture","Training Techniques"] },
{"key": "hsieh2023distilling", "citations": "110", "year": "2023", "title":"Distilling Step-by-step! Outperforming Larger Language Models With Less Training Data And Smaller Model Sizes", "abstract": "<p>Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step .</p>\n", "tags": ["Applications","Efficiency","Evaluation","Few-Shot","Has Code","Training Techniques"] },
{"key": "hsu2018unified", "citations": "279", "year": "2018", "title":"A Unified Model For Extractive And Abstractive Summarization Using Inconsistency Loss", "abstract": "<p>We propose a unified model combining the strength of extractive and\nabstractive summarization. On the one hand, a simple extractive model can\nobtain sentence-level attention with high ROUGE scores but less readable. On\nthe other hand, a more complicated abstractive model can obtain word-level\ndynamic attention to generate a more readable paragraph. In our model,\nsentence-level attention is used to modulate the word-level attention such that\nwords in less attended sentences are less likely to be generated. Moreover, a\nnovel inconsistency loss function is introduced to penalize the inconsistency\nbetween two levels of attentions. By end-to-end training our model with the\ninconsistency loss and original losses of extractive and abstractive models, we\nachieve state-of-the-art ROUGE scores while being the most informative and\nreadable summarization on the CNN/Daily Mail dataset in a solid human\nevaluation.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "htut2019do", "citations": "103", "year": "2019", "title":"Do Attention Heads In BERT Track Syntactic Dependencies?", "abstract": "<p>We investigate the extent to which individual attention heads in pretrained\ntransformer language models, such as BERT and RoBERTa, implicitly capture\nsyntactic dependency relations. We employ two methods—taking the maximum\nattention weight and computing the maximum spanning tree—to extract implicit\ndependency relations from the attention weights of each layer/head, and compare\nthem to the ground-truth Universal Dependency (UD) trees. We show that, for\nsome UD relation types, there exist heads that can recover the dependency type\nsignificantly better than baselines on parsed English text, suggesting that\nsome self-attention heads act as a proxy for syntactic structure. We also\nanalyze BERT fine-tuned on two datasets—the syntax-oriented CoLA and the\nsemantics-oriented MNLI—to investigate whether fine-tuning affects the\npatterns of their self-attention, but we do not observe substantial differences\nin the overall dependency relations extracted using our methods. Our results\nsuggest that these models have some specialist attention heads that track\nindividual dependency types, but no generalist head that performs holistic\nparsing significantly better than a trivial baseline, and that analyzing\nattention weights directly may not reveal much of the syntactic knowledge that\nBERT-style models are known to learn.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "hu2017learning", "citations": "509", "year": "2017", "title":"Learning To Reason: End-to-end Module Networks For Visual Question Answering", "abstract": "<p>Natural language questions are inherently compositional, and many are most\neasily answered by reasoning about their decomposition into modular\nsub-problems. For example, to answer “is there an equal number of balls and\nboxes?” we can look for balls, look for boxes, count them, and compare the\nresults. The recently proposed Neural Module Network (NMN) architecture\nimplements this approach to question answering by parsing questions into\nlinguistic substructures and assembling question-specific deep networks from\nsmaller modules that each solve one subtask. However, existing NMN\nimplementations rely on brittle off-the-shelf parsers, and are restricted to\nthe module configurations proposed by these parsers rather than learning them\nfrom data. In this paper, we propose End-to-End Module Networks (N2NMNs), which\nlearn to reason by directly predicting instance-specific network layouts\nwithout the aid of a parser. Our model learns to generate network structures\n(by imitating expert demonstrations) while simultaneously learning network\nparameters (using the downstream task loss). Experimental results on the new\nCLEVR dataset targeted at compositional question answering show that N2NMNs\nachieve an error reduction of nearly 50% relative to state-of-the-art\nattentional approaches, while discovering interpretable network architectures\nspecialized for each question.</p>\n", "tags": ["Datasets","ICCV","Model Architecture"] },
{"key": "hu2017reinforced", "citations": "197", "year": "2018", "title":"Reinforced Mnemonic Reader For Machine Reading Comprehension", "abstract": "<p>In this paper, we introduce the Reinforced Mnemonic Reader for machine\nreading comprehension tasks, which enhances previous attentive readers in two\naspects. First, a reattention mechanism is proposed to refine current\nattentions by directly accessing to past attentions that are temporally\nmemorized in a multi-round alignment architecture, so as to avoid the problems\nof attention redundancy and attention deficiency. Second, a new optimization\napproach, called dynamic-critical reinforcement learning, is introduced to\nextend the standard supervised method. It always encourages to predict a more\nacceptable answer so as to address the convergence suppression problem occurred\nin traditional reinforcement learning algorithms. Extensive experiments on the\nStanford Question Answering Dataset (SQuAD) show that our model achieves\nstate-of-the-art results. Meanwhile, our model outperforms previous systems by\nover 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD\ndatasets.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","IJCAI","Model Architecture","Reinforcement Learning"] },
{"key": "hu2017toward", "citations": "781", "year": "2017", "title":"Toward Controlled Generation Of Text", "abstract": "<p>Generic generation and manipulation of text is challenging and has limited\nsuccess compared to recent deep generative modeling in visual domain. This\npaper aims at generating plausible natural language sentences, whose attributes\nare dynamically controlled by learning disentangled latent representations with\ndesignated semantics. We propose a new neural generative model which combines\nvariational auto-encoders and holistic attribute discriminators for effective\nimposition of semantic structures. With differentiable approximation to\ndiscrete text samples, explicit constraints on independent attribute controls,\nand efficient collaborative learning of generator and discriminators, our model\nlearns highly interpretable representations from even only word annotations,\nand produces realistic sentences with desired attributes. Quantitative\nevaluation validates the accuracy of sentence and attribute generation.</p>\n", "tags": ["Evaluation"] },
{"key": "hu2018explainable", "citations": "189", "year": "2018", "title":"Explainable Neural Computation Via Stack Neural Module Networks", "abstract": "<p>In complex inferential tasks like question answering, machine learning models\nmust confront two challenges: the need to implement a compositional reasoning\nprocess, and, in many applications, the need for this reasoning process to be\ninterpretable to assist users in both development and prediction. Existing\nmodels designed to produce interpretable traces of their decision-making\nprocess typically require these traces to be supervised at training time. In\nthis paper, we present a novel neural modular approach that performs\ncompositional reasoning by automatically inducing a desired sub-task\ndecomposition without relying on strong supervision. Our model allows linking\ndifferent reasoning tasks though shared modules that handle common routines\nacross tasks. Experiments show that the model is more interpretable to human\nevaluators compared to other state-of-the-art models: users can better\nunderstand the model’s underlying reasoning procedure and predict when it will\nsucceed or fail based on observing its intermediate outputs.</p>\n", "tags": ["Applications","Training Techniques"] },
{"key": "hu2018read", "citations": "149", "year": "2019", "title":"Read + Verify: Machine Reading Comprehension With Unanswerable Questions", "abstract": "<p>Machine reading comprehension with unanswerable questions aims to abstain\nfrom answering when no answer can be inferred. In addition to extract answers,\nprevious works usually predict an additional “no-answer” probability to detect\nunanswerable cases. However, they fail to validate the answerability of the\nquestion by verifying the legitimacy of the predicted answer. To address this\nproblem, we propose a novel read-then-verify system, which not only utilizes a\nneural reader to extract candidate answers and produce no-answer probabilities,\nbut also leverages an answer verifier to decide whether the predicted answer is\nentailed by the input snippets. Moreover, we introduce two auxiliary losses to\nhelp the reader better handle answer extraction as well as no-answer detection,\nand investigate three different architectures for the answer verifier. Our\nexperiments on the SQuAD 2.0 dataset show that our system achieves a score of\n74.2 F1 on the test set, achieving state-of-the-art results at the time of\nsubmission (Aug. 28th, 2018).</p>\n", "tags": ["AAAI","Datasets","Evaluation"] },
{"key": "hu2019are", "citations": "77", "year": "2019", "title":"Are You Looking? Grounding To Multiple Modalities In Vision-and-language Navigation", "abstract": "<p>Vision-and-Language Navigation (VLN) requires grounding instructions, such as\n“turn right and stop at the door”, to routes in a visual environment. The\nactual grounding can connect language to the environment through multiple\nmodalities, e.g. “stop at the door” might ground into visual objects, while\n“turn right” might rely only on the geometric structure of a route. We\ninvestigate where the natural language empirically grounds under two recent\nstate-of-the-art VLN models. Surprisingly, we discover that visual features may\nactually hurt these models: models which only use route structure, ablating\nvisual features, outperform their visual counterparts in unseen new\nenvironments on the benchmark Room-to-Room dataset. To better use all the\navailable modalities, we propose to decompose the grounding procedure into a\nset of expert models with access to different modalities (including object\ndetections) and ensemble them at prediction time, improving the performance of\nstate-of-the-art models on the VLN task.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "hu2019domain", "citations": "65", "year": "2019", "title":"Domain Adaptation Of Neural Machine Translation By Lexicon Induction", "abstract": "<p>It has been previously noted that neural machine translation (NMT) is very\nsensitive to domain shift. In this paper, we argue that this is a dual effect\nof the highly lexicalized nature of NMT, resulting in failure for sentences\nwith large numbers of unknown words, and lack of supervision for\ndomain-specific words. To remedy this problem, we propose an unsupervised\nadaptation method which fine-tunes a pre-trained out-of-domain NMT model using\na pseudo-in-domain corpus. Specifically, we perform lexicon induction to\nextract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus\nby performing word-for-word back-translation of monolingual in-domain target\nsentences. In five domains over twenty pairwise adaptation settings and two\nmodel architectures, our method achieves consistent improvements without using\nany in-domain parallel sentences, improving up to 14 BLEU over unadapted\nmodels, and up to 2 BLEU over strong back-translation baselines.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture"] },
{"key": "hu2019iterative", "citations": "188", "year": "2020", "title":"Iterative Answer Prediction With Pointer-augmented Multimodal Transformers For Textvqa", "abstract": "<p>Many visual scenes contain text that carries crucial information, and it is\nthus essential to understand text in images for downstream reasoning tasks. For\nexample, a deep water label on a warning sign warns people about the danger in\nthe scene. Recent work has explored the TextVQA task that requires reading and\nunderstanding text in images to answer a question. However, existing approaches\nfor TextVQA are mostly based on custom pairwise fusion mechanisms between a\npair of two modalities and are restricted to a single prediction step by\ncasting TextVQA as a classification task. In this work, we propose a novel\nmodel for the TextVQA task based on a multimodal transformer architecture\naccompanied by a rich representation for text in images. Our model naturally\nfuses different modalities homogeneously by embedding them into a common\nsemantic space where self-attention is applied to model inter- and intra-\nmodality context. Furthermore, it enables iterative answer decoding with a\ndynamic pointer network, allowing the model to form an answer through\nmulti-step prediction instead of one-step classification. Our model outperforms\nexisting approaches on three benchmark datasets for the TextVQA task by a large\nmargin.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture"] },
{"key": "hu2019multi", "citations": "83", "year": "2019", "title":"A Multi-type Multi-span Network For Reading Comprehension That Requires Discrete Reasoning", "abstract": "<p>Rapid progress has been made in the field of reading comprehension and\nquestion answering, where several systems have achieved human parity in some\nsimplified settings. However, the performance of these models degrades\nsignificantly when they are applied to more realistic scenarios, such as\nanswers involve various types, multiple text strings are correct answers, or\ndiscrete reasoning abilities are required. In this paper, we introduce the\nMulti-Type Multi-Span Network (MTMSN), a neural reading comprehension model\nthat combines a multi-type answer predictor designed to support various answer\ntypes (e.g., span, count, negation, and arithmetic expression) with a\nmulti-span extraction method for dynamically producing one or multiple text\nspans. In addition, an arithmetic expression reranking mechanism is proposed to\nrank expression candidates for further confirming the prediction. Experiments\nshow that our model achieves 79.9 F1 on the DROP hidden test set, creating new\nstate-of-the-art results. Source\ncode\\footnote{https://github.com/huminghao16/MTMSN} is released to\nfacilitate future work.</p>\n", "tags": ["EMNLP","Evaluation","Has Code"] },
{"key": "hu2019parabank", "citations": "85", "year": "2019", "title":"Parabank: Monolingual Bitext Generation And Sentential Paraphrasing Via Lexically-constrained Neural Machine Translation", "abstract": "<p>We present ParaBank, a large-scale English paraphrase dataset that surpasses\nprior work in both quantity and quality. Following the approach of ParaNMT, we\ntrain a Czech-English neural machine translation (NMT) system to generate novel\nparaphrases of English reference sentences. By adding lexical constraints to\nthe NMT decoding procedure, however, we are able to produce multiple\nhigh-quality sentential paraphrases per source sentence, yielding an English\nparaphrase resource with more than 4 billion generated tokens and exhibiting\ngreater lexical diversity. Using human judgments, we also demonstrate that\nParaBank’s paraphrases improve over ParaNMT on both semantic similarity and\nfluency. Finally, we use ParaBank to train a monolingual NMT model with the\nsame support for lexically-constrained decoding for sentence rewriting tasks.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "hu2020ocnli", "citations": "72", "year": "2020", "title":"OCNLI: Original Chinese Natural Language Inference", "abstract": "<p>Despite the tremendous recent progress on natural language inference (NLI),\ndriven largely by large-scale investment in new datasets (e.g., SNLI, MNLI) and\nadvances in modeling, most progress has been limited to English due to a lack\nof reliable datasets for most of the world’s languages. In this paper, we\npresent the first large-scale NLI dataset (consisting of ~56,000 annotated\nsentence pairs) for Chinese called the Original Chinese Natural Language\nInference dataset (OCNLI). Unlike recent attempts at extending NLI to other\nlanguages, our dataset does not rely on any automatic translation or non-expert\nannotation. Instead, we elicit annotations from native speakers specializing in\nlinguistics. We follow closely the annotation protocol used for MNLI, but\ncreate new strategies for eliciting diverse hypotheses. We establish several\nbaseline results on our dataset using state-of-the-art pre-trained models for\nChinese, and find even the best performing models to be far outpaced by human\nperformance (~12% absolute performance gap), making it a challenging new\nresource that we hope will help to accelerate progress in Chinese NLU. To the\nbest of our knowledge, this is the first human-elicited MNLI-style corpus for a\nnon-English language.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "hu2020xtreme", "citations": "326", "year": "2020", "title":"XTREME: A Massively Multilingual Multi-task Benchmark For Evaluating Cross-lingual Generalization", "abstract": "<p>Much recent progress in applications of machine learning models to NLP has\nbeen driven by benchmarks that evaluate models across a wide variety of tasks.\nHowever, these broad-coverage benchmarks have been mostly limited to English,\nand despite an increasing interest in multilingual models, a benchmark that\nenables the comprehensive evaluation of such methods on a diverse range of\nlanguages and tasks is still missing. To this end, we introduce the\nCross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a\nmulti-task benchmark for evaluating the cross-lingual generalization\ncapabilities of multilingual representations across 40 languages and 9 tasks.\nWe demonstrate that while models tested on English reach human performance on\nmany tasks, there is still a sizable gap in the performance of cross-lingually\ntransferred models, particularly on syntactic and sentence retrieval tasks.\nThere is also a wide spread of results across languages. We release the\nbenchmark to encourage research on cross-lingual learning methods that transfer\nlinguistic knowledge across a diverse and representative set of languages and\ntasks.</p>\n", "tags": ["Applications","Datasets","Evaluation"] },
{"key": "hu2021knowledgeable", "citations": "194", "year": "2022", "title":"Knowledgeable Prompt-tuning: Incorporating Knowledge Into Prompt Verbalizer For Text Classification", "abstract": "<p>Tuning pre-trained language models (PLMs) with task-specific prompts has been\na promising approach for text classification. Particularly, previous studies\nsuggest that prompt-tuning has remarkable superiority in the low-data scenario\nover the generic fine-tuning methods with extra classifiers. The core idea of\nprompt-tuning is to insert text pieces, i.e., template, to the input and\ntransform a classification problem into a masked language modeling problem,\nwhere a crucial step is to construct a projection, i.e., verbalizer, between a\nlabel space and a label word space. A verbalizer is usually handcrafted or\nsearched by gradient descent, which may lack coverage and bring considerable\nbias and high variances to the results. In this work, we focus on incorporating\nexternal knowledge into the verbalizer, forming a knowledgeable prompt-tuning\n(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the\nlabel word space of the verbalizer using external knowledge bases (KBs) and\nrefine the expanded label word space with the PLM itself before predicting with\nthe expanded label word space. Extensive experiments on zero and few-shot text\nclassification tasks demonstrate the effectiveness of knowledgeable\nprompt-tuning.</p>\n", "tags": ["Few-Shot","Fine-Tuning","Prompting","Training Techniques"] },
{"key": "hu2021text", "citations": "123", "year": "2022", "title":"Text To Image Generation With Semantic-spatial Aware GAN", "abstract": "<p>Text-to-image synthesis (T2I) aims to generate photo-realistic images which\nare semantically consistent with the text descriptions. Existing methods are\nusually built upon conditional generative adversarial networks (GANs) and\ninitialize an image from noise with sentence embedding, and then refine the\nfeatures with fine-grained word embedding iteratively. A close inspection of\ntheir generated images reveals a major limitation: even though the generated\nimage holistically matches the description, individual image regions or parts\nof somethings are often not recognizable or consistent with words in the\nsentence, e.g. “a white crown”. To address this problem, we propose a novel\nframework Semantic-Spatial Aware GAN for synthesizing images from input text.\nConcretely, we introduce a simple and effective Semantic-Spatial Aware block,\nwhich (1) learns semantic-adaptive transformation conditioned on text to\neffectively fuse text features and image features, and (2) learns a semantic\nmask in a weakly-supervised way that depends on the current text-image fusion\nprocess in order to guide the transformation spatially. Experiments on the\nchallenging COCO and CUB bird datasets demonstrate the advantage of our method\nover the recent state-of-the-art approaches, regarding both visual fidelity and\nalignment with input text description.</p>\n", "tags": ["CVPR","Datasets","Tools"] },
{"key": "hu2021unit", "citations": "213", "year": "2021", "title":"Unit: Multimodal Multitask Learning With A Unified Transformer", "abstract": "<p>We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with significantly\nfewer parameters. Our code is available in MMF at https://mmf.sh.</p>\n", "tags": ["Datasets","Fine-Tuning","ICCV","Model Architecture","Training Techniques"] },
{"key": "hu2023llm", "citations": "99", "year": "2023", "title":"Llm-adapters: An Adapter Family For Parameter-efficient Fine-tuning Of Large Language Models", "abstract": "<p>The success of large language models (LLMs), like GPT-4 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by finetuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as\nSeries adapters, Parallel adapter, Prompt-based learning and\nReparametrization-based methods. Moreover, we conduct extensive empirical\nstudies on the impact of adapter types, placement locations, and\nhyper-parameters to the best design for each adapter-based methods. We evaluate\nthe effectiveness of the adapters on fourteen datasets from two different\nreasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results\ndemonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few\nextra trainable parameters yields comparable, and in some cases superior,\nperformance to powerful LLMs (175B) in zero-shot inference on both reasoning\ntasks.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "hu2023opportunities", "citations": "64", "year": "2023", "title":"Opportunities And Challenges Of Chatgpt For Design Knowledge Management", "abstract": "<p>Recent advancements in Natural Language Processing have opened up new\npossibilities for the development of large language models like ChatGPT, which\ncan facilitate knowledge management in the design process by providing\ndesigners with access to a vast array of relevant information. However,\nintegrating ChatGPT into the design process also presents new challenges. In\nthis paper, we provide a concise review of the classification and\nrepresentation of design knowledge, and past efforts to support designers in\nacquiring knowledge. We analyze the opportunities and challenges that ChatGPT\npresents for knowledge management in design and propose promising future\nresearch directions. A case study is conducted to validate the advantages and\ndrawbacks of ChatGPT, showing that designers can acquire targeted knowledge\nfrom various domains, but the quality of the acquired knowledge is highly\ndependent on the prompt.</p>\n", "tags": ["Prompting"] },
{"key": "huang2016instance", "citations": "242", "year": "2017", "title":"Instance-aware Image And Sentence Matching With Selective Multimodal LSTM", "abstract": "<p>Effective image and sentence matching depends on how to well measure their\nglobal visual-semantic similarity. Based on the observation that such a global\nsimilarity arises from a complex aggregation of multiple local similarities\nbetween pairwise instances of image (objects) and sentence (words), we propose\na selective multimodal Long Short-Term Memory network (sm-LSTM) for\ninstance-aware image and sentence matching. The sm-LSTM includes a multimodal\ncontext-modulated attention scheme at each timestep that can selectively attend\nto a pair of instances of image and sentence, by predicting pairwise\ninstance-aware saliency maps for image and sentence. For selected pairwise\ninstances, their representations are obtained based on the predicted saliency\nmaps, and then compared to measure their local similarity. By similarly\nmeasuring multiple local similarities within a few timesteps, the sm-LSTM\nsequentially aggregates them with hidden states to obtain a final matching\nscore as the desired global similarity. Extensive experiments show that our\nmodel can well match image and sentence with complex content, and achieve the\nstate-of-the-art results on two public benchmark datasets.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture"] },
{"key": "huang2017fusionnet", "citations": "90", "year": "2017", "title":"Fusionnet: Fusing Via Fully-aware Attention With Application To Machine Comprehension", "abstract": "<p>This paper introduces a new neural structure called FusionNet, which extends\nexisting attention approaches from three perspectives. First, it puts forward a\nnovel concept of “history of word” to characterize attention information from\nthe lowest word-level embedding up to the highest semantic-level\nrepresentation. Second, it introduces an improved attention scoring function\nthat better utilizes the “history of word” concept. Third, it proposes a\nfully-aware multi-level attention mechanism to capture the complete information\nin one text (such as a question) and exploit it in its counterpart (such as\ncontext or passage) layer by layer. We apply FusionNet to the Stanford Question\nAnswering Dataset (SQuAD) and it achieves the first position for both single\nand ensemble model on the official SQuAD leaderboard at the time of writing\n(Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two\nadversarial SQuAD datasets and it sets up the new state-of-the-art on both\ndatasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to\n51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "huang2018flowqa", "citations": "86", "year": "2018", "title":"Flowqa: Grasping Flow In History For Conversational Machine Comprehension", "abstract": "<p>Conversational machine comprehension requires the understanding of the\nconversation history, such as previous question/answer pairs, the document\ncontext, and the current question. To enable traditional, single-turn models to\nencode the history comprehensively, we introduce Flow, a mechanism that can\nincorporate intermediate representations generated during the process of\nanswering previous questions, through an alternating parallel processing\nstructure. Compared to approaches that concatenate previous questions/answers\nas input, Flow integrates the latent semantics of the conversation history more\ndeeply. Our model, FlowQA, shows superior performance on two recently proposed\nconversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The\neffectiveness of Flow also shows in other tasks. By reducing sequential\ninstruction understanding to conversational machine comprehension, FlowQA\noutperforms the best models on all three domains in SCONE, with +1.8% to +4.4%\nimprovement in accuracy.</p>\n", "tags": [] },
{"key": "huang2018gpipe", "citations": "830", "year": "2018", "title":"Gpipe: Efficient Training Of Giant Neural Networks Using Pipeline Parallelism", "abstract": "<p>Scaling up deep neural network capacity has been known as an effective\napproach to improving model quality for several different machine learning\ntasks. In many cases, increasing model capacity beyond the memory limit of a\nsingle accelerator has required developing special algorithms or\ninfrastructure. These solutions are often architecture-specific and do not\ntransfer to other tasks. To address the need for efficient and task-independent\nmodel parallelism, we introduce GPipe, a pipeline parallelism library that\nallows scaling any network that can be expressed as a sequence of layers. By\npipelining different sub-sequences of layers on separate accelerators, GPipe\nprovides the flexibility of scaling a variety of different networks to gigantic\nsizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining\nalgorithm, resulting in almost linear speedup when a model is partitioned\nacross multiple accelerators. We demonstrate the advantages of GPipe by\ntraining large-scale neural networks on two different tasks with distinct\nnetwork architectures: (i) Image Classification: We train a\n557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on\nImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single\n6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100\nlanguages and achieve better quality than all bilingual models.</p>\n", "tags": ["Efficiency","Model Architecture","Tools","Training Techniques"] },
{"key": "huang2018hierarchically", "citations": "96", "year": "2019", "title":"Hierarchically Structured Reinforcement Learning For Topically Coherent Visual Story Generation", "abstract": "<p>We propose a hierarchically structured reinforcement learning approach to\naddress the challenges of planning for generating coherent multi-sentence\nstories for the visual storytelling task. Within our framework, the task of\ngenerating a story given a sequence of images is divided across a two-level\nhierarchical decoder. The high-level decoder constructs a plan by generating a\nsemantic concept (i.e., topic) for each image in sequence. The low-level\ndecoder generates a sentence for each image using a semantic compositional\nnetwork, which effectively grounds the sentence generation conditioned on the\ntopic. The two decoders are jointly trained end-to-end using reinforcement\nlearning. We evaluate our model on the visual storytelling (VIST) dataset.\nEmpirical results from both automatic and human evaluations demonstrate that\nthe proposed hierarchically structured reinforced training achieves\nsignificantly better performance compared to a strong flat deep reinforcement\nlearning baseline.</p>\n", "tags": ["AAAI","Agentic","Datasets","Reinforcement Learning","Training Techniques"] },
{"key": "huang2018natural", "citations": "125", "year": "2018", "title":"Natural Language To Structured Query Generation Via Meta-learning", "abstract": "<p>In conventional supervised training, a model is trained to fit all the\ntraining examples. However, having a monolithic model may not always be the\nbest strategy, as examples could vary widely. In this work, we explore a\ndifferent learning protocol that treats each example as a unique pseudo-task,\nby reducing the original learning problem to a few-shot meta-learning scenario\nwith the help of a domain-dependent relevance function. When evaluated on the\nWikiSQL dataset, our approach leads to faster convergence and achieves\n1.1%-5.4% absolute accuracy gains over the non-meta-learning counterparts.</p>\n", "tags": ["Datasets","Few-Shot","NAACL","Training Techniques"] },
{"key": "huang2019ana", "citations": "71", "year": "2019", "title":"ANA At Semeval-2019 Task 3: Contextual Emotion Detection In Conversations Through Hierarchical Lstms And BERT", "abstract": "<p>This paper describes the system submitted by ANA Team for the SemEval-2019\nTask 3: EmoContext. We propose a novel Hierarchical LSTMs for Contextual\nEmotion Detection (HRLCE) model. It classifies the emotion of an utterance\ngiven its conversational context. The results show that, in this task, our\nHRCLE outperforms the most recent state-of-the-art text classification\nframework: BERT. We combine the results generated by BERT and HRCLE to achieve\nan overall score of 0.7709 which ranked 5th on the final leader board of the\ncompetition among 165 Teams.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","Model Architecture","Tools"] },
{"key": "huang2019challenges", "citations": "273", "year": "2020", "title":"Challenges In Building Intelligent Open-domain Dialog Systems", "abstract": "<p>There is a resurgent interest in developing intelligent open-domain dialog\nsystems due to the availability of large amounts of conversational data and the\nrecent progress on neural approaches to conversational AI. Unlike traditional\ntask-oriented bots, an open-domain dialog system aims to establish long-term\nconnections with users by satisfying the human need for communication,\naffection, and social belonging. This paper reviews the recent works on neural\napproaches that are devoted to addressing three challenges in developing such\nsystems: semantics, consistency, and interactiveness. Semantics requires a\ndialog system to not only understand the content of the dialog but also\nidentify user’s social needs during the conversation. Consistency requires the\nsystem to demonstrate a consistent personality to win users trust and gain\ntheir long-term confidence. Interactiveness refers to the system’s ability to\ngenerate interpersonal responses to achieve particular social goals such as\nentertainment, conforming, and task completion. The works we select to present\nhere is based on our unique views and are by no means complete. Nevertheless,\nwe hope that the discussion will inspire new research in developing more\nintelligent dialog systems.</p>\n", "tags": [] },
{"key": "huang2019cosmos", "citations": "268", "year": "2019", "title":"Cosmos QA: Machine Reading Comprehension With Contextual Commonsense Reasoning", "abstract": "<p>Understanding narratives requires reading between the lines, which in turn,\nrequires interpreting the likely causes and effects of events, even when they\nare not mentioned explicitly. In this paper, we introduce Cosmos QA, a\nlarge-scale dataset of 35,600 problems that require commonsense-based reading\ncomprehension, formulated as multiple-choice questions. In stark contrast to\nmost existing reading comprehension datasets where the questions focus on\nfactual and literal understanding of the context paragraph, our dataset focuses\non reading between the lines over a diverse collection of people’s everyday\nnarratives, asking such questions as “what might be the possible reason of\n…?”, or “what would have happened if …” that require reasoning beyond the\nexact text spans in the context. To establish baseline performances on Cosmos\nQA, we experiment with several state-of-the-art neural architectures for\nreading comprehension, and also propose a new architecture that improves over\nthe competitive baselines. Experimental results demonstrate a significant gap\nbetween machine (68.4%) and human performance (94%), pointing to avenues for\nfuture research on commonsense machine comprehension. Dataset, code and\nleaderboard is publicly available at https://wilburone.github.io/cosmos.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code","Model Architecture"] },
{"key": "huang2019reducing", "citations": "113", "year": "2020", "title":"Reducing Sentiment Bias In Language Models Via Counterfactual Evaluation", "abstract": "<p>Advances in language modeling architectures and the availability of large\ntext corpora have driven progress in automatic text generation. While this\nresults in models capable of generating coherent texts, it also prompts models\nto internalize social biases present in the training corpus. This paper aims to\nquantify and reduce a particular type of bias exhibited by language models:\nbias in the sentiment of generated text. Given a conditioning context (e.g., a\nwriting prompt) and a language model, we analyze if (and how) the sentiment of\nthe generated text is affected by changes in values of sensitive attributes\n(e.g., country names, occupations, genders) in the conditioning context using a\nform of counterfactual evaluation. We quantify sentiment bias by adopting\nindividual and group fairness metrics from the fair machine learning\nliterature, and demonstrate that large-scale models trained on two different\ncorpora (news articles, and Wikipedia) exhibit considerable levels of bias. We\nthen propose embedding and sentiment prediction-derived regularization on the\nlanguage model’s latent representations. The regularizations improve fairness\nmetrics while retaining comparable levels of perplexity and semantic\nsimilarity.</p>\n", "tags": ["Datasets","EMNLP","Ethics & Fairness","Evaluation","Training Techniques"] },
{"key": "huang2019transferable", "citations": "83", "year": "2019", "title":"Transferable Representation Learning In Vision-and-language Navigation", "abstract": "<p>Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require\nmachine agents to interpret natural language instructions and learn to act in\nvisually realistic environments to achieve navigation goals. The overall task\nrequires competence in several perception problems: successful agents combine\nspatio-temporal, vision and language understanding to produce appropriate\naction sequences. Our approach adapts pre-trained vision and language\nrepresentations to relevant in-domain tasks making them more effective for VLN.\nSpecifically, the representations are adapted to solve both a cross-modal\nsequence alignment and sequence coherence task. In the sequence alignment task,\nthe model determines whether an instruction corresponds to a sequence of visual\nframes. In the sequence coherence task, the model determines whether the\nperceptual sequences are predictive sequentially in the instruction-conditioned\nlatent space. By transferring the domain-adapted representations, we improve\ncompetitive agents in R2R as measured by the success rate weighted by path\nlength (SPL) metric.</p>\n", "tags": ["ICCV"] },
{"key": "huang2019unicoder", "citations": "166", "year": "2019", "title":"Unicoder: A Universal Language Encoder By Pre-training With Multiple Cross-lingual Tasks", "abstract": "<p>We present Unicoder, a universal language encoder that is insensitive to\ndifferent languages. Given an arbitrary NLP task, a model can be trained with\nUnicoder using training data in one language and directly applied to inputs of\nthe same task in other languages. Comparing to similar efforts such as\nMultilingual BERT and XLM, three new cross-lingual pre-training tasks are\nproposed, including cross-lingual word recovery, cross-lingual paraphrase\nclassification and cross-lingual masked language model. These tasks help\nUnicoder learn the mappings among different languages from more perspectives.\nWe also find that doing fine-tuning on multiple languages together can bring\nfurther improvement. Experiments are performed on two tasks: cross-lingual\nnatural language inference (XNLI) and cross-lingual question answering (XQA),\nwhere XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement (on 15\nlanguages) is obtained. On XQA, which is a new cross-lingual dataset built by\nus, 5.5% averaged accuracy improvement (on French and German) is obtained.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "huang2019voice", "citations": "73", "year": "2020", "title":"Voice Transformer Network: Sequence-to-sequence Voice Conversion Using Transformer With Text-to-speech Pretraining", "abstract": "<p>We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC)\nmodel based on the Transformer architecture with text-to-speech (TTS)\npretraining. Seq2seq VC models are attractive owing to their ability to convert\nprosody. While seq2seq models based on recurrent neural networks (RNNs) and\nconvolutional neural networks (CNNs) have been successfully applied to VC, the\nuse of the Transformer network, which has shown promising results in various\nspeech processing tasks, has not yet been investigated. Nonetheless, their\ndata-hungry property and the mispronunciation of converted speech make seq2seq\nmodels far from practical. To this end, we propose a simple yet effective\npretraining technique to transfer knowledge from learned TTS models, which\nbenefit from large-scale, easily accessible TTS corpora. VC models initialized\nwith such pretrained model parameters are able to generate effective hidden\nrepresentations for high-fidelity, highly intelligible converted speech.\nExperimental results show that such a pretraining scheme can facilitate\ndata-efficient training and outperform an RNN-based seq2seq VC model in terms\nof intelligibility, naturalness, and similarity.</p>\n", "tags": ["INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "huang2020knowledge", "citations": "151", "year": "2020", "title":"Knowledge Graph-augmented Abstractive Summarization With Semantic-driven Cloze Reward", "abstract": "<p>Sequence-to-sequence models for abstractive summarization have been studied\nextensively, yet the generated summaries commonly suffer from fabricated\ncontent, and are often found to be near-extractive. We argue that, to address\nthese issues, the summarizer should acquire semantic interpretation over input,\ne.g., via structured representation, to allow the generation of more\ninformative summaries. In this paper, we present ASGARD, a novel framework for\nAbstractive Summarization with Graph-Augmentation and semantic-driven RewarD.\nWe propose the use of dual encoders—a sequential document encoder and a\ngraph-structured encoder—to maintain the global context and local\ncharacteristics of entities, complementing each other. We further design a\nreward based on a multiple choice cloze test to drive the model to better\ncapture entity interactions. Results show that our models produce significantly\nhigher ROUGE scores than a variant without knowledge graph as input on both New\nYork Times and CNN/Daily Mail datasets. We also obtain better or comparable\nperformance compared to systems that are fine-tuned from large pretrained\nlanguage models. Human judges further rate our model outputs as more\ninformative and containing fewer unfaithful errors.</p>\n", "tags": ["Datasets","Reinforcement Learning","Tools"] },
{"key": "huang2020pixel", "citations": "283", "year": "2020", "title":"Pixel-bert: Aligning Image Pixels With Text By Deep Multi-modal Transformers", "abstract": "<p>We propose Pixel-BERT to align image pixels with text by deep multi-modal\ntransformers that jointly learn visual and language embedding in a unified\nend-to-end framework. We aim to build a more accurate and thorough connection\nbetween image pixels and language semantics directly from image and sentence\npairs instead of using region-based image features as the most recent vision\nand language tasks. Our Pixel-BERT which aligns semantic connection in pixel\nand text level solves the limitation of task-specific visual representation for\nvision and language tasks. It also relieves the cost of bounding box\nannotations and overcomes the unbalance between semantic labels in visual task\nand language semantic. To provide a better representation for down-stream\ntasks, we pre-train a universal end-to-end model with image and sentence pairs\nfrom Visual Genome dataset and MS-COCO dataset. We propose to use a random\npixel sampling mechanism to enhance the robustness of visual representation and\nto apply the Masked Language Model and Image-Text Matching as pre-training\ntasks. Extensive experiments on downstream tasks with our pre-trained model\nshow that our approach makes the most state-of-the-arts in downstream tasks,\nincluding Visual Question Answering (VQA), image-text retrieval, Natural\nLanguage for Visual Reasoning for Real (NLVR). Particularly, we boost the\nperformance of a single model in VQA task by 2.17 points compared with SOTA\nunder fair comparison.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "huang2020tabtransformer", "citations": "110", "year": "2020", "title":"Tabtransformer: Tabular Data Modeling Using Contextual Embeddings", "abstract": "<p>We propose TabTransformer, a novel deep tabular data modeling architecture\nfor supervised and semi-supervised learning. The TabTransformer is built upon\nself-attention based Transformers. The Transformer layers transform the\nembeddings of categorical features into robust contextual embeddings to achieve\nhigher prediction accuracy. Through extensive experiments on fifteen publicly\navailable datasets, we show that the TabTransformer outperforms the\nstate-of-the-art deep learning methods for tabular data by at least 1.0% on\nmean AUC, and matches the performance of tree-based ensemble models.\nFurthermore, we demonstrate that the contextual embeddings learned from\nTabTransformer are highly robust against both missing and noisy data features,\nand provide better interpretability. Lastly, for the semi-supervised setting we\ndevelop an unsupervised pre-training procedure to learn data-driven contextual\nembeddings, resulting in an average 2.1% AUC lift over the state-of-the-art\nmethods.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "huang2021cleanrl", "citations": "62", "year": "2021", "title":"Cleanrl: High-quality Single-file Implementations Of Deep Reinforcement Learning Algorithms", "abstract": "<p>CleanRL is an open-source library that provides high-quality single-file\nimplementations of Deep Reinforcement Learning algorithms. It provides a\nsimpler yet scalable developing experience by having a straightforward codebase\nand integrating production tools to help interact and scale experiments. In\nCleanRL, we put all details of an algorithm into a single file, making these\nperformance-relevant details easier to recognize. Additionally, an experiment\ntracking feature is available to help log metrics, hyperparameters, videos of\nan agent’s gameplay, dependencies, and more to the cloud. Despite succinct\nimplementations, we have also designed tools to help scale, at one point\norchestrating experiments on more than 2000 machines simultaneously via Docker\nand cloud providers. Finally, we have ensured the quality of the\nimplementations by benchmarking against a variety of environments. The source\ncode of CleanRL can be found at https://github.com/vwxyzjn/cleanrl</p>\n", "tags": ["Datasets","Evaluation","Has Code","Reinforcement Learning","Tools"] },
{"key": "huang2021efficient", "citations": "108", "year": "2021", "title":"Efficient Attentions For Long Document Summarization", "abstract": "<p>The quadratic computational and memory complexities of large Transformers\nhave limited their scalability for long document summarization. In this paper,\nwe propose Hepos, a novel efficient encoder-decoder attention with head-wise\npositional strides to effectively pinpoint salient information from the source.\nWe further conduct a systematic study of existing efficient self-attentions.\nCombined with Hepos, we are able to process ten times more tokens than existing\nmodels that use full attentions. For evaluation, we present a new dataset,\nGovReport, with significantly longer documents and summaries. Results show that\nour models produce significantly higher ROUGE scores than competitive\ncomparisons, including new state-of-the-art results on PubMed. Human evaluation\nalso shows that our models generate more informative summaries with fewer\nunfaithful errors.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Model Architecture","NAACL"] },
{"key": "huang2021seeing", "citations": "186", "year": "2021", "title":"Seeing Out Of The Box: End-to-end Pre-training For Vision-language Representation Learning", "abstract": "<p>We study joint learning of Convolutional Neural Network (CNN) and Transformer\nfor vision-language pre-training (VLPT) which aims to learn cross-modal\nalignments from millions of image-text pairs. State-of-the-art approaches\nextract salient image regions and align regions with words step-by-step. As\nregion-based visual features usually represent parts of an image, it is\nchallenging for existing vision-language models to fully understand the\nsemantics from paired natural languages. In this paper, we propose SOHO to “See\nOut of tHe bOx” that takes a whole image as input, and learns vision-language\nrepresentation in an end-to-end manner. SOHO does not require bounding box\nannotations which enables inference 10 times faster than region-based\napproaches. In particular, SOHO learns to extract comprehensive yet compact\nimage features through a visual dictionary (VD) that facilitates cross-modal\nunderstanding. VD is designed to represent consistent visual abstractions of\nsimilar semantics. It is updated on-the-fly and utilized in our proposed\npre-training task Masked Visual Modeling (MVM). We conduct experiments on four\nwell-established vision-language tasks by following standard VLPT settings. In\nparticular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text\nretrieval 5k test split, 1.5% accuracy on NLVR\\(^2\\) test-P split, 6.7% accuracy\non SNLI-VE test split, respectively.</p>\n", "tags": ["CVPR","Model Architecture","Training Techniques"] },
{"key": "huang2022inner", "citations": "151", "year": "2022", "title":"Inner Monologue: Embodied Reasoning Through Planning With Language Models", "abstract": "<p>Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent’s own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.</p>\n", "tags": ["Agentic","Training Techniques"] },
{"key": "huang2022language", "citations": "116", "year": "2022", "title":"Language Models As Zero-shot Planners: Extracting Actionable Knowledge For Embodied Agents", "abstract": "<p>Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. “make\nbreakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner</p>\n", "tags": ["Evaluation","Has Code","Training Techniques"] },
{"key": "huang2022large", "citations": "114", "year": "2023", "title":"Large Language Models Can Self-improve", "abstract": "<p>Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate “high-confidence” rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%-&gt;82.1% on GSM8K, 78.2%-&gt;83.0% on DROP, 90.0%-&gt;94.4% on OpenBookQA, and\n63.4%-&gt;67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Prompting","Training Techniques"] },
{"key": "huang2022layoutlmv3", "citations": "270", "year": "2022", "title":"Layoutlmv3: Pre-training For Document AI With Unified Text And Image Masking", "abstract": "<p>Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\n\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with\nunified text and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\nhttps://aka.ms/layoutlmv3.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "huang2022towards", "citations": "208", "year": "2023", "title":"Towards Reasoning In Large Language Models: A Survey", "abstract": "<p>Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.</p>\n", "tags": ["Survey Paper"] },
{"key": "huang2023chatgpt", "citations": "178", "year": "2023", "title":"Chatgpt For Shaping The Future Of Dentistry: The Potential Of Multi-modal Large Language Model", "abstract": "<p>The ChatGPT, a lite and conversational variant of Generative Pretrained\nTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large\nLanguage Models (LLMs) with billions of parameters. LLMs have stirred up much\ninterest among researchers and practitioners in their impressive skills in\nnatural language processing tasks, which profoundly impact various fields. This\npaper mainly discusses the future applications of LLMs in dentistry. We\nintroduce two primary LLM deployment methods in dentistry, including automated\ndental diagnosis and cross-modal dental diagnosis, and examine their potential\napplications. Especially, equipped with a cross-modal encoder, a single LLM can\nmanage multi-source data and conduct advanced natural language reasoning to\nperform complex clinical operations. We also present cases to demonstrate the\npotential of a fully automatic Multi-Modal LLM AI system for dentistry clinical\napplication. While LLMs offer significant potential benefits, the challenges,\nsuch as data privacy, data quality, and model bias, need further study.\nOverall, LLMs have the potential to revolutionize dental diagnosis and\ntreatment, which indicates a promising avenue for clinical application and\nresearch in dentistry.</p>\n", "tags": ["Applications","Ethics & Fairness","Model Architecture"] },
{"key": "huang2023is", "citations": "151", "year": "2023", "title":"Is Chatgpt Better Than Human Annotators? Potential And Limitations Of Chatgpt In Explaining Implicit Hate Speech", "abstract": "<p>Recent studies have alarmed that many online hate speeches are implicit. With\nits subtle nature, the explainability of the detection of such hateful speech\nhas been a challenging problem. In this work, we examine whether ChatGPT can be\nused for providing natural language explanations (NLEs) for implicit hateful\nspeech detection. We design our prompt to elicit concise ChatGPT-generated NLEs\nand conduct user studies to evaluate their qualities by comparison with\nhuman-written NLEs. We discuss the potential and limitations of ChatGPT in the\ncontext of implicit hateful speech research.</p>\n", "tags": ["Prompting"] },
{"key": "huang2023language", "citations": "133", "year": "2023", "title":"Language Is Not All You Need: Aligning Perception With Language Models", "abstract": "<p>A big convergence of language, multimodal perception, action, and world\nmodeling is a key step toward artificial general intelligence. In this work, we\nintroduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\ngeneral modalities, learn in context (i.e., few-shot), and follow instructions\n(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\nmultimodal corpora, including arbitrarily interleaved text and images,\nimage-caption pairs, and text data. We evaluate various settings, including\nzero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\nof tasks without any gradient updates or finetuning. Experimental results show\nthat Kosmos-1 achieves impressive performance on (i) language understanding,\ngeneration, and even OCR-free NLP (directly fed with document images), (ii)\nperception-language tasks, including multimodal dialogue, image captioning,\nvisual question answering, and (iii) vision tasks, such as image recognition\nwith descriptions (specifying classification via text instructions). We also\nshow that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\nfrom language to multimodal, and from multimodal to language. In addition, we\nintroduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\ncapability of MLLMs.</p>\n", "tags": ["Datasets","Few-Shot","Instruction Following","Prompting"] },
{"key": "hudson2018compositional", "citations": "241", "year": "2018", "title":"Compositional Attention Networks For Machine Reasoning", "abstract": "<p>We present the MAC network, a novel fully differentiable neural network\narchitecture, designed to facilitate explicit and expressive reasoning. MAC\nmoves away from monolithic black-box neural architectures towards a design that\nencourages both transparency and versatility. The model approaches problems by\ndecomposing them into a series of attention-based reasoning steps, each\nperformed by a novel recurrent Memory, Attention, and Composition (MAC) cell\nthat maintains a separation between control and memory. By stringing the cells\ntogether and imposing structural constraints that regulate their interaction,\nMAC effectively learns to perform iterative reasoning processes that are\ndirectly inferred from the data in an end-to-end approach. We demonstrate the\nmodel’s strength, robustness and interpretability on the challenging CLEVR\ndataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy,\nhalving the error rate of the previous best model. More importantly, we show\nthat the model is computationally-efficient and data-efficient, in particular\nrequiring 5x less data than existing models to achieve strong results.</p>\n", "tags": ["Datasets","Model Architecture","Security"] },
{"key": "hudson2019gqa", "citations": "967", "year": "2019", "title":"GQA: A New Dataset For Real-world Visual Reasoning And Compositional Question Answering", "abstract": "<p>We introduce GQA, a new dataset for real-world visual reasoning and\ncompositional question answering, seeking to address key shortcomings of\nprevious VQA datasets. We have developed a strong and robust question engine\nthat leverages scene graph structures to create 22M diverse reasoning\nquestions, all come with functional programs that represent their semantics. We\nuse the programs to gain tight control over the answer distribution and present\na new tunable smoothing technique to mitigate question biases. Accompanying the\ndataset is a suite of new metrics that evaluate essential qualities such as\nconsistency, grounding and plausibility. An extensive analysis is performed for\nbaselines as well as state-of-the-art models, providing fine-grained results\nfor different question types and topologies. Whereas a blind LSTM obtains mere\n42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%,\noffering ample opportunity for new research to explore. We strongly hope GQA\nwill provide an enabling resource for the next generation of models with\nenhanced robustness, improved consistency, and deeper semantic understanding\nfor images and language.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture"] },
{"key": "hudson2019learning", "citations": "124", "year": "2019", "title":"Learning By Abstraction: The Neural State Machine", "abstract": "<p>We introduce the Neural State Machine, seeking to bridge the gap between the\nneural and symbolic views of AI and integrate their complementary strengths for\nthe task of visual reasoning. Given an image, we first predict a probabilistic\ngraph that represents its underlying semantics and serves as a structured world\nmodel. Then, we perform sequential reasoning over the graph, iteratively\ntraversing its nodes to answer a given question or draw a new inference. In\ncontrast to most neural architectures that are designed to closely interact\nwith the raw sensory data, our model operates instead in an abstract latent\nspace, by transforming both the visual and linguistic modalities into semantic\nconcept-based representations, thereby achieving enhanced transparency and\nmodularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets\nthat involve compositionality, multi-step inference and diverse reasoning\nskills, achieving state-of-the-art results in both cases. We provide further\nexperiments that illustrate the model’s strong generalization capacity across\nmultiple dimensions, including novel compositions of concepts, changes in the\nanswer distribution, and unseen linguistic structures, demonstrating the\nqualities and efficacy of our approach.</p>\n", "tags": ["Datasets","Ethics & Fairness"] },
{"key": "hugo2023llama", "citations": "1805", "year": "2023", "title":"Llama: Open And Efficient Foundation Language Models", "abstract": "<p>We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.</p>\n", "tags": ["Datasets","Efficiency","Model Architecture"] },
{"key": "humeau2019poly", "citations": "172", "year": "2019", "title":"Poly-encoders: Transformer Architectures And Pre-training Strategies For Fast And Accurate Multi-sentence Scoring", "abstract": "<p>The use of deep pre-trained bidirectional transformers has led to remarkable\nprogress in a number of applications (Devlin et al., 2018). For tasks that make\npairwise comparisons between sequences, matching a given input with a\ncorresponding label, two approaches are common: Cross-encoders performing full\nself-attention over the pair and Bi-encoders encoding the pair separately. The\nformer often performs better, but is too slow for practical use. In this work,\nwe develop a new transformer architecture, the Poly-encoder, that learns global\nrather than token level self-attention features. We perform a detailed\ncomparison of all three approaches, including what pre-training and fine-tuning\nstrategies work best. We show our models achieve state-of-the-art results on\nthree existing tasks; that Poly-encoders are faster than Cross-encoders and\nmore accurate than Bi-encoders; and that the best results are obtained by\npre-training on large datasets similar to the downstream tasks.</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "huo2021wenlan", "citations": "77", "year": "2021", "title":"Wenlan: Bridging Vision And Language By Large-scale Multi-modal Pre-training", "abstract": "<p>Multi-modal pre-training models have been intensively explored to bridge\nvision and language in recent years. However, most of them explicitly model the\ncross-modal interaction between image-text pairs, by assuming that there exists\nstrong semantic correlation between the text and image modalities. Since this\nstrong assumption is often invalid in real-world scenarios, we choose to\nimplicitly model the cross-modal correlation for large-scale multi-modal\npre-training, which is the focus of the Chinese project `WenLan’ led by our\nteam. Specifically, with the weak correlation assumption over image-text pairs,\nwe propose a two-tower pre-training model called BriVL within the cross-modal\ncontrastive learning framework. Unlike OpenAI CLIP that adopts a simple\ncontrastive learning method, we devise a more advanced algorithm by adapting\nthe latest method MoCo into the cross-modal scenario. By building a large\nqueue-based dictionary, our BriVL can incorporate more negative samples in\nlimited GPU resources. We further construct a large Chinese multi-source\nimage-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.\nExtensive experiments demonstrate that the pre-trained BriVL model outperforms\nboth UNITER and OpenAI CLIP on various downstream tasks.</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "hutchinson2020social", "citations": "245", "year": "2020", "title":"Social Biases In NLP Models As Barriers For Persons With Disabilities", "abstract": "<p>Building equitable and inclusive NLP technologies demands consideration of\nwhether and how social attitudes are represented in ML models. In particular,\nrepresentations encoded in models often inadvertently perpetuate undesirable\nsocial biases from the data on which they are trained. In this paper, we\npresent evidence of such undesirable biases towards mentions of disability in\ntwo different English language models: toxicity prediction and sentiment\nanalysis. Next, we demonstrate that the neural embeddings that are the critical\nfirst step in most NLP pipelines similarly contain undesirable biases towards\nmentions of disability. We end by highlighting topical biases in the discourse\nabout disability which may contribute to the observed model biases; for\ninstance, gun violence, homelessness, and drug addiction are over-represented\nin texts discussing mental illness.</p>\n", "tags": ["Ethics & Fairness"] },
{"key": "hwang2019comprehensive", "citations": "119", "year": "2019", "title":"A Comprehensive Exploration On Wikisql With Table-aware Word Contextualization", "abstract": "<p>We present SQLova, the first Natural-language-to-SQL (NL2SQL) model to\nachieve human performance in WikiSQL dataset. We revisit and discuss diverse\npopular methods in NL2SQL literature, take a full advantage of BERT {Devlin et\nal., 2018) through an effective table contextualization method, and coherently\ncombine them, outperforming the previous state of the art by 8.2% and 2.5% in\nlogical form and execution accuracy, respectively. We particularly note that\nBERT with a seq2seq decoder leads to a poor performance in the task, indicating\nthe importance of a careful design when using such large pretrained models. We\nalso provide a comprehensive analysis on the dataset and our model, which can\nbe helpful for designing future NL2SQL datsets and models. We especially show\nthat our model’s performance is near the upper bound in WikiSQL, where we\nobserve that a large portion of the evaluation errors are due to wrong\nannotations, and our model is already exceeding human performance by 1.3% in\nexecution accuracy.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "hwang2020comet", "citations": "258", "year": "2021", "title":"COMET-ATOMIC 2020: On Symbolic And Neural Commonsense Knowledge Graphs", "abstract": "<p>Recent years have brought about a renewed interest in commonsense\nrepresentation and reasoning in the field of natural language understanding.\nThe development of new commonsense knowledge graphs (CSKG) has been central to\nthese advances as their diverse facts can be used and referenced by machine\nlearning models for tackling new and challenging tasks. At the same time, there\nremain questions about the quality and coverage of these resources due to the\nmassive scale required to comprehensively encompass general commonsense\nknowledge.\n  In this work, we posit that manually constructed CSKGs will never achieve the\ncoverage necessary to be applicable in all situations encountered by NLP\nagents. Therefore, we propose a new evaluation framework for testing the\nutility of KGs based on how effectively implicit knowledge representations can\nbe learned from them.\n  With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose\ncommonsense knowledge containing knowledge that is not readily available in\npretrained language models. We evaluate its properties in comparison with other\nleading CSKGs, performing the first large-scale pairwise study of commonsense\nknowledge resources. Next, we show that ATOMIC 2020 is better suited for\ntraining knowledge models that can generate accurate, representative knowledge\nfor new, unseen entities and events. Finally, through human evaluation, we show\nthat the few-shot performance of GPT-3 (175B parameters), while impressive,\nremains ~12 absolute points lower than a BART-based knowledge model trained on\nATOMIC 2020 despite using over 430x fewer parameters.</p>\n", "tags": ["AAAI","Evaluation","Few-Shot","Model Architecture","Tools","Training Techniques"] },
{"key": "ilharco2019large", "citations": "64", "year": "2019", "title":"Large-scale Representation Learning From Visually Grounded Untranscribed Speech", "abstract": "<p>Systems that can associate images with their spoken audio captions are an\nimportant step towards visually grounded language learning. We describe a\nscalable method to automatically generate diverse audio for image captioning\ndatasets. This supports pretraining deep networks for encoding both audio and\nimages, which we do via a dual encoder that learns to align latent\nrepresentations from both modalities. We show that a masked margin softmax loss\nfor such models is superior to the standard triplet loss. We fine-tune these\nmodels on the Flickr8k Audio Captions Corpus and obtain state-of-the-art\nresults—improving recall in the top 10 from 29.6% to 49.5%. We also obtain\nhuman ratings on retrieval outputs to better assess the impact of incidentally\nmatching image-caption pairs that were not associated in the data, finding that\nautomatic evaluation substantially underestimates the quality of the retrieved\nresults.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "imani2023mathprompter", "citations": "68", "year": "2023", "title":"Mathprompter: Mathematical Reasoning Using Large Language Models", "abstract": "<p>Large Language Models (LLMs) have limited performance when solving arithmetic\nreasoning tasks and often provide incorrect answers. Unlike natural language\nunderstanding, math problems typically have a single correct answer, making the\ntask of generating accurate solutions more challenging for LLMs. To the best of\nour knowledge, we are not aware of any LLMs that indicate their level of\nconfidence in their responses which fuels a trust deficit in these models\nimpeding their adoption. To address this deficiency, we propose `MathPrompter’,\na technique that improves performance of LLMs on arithmetic problems along with\nincreased reliance in the predictions. MathPrompter uses the Zero-shot\nchain-of-thought prompting technique to generate multiple Algebraic expressions\nor Python functions to solve the same math problem in different ways and\nthereby raise the confidence level in the output results. This is in contrast\nto other prompt based CoT methods, where there is no check on the validity of\nthe intermediate steps followed. Our technique improves over state-of-the-art\non the MultiArith dataset (\\(78.7%\\rightarrow92.5%\\)) evaluated using 175B\nparameter GPT-based LLM.</p>\n", "tags": ["Datasets","Model Architecture","Prompting"] },
{"key": "inaguma2019multilingual", "citations": "80", "year": "2019", "title":"Multilingual End-to-end Speech Translation", "abstract": "<p>In this paper, we propose a simple yet effective framework for multilingual\nend-to-end speech translation (ST), in which speech utterances in source\nlanguages are directly translated to the desired target languages with a\nuniversal sequence-to-sequence architecture. While multilingual models have\nshown to be useful for automatic speech recognition (ASR) and machine\ntranslation (MT), this is the first time they are applied to the end-to-end ST\nproblem. We show the effectiveness of multilingual end-to-end ST in two\nscenarios: one-to-many and many-to-many translations with publicly available\ndata. We experimentally confirm that multilingual end-to-end ST models\nsignificantly outperform bilingual ones in both scenarios. The generalization\nof multilingual training is also evaluated in a transfer learning scenario to a\nvery low-resource language pair. All of our codes and the database are publicly\navailable to encourage further research in this emergent multilingual ST topic.</p>\n", "tags": ["ASRU","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "inaguma2020espnet", "citations": "139", "year": "2020", "title":"Espnet-st: All-in-one Speech Translation Toolkit", "abstract": "<p>We present ESPnet-ST, which is designed for the quick development of\nspeech-to-speech translation systems in a single framework. ESPnet-ST is a new\nproject inside end-to-end speech processing toolkit, ESPnet, which integrates\nor newly implements automatic speech recognition, machine translation, and\ntext-to-speech functions for speech translation. We provide all-in-one recipes\nincluding data pre-processing, feature extraction, training, and decoding\npipelines for a wide range of benchmark datasets. Our reproducible results can\nmatch or even outperform the current state-of-the-art performances; these\npre-trained models are downloadable. The toolkit is publicly available at\nhttps://github.com/espnet/espnet.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Tools","Training Techniques"] },
{"key": "inie2023designing", "citations": "77", "year": "2023", "title":"Designing Participatory AI: Creative Professionals' Worries And Expectations About Generative AI", "abstract": "<p>Generative AI, i.e., the group of technologies that automatically generate\nvisual or written content based on text prompts, has undergone a leap in\ncomplexity and become widely available within just a few years. Such\ntechnologies potentially introduce a massive disruption to creative fields.\nThis paper presents the results of a qualitative survey (\\(N\\) = 23)\ninvestigating how creative professionals think about generative AI. The results\nshow that the advancement of these AI models prompts important reflections on\nwhat defines creativity and how creatives imagine using AI to support their\nworkflows. Based on these reflections, we discuss how we might design\n\\textit{participatory AI} in the domain of creative expertise with the goal of\nempowering creative professionals in their present and future coexistence with\nAI.</p>\n", "tags": ["Survey Paper"] },
{"key": "ippolito2019automatic", "citations": "180", "year": "2020", "title":"Automatic Detection Of Generated Text Is Easiest When Humans Are Fooled", "abstract": "<p>Recent advancements in neural language modelling make it possible to rapidly\ngenerate vast amounts of human-sounding text. The capabilities of humans and\nautomatic discriminators to detect machine-generated text have been a large\nsource of research interest, but humans and machines rely on different cues to\nmake their decisions. Here, we perform careful benchmarking and analysis of\nthree popular sampling-based decoding strategies—top-\\(k\\), nucleus sampling,\nand untruncated random sampling—and show that improvements in decoding\nmethods have primarily optimized for fooling humans. This comes at the expense\nof introducing statistical abnormalities that make detection easy for automatic\nsystems. We also show that though both human and automatic detector performance\nimprove with longer excerpt length, even multi-sentence excerpts can fool\nexpert human raters over 30% of the time. Our findings reveal the importance of\nusing both human and automatic detectors to assess the humanness of text\ngeneration systems.</p>\n", "tags": ["Datasets"] },
{"key": "ippolito2019comparison", "citations": "88", "year": "2019", "title":"Comparison Of Diverse Decoding Methods From Conditional Language Models", "abstract": "<p>While conditional language models have greatly improved in their ability to\noutput high-quality natural language, many NLP applications benefit from being\nable to generate a diverse set of candidate sequences. Diverse decoding\nstrategies aim to, within a given-sized candidate list, cover as much of the\nspace of high-quality outputs as possible, leading to improvements for tasks\nthat re-rank and combine candidate outputs. Standard decoding methods, such as\nbeam search, optimize for generating high likelihood sequences rather than\ndiverse ones, though recent work has focused on increasing diversity in these\nmethods. In this work, we perform an extensive survey of decoding-time\nstrategies for generating diverse outputs from conditional language models. We\nalso show how diversity can be improved without sacrificing quality by\nover-sampling additional candidates, then filtering to the desired number.</p>\n", "tags": ["Applications","Survey Paper"] },
{"key": "irie2019language", "citations": "179", "year": "2019", "title":"Language Modeling With Deep Transformers", "abstract": "<p>We explore deep autoregressive Transformer models in language modeling for\nspeech recognition. We focus on two aspects. First, we revisit Transformer\nmodel configurations specifically for language modeling. We show that well\nconfigured Transformer models outperform our baseline models based on the\nshallow stack of LSTM recurrent neural network layers. We carry out experiments\non the open-source LibriSpeech 960hr task, for both 200K vocabulary word-level\nand 10K byte-pair encoding subword-level language modeling. We apply our\nword-level models to conventional hybrid speech recognition by lattice\nrescoring, and the subword-level models to attention based encoder-decoder\nmodels by shallow fusion. Second, we show that deep Transformer language models\ndo not require positional encoding. The positional encoding is an essential\naugmentation for the self-attention mechanism which is invariant to sequence\nordering. However, in autoregressive setup, as is the case for language\nmodeling, the amount of information increases along the position dimension,\nwhich is a positional signal by its own. The analysis of attention weights\nshows that deep autoregressive self-attention models can automatically make use\nof such positional information. We find that removing the positional encoding\neven slightly improves the performance of these models.</p>\n", "tags": ["INTERSPEECH","Model Architecture"] },
{"key": "iter2020pretraining", "citations": "73", "year": "2020", "title":"Pretraining With Contrastive Sentence Objectives Improves Discourse Performance Of Language Models", "abstract": "<p>Recent models for unsupervised representation learning of text have employed\na number of techniques to improve contextual word representations but have put\nlittle focus on discourse-level representations. We propose CONPONO, an\ninter-sentence objective for pretraining language models that models discourse\ncoherence and the distance between sentences. Given an anchor sentence, our\nmodel is trained to predict the text k sentences away using a sampled-softmax\nobjective where the candidates consist of neighboring sentences and sentences\nrandomly sampled from the corpus. On the discourse representation benchmark\nDiscoEval, our model improves over the previous state-of-the-art by up to 13%\nand on average 4% absolute across 7 tasks. Our model is the same size as\nBERT-Base, but outperforms the much larger BERT- Large model and other more\nrecent approaches that incorporate discourse. We also show that CONPONO yields\ngains of 2%-6% absolute even for tasks that do not explicitly evaluate\ndiscourse: textual entailment (RTE), common sense reasoning (COPA) and reading\ncomprehension (ReCoRD).</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "ivan2020probing", "citations": "176", "year": "2020", "title":"Probing Pretrained Language Models For Lexical Semantics", "abstract": "<p>The success of large pretrained language models (LMs) such as BERT and\nRoBERTa has sparked interest in probing their representations, in order to\nunveil what types of knowledge they implicitly capture. While prior research\nfocused on morphosyntactic, semantic, and world knowledge, it remains unclear\nto which extent LMs also derive lexical type-level knowledge from words in\ncontext. In this work, we present a systematic empirical analysis across six\ntypologically diverse languages and five different lexical tasks, addressing\nthe following questions: 1) How do different lexical knowledge extraction\nstrategies (monolingual versus multilingual source LM, out-of-context versus\nin-context encoding, inclusion of special tokens, and layer-wise averaging)\nimpact performance? How consistent are the observed effects across tasks and\nlanguages? 2) Is lexical knowledge stored in few parameters, or is it scattered\nthroughout the network? 3) How do these representations fare against\ntraditional static word vectors in lexical tasks? 4) Does the lexical\ninformation emerging from independently trained monolingual LMs display latent\nsimilarities? Our main results indicate patterns and best practices that hold\nuniversally, but also point to prominent variations across languages and tasks.\nMoreover, we validate the claim that lower Transformer layers carry more\ntype-level lexical knowledge, but also show that this knowledge is distributed\nacross multiple layers.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "ive2019distilling", "citations": "84", "year": "2019", "title":"Distilling Translations With Visual Awareness", "abstract": "<p>Previous work on multimodal machine translation has shown that visual\ninformation is only needed in very specific cases, for example in the presence\nof ambiguous words where the textual context is not sufficient. As a\nconsequence, models tend to learn to ignore this information. We propose a\ntranslate-and-refine approach to this problem where images are only used by a\nsecond stage decoder. This approach is trained jointly to generate a good first\ndraft translation and to improve over this draft by (i) making better use of\nthe target language textual context (both left and right-side contexts) and\n(ii) making use of visual context. This approach leads to the state of the art\nresults. Additionally, we show that it has the ability to recover from\nerroneous or missing words in the source language.</p>\n", "tags": [] },
{"key": "iyer2022opt", "citations": "77", "year": "2022", "title":"OPT-IML: Scaling Language Model Instruction Meta Learning Through The Lens Of Generalization", "abstract": "<p>Recent work has shown that fine-tuning large pre-trained language models on a\ncollection of tasks described via instructions, a.k.a. instruction-tuning,\nimproves their zero and few-shot generalization to unseen tasks. However, there\nis a limited understanding of the performance trade-offs of different decisions\nmade during the instruction-tuning process. These decisions include the scale\nand diversity of the instruction-tuning benchmark, different task sampling\nstrategies, fine-tuning with and without demonstrations, training using\nspecialized datasets for reasoning and dialogue, and finally, the fine-tuning\nobjectives themselves. In this paper, we characterize the effect of\ninstruction-tuning decisions on downstream task performance when scaling both\nmodel and benchmark sizes. To this end, we create OPT-IML Bench: a large\nbenchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated\ninto task categories from 8 existing benchmarks, and prepare an evaluation\nframework to measure three types of model generalizations: to tasks from fully\nheld-out categories, to held-out tasks from seen categories, and to held-out\ninstances from seen tasks. Through the lens of this framework, we first present\ninsights about instruction-tuning decisions as applied to OPT-30B and further\nexploit these insights to train OPT-IML 30B and 175B, which are\ninstruction-tuned versions of OPT. OPT-IML demonstrates all three\ngeneralization abilities at both scales on four different evaluation benchmarks\nwith diverse tasks and input formats – PromptSource, FLAN,\nSuper-NaturalInstructions, and UnifiedSKG. Not only does it significantly\noutperform OPT on all benchmarks but is also highly competitive with existing\nmodels fine-tuned on each specific benchmark. We release OPT-IML at both\nscales, together with the OPT-IML Bench evaluation framework.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Tools","Training Techniques"] },
{"key": "iyyer2018adversarial", "citations": "623", "year": "2018", "title":"Adversarial Example Generation With Syntactically Controlled Paraphrase Networks", "abstract": "<p>We propose syntactically controlled paraphrase networks (SCPNs) and use them\nto generate adversarial examples. Given a sentence and a target syntactic form\n(e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the\nsentence with the desired syntax. We show it is possible to create training\ndata for this task by first doing backtranslation at a very large scale, and\nthen using a parser to label the syntactic transformations that naturally occur\nduring this process. Such data allows us to train a neural encoder-decoder\nmodel with extra inputs to specify the target syntax. A combination of\nautomated and human evaluations show that SCPNs generate paraphrases that\nfollow their target specifications without decreasing paraphrase quality when\ncompared to baseline (uncontrolled) paraphrase systems. Furthermore, they are\nmore capable of generating syntactically adversarial examples that both (1)\n“fool” pretrained models and (2) improve the robustness of these models to\nsyntactic variation when used to augment their training data.</p>\n", "tags": ["NAACL","Security","Training Techniques"] },
{"key": "izacard2020leveraging", "citations": "553", "year": "2021", "title":"Leveraging Passage Retrieval With Generative Models For Open Domain Question Answering", "abstract": "<p>Generative models for open domain question answering have proven to be\ncompetitive, without resorting to external knowledge. While promising, this\napproach requires to use models with billions of parameters, which are\nexpensive to train and query. In this paper, we investigate how much these\nmodels can benefit from retrieving text passages, potentially containing\nevidence. We obtain state-of-the-art results on the Natural Questions and\nTriviaQA open benchmarks. Interestingly, we observe that the performance of\nthis method significantly improves when increasing the number of retrieved\npassages. This is evidence that generative models are good at aggregating and\ncombining evidence from multiple passages.</p>\n", "tags": [] },
{"key": "izacard2022atlas", "citations": "153", "year": "2022", "title":"Atlas: Few-shot Learning With Retrieval Augmented Language Models", "abstract": "<p>Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\non Natural Questions using only 64 examples, outperforming a 540B parameters\nmodel by 3% despite having 50x fewer parameters.</p>\n", "tags": ["Evaluation Frameworks","Few-Shot","Training Techniques"] },
{"key": "jablonka202314", "citations": "138", "year": "2023", "title":"14 Examples Of How Llms Can Transform Materials Science And Chemistry: A Reflection On A Large Language Model Hackathon", "abstract": "<p>Large-language models (LLMs) such as GPT-4 caught the interest of many\nscientists. Recent studies suggested that these models could be useful in\nchemistry and materials science. To explore these possibilities, we organized a\nhackathon.\n  This article chronicles the projects built as part of this hackathon.\nParticipants employed LLMs for various applications, including predicting\nproperties of molecules and materials, designing novel interfaces for tools,\nextracting knowledge from unstructured data, and developing new educational\napplications.\n  The diverse topics and the fact that working prototypes could be generated in\nless than two days highlight that LLMs will profoundly impact the future of our\nfields. The rich collection of ideas and projects also indicates that the\napplications of LLMs are not limited to materials science and chemistry but\noffer potential benefits to a wide range of scientific disciplines.</p>\n", "tags": ["Applications","Model Architecture","Tools"] },
{"key": "jabri2016revisiting", "citations": "247", "year": "2016", "title":"Revisiting Visual Question Answering Baselines", "abstract": "<p>Visual question answering (VQA) is an interesting learning setting for\nevaluating the abilities and shortcomings of current systems for image\nunderstanding. Many of the recently proposed VQA systems include attention or\nmemory mechanisms designed to support “reasoning”. For multiple-choice VQA,\nnearly all of these systems train a multi-class classifier on image and\nquestion features to predict an answer. This paper questions the value of these\ncommon practices and develops a simple alternative model based on binary\nclassification. Instead of treating answers as competing choices, our model\nreceives the answer as input and predicts whether or not an\nimage-question-answer triplet is correct. We evaluate our model on the Visual7W\nTelling and the VQA Real Multiple Choice tasks, and find that even simple\nversions of our model perform competitively. Our best model achieves\nstate-of-the-art performance on the Visual7W Telling task and compares\nsurprisingly well with the most complex systems proposed for the VQA Real\nMultiple Choice task. We explore variants of the model and study its\ntransferability between both datasets. We also present an error analysis of our\nmodel that suggests a key problem of current VQA systems lies in the lack of\nvisual grounding of concepts that occur in the questions and answers. Overall,\nour results suggest that the performance of current VQA systems is not\nsignificantly better than that of systems designed to exploit dataset biases.</p>\n", "tags": ["Datasets","Memory & Context","Model Architecture"] },
{"key": "jacovi2020towards", "citations": "332", "year": "2020", "title":"Towards Faithfully Interpretable NLP Systems: How Should We Define And Evaluate Faithfulness?", "abstract": "<p>With the growing popularity of deep-learning based NLP models, comes a need\nfor interpretable systems. But what is interpretability, and what constitutes a\nhigh-quality interpretation? In this opinion piece we reflect on the current\nstate of interpretability evaluation research. We call for more clearly\ndifferentiating between different desired criteria an interpretation should\nsatisfy, and focus on the faithfulness criteria. We survey the literature with\nrespect to faithfulness evaluation, and arrange the current approaches around\nthree assumptions, providing an explicit form to how faithfulness is “defined”\nby the community. We provide concrete guidelines on how evaluation of\ninterpretation methods should and should not be conducted. Finally, we claim\nthat the current binary definition for faithfulness sets a potentially\nunrealistic bar for being considered faithful. We call for discarding the\nbinary notion of faithfulness in favor of a more graded one, which we believe\nwill be of greater practical utility.</p>\n", "tags": ["Evaluation","Survey Paper"] },
{"key": "jaegle2021perceiver", "citations": "171", "year": "2021", "title":"Perceiver IO: A General Architecture For Structured Inputs & Outputs", "abstract": "<p>A central goal of machine learning is the development of systems that can\nsolve many problems in as many data domains as possible. Current architectures,\nhowever, cannot be applied beyond a small set of stereotyped settings, as they\nbake in domain &amp; task assumptions or scale poorly to large inputs or outputs.\nIn this work, we propose Perceiver IO, a general-purpose architecture that\nhandles data from arbitrary settings while scaling linearly with the size of\ninputs and outputs. Our model augments the Perceiver with a flexible querying\nmechanism that enables outputs of various sizes and semantics, doing away with\nthe need for task-specific architecture engineering. The same architecture\nachieves strong results on tasks spanning natural language and visual\nunderstanding, multi-task and multi-modal reasoning, and StarCraft II. As\nhighlights, Perceiver IO outperforms a Transformer-based BERT baseline on the\nGLUE language benchmark despite removing input tokenization and achieves\nstate-of-the-art performance on Sintel optical flow estimation with no explicit\nmechanisms for multiscale correspondence.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "jain2017creativity", "citations": "134", "year": "2017", "title":"Creativity: Generating Diverse Questions Using Variational Autoencoders", "abstract": "<p>Generating diverse questions for given images is an important task for\ncomputational education, entertainment and AI assistants. Different from many\nconventional prediction techniques is the need for algorithms to generate a\ndiverse set of plausible questions, which we refer to as “creativity”. In this\npaper we propose a creative algorithm for visual question generation which\ncombines the advantages of variational autoencoders with long short-term memory\nnetworks. We demonstrate that our framework is able to generate a large set of\nvarying questions given a single input image.</p>\n", "tags": ["CVPR","Tools"] },
{"key": "jain2017story", "citations": "84", "year": "2017", "title":"Story Generation From Sequence Of Independent Short Descriptions", "abstract": "<p>Existing Natural Language Generation (NLG) systems are weak AI systems and\nexhibit limited capabilities when language generation tasks demand higher\nlevels of creativity, originality and brevity. Effective solutions or, at least\nevaluations of modern NLG paradigms for such creative tasks have been elusive,\nunfortunately. This paper introduces and addresses the task of coherent story\ngeneration from independent descriptions, describing a scene or an event.\nTowards this, we explore along two popular text-generation paradigms – (1)\nStatistical Machine Translation (SMT), posing story generation as a translation\nproblem and (2) Deep Learning, posing story generation as a sequence to\nsequence learning problem. In SMT, we chose two popular methods such as phrase\nbased SMT (PB-SMT) and syntax based SMT (SYNTAX-SMT) to `translate’ the\nincoherent input text into stories. We then implement a deep recurrent neural\nnetwork (RNN) architecture that encodes sequence of variable length input\ndescriptions to corresponding latent representations and decodes them to\nproduce well formed comprehensive story like summaries. The efficacy of the\nsuggested approaches is demonstrated on a publicly available dataset with the\nhelp of popular machine translation and summarization evaluation metrics.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "jain2018two", "citations": "94", "year": "2018", "title":"Two Can Play This Game: Visual Dialog With Discriminative Question Generation And Answering", "abstract": "<p>Human conversation is a complex mechanism with subtle nuances. It is hence an\nambitious goal to develop artificial intelligence agents that can participate\nfluently in a conversation. While we are still far from achieving this goal,\nrecent progress in visual question answering, image captioning, and visual\nquestion generation shows that dialog systems may be realizable in the not too\ndistant future. To this end, a novel dataset was introduced recently and\nencouraging results were demonstrated, particularly for question answering. In\nthis paper, we demonstrate a simple symmetric discriminative baseline, that can\nbe applied to both predicting an answer as well as predicting a question. We\nshow that this method performs on par with the state of the art, even memory\nnet based methods. In addition, for the first time on the visual dialog\ndataset, we assess the performance of a system asking questions, and\ndemonstrate how visual dialog can be generated from discriminative question\ngeneration and question answering.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "jain2019attention", "citations": "537", "year": "2019", "title":"Attention Is Not Explanation", "abstract": "<p>Attention mechanisms have seen wide adoption in neural NLP models. In\naddition to improving predictive performance, these are often touted as\naffording transparency: models equipped with attention provide a distribution\nover attended-to input units, and this is often presented (at least implicitly)\nas communicating the relative importance of inputs. However, it is unclear what\nrelationship exists between attention weights and model outputs. In this work,\nwe perform extensive experiments across a variety of NLP tasks that aim to\nassess the degree to which attention weights provide meaningful `explanations’\nfor predictions. We find that they largely do not. For example, learned\nattention weights are frequently uncorrelated with gradient-based measures of\nfeature importance, and one can identify very different attention distributions\nthat nonetheless yield equivalent predictions. Our findings show that standard\nattention modules do not provide meaningful explanations and should not be\ntreated as though they do. Code for all experiments is available at\nhttps://github.com/successar/AttentionExplanation.</p>\n", "tags": ["Has Code","Model Architecture"] },
{"key": "jain2019stay", "citations": "131", "year": "2019", "title":"Stay On The Path: Instruction Fidelity In Vision-and-language Navigation", "abstract": "<p>Advances in learning and representations have reinvigorated work that\nconnects language to other modalities. A particularly exciting direction is\nVision-and-Language Navigation(VLN), in which agents interpret natural language\ninstructions and visual scenes to move through environments and reach goals.\nDespite recent progress, current research leaves unclear how much of a role\nlanguage understanding plays in this task, especially because dominant\nevaluation metrics have focused on goal completion rather than the sequence of\nactions corresponding to the instructions. Here, we highlight shortcomings of\ncurrent metrics for the Room-to-Room dataset (Anderson et al.,2018b) and\npropose a new metric, Coverage weighted by Length Score (CLS). We also show\nthat the existing paths in the dataset are not ideal for evaluating instruction\nfollowing because they are direct-to-goal shortest paths. We join existing\nshort paths to form more challenging extended paths to create a new data set,\nRoom-for-Room (R4R). Using R4R and CLS, we show that agents that receive\nrewards for instruction fidelity outperform agents that focus on goal\ncompletion.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "jain2020contrastive", "citations": "91", "year": "2021", "title":"Contrastive Code Representation Learning", "abstract": "<p>Recent work learns contextual representations of source code by\nreconstructing tokens from their context. For downstream semantic understanding\ntasks like summarizing code in English, these representations should ideally\ncapture program functionality. However, we show that the popular\nreconstruction-based BERT model is sensitive to source code edits, even when\nthe edits preserve semantics. We propose ContraCode: a contrastive pre-training\ntask that learns code functionality, not form. ContraCode pre-trains a neural\nnetwork to identify functionally similar variants of a program among many\nnon-equivalent distractors. We scalably generate these variants using an\nautomated source-to-source compiler as a form of data augmentation. Contrastive\npre-training improves JavaScript summarization and TypeScript type inference\naccuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone\ndetection dataset, showing that ContraCode is both more robust and semantically\nmeaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting\nand up to 5% on natural code.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "jain2020learning", "citations": "145", "year": "2020", "title":"Learning To Faithfully Rationalize By Construction", "abstract": "<p>In many settings it is important for one to be able to understand why a model\nmade a particular prediction. In NLP this often entails extracting snippets of\nan input text <code class=\"language-plaintext highlighter-rouge\">responsible for' corresponding model output; when such a snippet\ncomprises tokens that indeed informed the model's prediction, it is a faithful\nexplanation. In some settings, faithfulness may be critical to ensure\ntransparency. Lei et al. (2016) proposed a model to produce faithful rationales\nfor neural text classification by defining independent snippet extraction and\nprediction modules. However, the discrete selection over input tokens performed\nby this method complicates training, leading to high variance and requiring\ncareful hyperparameter tuning. We propose a simpler variant of this approach\nthat provides faithful explanations by construction. In our scheme, named\nFRESH, arbitrary feature importance scores (e.g., gradients from a trained\nmodel) are used to induce binary labels over token inputs, which an extractor\ncan be trained to predict. An independent classifier module is then trained\nexclusively on snippets provided by the extractor; these snippets thus\nconstitute faithful explanations, even if the classifier is arbitrarily\ncomplex. In both automatic and manual evaluations we find that variants of this\nsimple framework yield predictive performance superior to </code>end-to-end’\napproaches, while being more general and easier to train. Code is available at\nhttps://github.com/successar/FRESH</p>\n", "tags": ["Ethics & Fairness","Has Code","Tools","Training Techniques"] },
{"key": "jain2021zero", "citations": "279", "year": "2022", "title":"Zero-shot Text-guided Object Generation With Dream Fields", "abstract": "<p>We combine neural rendering with multi-modal image and text representations\nto synthesize diverse 3D objects solely from natural language descriptions. Our\nmethod, Dream Fields, can generate the geometry and color of a wide range of\nobjects without 3D supervision. Due to the scarcity of diverse, captioned 3D\ndata, prior methods only generate objects from a handful of categories, such as\nShapeNet. Instead, we guide generation with image-text models pre-trained on\nlarge datasets of captioned images from the web. Our method optimizes a Neural\nRadiance Field from many camera views so that rendered images score highly with\na target caption according to a pre-trained CLIP model. To improve fidelity and\nvisual quality, we introduce simple geometric priors, including\nsparsity-inducing transmittance regularization, scene bounds, and new MLP\narchitectures. In experiments, Dream Fields produce realistic, multi-view\nconsistent object geometry and color from a variety of natural language\ncaptions.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "jakesch2022human", "citations": "158", "year": "2023", "title":"Human Heuristics For Ai-generated Language Are Flawed", "abstract": "<p>Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems suggest words, complete\nsentences, or produce entire conversations. AI-generated language is often not\nidentified as such but presented as language written by humans, raising\nconcerns about novel forms of deception and manipulation. Here, we study how\nhumans discern whether verbal self-presentations, one of the most personal and\nconsequential forms of language, were generated by AI. In six experiments,\nparticipants (N = 4,600) were unable to detect self-presentations generated by\nstate-of-the-art AI language models in professional, hospitality, and dating\ncontexts. A computational analysis of language features shows that human\njudgments of AI-generated language are hindered by intuitive but flawed\nheuristics such as associating first-person pronouns, use of contractions, or\nfamily topics with human-written language. We experimentally demonstrate that\nthese heuristics make human judgment of AI-generated language predictable and\nmanipulable, allowing AI systems to produce text perceived as “more human than\nhuman.” We discuss solutions, such as AI accents, to reduce the deceptive\npotential of language generated by AI, limiting the subversion of human\nintuition.</p>\n", "tags": [] },
{"key": "jakesch2023co", "citations": "137", "year": "2023", "title":"Co-writing With Opinionated Language Models Affects Users' Views", "abstract": "<p>If large language models like GPT-3 preferably produce a particular point of\nview, they may influence people’s opinions on an unknown scale. This study\ninvestigates whether a language-model-powered writing assistant that generates\nsome opinions more often than others impacts what users write - and what they\nthink. In an online experiment, we asked participants (N=1,506) to write a post\ndiscussing whether social media is good for society. Treatment group\nparticipants used a language-model-powered writing assistant configured to\nargue that social media is good or bad for society. Participants then completed\na social media attitude survey, and independent judges (N=500) evaluated the\nopinions expressed in their writing. Using the opinionated language model\naffected the opinions expressed in participants’ writing and shifted their\nopinions in the subsequent attitude survey. We discuss the wider implications\nof our results and argue that the opinions built into AI language technologies\nneed to be monitored and engineered more carefully.</p>\n", "tags": ["Model Architecture","Survey Paper"] },
{"key": "jalil2023chatgpt", "citations": "165", "year": "2023", "title":"Chatgpt And Software Testing Education: Promises & Perils", "abstract": "<p>Over the past decade, predictive language modeling for code has proven to be\na valuable tool for enabling new forms of automation for developers. More\nrecently, we have seen the advent of general purpose “large language models”,\nbased on neural transformer architectures, that have been trained on massive\ndatasets of human written text spanning code and natural language. However,\ndespite the demonstrated representational power of such models, interacting\nwith them has historically been constrained to specific task settings, limiting\ntheir general applicability. Many of these limitations were recently overcome\nwith the introduction of ChatGPT, a language model created by OpenAI and\ntrained to operate as a conversational agent, enabling it to answer questions\nand respond to a wide variety of commands from end users. The introduction of\nmodels, such as ChatGPT, has already spurred fervent discussion from educators,\nranging from fear that students could use these AI tools to circumvent\nlearning, to excitement about the new types of learning opportunities that they\nmight unlock. However, given the nascent nature of these tools, we currently\nlack fundamental knowledge related to how well they perform in different\neducational settings, and the potential promise (or danger) that they might\npose to traditional forms of instruction. As such, in this paper, we examine\nhow well ChatGPT performs when tasked with answering common questions in a\npopular software testing curriculum. Our findings indicate that ChatGPT can\nprovide correct or partially correct answers in 55.6% of cases, provide correct\nor partially correct explanations of answers in 53.0% of cases, and that\nprompting the tool in a shared question context leads to a marginally higher\nrate of correct responses. Based on these findings, we discuss the potential\npromises and perils related to the use of ChatGPT by students and instructors.</p>\n", "tags": ["Agentic","Datasets","Model Architecture","Prompting","Tools"] },
{"key": "jaques2019way", "citations": "141", "year": "2019", "title":"Way Off-policy Batch Deep Reinforcement Learning Of Implicit Human Preferences In Dialog", "abstract": "<p>Most deep reinforcement learning (RL) systems are not able to learn\neffectively from off-policy data, especially if they cannot explore online in\nthe environment. These are critical shortcomings for applying RL to real-world\nproblems where collecting data is expensive, and models must be tested offline\nbefore being deployed to interact with the environment – e.g. systems that\nlearn from human interaction. Thus, we develop a novel class of off-policy\nbatch RL algorithms, which are able to effectively learn offline, without\nexploring, from a fixed batch of human interaction data. We leverage models\npre-trained on data as a strong prior, and use KL-control to penalize\ndivergence from this prior during RL training. We also use dropout-based\nuncertainty estimates to lower bound the target Q-values as a more efficient\nalternative to Double Q-Learning. The algorithms are tested on the problem of\nopen-domain dialog generation – a challenging reinforcement learning problem\nwith a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we\ncan extract multiple different reward functions post-hoc from collected human\ninteraction data, and learn effectively from all of these. We test the\nreal-world generalization of these systems by deploying them live to converse\nwith humans in an open-domain setting, and demonstrate that our algorithm\nachieves significant improvements over prior methods in off-policy batch RL.</p>\n", "tags": ["Agentic","Reinforcement Learning","Training Techniques"] },
{"key": "jared2020scaling", "citations": "1236", "year": "2020", "title":"Scaling Laws For Neural Language Models", "abstract": "<p>We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.</p>\n", "tags": ["Datasets","Efficiency","Training Techniques"] },
{"key": "jason2021finetuned", "citations": "707", "year": "2021", "title":"Finetuned Language Models Are Zero-shot Learners", "abstract": "<p>This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning – finetuning\nlanguage models on a collection of tasks described via instructions –\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Model Architecture"] },
{"key": "jason2022chain", "citations": "2518", "year": "2022", "title":"Chain-of-thought Prompting Elicits Reasoning In Large Language Models", "abstract": "<p>We explore how generating a chain of thought – a series of intermediate\nreasoning steps – significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Prompting"] },
{"key": "jason2022emergent", "citations": "829", "year": "2022", "title":"Emergent Abilities Of Large Language Models", "abstract": "<p>Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.</p>\n", "tags": ["Emergent Abilities"] },
{"key": "jawahar2020automatic", "citations": "109", "year": "2020", "title":"Automatic Detection Of Machine Generated Text: A Critical Survey", "abstract": "<p>Text generative models (TGMs) excel in producing text that matches the style\nof human language reasonably well. Such TGMs can be misused by adversaries,\ne.g., by automatically generating fake news and fake product reviews that can\nlook authentic and fool humans. Detectors that can distinguish text generated\nby TGM from human written text play a vital role in mitigating such misuse of\nTGMs. Recently, there has been a flurry of works from both natural language\nprocessing (NLP) and machine learning (ML) communities to build accurate\ndetectors for English. Despite the importance of this problem, there is\ncurrently no work that surveys this fast-growing literature and introduces\nnewcomers to important research challenges. In this work, we fill this void by\nproviding a critical survey and review of this literature to facilitate a\ncomprehensive understanding of this problem. We conduct an in-depth error\nanalysis of the state-of-the-art detector and discuss research directions to\nguide future work in this exciting area.</p>\n", "tags": ["COLING","Survey Paper"] },
{"key": "jean2017does", "citations": "138", "year": "2017", "title":"Does Neural Machine Translation Benefit From Larger Context?", "abstract": "<p>We propose a neural machine translation architecture that models the\nsurrounding text in addition to the source sentence. These models lead to\nbetter performance, both in terms of general translation quality and pronoun\nprediction, when trained on small corpora, although this improvement largely\ndisappears when trained with a larger corpus. We also discover that\nattention-based neural machine translation is well suited for pronoun\nprediction and compares favorably with other approaches that were specifically\ndesigned for this task.</p>\n", "tags": ["Datasets","Memory & Context","Model Architecture"] },
{"key": "jeblick2022chatgpt", "citations": "373", "year": "2023", "title":"Chatgpt Makes Medicine Easy To Swallow: An Exploratory Case Study On Simplified Radiology Reports", "abstract": "<p>The release of ChatGPT, a language model capable of generating text that\nappears human-like and authentic, has gained significant attention beyond the\nresearch community. We expect that the convincing performance of ChatGPT\nincentivizes users to apply it to a variety of downstream tasks, including\nprompting the model to simplify their own medical reports. To investigate this\nphenomenon, we conducted an exploratory case study. In a questionnaire, we\nasked 15 radiologists to assess the quality of radiology reports simplified by\nChatGPT. Most radiologists agreed that the simplified reports were factually\ncorrect, complete, and not potentially harmful to the patient. Nevertheless,\ninstances of incorrect statements, missed key medical findings, and potentially\nharmful passages were reported. While further studies are needed, the initial\ninsights of this study indicate a great potential in using large language\nmodels like ChatGPT to improve patient-centered care in radiology and other\nmedical domains.</p>\n", "tags": ["Model Architecture","Prompting"] },
{"key": "jeretic2020are", "citations": "84", "year": "2020", "title":"Are Natural Language Inference Models Imppressive? Learning Implicature And Presupposition", "abstract": "<p>Natural language inference (NLI) is an increasingly important task for\nnatural language understanding, which requires one to infer whether a sentence\nentails another. However, the ability of NLI models to make pragmatic\ninferences remains understudied. We create an IMPlicature and PRESupposition\ndiagnostic dataset (IMPPRES), consisting of &gt;25k semiautomatically generated\nsentence pairs illustrating well-studied pragmatic inference types. We use\nIMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on\nMultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although\nMultiNLI appears to contain very few pairs illustrating these inference types,\nwe find that BERT learns to draw pragmatic inferences. It reliably treats\nscalar implicatures triggered by “some” as entailments. For some presupposition\ntriggers like “only”, BERT reliably recognizes the presupposition as an\nentailment, even when the trigger is embedded under an entailment canceling\noperator like negation. BOW and InferSent show weaker evidence of pragmatic\nreasoning. We conclude that NLI training encourages models to learn some, but\nnot all, pragmatic inferences.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "jhamtani2018learning", "citations": "102", "year": "2018", "title":"Learning To Describe Differences Between Pairs Of Similar Images", "abstract": "<p>In this paper, we introduce the task of automatically generating text to\ndescribe the differences between two similar images. We collect a new dataset\nby crowd-sourcing difference descriptions for pairs of image frames extracted\nfrom video-surveillance footage. Annotators were asked to succinctly describe\nall the differences in a short paragraph. As a result, our novel dataset\nprovides an opportunity to explore models that align language and vision, and\ncapture visual salience. The dataset may also be a useful benchmark for\ncoherent multi-sentence generation. We perform a firstpass visual analysis that\nexposes clusters of differing pixels as a proxy for object-level differences.\nWe propose a model that captures visual salience by using a latent variable to\nalign clusters of differing pixels with output sentences. We find that, for\nboth single-sentence generation and as well as multi-sentence generation, the\nproposed model outperforms the models that use attention alone.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "ji2017dynamic", "citations": "108", "year": "2017", "title":"Dynamic Entity Representations In Neural Language Models", "abstract": "<p>Understanding a long document requires tracking how entities are introduced\nand evolve over time. We present a new type of language model, EntityNLM, that\ncan explicitly model entities, dynamically update their representations, and\ncontextually generate their mentions. Our model is generative and flexible; it\ncan model an arbitrary number of entities in context while generating each\nentity mention at an arbitrary length. In addition, it can be used for several\ndifferent tasks such as language modeling, coreference resolution, and entity\nprediction. Experimental results with all these tasks demonstrate that our\nmodel consistently outperforms strong baselines and prior work.</p>\n", "tags": ["EMNLP"] },
{"key": "ji2019saliency", "citations": "101", "year": "2019", "title":"Saliency-guided Attention Network For Image-sentence Matching", "abstract": "<p>This paper studies the task of matching image and sentence, where learning\nappropriate representations across the multi-modal data appears to be the main\nchallenge. Unlike previous approaches that predominantly deploy symmetrical\narchitecture to represent both modalities, we propose Saliency-guided Attention\nNetwork (SAN) that asymmetrically employs visual and textual attention modules\nto learn the fine-grained correlation intertwined between vision and language.\nThe proposed SAN mainly includes three components: saliency detector,\nSaliency-weighted Visual Attention (SVA) module, and Saliency-guided Textual\nAttention (STA) module. Concretely, the saliency detector provides the visual\nsaliency information as the guidance for the two attention modules. SVA is\ndesigned to leverage the advantage of the saliency information to improve\ndiscrimination of visual representations. By fusing the visual information from\nSVA and textual information as a multi-modal guidance, STA learns\ndiscriminative textual representations that are highly sensitive to visual\nclues. Extensive experiments demonstrate SAN can substantially improve the\nstate-of-the-art results on the benchmark Flickr30K and MSCOCO datasets by a\nlarge margin.</p>\n", "tags": ["Datasets","Evaluation","ICCV","Model Architecture"] },
{"key": "ji2020improving", "citations": "142", "year": "2021", "title":"Improving Image Captioning By Leveraging Intra- And Inter-layer Global Representation In Transformer Network", "abstract": "<p>Transformer-based architectures have shown great success in image captioning,\nwhere object regions are encoded and then attended into the vectorial\nrepresentations to guide the caption decoding. However, such vectorial\nrepresentations only contain region-level information without considering the\nglobal information reflecting the entire image, which fails to expand the\ncapability of complex multi-modal reasoning in image captioning. In this paper,\nwe introduce a Global Enhanced Transformer (termed GET) to enable the\nextraction of a more comprehensive global representation, and then adaptively\nguide the decoder to generate high-quality captions. In GET, a Global Enhanced\nEncoder is designed for the embedding of the global feature, and a Global\nAdaptive Decoder are designed for the guidance of the caption generation. The\nformer models intra- and inter-layer global representation by taking advantage\nof the proposed Global Enhanced Attention and a layer-wise fusion module. The\nlatter contains a Global Adaptive Controller that can adaptively fuse the\nglobal information into the decoder to guide the caption generation. Extensive\nexperiments on MS COCO dataset demonstrate the superiority of our GET over many\nstate-of-the-arts.</p>\n", "tags": ["AAAI","Datasets","Model Architecture"] },
{"key": "ji2020language", "citations": "93", "year": "2020", "title":"Language Generation With Multi-hop Reasoning On Commonsense Knowledge Graph", "abstract": "<p>Despite the success of generative pre-trained language models on a series of\ntext generation tasks, they still suffer in cases where reasoning over\nunderlying commonsense knowledge is required during generation. Existing\napproaches that integrate commonsense knowledge into generative pre-trained\nlanguage models simply transfer relational knowledge by post-training on\nindividual knowledge triples while ignoring rich connections within the\nknowledge graph. We argue that exploiting both the structural and semantic\ninformation of the knowledge graph facilitates commonsense-aware text\ngeneration. In this paper, we propose Generation with Multi-Hop Reasoning Flow\n(GRF) that enables pre-trained models with dynamic multi-hop reasoning on\nmulti-relational paths extracted from the external commonsense knowledge graph.\nWe empirically show that our model outperforms existing baselines on three text\ngeneration tasks that require reasoning over commonsense knowledge. We also\ndemonstrate the effectiveness of the dynamic multi-hop reasoning module with\nreasoning paths inferred by the model that provide rationale to the generation.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "ji2021does", "citations": "70", "year": "2021", "title":"Does The Magic Of BERT Apply To Medical Code Assignment? A Quantitative Study", "abstract": "<p>Unsupervised pretraining is an integral part of many natural language\nprocessing systems, and transfer learning with language models has achieved\nremarkable results in many downstream tasks. In the clinical application of\nmedical code assignment, diagnosis and procedure codes are inferred from\nlengthy clinical notes such as hospital discharge summaries. However, it is not\nclear if pretrained models are useful for medical code prediction without\nfurther architecture engineering. This paper conducts a comprehensive\nquantitative analysis of various contextualized language models’ performance,\npretrained in different domains, for medical code assignment from clinical\nnotes. We propose a hierarchical fine-tuning architecture to capture\ninteractions between distant words and adopt label-wise attention to exploit\nlabel information. Contrary to current trends, we demonstrate that a carefully\ntrained classical CNN outperforms attention-based models on a MIMIC-III subset\nwith frequent codes. Our empirical findings suggest directions for improving\nthe medical code assignment application.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "ji2021mentalbert", "citations": "102", "year": "2021", "title":"Mentalbert: Publicly Available Pretrained Language Models For Mental Healthcare", "abstract": "<p>Mental health is a critical issue in modern society, and mental disorders\ncould sometimes turn to suicidal ideation without adequate treatment. Early\ndetection of mental disorders and suicidal ideation from social content\nprovides a potential way for effective social intervention. Recent advances in\npretrained contextualized language representations have promoted the\ndevelopment of several domain-specific pretrained models and facilitated\nseveral downstream applications. However, there are no existing pretrained\nlanguage models for mental healthcare. This paper trains and release two\npretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to\nbenefit machine learning for the mental healthcare research community. Besides,\nwe evaluate our trained domain-specific models and several variants of\npretrained language models on several mental disorder detection benchmarks and\ndemonstrate that language representations pretrained in the target domain\nimprove the performance of mental health detection tasks.</p>\n", "tags": ["Applications","Evaluation","LREC"] },
{"key": "ji2022survey", "citations": "1637", "year": "2022", "title":"Survey Of Hallucination In Natural Language Generation", "abstract": "<p>Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, machine translation, and visual-language generation;\nand (3) hallucinations in large language models (LLMs). This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.</p>\n", "tags": ["Evaluation","Model Architecture","Survey Paper"] },
{"key": "jia2016data", "citations": "494", "year": "2016", "title":"Data Recombination For Neural Semantic Parsing", "abstract": "<p>Modeling crisp logical regularities is crucial in semantic parsing, making it\ndifficult for neural models with no task-specific prior knowledge to achieve\ngood results. In this paper, we introduce data recombination, a novel framework\nfor injecting such prior knowledge into a model. From the training data, we\ninduce a high-precision synchronous context-free grammar, which captures\nimportant conditional independence properties commonly found in semantic\nparsing. We then train a sequence-to-sequence recurrent network (RNN) model\nwith a novel attention-based copying mechanism on datapoints sampled from this\ngrammar, thereby teaching the model about these structural properties. Data\nrecombination improves the accuracy of our RNN model on three semantic parsing\ndatasets, leading to new state-of-the-art performance on the standard GeoQuery\ndataset for models with comparable supervision.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "jia2017adversarial", "citations": "1352", "year": "2017", "title":"Adversarial Examples For Evaluating Reading Comprehension Systems", "abstract": "<p>Standard accuracy metrics indicate that reading comprehension systems are\nmaking rapid progress, but the extent to which these systems truly understand\nlanguage remains unclear. To reward systems with real language understanding\nabilities, we propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). Our method tests whether systems can answer\nquestions about paragraphs that contain adversarially inserted sentences, which\nare automatically generated to distract computer systems without changing the\ncorrect answer or misleading humans. In this adversarial setting, the accuracy\nof sixteen published models drops from an average of \\(75%\\) F1 score to \\(36%\\);\nwhen the adversary is allowed to add ungrammatical sequences of words, average\naccuracy on four models decreases further to \\(7%\\). We hope our insights will\nmotivate the development of new models that understand language more precisely.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Security"] },
{"key": "jia2018leveraging", "citations": "163", "year": "2019", "title":"Leveraging Weakly Supervised Data To Improve End-to-end Speech-to-text Translation", "abstract": "<p>End-to-end Speech Translation (ST) models have many potential advantages when\ncompared to the cascade of Automatic Speech Recognition (ASR) and text Machine\nTranslation (MT) models, including lowered inference latency and the avoidance\nof error compounding. However, the quality of end-to-end ST is often limited by\na paucity of training data, since it is difficult to collect large parallel\ncorpora of speech and translated transcript pairs. Previous studies have\nproposed the use of pre-trained components and multi-task learning in order to\nbenefit from weakly supervised training data, such as speech-to-transcript or\ntext-to-foreign-text pairs. In this paper, we demonstrate that using\npre-trained MT or text-to-speech (TTS) synthesis models to convert weakly\nsupervised data into speech-to-translation pairs for ST training can be more\neffective than multi-task learning. Furthermore, we demonstrate that a high\nquality end-to-end ST model can be trained using only weakly supervised\ndatasets, and that synthetic data sourced from unlabeled monolingual text or\nspeech can be used to improve performance. Finally, we discuss methods for\navoiding overfitting to synthetic speech with a quantitative ablation study.</p>\n", "tags": ["Datasets","ICASSP","Training Techniques"] },
{"key": "jia2019certified", "citations": "234", "year": "2019", "title":"Certified Robustness To Adversarial Word Substitutions", "abstract": "<p>State-of-the-art NLP models can often be fooled by adversaries that apply\nseemingly innocuous label-preserving transformations (e.g., paraphrasing) to\ninput text. The number of possible transformations scales exponentially with\ntext length, so data augmentation cannot cover all transformations of an input.\nThis paper considers one exponentially large family of label-preserving\ntransformations, in which every word in the input can be replaced with a\nsimilar word. We train the first models that are provably robust to all word\nsubstitutions in this family. Our training procedure uses Interval Bound\nPropagation (IBP) to minimize an upper bound on the worst-case loss that any\ncombination of word substitutions can induce. To evaluate models’ robustness to\nthese transformations, we measure accuracy on adversarially chosen word\nsubstitutions applied to test examples. Our IBP-trained models attain \\(75%\\)\nadversarial accuracy on both sentiment analysis on IMDB and natural language\ninference on SNLI. In comparison, on IMDB, models trained normally and ones\ntrained with data augmentation achieve adversarial accuracy of only \\(8%\\) and\n\\(35%\\), respectively.</p>\n", "tags": ["EMNLP","Security","Training Techniques"] },
{"key": "jia2019direct", "citations": "150", "year": "2019", "title":"Direct Speech-to-speech Translation With A Sequence-to-sequence Model", "abstract": "<p>We present an attention-based sequence-to-sequence neural network which can\ndirectly translate speech from one language into speech in another language,\nwithout relying on an intermediate text representation. The network is trained\nend-to-end, learning to map speech spectrograms into target spectrograms in\nanother language, corresponding to the translated content (in a different\ncanonical voice). We further demonstrate the ability to synthesize translated\nspeech using the voice of the source speaker. We conduct experiments on two\nSpanish-to-English speech translation datasets, and find that the proposed\nmodel slightly underperforms a baseline cascade of a direct speech-to-text\ntranslation model and a text-to-speech synthesis model, demonstrating the\nfeasibility of the approach on this very challenging task.</p>\n", "tags": ["Datasets","INTERSPEECH","Model Architecture"] },
{"key": "jia2021complex", "citations": "70", "year": "2021", "title":"Complex Temporal Question Answering On Knowledge Graphs", "abstract": "<p>Question answering over knowledge graphs (KG-QA) is a vital topic in IR.\nQuestions with temporal intent are a special class of practical importance, but\nhave not received much attention in research. This work presents EXAQT, the\nfirst end-to-end system for answering complex temporal questions that have\nmultiple entities and predicates, and associated temporal conditions. EXAQT\nanswers natural language questions over KGs in two stages, one geared towards\nhigh recall, the other towards precision at top ranks. The first step computes\nquestion-relevant compact subgraphs within the KG, and judiciously enhances\nthem with pertinent temporal facts, using Group Steiner Trees and fine-tuned\nBERT models. The second step constructs relational graph convolutional networks\n(R-GCNs) from the first step’s output, and enhances the R-GCNs with time-aware\nentity embeddings and attention over temporal relations. We evaluate EXAQT on\nTimeQuestions, a large dataset of 16k temporal questions we compiled from a\nvariety of general purpose KG-QA benchmarks. Results show that EXAQT\noutperforms three state-of-the-art systems for answering complex questions over\nKGs, thereby justifying specialized treatment of temporal QA.</p>\n", "tags": ["CIKM","Datasets","Model Architecture"] },
{"key": "jia2021scaling", "citations": "712", "year": "2021", "title":"Scaling Up Visual And Vision-language Representation Learning With Noisy Text Supervision", "abstract": "<p>Pre-trained representations are becoming crucial for many NLP and perception\ntasks. While representation learning in NLP has transitioned to training on raw\ntext without human annotations, visual and vision-language representations\nstill rely heavily on curated training datasets that are expensive or require\nexpert knowledge. For vision applications, representations are mostly learned\nusing datasets with explicit class labels such as ImageNet or OpenImages. For\nvision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all\ninvolve a non-trivial data collection (and cleaning) process. This costly\ncuration process limits the size of datasets and hence hinders the scaling of\ntrained models. In this paper, we leverage a noisy dataset of over one billion\nimage alt-text pairs, obtained without expensive filtering or post-processing\nsteps in the Conceptual Captions dataset. A simple dual-encoder architecture\nlearns to align visual and language representations of the image and text pairs\nusing a contrastive loss. We show that the scale of our corpus can make up for\nits noise and leads to state-of-the-art representations even with such a simple\nlearning scheme. Our visual representation achieves strong performance when\ntransferred to classification tasks such as ImageNet and VTAB. The aligned\nvisual and language representations enables zero-shot image classification and\nalso set new state-of-the-art results on Flickr30K and MSCOCO image-text\nretrieval benchmarks, even when compared with more sophisticated\ncross-attention models. The representations also enable cross-modality search\nwith complex text and text + image queries.</p>\n", "tags": ["Applications","Datasets","ICML","Model Architecture","Training Techniques"] },
{"key": "jia2022visual", "citations": "635", "year": "2022", "title":"Visual Prompt Tuning", "abstract": "<p>The current modus operandi in adapting pre-trained models involves updating\nall the backbone parameters, ie, full fine-tuning. This paper introduces Visual\nPrompt Tuning (VPT) as an efficient and effective alternative to full\nfine-tuning for large-scale Transformer models in vision. Taking inspiration\nfrom recent advances in efficiently tuning large language models, VPT\nintroduces only a small amount (less than 1% of model parameters) of trainable\nparameters in the input space while keeping the model backbone frozen. Via\nextensive experiments on a wide variety of downstream recognition tasks, we\nshow that VPT achieves significant performance gains compared to other\nparameter efficient tuning protocols. Most importantly, VPT even outperforms\nfull fine-tuning in many cases across model capacities and training data\nscales, while reducing per-task storage cost.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "jiaan2023is", "citations": "138", "year": "2023", "title":"Is Chatgpt A Good NLG Evaluator? A Preliminary Study", "abstract": "<p>Recently, the emergence of ChatGPT has attracted wide attention from the\ncomputational linguistics community. Many prior studies have shown that ChatGPT\nachieves remarkable performance on various NLP tasks in terms of automatic\nevaluation metrics. However, the ability of ChatGPT to serve as an evaluation\nmetric is still underexplored. Considering assessing the quality of natural\nlanguage generation (NLG) models is an arduous task and NLG metrics notoriously\nshow their poor correlation with human judgments, we wonder whether ChatGPT is\na good NLG evaluation metric. In this report, we provide a preliminary\nmeta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail,\nwe regard ChatGPT as a human evaluator and give task-specific (e.g.,\nsummarization) and aspect-specific (e.g., relevance) instruction to prompt\nChatGPT to evaluate the generated results of NLG models. We conduct experiments\non five NLG meta-evaluation datasets (including summarization, story generation\nand data-to-text tasks). Experimental results show that compared with previous\nautomatic metrics, ChatGPT achieves state-of-the-art or competitive correlation\nwith human judgments in most cases. In addition, we find that the effectiveness\nof the ChatGPT evaluator might be influenced by the creation method of the\nmeta-evaluation datasets. For the meta-evaluation datasets which are created\ngreatly depending on the reference and thus are biased, the ChatGPT evaluator\nmight lose its effectiveness. We hope our preliminary study could prompt the\nemergence of a general-purposed reliable NLG metric.</p>\n", "tags": ["Evaluation"] },
{"key": "jiang2019dualvd", "citations": "70", "year": "2020", "title":"Dualvd: An Adaptive Dual Encoding Model For Deep Visual Understanding In Visual Dialogue", "abstract": "<p>Different from Visual Question Answering task that requires to answer only\none question about an image, Visual Dialogue involves multiple questions which\ncover a broad range of visual content that could be related to any objects,\nrelationships or semantics. The key challenge in Visual Dialogue task is thus\nto learn a more comprehensive and semantic-rich image representation which may\nhave adaptive attentions on the image for variant questions. In this research,\nwe propose a novel model to depict an image from both visual and semantic\nperspectives. Specifically, the visual view helps capture the appearance-level\ninformation, including objects and their relationships, while the semantic view\nenables the agent to understand high-level visual semantics from the whole\nimage to the local regions. Futhermore, on top of such multi-view image\nfeatures, we propose a feature selection framework which is able to adaptively\ncapture question-relevant information hierarchically in fine-grained level. The\nproposed method achieved state-of-the-art results on benchmark Visual Dialogue\ndatasets. More importantly, we can tell which modality (visual or semantic) has\nmore contribution in answering the current question by visualizing the gate\nvalues. It gives us insights in understanding of human cognition in Visual\nDialogue.</p>\n", "tags": ["AAAI","Agentic","Datasets","Evaluation","Tools"] },
{"key": "jiang2019how", "citations": "823", "year": "2020", "title":"How Can We Know What Language Models Know?", "abstract": "<p>Recent work has presented intriguing results examining the knowledge\ncontained in language models (LM) by having the LM fill in the blanks of\nprompts such as “Obama is a _ by profession”. These prompts are usually\nmanually created, and quite possibly sub-optimal; another prompt such as “Obama\nworked as a _” may result in more accurately predicting the correct profession.\nBecause of this, given an inappropriate prompt, we might fail to retrieve facts\nthat the LM does know, and thus any given prompt only provides a lower bound\nestimate of the knowledge contained in an LM. In this paper, we attempt to more\naccurately estimate the knowledge contained in LMs by automatically discovering\nbetter prompts to use in this querying process. Specifically, we propose\nmining-based and paraphrasing-based methods to automatically generate\nhigh-quality and diverse prompts, as well as ensemble methods to combine\nanswers from different prompts. Extensive experiments on the LAMA benchmark for\nextracting relational knowledge from LMs demonstrate that our methods can\nimprove accuracy from 31.1% to 39.6%, providing a tighter lower bound on what\nLMs know. We have released the code and the resulting LM Prompt And Query\nArchive (LPAQA) at https://github.com/jzbjyb/LPAQA.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Prompting","TACL"] },
{"key": "jiang2019self", "citations": "63", "year": "2019", "title":"Self-assembling Modular Networks For Interpretable Multi-hop Reasoning", "abstract": "<p>Multi-hop QA requires a model to connect multiple pieces of evidence\nscattered in a long context to answer the question. The recently proposed\nHotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four\ndifferent multi-hop reasoning paradigms (two bridge entity setups, checking\nmultiple properties, and comparing two entities), making it challenging for a\nsingle neural network to handle all four. In this work, we present an\ninterpretable, controller-based Self-Assembling Neural Modular Network (Hu et\nal., 2017, 2018) for multi-hop reasoning, where we design four novel modules\n(Find, Relocate, Compare, NoOp) to perform unique types of language reasoning.\nBased on a question, our layout controller RNN dynamically infers a series of\nreasoning modules to construct the entire network. Empirically, we show that\nour dynamic, multi-hop modular network achieves significant improvements over\nthe static, single-hop baseline (on both regular and adversarial evaluation).\nWe further demonstrate the interpretability of our model via three analyses.\nFirst, the controller can softly decompose the multi-hop question into multiple\nsingle-hop sub-questions to promote compositional reasoning behavior of the\nmain network. Second, the controller can predict layouts that conform to the\nlayouts designed by human experts. Finally, the intermediate module can infer\nthe entity that connects two distantly-located supporting facts by addressing\nthe sub-question from the controller.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "jiang2019smart", "citations": "320", "year": "2020", "title":"SMART: Robust And Efficient Fine-tuning For Pre-trained Natural Language Models Through Principled Regularized Optimization", "abstract": "<p>Transfer learning has fundamentally changed the landscape of natural language\nprocessing (NLP) research. Many existing state-of-the-art models are first\npre-trained on a large text corpus and then fine-tuned on downstream tasks.\nHowever, due to limited data resources from downstream tasks and the extremely\nlarge capacity of pre-trained models, aggressive fine-tuning often causes the\nadapted model to overfit the data of downstream tasks and forget the knowledge\nof the pre-trained model. To address the above issue in a more principled\nmanner, we propose a new computational framework for robust and efficient\nfine-tuning for pre-trained language models. Specifically, our proposed\nframework contains two important ingredients: 1. Smoothness-inducing\nregularization, which effectively manages the capacity of the model; 2. Bregman\nproximal point optimization, which is a class of trust-region methods and can\nprevent knowledge forgetting. Our experiments demonstrate that our proposed\nmethod achieves the state-of-the-art performance on multiple NLP benchmarks.</p>\n", "tags": ["Efficiency","Fine-Tuning","Tools","Training Techniques"] },
{"key": "jiang2020convbert", "citations": "97", "year": "2020", "title":"Convbert: Improving BERT With Span-based Dynamic Convolution", "abstract": "<p>Pre-trained language models like BERT and its variants have recently achieved\nimpressive performance in various natural language understanding tasks.\nHowever, BERT heavily relies on the global self-attention block and thus\nsuffers large memory footprint and computation cost. Although all its attention\nheads query on the whole input sequence for generating the attention map from a\nglobal perspective, we observe some heads only need to learn local\ndependencies, which means the existence of computation redundancy. We therefore\npropose a novel span-based dynamic convolution to replace these self-attention\nheads to directly model local dependencies. The novel convolution heads,\ntogether with the rest self-attention heads, form a new mixed attention block\nthat is more efficient at both global and local context learning. We equip BERT\nwith this mixed attention design and build a ConvBERT model. Experiments have\nshown that ConvBERT significantly outperforms BERT and its variants in various\ndownstream tasks, with lower training cost and fewer model parameters.\nRemarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than\nELECTRAbase, while using less than 1/4 training cost. Code and pre-trained\nmodels will be released.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "jiang2020defense", "citations": "339", "year": "2020", "title":"In Defense Of Grid Features For Visual Question Answering", "abstract": "<p>Popularized as ‘bottom-up’ attention, bounding box (or region) based visual\nfeatures have recently surpassed vanilla grid-based convolutional features as\nthe de facto standard for vision and language tasks like visual question\nanswering (VQA). However, it is not clear whether the advantages of regions\n(e.g. better localization) are the key reasons for the success of bottom-up\nattention. In this paper, we revisit grid features for VQA, and find they can\nwork surprisingly well - running more than an order of magnitude faster with\nthe same accuracy (e.g. if pre-trained in a similar fashion). Through extensive\nexperiments, we verify that this observation holds true across different VQA\nmodels (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71),\ndatasets, and generalizes well to other tasks like image captioning. As grid\nfeatures make the model design and training process much simpler, this enables\nus to train them end-to-end and also use a more flexible network design. We\nlearn VQA models end-to-end, from pixels directly to answers, and show that\nstrong performance is achievable without using any region annotations in\npre-training. We hope our findings help further improve the scientific\nunderstanding and the practical application of VQA. Code and features will be\nmade available.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "jiang2020how", "citations": "107", "year": "2021", "title":"How Can We Know When Language Models Know? On The Calibration Of Language Models For Question Answering", "abstract": "<p>Recent works have shown that language models (LM) capture different types of\nknowledge regarding facts or common sense. However, because no model is\nperfect, they still fail to provide appropriate answers in many cases. In this\npaper, we ask the question “how can we know when language models know, with\nconfidence, the answer to a particular query?” We examine this question from\nthe point of view of calibration, the property of a probabilistic model’s\npredicted probabilities actually being well correlated with the probabilities\nof correctness. We examine three strong generative models – T5, BART, and\nGPT-2 – and study whether their probabilities on QA tasks are well calibrated,\nfinding the answer is a relatively emphatic no. We then examine methods to\ncalibrate such models to make their confidence scores correlate better with the\nlikelihood of correctness through fine-tuning, post-hoc probability\nmodification, or adjustment of the predicted outputs or inputs. Experiments on\na diverse range of datasets demonstrate the effectiveness of our methods. We\nalso perform analysis to study the strengths and limitations of these methods,\nshedding light on further improvements that may be made in methods for\ncalibrating LMs. We have released the code at\nhttps://github.com/jzbjyb/lm-calibration.</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Model Architecture","TACL","Training Techniques"] },
{"key": "jiang2020neural", "citations": "95", "year": "2020", "title":"Neural CRF Model For Sentence Alignment In Text Simplification", "abstract": "<p>The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "jiang2020robust", "citations": "66", "year": "2020", "title":"Robust Pre-training By Adversarial Contrastive Learning", "abstract": "<p>Recent work has shown that, when integrated with adversarial training,\nself-supervised pre-training can lead to state-of-the-art robustness In this\nwork, we improve robustness-aware self-supervised pre-training by learning\nrepresentations that are consistent under both data augmentations and\nadversarial perturbations. Our approach leverages a recent contrastive learning\nframework, which learns representations by maximizing feature consistency under\ndifferently augmented views. This fits particularly well with the goal of\nadversarial robustness, as one cause of adversarial fragility is the lack of\nfeature invariance, i.e., small input perturbations can result in undesirable\nlarge changes in features or even predicted labels. We explore various options\nto formulate the contrastive task, and demonstrate that by injecting\nadversarial perturbations, contrastive pre-training can lead to models that are\nboth label-efficient and robust. We empirically evaluate the proposed\nAdversarial Contrastive Learning (ACL) and show it can consistently outperform\nexisting methods. For example on the CIFAR-10 dataset, ACL outperforms the\nprevious state-of-the-art unsupervised robust pre-training approach by 2.99% on\nrobust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL\npre-training can improve semi-supervised adversarial training, even when only a\nfew labeled examples are available. Our codes and pre-trained models have been\nreleased at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.</p>\n", "tags": ["Has Code","Security","Tools","Training Techniques"] },
{"key": "jiang2020x", "citations": "84", "year": "2020", "title":"X-FACTR: Multilingual Factual Knowledge Retrieval From Pretrained Language Models", "abstract": "<p>Language models (LMs) have proven surprisingly successful at capturing\nfactual knowledge by completing cloze-style fill-in-the-blank questions such as\n“Punta Cana is located in _.” However, while knowledge is both written and\nqueried in many languages, studies on LMs’ factual representation ability have\nalmost invariably been performed on English. To assess factual knowledge\nretrieval in LMs in different languages, we create a multilingual benchmark of\ncloze-style probes for 23 typologically diverse languages. To properly handle\nlanguage variations, we expand probing methods from single- to multi-word\nentities, and develop several decoding algorithms to generate multi-token\npredictions. Extensive experimental results provide insights about how well (or\npoorly) current state-of-the-art LMs perform at this task in languages with\nmore or fewer available resources. We further propose a code-switching-based\nmethod to improve the ability of multilingual LMs to access knowledge, and\nverify its effectiveness on several benchmark languages. Benchmark data and\ncode have been released at https://x-factr.github.io.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code","RAG"] },
{"key": "jiang2021cure", "citations": "204", "year": "2021", "title":"CURE: Code-aware Neural Machine Translation For Automatic Program Repair", "abstract": "<p>Automatic program repair (APR) is crucial to improve software reliability.\nRecently, neural machine translation (NMT) techniques have been used to fix\nsoftware bugs automatically. While promising, these approaches have two major\nlimitations. Their search space often does not contain the correct fix, and\ntheir search strategy ignores software knowledge such as strict code syntax.\nDue to these limitations, existing NMT-based techniques underperform the best\ntemplate-based approaches.\n  We propose CURE, a new NMT-based APR technique with three major novelties.\nFirst, CURE pre-trains a programming language (PL) model on a large software\ncodebase to learn developer-like source code before the APR task. Second, CURE\ndesigns a new code-aware search strategy that finds more correct fixes by\nfocusing on compilable patches and patches that are close in length to the\nbuggy code. Finally, CURE uses a subword tokenization technique to generate a\nsmaller search space that contains more correct fixes.\n  Our evaluation on two widely-used benchmarks shows that CURE correctly fixes\n57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR\ntechniques on both benchmarks.</p>\n", "tags": ["Evaluation","Llm For Code"] },
{"key": "jiang2021talk", "citations": "84", "year": "2021", "title":"Talk-to-edit: Fine-grained Facial Editing Via Dialog", "abstract": "<p>Facial editing is an important task in vision and graphics with numerous\napplications. However, existing works are incapable to deliver a continuous and\nfine-grained editing mode (e.g., editing a slightly smiling face to a big\nlaughing one) with natural interactions with users. In this work, we propose\nTalk-to-Edit, an interactive facial editing framework that performs\nfine-grained attribute manipulation through dialog between the user and the\nsystem. Our key insight is to model a continual “semantic field” in the GAN\nlatent space. 1) Unlike previous works that regard the editing as traversing\nstraight lines in the latent space, here the fine-grained editing is formulated\nas finding a curving trajectory that respects fine-grained attribute landscape\non the semantic field. 2) The curvature at each step is location-specific and\ndetermined by the input image as well as the users’ language requests. 3) To\nengage the users in a meaningful dialog, our system generates language feedback\nby considering both the user request and the current state of the semantic\nfield.\n  We also contribute CelebA-Dialog, a visual-language facial editing dataset to\nfacilitate large-scale study. Specifically, each image has manually annotated\nfine-grained attribute annotations as well as template-based textual\ndescriptions in natural language. Extensive quantitative and qualitative\nexperiments demonstrate the superiority of our framework in terms of 1) the\nsmoothness of fine-grained editing, 2) the identity/attribute preservation, and\n3) the visual photorealism and dialog fluency. Notably, user study validates\nthat our overall system is consistently favored by around 80% of the\nparticipants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.</p>\n", "tags": ["Applications","Datasets","ICCV","Tools"] },
{"key": "jiang2022promptbert", "citations": "115", "year": "2022", "title":"Promptbert: Improving BERT Sentence Embeddings With Prompts", "abstract": "<p>We propose PromptBERT, a novel contrastive learning method for learning\nbetter sentence representation. We firstly analyze the drawback of current\nsentence embedding from original BERT and find that it is mainly due to the\nstatic token embedding bias and ineffective BERT layers. Then we propose the\nfirst prompt-based sentence embeddings method and discuss two prompt\nrepresenting methods and three prompt searching methods to make BERT achieve\nbetter sentence embeddings. Moreover, we propose a novel unsupervised training\nobjective by the technology of template denoising, which substantially shortens\nthe performance gap between the supervised and unsupervised settings. Extensive\nexperiments show the effectiveness of our method. Compared to SimCSE,\nPromptBert achieves 2.29 and 2.58 points of improvement based on BERT and\nRoBERTa in the unsupervised setting.</p>\n", "tags": ["EMNLP","Model Architecture","Prompting","Training Techniques"] },
{"key": "jiang2023active", "citations": "112", "year": "2023", "title":"Active Retrieval Augmented Generation", "abstract": "<p>Despite the remarkable ability of large language models (LMs) to comprehend\nand generate language, they have a tendency to hallucinate and create factually\ninaccurate output. Augmenting LMs by retrieving information from external\nknowledge resources is one promising solution. Most existing retrieval\naugmented LMs employ a retrieve-and-generate setup that only retrieves\ninformation once based on the input. This is limiting, however, in more general\nscenarios involving generation of long texts, where continually gathering\ninformation throughout generation is essential. In this work, we provide a\ngeneralized view of active retrieval augmented generation, methods that\nactively decide when and what to retrieve across the course of the generation.\nWe propose Forward-Looking Active REtrieval augmented generation (FLARE), a\ngeneric method which iteratively uses a prediction of the upcoming sentence to\nanticipate future content, which is then utilized as a query to retrieve\nrelevant documents to regenerate the sentence if it contains low-confidence\ntokens. We test FLARE along with baselines comprehensively over 4 long-form\nknowledge-intensive generation tasks/datasets. FLARE achieves superior or\ncompetitive performance on all tasks, demonstrating the effectiveness of our\nmethod. Code and datasets are available at https://github.com/jzbjyb/FLARE.</p>\n", "tags": ["Datasets","EMNLP","Has Code"] },
{"key": "jiang2023cross", "citations": "120", "year": "2023", "title":"Cross-modal Implicit Relation Reasoning And Aligning For Text-to-image Person Retrieval", "abstract": "<p>Text-to-image person retrieval aims to identify the target person based on a\ngiven textual description query. The primary challenge is to learn the mapping\nof visual and textual modalities into a common latent space. Prior works have\nattempted to address this challenge by leveraging separately pre-trained\nunimodal models to extract visual and textual features. However, these\napproaches lack the necessary underlying alignment capabilities required to\nmatch multimodal data effectively. Besides, these works use prior information\nto explore explicit part alignments, which may lead to the distortion of\nintra-modality information. To alleviate these issues, we present IRRA: a\ncross-modal Implicit Relation Reasoning and Aligning framework that learns\nrelations between local visual-textual tokens and enhances global image-text\nmatching without requiring additional prior supervision. Specifically, we first\ndesign an Implicit Relation Reasoning module in a masked language modeling\nparadigm. This achieves cross-modal interaction by integrating the visual cues\ninto the textual tokens with a cross-modal multimodal interaction encoder.\nSecondly, to globally align the visual and textual embeddings, Similarity\nDistribution Matching is proposed to minimize the KL divergence between\nimage-text similarity distributions and the normalized label matching\ndistributions. The proposed method achieves new state-of-the-art results on all\nthree public datasets, with a notable margin of about 3%-9% for Rank-1 accuracy\ncompared to prior methods.</p>\n", "tags": ["CVPR","Datasets","Tools"] },
{"key": "jiang2023graphologue", "citations": "61", "year": "2023", "title":"Graphologue: Exploring Large Language Model Responses With Interactive Diagrams", "abstract": "<p>Large language models (LLMs) have recently soared in popularity due to their\nease of access and the unprecedented ability to synthesize text responses to\ndiverse user questions. However, LLMs like ChatGPT present significant\nlimitations in supporting complex information tasks due to the insufficient\naffordances of the text-based medium and linear conversational structure.\nThrough a formative study with ten participants, we found that LLM interfaces\noften present long-winded responses, making it difficult for people to quickly\ncomprehend and interact flexibly with various pieces of information,\nparticularly during more complex tasks. We present Graphologue, an interactive\nsystem that converts text-based responses from LLMs into graphical diagrams to\nfacilitate information-seeking and question-answering tasks. Graphologue\nemploys novel prompting strategies and interface designs to extract entities\nand relationships from LLM responses and constructs node-link diagrams in\nreal-time. Further, users can interact with the diagrams to flexibly adjust the\ngraphical presentation and to submit context-specific prompts to obtain more\ninformation. Utilizing diagrams, Graphologue enables graphical, non-linear\ndialogues between humans and LLMs, facilitating information exploration,\norganization, and comprehension.</p>\n", "tags": ["Prompting"] },
{"key": "jiang2023structgpt", "citations": "67", "year": "2023", "title":"Structgpt: A General Framework For Large Language Model To Reason Over Structured Data", "abstract": "<p>In this paper, we study how to improve the zero-shot reasoning ability of\nlarge language models~(LLMs) over structured data in a unified way. Inspired by\nthe study on tool augmentation for LLMs, we develop an <em>Iterative\nReading-then-Reasoning~(IRR)</em> approach for solving question answering tasks\nbased on structured data, called \\textbf{StructGPT}. In our approach, we\nconstruct the specialized function to collect relevant evidence from structured\ndata (\\ie <em>reading</em>), and let LLMs concentrate the reasoning task based on\nthe collected information (\\ie <em>reasoning</em>). Specially, we propose an\n<em>invoking-linearization-generation</em> procedure to support LLMs in reasoning\non the structured data with the help of the external interfaces. By iterating\nthis procedures with provided interfaces, our approach can gradually approach\nthe target answer to a given query. Extensive experiments conducted on three\ntypes of structured data demonstrate the effectiveness of our approach, which\ncan significantly boost the performance of ChatGPT and achieve comparable\nperformance against the full-data supervised-tuning baselines. Our codes and\ndata are publicly available at~https://github.com/RUCAIBox/StructGPT.</p>\n", "tags": ["EMNLP","Has Code","Tools"] },
{"key": "jiang2024mixtral", "citations": "64", "year": "2024", "title":"Mixtral Of Experts", "abstract": "<p>We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.</p>\n", "tags": ["Llm For Code","Model Architecture"] },
{"key": "jiao2019real", "citations": "126", "year": "2020", "title":"Real-time Emotion Recognition Via Attention Gated Hierarchical Memory Network", "abstract": "<p>Real-time emotion recognition (RTER) in conversations is significant for\ndeveloping emotionally intelligent chatting machines. Without the future\ncontext in RTER, it becomes critical to build the memory bank carefully for\ncapturing historical context and summarize the memories appropriately to\nretrieve relevant information. We propose an Attention Gated Hierarchical\nMemory Network (AGHMN) to address the problems of prior work: (1) Commonly used\nconvolutional neural networks (CNNs) for utterance feature extraction are less\ncompatible in the memory modules; (2) Unidirectional gated recurrent units\n(GRUs) only allow each historical utterance to have context before it,\npreventing information propagation in the opposite direction; (3) The Soft\nAttention for summarizing loses the positional and ordering information of\nmemories, regardless of how the memory bank is built. Particularly, we propose\na Hierarchical Memory Network (HMN) with a bidirectional GRU (BiGRU) as the\nutterance reader and a BiGRU fusion layer for the interaction between\nhistorical utterances. For memory summarizing, we propose an Attention GRU\n(AGRU) where we utilize the attention weights to update the internal state of\nGRU. We further promote the AGRU to a bidirectional variant (BiAGRU) to balance\nthe contextual information from recent memories and that from distant memories.\nWe conduct experiments on two emotion conversation datasets with extensive\nanalysis, demonstrating the efficacy of our AGHMN models.</p>\n", "tags": ["AAAI","Datasets","Model Architecture"] },
{"key": "jiao2019tinybert", "citations": "1378", "year": "2020", "title":"Tinybert: Distilling BERT For Natural Language Understanding", "abstract": "<p>Language model pre-training, such as BERT, has significantly improved the\nperformances of many natural language processing tasks. However, pre-trained\nlanguage models are usually computationally expensive, so it is difficult to\nefficiently execute them on resource-restricted devices. To accelerate\ninference and reduce model size while maintaining accuracy, we first propose a\nnovel Transformer distillation method that is specially designed for knowledge\ndistillation (KD) of the Transformer-based models. By leveraging this new KD\nmethod, the plenty of knowledge encoded in a large teacher BERT can be\neffectively transferred to a small student Tiny-BERT. Then, we introduce a new\ntwo-stage learning framework for TinyBERT, which performs Transformer\ndistillation at both the pretraining and task-specific learning stages. This\nframework ensures that TinyBERT can capture he general-domain as well as the\ntask-specific knowledge in BERT.\n  TinyBERT with 4 layers is empirically effective and achieves more than 96.8%\nthe performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x\nsmaller and 9.4x faster on inference. TinyBERT with 4 layers is also\nsignificantly better than 4-layer state-of-the-art baselines on BERT\ndistillation, with only about 28% parameters and about 31% inference time of\nthem. Moreover, TinyBERT with 6 layers performs on-par with its teacher\nBERTBASE.</p>\n", "tags": ["Datasets","EMNLP","Efficiency","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "jiao2023is", "citations": "243", "year": "2023", "title":"Is Chatgpt A Good Translator? Yes With GPT-4 As The Engine", "abstract": "<p>This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well\nwith minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. As for the\ntranslation robustness, ChatGPT does not perform as well as the commercial\nsystems on biomedical abstracts or Reddit comments but exhibits good results on\nspoken language. Further, we explore an interesting strategy named\n\\(\\mathbf{pivot~prompting}\\) for distant languages, which asks ChatGPT to\ntranslate the source sentence into a high-resource pivot language before into\nthe target language, improving the translation performance noticeably. With the\nlaunch of the GPT-4 engine, the translation performance of ChatGPT is\nsignificantly boosted, becoming comparable to commercial translation products,\neven for distant languages. Human analysis on Google Translate and ChatGPT\nsuggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and\nmis-translation errors while that with GPT-4 makes the least errors. In other\nwords, ChatGPT has already become a good translator. Please refer to our Github\nproject for more details:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator</p>\n", "tags": ["Evaluation","Prompting"] },
{"key": "jin2019is", "citations": "729", "year": "2020", "title":"Is BERT Really Robust? A Strong Baseline For Natural Language Attack On Text Classification And Entailment", "abstract": "<p>Machine learning algorithms are often vulnerable to adversarial examples that\nhave imperceptible alterations from the original counterparts but can fool the\nstate-of-the-art models. It is helpful to evaluate or even improve the\nrobustness of these models by exposing the maliciously crafted adversarial\nexamples. In this paper, we present TextFooler, a simple but strong baseline to\ngenerate natural adversarial text. By applying it to two fundamental natural\nlanguage tasks, text classification and textual entailment, we successfully\nattacked three target models, including the powerful pre-trained BERT, and the\nwidely used convolutional and recurrent neural networks. We demonstrate the\nadvantages of this framework in three ways: (1) effective—it outperforms\nstate-of-the-art attacks in terms of success rate and perturbation rate, (2)\nutility-preserving—it preserves semantic content and grammaticality, and\nremains correctly classified by humans, and (3) efficient—it generates\nadversarial text with computational complexity linear to the text length. *The\ncode, pre-trained target models, and test examples are available at\nhttps://github.com/jind11/TextFooler.</p>\n", "tags": ["AAAI","Has Code","Model Architecture","Security","Tools"] },
{"key": "jin2019mmm", "citations": "66", "year": "2020", "title":"MMM: Multi-stage Multi-task Learning For Multi-choice Reading Comprehension", "abstract": "<p>Machine Reading Comprehension (MRC) for question answering (QA), which aims\nto answer a question given the relevant context passages, is an important way\nto test the ability of intelligence systems to understand human language.\nMultiple-Choice QA (MCQA) is one of the most difficult tasks in MRC because it\noften requires more advanced reading comprehension skills such as logical\nreasoning, summarization, and arithmetic operations, compared to the extractive\ncounterpart where answers are usually spans of text within given passages.\nMoreover, most existing MCQA datasets are small in size, making the learning\ntask even harder. We introduce MMM, a Multi-stage Multi-task learning framework\nfor Multi-choice reading comprehension. Our method involves two sequential\nstages: coarse-tuning stage using out-of-domain datasets and multi-task\nlearning stage using a larger in-domain dataset to help model generalize better\nwith limited data. Furthermore, we propose a novel multi-step attention network\n(MAN) as the top-level classifier for this task. We demonstrate MMM\nsignificantly advances the state-of-the-art on four representative MCQA\ndatasets.</p>\n", "tags": ["AAAI","Datasets","Model Architecture","Tools"] },
{"key": "jin2019probing", "citations": "105", "year": "2019", "title":"Probing Biomedical Embeddings From Language Models", "abstract": "<p>Contextualized word embeddings derived from pre-trained language models (LMs)\nshow significant improvements on downstream NLP tasks. Pre-training on\ndomain-specific corpora, such as biomedical articles, further improves their\nperformance. In this paper, we conduct probing experiments to determine what\nadditional information is carried intrinsically by the in-domain trained\ncontextualized embeddings. For this we use the pre-trained LMs as fixed feature\nextractors and restrict the downstream task models to not have additional\nsequence modeling layers. We compare BERT, ELMo, BioBERT and BioELMo, a\nbiomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while\nfine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a\nfixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We\nuse visualization and nearest neighbor analysis to show that better encoding of\nentity-type and relational information leads to this superiority.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "jin2020deep", "citations": "141", "year": "2021", "title":"Deep Learning For Text Style Transfer: A Survey", "abstract": "<p>Text style transfer is an important task in natural language generation,\nwhich aims to control certain attributes in the generated text, such as\npoliteness, emotion, humor, and many others. It has a long history in the field\nof natural language processing, and recently has re-gained significant\nattention thanks to the promising performance brought by deep neural models. In\nthis paper, we present a systematic survey of the research on neural text style\ntransfer, spanning over 100 representative articles since the first neural text\nstyle transfer work in 2017. We discuss the task formulation, existing datasets\nand subtasks, evaluation, as well as the rich methodologies in the presence of\nparallel and non-parallel data. We also provide discussions on a variety of\nimportant topics regarding the future development of this task. Our curated\npaper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Survey Paper"] },
{"key": "jin2020what", "citations": "202", "year": "2021", "title":"What Disease Does This Patient Have? A Large-scale Open Domain Question Answering Dataset From Medical Exams", "abstract": "<p>Open domain question answering (OpenQA) tasks have been recently attracting\nmore and more attention from the natural language processing (NLP) community.\nIn this work, we present the first free-form multiple-choice OpenQA dataset for\nsolving medical problems, MedQA, collected from the professional medical board\nexams. It covers three languages: English, simplified Chinese, and traditional\nChinese, and contains 12,723, 34,251, and 14,123 questions for the three\nlanguages, respectively. We implement both rule-based and popular neural\nmethods by sequentially combining a document retriever and a machine\ncomprehension model. Through experiments, we find that even the current best\nmethod can only achieve 36.7%, 42.0%, and 70.1% of test accuracy on the\nEnglish, traditional Chinese, and simplified Chinese questions, respectively.\nWe expect MedQA to present great challenges to existing OpenQA systems and hope\nthat it can serve as a platform to promote much stronger OpenQA models from the\nNLP community in the future.</p>\n", "tags": ["Datasets","Model Architecture","Retrieval Systems","Tools"] },
{"key": "jin2021biomedical", "citations": "72", "year": "2022", "title":"Biomedical Question Answering: A Survey Of Approaches And Challenges", "abstract": "<p>Automatic Question Answering (QA) has been successfully applied in various\ndomains such as search engines and chatbots. Biomedical QA (BQA), as an\nemerging QA task, enables innovative applications to effectively perceive,\naccess and understand complex biomedical knowledge. There have been tremendous\ndevelopments of BQA in the past two decades, which we classify into 5\ndistinctive approaches: classic, information retrieval, machine reading\ncomprehension, knowledge base and question entailment approaches. In this\nsurvey, we introduce available datasets and representative methods of each BQA\napproach in detail. Despite the developments, BQA systems are still immature\nand rarely used in real-life settings. We identify and characterize several key\nchallenges in BQA that might lead to this issue, and discuss some potential\nfuture directions to explore.</p>\n", "tags": ["Applications","Datasets","Survey Paper"] },
{"key": "jin2021good", "citations": "68", "year": "2022", "title":"A Good Prompt Is Worth Millions Of Parameters: Low-resource Prompt-based Learning For Vision-language Models", "abstract": "<p>Large pre-trained vision-language (VL) models can learn a new task with a\nhandful of examples and generalize to a new task without fine-tuning. However,\nthese VL models are hard to deploy for real-world applications due to their\nimpractically huge sizes and slow inference speed. To solve this limitation, we\nstudy prompt-based low-resource learning of VL tasks with our proposed method,\nFewVLM, relatively smaller than recent few-shot learners. For FewVLM, we\npre-train a sequence-to-sequence transformer model with prefix language\nmodeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we\nanalyze the effect of diverse prompts for few-shot tasks. Experimental results\non VQA show that FewVLM with prompt-based learning outperforms Frozen which is\n31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x\nlarger model, PICa. In our analysis, we observe that (1) prompts significantly\naffect zero-shot performance but marginally affect few-shot performance, (2)\nmodels with noisy prompts learn as quickly as hand-crafted prompts given larger\ntraining data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts\ncaptioning performance. Our code is publicly available at\nhttps://github.com/woojeongjin/FewVLM</p>\n", "tags": ["Applications","Few-Shot","Fine-Tuning","Has Code","Model Architecture","Prompting","Training Techniques"] },
{"key": "jin2023genegpt", "citations": "81", "year": "2024", "title":"Genegpt: Augmenting Large Language Models With Domain Tools For Improved Access To Biomedical Information", "abstract": "<p>While large language models (LLMs) have been successfully applied to various\ntasks, they still face challenges with hallucinations. Augmenting LLMs with\ndomain-specific tools such as database utilities can facilitate easier and more\nprecise access to specialized knowledge. In this paper, we present GeneGPT, a\nnovel method for teaching LLMs to use the Web APIs of the National Center for\nBiotechnology Information (NCBI) for answering genomics questions.\nSpecifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs\nby in-context learning and an augmented decoding algorithm that can detect and\nexecute API calls. Experimental results show that GeneGPT achieves\nstate-of-the-art performance on eight tasks in the GeneTuring benchmark with an\naverage score of 0.83, largely surpassing retrieval-augmented LLMs such as the\nnew Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as\nwell as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)\nAPI demonstrations have good cross-task generalizability and are more useful\nthan documentations for in-context learning; (2) GeneGPT can generalize to\nlonger chains of API calls and answer multi-hop questions in GeneHop, a novel\ndataset introduced in this work; (3) Different types of errors are enriched in\ndifferent tasks, providing valuable insights for future improvements.</p>\n", "tags": ["Datasets","Evaluation","In Context Learning","Model Architecture","Prompting","RAG","Tools"] },
{"key": "jin2023inferfix", "citations": "69", "year": "2023", "title":"Inferfix: End-to-end Program Repair With Llms", "abstract": "<p>Software development life cycle is profoundly influenced by bugs: their\nintroduction, identification, and eventual resolution account for a significant\nportion of software cost. This has motivated software engineering researchers\nand practitioners to propose different approaches for automating the\nidentification and repair of software defects. Large language models have been\nadapted to the program repair task through few-shot demonstration learning and\ninstruction prompting, treating this as an infilling task. However, these\nmodels have only focused on learning general bug-fixing patterns for\nuncategorized bugs mined from public repositories. In this paper, we propose\nInferFix: a transformer-based program repair framework paired with a\nstate-of-the-art static analyzer to fix critical security and performance bugs.\nInferFix combines a Retriever – transformer encoder model pretrained via\ncontrastive learning objective, which aims at searching for semantically\nequivalent bugs and corresponding fixes; and a Generator – a large language\nmodel (Codex Cushman) finetuned on supervised bug-fix data with prompts\naugmented via bug type annotations and semantically similar fixes retrieved\nfrom an external non-parametric memory. To train and evaluate our approach, we\ncurated InferredBugs, a novel, metadata-rich dataset of bugs extracted by\nexecuting the Infer static analyzer on the change histories of thousands of\nJava and C# repositories. Our evaluation demonstrates that InferFix outperforms\nstrong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C#\nand 76.8% in Java. We discuss the deployment of InferFix alongside Infer at\nMicrosoft which offers an end-to-end solution for detection, classification,\nand localization of bugs, as well as fixing and validation of candidate\npatches, integrated in the continuous integration pipeline to automate the\nsoftware development workflow.</p>\n", "tags": ["Evaluation","Few-Shot","Llm For Code","Tools"] },
{"key": "jin2023time", "citations": "63", "year": "2023", "title":"Time-llm: Time Series Forecasting By Reprogramming Large Language Models", "abstract": "<p>Time series forecasting holds significant importance in many real-world\ndynamic systems and has been extensively studied. Unlike natural language\nprocess (NLP) and computer vision (CV), where a single large model can tackle\nmultiple tasks, models for time series forecasting are often specialized,\nnecessitating distinct designs for different tasks and applications. While\npre-trained foundation models have made impressive strides in NLP and CV, their\ndevelopment in time series domains has been constrained by data sparsity.\nRecent studies have revealed that large language models (LLMs) possess robust\npattern recognition and reasoning abilities over complex sequences of tokens.\nHowever, the challenge remains in effectively aligning the modalities of time\nseries data and natural language to leverage these capabilities. In this work,\nwe present Time-LLM, a reprogramming framework to repurpose LLMs for general\ntime series forecasting with the backbone language models kept intact. We begin\nby reprogramming the input time series with text prototypes before feeding it\ninto the frozen LLM to align the two modalities. To augment the LLM’s ability\nto reason with time series data, we propose Prompt-as-Prefix (PaP), which\nenriches the input context and directs the transformation of reprogrammed input\npatches. The transformed time series patches from the LLM are finally projected\nto obtain the forecasts. Our comprehensive evaluations demonstrate that\nTime-LLM is a powerful time series learner that outperforms state-of-the-art,\nspecialized forecasting models. Moreover, Time-LLM excels in both few-shot and\nzero-shot learning scenarios.</p>\n", "tags": ["Applications","Few-Shot","Prompting","Time Series","Tools"] },
{"key": "jin2023when", "citations": "89", "year": "2024", "title":"When Large Language Models Meet Personalization: Perspectives Of Challenges And Opportunities", "abstract": "<p>The advent of large language models marks a revolutionary breakthrough in\nartificial intelligence. With the unprecedented scale of training and model\nparameters, the capability of large language models has been dramatically\nimproved, leading to human-like performances in understanding, language\nsynthesizing, and common-sense reasoning, etc. Such a major leap-forward in\ngeneral AI capacity will change the pattern of how personalization is\nconducted. For one thing, it will reform the way of interaction between humans\nand personalization systems. Instead of being a passive medium of information\nfiltering, large language models present the foundation for active user\nengagement. On top of such a new foundation, user requests can be proactively\nexplored, and user’s required information can be delivered in a natural and\nexplainable way. For another thing, it will also considerably expand the scope\nof personalization, making it grow from the sole function of collecting\npersonalized information to the compound function of providing personalized\nservices. By leveraging large language models as general-purpose interface, the\npersonalization systems may compile user requests into plans, calls the\nfunctions of external tools to execute the plans, and integrate the tools’\noutputs to complete the end-to-end personalization tasks. Today, large language\nmodels are still being developed, whereas the application in personalization is\nlargely unexplored. Therefore, we consider it to be the right time to review\nthe challenges in personalization and the opportunities to address them with\nLLMs. In particular, we dedicate this perspective paper to the discussion of\nthe following aspects: the development and challenges for the existing\npersonalization system, the newly emerged capabilities of large language\nmodels, and the potential ways of making use of large language models for\npersonalization.</p>\n", "tags": ["Reinforcement Learning","Tools","Training Techniques"] },
{"key": "jing2017automatic", "citations": "491", "year": "2018", "title":"On The Automatic Generation Of Medical Imaging Reports", "abstract": "<p>Medical imaging is widely used in clinical practice for diagnosis and\ntreatment. Report-writing can be error-prone for unexperienced physicians, and\ntime- consuming and tedious for experienced physicians. To address these\nissues, we study the automatic generation of medical imaging reports. This task\npresents several challenges. First, a complete report contains multiple\nheterogeneous forms of information, including findings and tags. Second,\nabnormal regions in medical images are difficult to identify. Third, the re-\nports are typically long, containing multiple sentences. To cope with these\nchallenges, we (1) build a multi-task learning framework which jointly performs\nthe pre- diction of tags and the generation of para- graphs, (2) propose a\nco-attention mechanism to localize regions containing abnormalities and\ngenerate narrations for them, (3) develop a hierarchical LSTM model to generate\nlong paragraphs. We demonstrate the effectiveness of the proposed methods on\ntwo publicly available datasets.</p>\n", "tags": ["Datasets","Model Architecture","Tools"] },
{"key": "jizhi2023is", "citations": "69", "year": "2023", "title":"Is Chatgpt Fair For Recommendation? Evaluating Fairness In Large Language Model Recommendation", "abstract": "<p>The remarkable achievements of Large Language Models (LLMs) have led to the\nemergence of a novel recommendation paradigm – Recommendation via LLM\n(RecLLM). Nevertheless, it is important to note that LLMs may contain social\nprejudices, and therefore, the fairness of recommendations made by RecLLM\nrequires further investigation. To avoid the potential risks of RecLLM, it is\nimperative to evaluate the fairness of RecLLM with respect to various sensitive\nattributes on the user side. Due to the differences between the RecLLM paradigm\nand the traditional recommendation paradigm, it is problematic to directly use\nthe fairness benchmark of traditional recommendation. To address the dilemma,\nwe propose a novel benchmark called Fairness of Recommendation via LLM\n(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset\nthat accounts for eight sensitive attributes1 in two recommendation scenarios:\nmusic and movies. By utilizing our FaiRLLM benchmark, we conducted an\nevaluation of ChatGPT and discovered that it still exhibits unfairness to some\nsensitive attributes when generating recommendations. Our code and dataset can\nbe found at https://github.com/jizhi-zhang/FaiRLLM.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Has Code"] },
{"key": "joglekar2019neural", "citations": "88", "year": "2020", "title":"Neural Input Search For Large Scale Recommendation Models", "abstract": "<p>Recommendation problems with large numbers of discrete items, such as\nproducts, webpages, or videos, are ubiquitous in the technology industry. Deep\nneural networks are being increasingly used for these recommendation problems.\nThese models use embeddings to represent discrete items as continuous vectors,\nand the vocabulary sizes and embedding dimensions, although heavily influence\nthe model’s accuracy, are often manually selected in a heuristical manner. We\npresent Neural Input Search (NIS), a technique for learning the optimal\nvocabulary sizes and embedding dimensions for categorical features. The goal is\nto maximize prediction accuracy subject to a constraint on the total memory\nused by all embeddings. Moreover, we argue that the traditional Single-size\nEmbedding (SE), which uses the same embedding dimension for all values of a\nfeature, suffers from inefficient usage of model capacity and training data. We\npropose a novel type of embedding, namely Multi-size Embedding (ME), which\nallows the embedding dimension to vary for different values of the feature.\nDuring training we use reinforcement learning to find the optimal vocabulary\nsize for each feature and embedding dimension for each value of the feature. In\nexperiments on two common types of large scale recommendation problems, i.e.\nretrieval and ranking problems, NIS automatically found better vocabulary and\nembedding sizes that result in \\(6.8%\\) and \\(1.8%\\) relative improvements on\nRecall@1 and ROC-AUC over manually optimized ones.</p>\n", "tags": ["Agentic","KDD","Reinforcement Learning","Training Techniques"] },
{"key": "johnson2016clevr", "citations": "1740", "year": "2017", "title":"CLEVR: A Diagnostic Dataset For Compositional Language And Elementary Visual Reasoning", "abstract": "<p>When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "johnson2016google", "citations": "121", "year": "2016", "title":"Google's Multilingual Neural Machine Translation System: Enabling Zero-shot Translation", "abstract": "<p>We propose a simple solution to use a single Neural Machine Translation (NMT)\nmodel to translate between multiple languages. Our solution requires no change\nin the model architecture from our base system but instead introduces an\nartificial token at the beginning of the input sentence to specify the required\ntarget language. The rest of the model, which includes encoder, decoder and\nattention, remains unchanged and is shared across all languages. Using a shared\nwordpiece vocabulary, our approach enables Multilingual NMT using a single\nmodel without any increase in parameters, which is significantly simpler than\nprevious proposals for Multilingual NMT. Our method often improves the\ntranslation quality of all involved language pairs, even while keeping the\ntotal number of model parameters constant. On the WMT’14 benchmarks, a single\nmultilingual model achieves comparable performance for\nEnglish\\(\\rightarrow\\)French and surpasses state-of-the-art results for\nEnglish\\(\\rightarrow\\)German. Similarly, a single multilingual model surpasses\nstate-of-the-art results for French\\(\\rightarrow\\)English and\nGerman\\(\\rightarrow\\)English on WMT’14 and WMT’15 benchmarks respectively. On\nproduction corpora, multilingual models of up to twelve language pairs allow\nfor better translation of many individual pairs. In addition to improving the\ntranslation quality of language pairs that the model was trained with, our\nmodels can also learn to perform implicit bridging between language pairs never\nseen explicitly during training, showing that transfer learning and zero-shot\ntranslation is possible for neural translation. Finally, we show analyses that\nhints at a universal interlingua representation in our models and show some\ninteresting examples when mixing languages.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "johnson2017inferring", "citations": "529", "year": "2017", "title":"Inferring And Executing Programs For Visual Reasoning", "abstract": "<p>Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings.</p>\n", "tags": ["Datasets","Evaluation","ICCV"] },
{"key": "joshi2017personalization", "citations": "68", "year": "2017", "title":"Personalization In Goal-oriented Dialog", "abstract": "<p>The main goal of modeling human conversation is to create agents which can\ninteract with people in both open-ended and goal-oriented scenarios. End-to-end\ntrained neural dialog systems are an important line of research for such\ngeneralized dialog models as they do not resort to any situation-specific\nhandcrafting of rules. However, incorporating personalization into such systems\nis a largely unexplored topic as there are no existing corpora to facilitate\nsuch work. In this paper, we present a new dataset of goal-oriented dialogs\nwhich are influenced by speaker profiles attached to them. We analyze the\nshortcomings of an existing end-to-end dialog system based on Memory Networks\nand propose modifications to the architecture which enable personalization. We\nalso investigate personalization in dialog as a multi-task learning problem,\nand show that a single model which shares features among various profiles\noutperforms separate models for each profile.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "joshi2017triviaqa", "citations": "1377", "year": "2017", "title":"Triviaqa: A Large Scale Distantly Supervised Challenge Dataset For Reading Comprehension", "abstract": "<p>We present TriviaQA, a challenging reading comprehension dataset containing\nover 650K question-answer-evidence triples. TriviaQA includes 95K\nquestion-answer pairs authored by trivia enthusiasts and independently gathered\nevidence documents, six per question on average, that provide high quality\ndistant supervision for answering the questions. We show that, in comparison to\nother recently introduced large-scale datasets, TriviaQA (1) has relatively\ncomplex, compositional questions, (2) has considerable syntactic and lexical\nvariability between questions and corresponding answer-evidence sentences, and\n(3) requires more cross sentence reasoning to find answers. We also present two\nbaseline algorithms: a feature-based classifier and a state-of-the-art neural\nnetwork, that performs well on SQuAD reading comprehension. Neither approach\ncomes close to human performance (23% and 40% vs. 80%), suggesting that\nTriviaQA is a challenging testbed that is worth significant future study. Data\nand code available at – http://nlp.cs.washington.edu/triviaqa/</p>\n", "tags": ["Datasets","Has Code"] },
{"key": "joshi2018extending", "citations": "98", "year": "2018", "title":"Extending A Parser To Distant Domains Using A Few Dozen Partially Annotated Examples", "abstract": "<p>We revisit domain adaptation for parsers in the neural era. First we show\nthat recent advances in word representations greatly diminish the need for\ndomain adaptation when the target domain is syntactically similar to the source\ndomain. As evidence, we train a parser on the Wall Street Jour- nal alone that\nachieves over 90% F1 on the Brown corpus. For more syntactically dis- tant\ndomains, we provide a simple way to adapt a parser using only dozens of partial\nannotations. For instance, we increase the percentage of error-free\ngeometry-domain parses in a held-out set from 45% to 73% using approximately\nfive dozen training examples. In the process, we demon- strate a new\nstate-of-the-art single model result on the Wall Street Journal test set of\n94.3%. This is an absolute increase of 1.7% over the previous state-of-the-art\nof 92.6%.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "joshi2019spanbert", "citations": "1696", "year": "2020", "title":"Spanbert: Improving Pre-training By Representing And Predicting Spans", "abstract": "<p>We present SpanBERT, a pre-training method that is designed to better\nrepresent and predict spans of text. Our approach extends BERT by (1) masking\ncontiguous random spans, rather than random tokens, and (2) training the span\nboundary representations to predict the entire content of the masked span,\nwithout relying on the individual token representations within it. SpanBERT\nconsistently outperforms BERT and our better-tuned baselines, with substantial\ngains on span selection tasks such as question answering and coreference\nresolution. In particular, with the same training data and model size as\nBERT-large, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0,\nrespectively. We also achieve a new state of the art on the OntoNotes\ncoreference resolution task (79.6% F1), strong performance on the TACRED\nrelation extraction benchmark, and even show gains on GLUE.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","TACL","Training Techniques"] },
{"key": "jovanović2020chatbots", "citations": "98", "year": "2020", "title":"Chatbots As Conversational Healthcare Services", "abstract": "<p>Chatbots are emerging as a promising platform for accessing and delivering\nhealthcare services. The evidence is in the growing number of publicly\navailable chatbots aiming at taking an active role in the provision of\nprevention, diagnosis, and treatment services. This article takes a closer look\nat how these emerging chatbots address design aspects relevant to healthcare\nservice provision, emphasizing the Human-AI interaction aspects and the\ntransparency in AI automation and decision making.</p>\n", "tags": ["Ethics & Fairness","Tools"] },
{"key": "jozefowicz2016exploring", "citations": "987", "year": "2016", "title":"Exploring The Limits Of Language Modeling", "abstract": "<p>In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "ju2021prompting", "citations": "209", "year": "2022", "title":"Prompting Visual-language Models For Efficient Video Understanding", "abstract": "<p>Image-based visual-language (I-VL) pre-training has shown great success for\nlearning joint visual-textual representations from large-scale web data,\nrevealing remarkable ability for zero-shot generalisation. This paper presents\na simple but strong baseline to efficiently adapt the pre-trained I-VL model,\nand exploit its powerful ability for resource-hungry video understanding tasks,\nwith minimal training. Specifically, we propose to optimise a few random\nvectors, termed as continuous prompt vectors, that convert video-related tasks\ninto the same format as the pre-training objectives. In addition, to bridge the\ngap between static images and videos, temporal information is encoded with\nlightweight Transformers stacking on top of frame-wise visual features.\nExperimentally, we conduct extensive ablation studies to analyse the critical\ncomponents. On 10 public benchmarks of action recognition, action localisation,\nand text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,\nwe achieve competitive or state-of-the-art performance to existing methods,\ndespite optimising significantly fewer parameters.</p>\n", "tags": ["Few-Shot","Prompting","Training Techniques"] },
{"key": "junczysdowmunt2016amu", "citations": "66", "year": "2016", "title":"The AMU-UEDIN Submission To The WMT16 News Translation Task: Attention-based NMT Models As Feature Functions In Phrase-based SMT", "abstract": "<p>This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on\nnews translation. We explore methods of decode-time integration of\nattention-based neural translation models with phrase-based statistical machine\ntranslation. Efficient batch-algorithms for GPU-querying are proposed and\nimplemented. For English-Russian, our system stays behind the state-of-the-art\npure neural models in terms of BLEU. Among restricted systems, manual\nevaluation places it in the first cluster tied with the pure neural model. For\nthe Russian-English task, our submission achieves the top BLEU result,\noutperforming the best pure neural system by 1.1 BLEU points and our own\nphrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the\nbest restricted system in its own cluster. In follow-up experiments we improve\nresults by additional 0.8 BLEU.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "junczysdowmunt2016is", "citations": "155", "year": "2016", "title":"Is Neural Machine Translation Ready For Deployment? A Case Study On 30 Translation Directions", "abstract": "<p>In this paper we provide the largest published comparison of translation\nquality for phrase-based SMT and neural machine translation across 30\ntranslation directions. For ten directions we also include hierarchical\nphrase-based MT. Experiments are performed for the recently published United\nNations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus.\nIn the second part of the paper we investigate aspects of translation speed,\nintroducing AmuNMT, our efficient neural machine translation decoder. We\ndemonstrate that current neural machine translation could already be used for\nin-production systems when comparing words-per-second ratios.</p>\n", "tags": ["Datasets"] },
{"key": "junczysdowmunt2016log", "citations": "110", "year": "2016", "title":"Log-linear Combinations Of Monolingual And Bilingual Neural Machine Translation Models For Automatic Post-editing", "abstract": "<p>This paper describes the submission of the AMU (Adam Mickiewicz University)\nteam to the Automatic Post-Editing (APE) task of WMT 2016. We explore the\napplication of neural translation models to the APE problem and achieve good\nresults by treating different models as components in a log-linear model,\nallowing for multiple inputs (the MT-output and the source) that are decoded to\nthe same target language (post-edited translations). A simple string-matching\npenalty integrated within the log-linear model is used to control for higher\nfaithfulness with regard to the raw machine translation output. To overcome the\nproblem of too little training data, we generate large amounts of artificial\ndata. Our submission improves over the uncorrected baseline on the unseen test\nset by -3.2% TER and +5.5% BLEU and outperforms any other system submitted to\nthe shared-task by a large margin.</p>\n", "tags": ["Training Techniques"] },
{"key": "junczysdowmunt2018approaching", "citations": "178", "year": "2018", "title":"Approaching Neural Grammatical Error Correction As A Low-resource Machine Translation Task", "abstract": "<p>Previously, neural methods in grammatical error correction (GEC) did not\nreach state-of-the-art results compared to phrase-based statistical machine\ntranslation (SMT) baselines. We demonstrate parallels between neural GEC and\nlow-resource neural MT and successfully adapt several methods from low-resource\nMT to neural GEC. We further establish guidelines for trustable results in\nneural GEC and propose a set of model-independent methods for neural GEC that\ncan be easily applied in most GEC settings. Proposed methods include adding\nsource-side noise, domain-adaptation techniques, a GEC-specific\ntraining-objective, transfer learning with monolingual data, and ensembling of\nindependently trained GEC models and language models. The combined effects of\nthese methods result in better than state-of-the-art neural GEC models that\noutperform previously best neural GEC systems by more than 10% M\\(^2\\) on the\nCoNLL-2014 benchmark and 5.9% on the JFLEG test set. Non-neural\nstate-of-the-art systems are outperformed by more than 2% on the CoNLL-2014\nbenchmark and by 4% on JFLEG.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","NAACL","Training Techniques"] },
{"key": "junczysdowmunt2018ms", "citations": "70", "year": "2018", "title":"Ms-uedin Submission To The WMT2018 APE Shared Task: Dual-source Transformer For Automatic Post-editing", "abstract": "<p>This paper describes the Microsoft and University of Edinburgh submission to\nthe Automatic Post-editing shared task at WMT2018. Based on training data and\nsystems from the WMT2017 shared task, we re-implement our own models from the\nlast shared task and introduce improvements based on extensive parameter\nsharing. Next we experiment with our implementation of dual-source transformer\nmodels and data selection for the IT domain. Our submissions decisively wins\nthe SMT post-editing sub-task establishing the new state-of-the-art and is a\nvery close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on\nthe rather weak results in the NMT sub-task, we hypothesize that\nneural-on-neural APE might not be actually useful.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "junczysdowmunt2019microsoft", "citations": "130", "year": "2019", "title":"Microsoft Translator At WMT 2019: Towards Large-scale Document-level Neural Machine Translation", "abstract": "<p>This paper describes the Microsoft Translator submissions to the WMT19 news\ntranslation shared task for English-German. Our main focus is document-level\nneural machine translation with deep transformer models. We start with strong\nsentence-level baselines, trained on large-scale data created via\ndata-filtering and noisy back-translation and find that back-translation seems\nto mainly help with translationese input. We explore fine-tuning techniques,\ndeeper models and different ensembling strategies to counter these effects.\nUsing document boundaries present in the authentic and synthetic parallel data,\nwe create sequences of up to 1000 subword segments and train transformer\ntranslation models. We experiment with data augmentation techniques for the\nsmaller authentic data with document-boundaries and for larger authentic data\nwithout boundaries. We further explore multi-task training for the\nincorporation of document-level source language monolingual data via the\nBERT-objective on the encoder and two-pass decoding for combinations of\nsentence-level and document-level systems. Based on preliminary human\nevaluation results, evaluators strongly prefer the document-level systems over\nour comparable sentence-level system. The document-level systems also seem to\nscore higher than the human references in source-based direct assessment.</p>\n", "tags": ["Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "junjie2023comprehensive", "citations": "153", "year": "2023", "title":"A Comprehensive Capability Analysis Of GPT-3 And GPT-3.5 Series Models", "abstract": "<p>GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,\nhave gained considerable attention due to their exceptional natural language\nprocessing capabilities. However, despite the abundance of research on the\ndifference in capabilities between GPT series models and fine-tuned models,\nthere has been limited attention given to the evolution of GPT series models’\ncapabilities over time. To conduct a comprehensive analysis of the capabilities\nof GPT series models, we select six representative models, comprising two GPT-3\nseries models (i.e., davinci and text-davinci-001) and four GPT-3.5 series\nmodels (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and\ngpt-3.5-turbo). We evaluate their performance on nine natural language\nunderstanding (NLU) tasks using 21 datasets. In particular, we compare the\nperformance and robustness of different models for each task under zero-shot\nand few-shot scenarios. Our extensive experiments reveal that the overall\nability of GPT series models on NLU tasks does not increase gradually as the\nmodels evolve, especially with the introduction of the RLHF training strategy.\nWhile this strategy enhances the models’ ability to generate human-like\nresponses, it also compromises their ability to solve some tasks. Furthermore,\nour findings indicate that there is still room for improvement in areas such as\nmodel robustness.</p>\n", "tags": ["Datasets","Few-Shot","Model Architecture","Security","Training Techniques"] },
{"key": "junnan2023blip", "citations": "630", "year": "2023", "title":"BLIP-2: Bootstrapping Language-image Pre-training With Frozen Image Encoders And Large Language Models", "abstract": "<p>The cost of vision-and-language pre-training has become increasingly\nprohibitive due to end-to-end training of large-scale models. This paper\nproposes BLIP-2, a generic and efficient pre-training strategy that bootstraps\nvision-language pre-training from off-the-shelf frozen pre-trained image\nencoders and frozen large language models. BLIP-2 bridges the modality gap with\na lightweight Querying Transformer, which is pre-trained in two stages. The\nfirst stage bootstraps vision-language representation learning from a frozen\nimage encoder. The second stage bootstraps vision-to-language generative\nlearning from a frozen language model. BLIP-2 achieves state-of-the-art\nperformance on various vision-language tasks, despite having significantly\nfewer trainable parameters than existing methods. For example, our model\noutperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable\nparameters. We also demonstrate the model’s emerging capabilities of zero-shot\nimage-to-text generation that can follow natural language instructions.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "juraska2018deep", "citations": "79", "year": "2018", "title":"A Deep Ensemble Model With Slot Alignment For Sequence-to-sequence Natural Language Generation", "abstract": "<p>Natural language generation lies at the core of generative dialogue systems\nand conversational agents. We describe an ensemble neural language generator,\nand present several novel methods for data representation and augmentation that\nyield improved results in our model. We test the model on three datasets in the\nrestaurant, TV and laptop domains, and report both objective and subjective\nevaluations of our best model. Using a range of automatic metrics, as well as\nhuman evaluators, we show that our approach achieves better results than\nstate-of-the-art models on the same datasets.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","NAACL"] },
{"key": "kadavath2022language", "citations": "126", "year": "2022", "title":"Language Models (mostly) Know What They Know", "abstract": "<p>We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability “P(True)” that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict “P(IK)”, the probability\nthat “I know” the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "kaddour2023challenges", "citations": "121", "year": "2023", "title":"Challenges And Applications Of Large Language Models", "abstract": "<p>Large Language Models (LLMs) went from non-existent to ubiquitous in the\nmachine learning discourse within a few years. Due to the fast pace of the\nfield, it is difficult to identify the remaining challenges and already\nfruitful application areas. In this paper, we aim to establish a systematic set\nof open problems and application successes so that ML researchers can\ncomprehend the field’s current state more quickly and become productive.</p>\n", "tags": ["Applications"] },
{"key": "kadlec2016text", "citations": "313", "year": "2016", "title":"Text Understanding With The Attention Sum Reader Network", "abstract": "<p>Several large cloze-style context-question-answer datasets have been\nintroduced recently: the CNN and Daily Mail news data and the Children’s Book\nTest. Thanks to the size of these datasets, the associated text comprehension\ntask is well suited for deep-learning techniques that currently seem to\noutperform all alternative approaches. We present a new, simple model that uses\nattention to directly pick the answer from the context as opposed to computing\nthe answer using a blended representation of words in the document as is usual\nin similar models. This makes the model particularly suitable for\nquestion-answering problems where the answer is a single word from the\ndocument. Ensemble of our models sets new state of the art on all evaluated\ndatasets.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "kafle2017analysis", "citations": "237", "year": "2017", "title":"An Analysis Of Visual Question Answering Algorithms", "abstract": "<p>In visual question answering (VQA), an algorithm must answer text-based\nquestions about images. While multiple datasets for VQA have been created since\nlate 2014, they all have flaws in both their content and the way algorithms are\nevaluated on them. As a result, evaluation scores are inflated and\npredominantly determined by answering easier questions, making it difficult to\ncompare different methods. In this paper, we analyze existing VQA algorithms\nusing a new dataset. It contains over 1.6 million questions organized into 12\ndifferent categories. We also introduce questions that are meaningless for a\ngiven image to force a VQA system to reason about image content. We propose new\nevaluation schemes that compensate for over-represented question-types and make\nit easier to study the strengths and weaknesses of algorithms. We analyze the\nperformance of both baseline and state-of-the-art VQA models, including\nmulti-modal compact bilinear pooling (MCB), neural module networks, and\nrecurrent answering units. Our experiments establish how attention helps\ncertain categories more than others, determine which models work better than\nothers, and explain how simple models (e.g. MLP) can surpass more complex\nmodels (MCB) by simply learning to answer large, easy question categories.</p>\n", "tags": ["Datasets","Evaluation","ICCV","Model Architecture"] },
{"key": "kaiser2017learning", "citations": "256", "year": "2017", "title":"Learning To Remember Rare Events", "abstract": "<p>Despite recent advances, memory-augmented deep neural networks are still\nlimited when it comes to life-long and one-shot learning, especially in\nremembering rare events. We present a large-scale life-long memory module for\nuse in deep learning. The module exploits fast nearest-neighbor algorithms for\nefficiency and thus scales to large memory sizes. Except for the\nnearest-neighbor query, the module is fully differentiable and trained\nend-to-end with no extra supervision. It operates in a life-long manner, i.e.,\nwithout the need to reset it during training.\n  Our memory module can be easily added to any part of a supervised neural\nnetwork. To show its versatility we add it to a number of networks, from simple\nconvolutional ones tested on image classification to deep sequence-to-sequence\nand recurrent-convolutional models. In all cases, the enhanced network gains\nthe ability to remember and do life-long one-shot learning. Our module\nremembers training examples shown many thousands of steps in the past and it\ncan successfully generalize from them. We set new state-of-the-art for one-shot\nlearning on the Omniglot dataset and demonstrate, for the first time, life-long\none-shot learning in recurrent neural networks on a large-scale machine\ntranslation task.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "kalamkar2019study", "citations": "125", "year": "2019", "title":"A Study Of BFLOAT16 For Deep Learning Training", "abstract": "<p>This paper presents the first comprehensive empirical study demonstrating the\nefficacy of the Brain Floating Point (BFLOAT16) half-precision format for Deep\nLearning training across image classification, speech recognition, language\nmodeling, generative networks and industrial recommendation systems. BFLOAT16\nis attractive for Deep Learning training for two reasons: the range of values\nit can represent is the same as that of IEEE 754 floating-point format (FP32)\nand conversion to/from FP32 is simple. Maintaining the same range as FP32 is\nimportant to ensure that no hyper-parameter tuning is required for convergence;\ne.g., IEEE 754 compliant half-precision floating point (FP16) requires\nhyper-parameter tuning. In this paper, we discuss the flow of tensors and\nvarious key operations in mixed precision training, and delve into details of\noperations, such as the rounding modes for converting FP32 tensors to BFLOAT16.\nWe have implemented a method to emulate BFLOAT16 operations in Tensorflow,\nCaffe2, IntelCaffe, and Neon for our experiments. Our results show that deep\nlearning training using BFLOAT16 tensors achieves the same state-of-the-art\n(SOTA) results across domains as FP32 tensors in the same number of iterations\nand with no changes to hyper-parameters.</p>\n", "tags": ["Training Techniques"] },
{"key": "kalchbrenner2016neural", "citations": "341", "year": "2016", "title":"Neural Machine Translation In Linear Time", "abstract": "<p>We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens.</p>\n", "tags": [] },
{"key": "kale2020text", "citations": "121", "year": "2020", "title":"Text-to-text Pre-training For Data-to-text Tasks", "abstract": "<p>We study the pre-train + fine-tune strategy for data-to-text tasks. Our\nexperiments indicate that text-to-text pre-training in the form of T5, enables\nsimple, end-to-end transformer based models to outperform pipelined neural\narchitectures tailored for data-to-text generation, as well as alternative\nlanguage model based pre-training techniques such as BERT and GPT-2.\nImportantly, T5 pre-training leads to better generalization, as evidenced by\nlarge improvements on out-of-domain test sets. We hope our work serves as a\nuseful baseline for future research, as transfer learning becomes ever more\nprevalent for data-to-text tasks.</p>\n", "tags": ["Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "kalyan2021ammu", "citations": "211", "year": "2021", "title":"AMMU : A Survey Of Transformer-based Biomedical Pretrained Language Models", "abstract": "<p>Transformer-based pretrained language models (PLMs) have started a new era in\nmodern natural language processing (NLP). These models combine the power of\ntransformers, transfer learning, and self-supervised learning (SSL). Following\nthe success of these models in the general domain, the biomedical research\ncommunity has developed various in-domain PLMs starting from BioBERT to the\nlatest BioELECTRA and BioALBERT models. We strongly believe there is a need for\na survey paper that can provide a comprehensive survey of various\ntransformer-based biomedical pretrained language models (BPLMs). In this\nsurvey, we start with a brief overview of foundational concepts like\nself-supervised learning, embedding layer and transformer encoder layers. We\ndiscuss core concepts of transformer-based PLMs like pretraining methods,\npretraining tasks, fine-tuning methods, and various embedding types specific to\nbiomedical domain. We introduce a taxonomy for transformer-based BPLMs and then\ndiscuss all the models. We discuss various challenges and present possible\nsolutions. We conclude by highlighting some of the open issues which will drive\nthe research community to further improve transformer-based BPLMs.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "kalyan2021ammus", "citations": "137", "year": "2021", "title":"AMMUS : A Survey Of Transformer-based Pretrained Models In Natural Language Processing", "abstract": "<p>Transformer-based pretrained language models (T-PTLMs) have achieved great\nsuccess in almost every NLP task. The evolution of these models started with\nGPT and BERT. These models are built on the top of transformers,\nself-supervised learning and transfer learning. Transformed-based PTLMs learn\nuniversal language representations from large volumes of text data using\nself-supervised learning and transfer this knowledge to downstream tasks. These\nmodels provide good background knowledge to downstream tasks which avoids\ntraining of downstream models from scratch. In this comprehensive survey paper,\nwe initially give a brief overview of self-supervised learning. Next, we\nexplain various core concepts like pretraining, pretraining methods,\npretraining tasks, embeddings and downstream adaptation methods. Next, we\npresent a new taxonomy of T-PTLMs and then give brief overview of various\nbenchmarks including both intrinsic and extrinsic. We present a summary of\nvarious useful libraries to work with T-PTLMs. Finally, we highlight some of\nthe future research directions which will further improve these models. We\nstrongly believe that this comprehensive survey paper will serve as a good\nreference to learn the core concepts as well as to stay updated with the recent\nhappenings in T-PTLMs.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "kalyan2023survey", "citations": "141", "year": "2023", "title":"A Survey Of GPT-3 Family Large Language Models Including Chatgpt And GPT-4", "abstract": "<p>Large language models (LLMs) are a special class of pretrained language\nmodels obtained by scaling model size, pretraining corpus and computation.\nLLMs, because of their large size and pretraining on large volumes of text\ndata, exhibit special abilities which allow them to achieve remarkable\nperformances without any task-specific training in many of the natural language\nprocessing tasks. The era of LLMs started with OpenAI GPT-3 model, and the\npopularity of LLMs is increasing exponentially after the introduction of models\nlike ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models,\nincluding ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With\nthe ever-rising popularity of GLLMs, especially in the research community,\nthere is a strong need for a comprehensive survey which summarizes the recent\nresearch progress in multiple dimensions and can guide the research community\nwith insightful future research directions. We start the survey paper with\nfoundation concepts like transformers, transfer learning, self-supervised\nlearning, pretrained language models and large language models. We then present\na brief overview of GLLMs and discuss the performances of GLLMs in various\ndownstream tasks, specific domains and multiple languages. We also discuss the\ndata labelling and data augmentation abilities of GLLMs, the robustness of\nGLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with\nmultiple insightful future research directions. To summarize, this\ncomprehensive survey paper will serve as a good resource for both academic and\nindustry people to stay updated with the latest research related to GPT-3\nfamily large language models.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Security","Survey Paper","Training Techniques"] },
{"key": "kamath2020selective", "citations": "116", "year": "2020", "title":"Selective Question Answering Under Domain Shift", "abstract": "<p>To avoid giving wrong answers, question answering (QA) models need to know\nwhen to abstain from answering. Moreover, users often ask questions that\ndiverge from the model’s training data, making errors more likely and thus\nabstention more critical. In this work, we propose the setting of selective\nquestion answering under domain shift, in which a QA model is tested on a\nmixture of in-domain and out-of-domain data, and must answer (i.e., not abstain\non) as many questions as possible while maintaining high accuracy. Abstention\npolicies based solely on the model’s softmax probabilities fare poorly, since\nmodels are overconfident on out-of-domain inputs. Instead, we train a\ncalibrator to identify inputs on which the QA model errs, and abstain when it\npredicts an error is likely. Crucially, the calibrator benefits from observing\nthe model’s behavior on out-of-domain data, even if from a different domain\nthan the test data. We combine this method with a SQuAD-trained QA model and\nevaluate on mixtures of SQuAD and five other QA datasets. Our method answers\n56% of questions while maintaining 80% accuracy; in contrast, directly using\nthe model’s probabilities only answers 48% at 80% accuracy.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "kamath2021mdetr", "citations": "476", "year": "2021", "title":"MDETR -- Modulated Detection For End-to-end Multi-modal Understanding", "abstract": "<p>Multi-modal reasoning systems rely on a pre-trained object detector to\nextract regions of interest from the image. However, this crucial module is\ntypically used as a black box, trained independently of the downstream task and\non a fixed vocabulary of objects and attributes. This makes it challenging for\nsuch systems to capture the long tail of visual concepts expressed in free form\ntext. In this paper we propose MDETR, an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or\na question. We use a transformer-based architecture to reason jointly over text\nand image by fusing the two modalities at an early stage of the model. We\npre-train the network on 1.3M text-image pairs, mined from pre-existing\nmulti-modal datasets having explicit alignment between phrases in text and\nobjects in the image. We then fine-tune on several downstream tasks such as\nphrase grounding, referring expression comprehension and segmentation,\nachieving state-of-the-art results on popular benchmarks. We also investigate\nthe utility of our model as an object detector on a given label set when\nfine-tuned in a few-shot setting. We show that our pre-training approach\nprovides a way to handle the long tail of object categories which have very few\nlabelled instances. Our approach can be easily extended for visual question\nanswering, achieving competitive performance on GQA and CLEVR. The code and\nmodels are available at https://github.com/ashkamath/mdetr.</p>\n", "tags": ["Datasets","Few-Shot","Has Code","ICCV","Model Architecture","Training Techniques"] },
{"key": "kanade2019learning", "citations": "142", "year": "2020", "title":"Learning And Evaluating Contextual Embedding Of Source Code", "abstract": "<p>Recent research has achieved impressive results on understanding and\nimproving source code by building up on machine-learning techniques developed\nfor natural languages. A significant advancement in natural-language\nunderstanding has come with the development of pre-trained contextual\nembeddings, such as BERT, which can be fine-tuned for downstream tasks with\nless labeled data and training budget, while achieving better accuracies.\nHowever, there is no attempt yet to obtain a high-quality contextual embedding\nof source code, and to evaluate it on multiple program-understanding tasks\nsimultaneously; that is the gap that this paper aims to mitigate. Specifically,\nfirst, we curate a massive, deduplicated corpus of 7.4M Python files from\nGitHub, which we use to pre-train CuBERT, an open-sourced code-understanding\nBERT model; and, second, we create an open-sourced benchmark that comprises\nfive classification tasks and one program-repair task, akin to\ncode-understanding tasks proposed in the literature before. We fine-tune CuBERT\non our benchmark tasks, and compare the resulting models to different variants\nof Word2Vec token embeddings, BiLSTM and Transformer models, as well as\npublished state-of-the-art models, showing that CuBERT outperforms them all,\neven with shorter training, and with fewer labeled examples. Future work on\nsource-code embedding can benefit from reusing our benchmark, and from\ncomparing against CuBERT models as a strong baseline.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "kaneko2021debiasing", "citations": "82", "year": "2021", "title":"Debiasing Pre-trained Contextualised Embeddings", "abstract": "<p>In comparison to the numerous debiasing methods proposed for the static\nnon-contextualised word embeddings, the discriminative biases in contextualised\nembeddings have received relatively little attention. We propose a fine-tuning\nmethod that can be applied at token- or sentence-levels to debias pre-trained\ncontextualised embeddings. Our proposed method can be applied to any\npre-trained contextualised embedding model, without requiring to retrain those\nmodels. Using gender bias as an illustrative example, we then conduct a\nsystematic study using several state-of-the-art (SoTA) contextualised\nrepresentations on multiple benchmark datasets to evaluate the level of biases\nencoded in different contextualised embeddings before and after debiasing using\nthe proposed method. We find that applying token-level debiasing for all tokens\nand across all layers of a contextualised embedding model produces the best\nperformance. Interestingly, we observe that there is a trade-off between\ncreating an accurate vs. unbiased contextualised embedding model, and different\ncontextualised embedding models respond differently to this trade-off.</p>\n", "tags": ["Datasets","EACL","Ethics & Fairness","Evaluation","Fine-Tuning","Model Architecture","NAACL","Training Techniques"] },
{"key": "kang2018adventure", "citations": "69", "year": "2018", "title":"Adventure: Adversarial Training For Textual Entailment With Knowledge-guided Examples", "abstract": "<p>We consider the problem of learning textual entailment models with limited\nsupervision (5K-10K training examples), and present two complementary\napproaches for it. First, we propose knowledge-guided adversarial example\ngenerators for incorporating large lexical resources in entailment models via\nonly a handful of rule templates. Second, to make the entailment model - a\ndiscriminator - more robust, we propose the first GAN-style approach for\ntraining it using a natural language example generator that iteratively adjusts\nbased on the discriminator’s performance. We demonstrate effectiveness using\ntwo entailment datasets, where the proposed methods increase accuracy by 4.7%\non SciTail and by 2.8% on a 1% training sub-sample of SNLI. Notably, even a\nsingle hand-written rule, negate, improves the accuracy on the negation\nexamples in SNLI by 6.1%.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "kang2018self", "citations": "1981", "year": "2018", "title":"Self-attentive Sequential Recommendation", "abstract": "<p>Sequential dynamics are a key feature of many modern recommender systems,\nwhich seek to capture the <code class=\"language-plaintext highlighter-rouge\">context' of users' activities on the basis of\nactions they have performed recently. To capture such patterns, two approaches\nhave proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs).\nMarkov Chains assume that a user's next action can be predicted on the basis of\njust their last (or last few) actions, while RNNs in principle allow for\nlonger-term semantics to be uncovered. Generally speaking, MC-based methods\nperform best in extremely sparse datasets, where model parsimony is critical,\nwhile RNNs perform better in denser datasets where higher model complexity is\naffordable. The goal of our work is to balance these two goals, by proposing a\nself-attention based sequential model (SASRec) that allows us to capture\nlong-term semantics (like an RNN), but, using an attention mechanism, makes its\npredictions based on relatively few actions (like an MC). At each time step,\nSASRec seeks to identify which items are </code>relevant’ from a user’s action\nhistory, and use them to predict the next item. Extensive empirical studies\nshow that our method outperforms various state-of-the-art sequential models\n(including MC/CNN/RNN-based approaches) on both sparse and dense datasets.\nMoreover, the model is an order of magnitude more efficient than comparable\nCNN/RNN-based models. Visualizations on attention weights also show how our\nmodel adaptively handles datasets with various density, and uncovers meaningful\npatterns in activity sequences.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "kang2019dual", "citations": "88", "year": "2019", "title":"Dual Attention Networks For Visual Reference Resolution In Visual Dialog", "abstract": "<p>Visual dialog (VisDial) is a task which requires an AI agent to answer a\nseries of questions grounded in an image. Unlike in visual question answering\n(VQA), the series of questions should be able to capture a temporal context\nfrom a dialog history and exploit visually-grounded information. A problem\ncalled visual reference resolution involves these challenges, requiring the\nagent to resolve ambiguous references in a given question and find the\nreferences in a given image. In this paper, we propose Dual Attention Networks\n(DAN) for visual reference resolution. DAN consists of two kinds of attention\nnetworks, REFER and FIND. Specifically, REFER module learns latent\nrelationships between a given question and a dialog history by employing a\nself-attention mechanism. FIND module takes image features and reference-aware\nrepresentations (i.e., the output of REFER module) as input, and performs\nvisual grounding via bottom-up attention mechanism. We qualitatively and\nquantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showing\nthat DAN outperforms the previous state-of-the-art model by a significant\nmargin.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "kang2019recommendation", "citations": "70", "year": "2019", "title":"Recommendation As A Communication Game: Self-supervised Bot-play For Goal-oriented Dialogue", "abstract": "<p>Traditional recommendation systems produce static rather than interactive\nrecommendations invariant to a user’s specific requests, clarifications, or\ncurrent mood, and can suffer from the cold-start problem if their tastes are\nunknown. These issues can be alleviated by treating recommendation as an\ninteractive dialogue task instead, where an expert recommender can sequentially\nask about someone’s preferences, react to their requests, and recommend more\nappropriate items. In this work, we collect a goal-driven recommendation\ndialogue dataset (GoRecDial), which consists of 9,125 dialogue games and 81,260\nconversation turns between pairs of human workers recommending movies to each\nother. The task is specifically designed as a cooperative game between two\nplayers working towards a quantifiable common goal. We leverage the dataset to\ndevelop an end-to-end dialogue system that can simultaneously converse and\nrecommend. Models are first trained to imitate the behavior of human players\nwithout considering the task goal itself (supervised training). We then\nfinetune our models on simulated bot-bot conversations between two paired\npre-trained models (bot-play), in order to achieve the dialogue goal. Our\nexperiments show that models finetuned with bot-play learn improved dialogue\nstrategies, reach the dialogue goal more often when paired with a human, and\nare rated as more consistent by humans compared to models trained without\nbot-play. The dataset and code are publicly available through the ParlAI\nframework.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","EMNLP","Tools","Training Techniques"] },
{"key": "kang2020dynamic", "citations": "61", "year": "2020", "title":"Dynamic Context Selection For Document-level Neural Machine Translation Via Reinforcement Learning", "abstract": "<p>Document-level neural machine translation has yielded attractive\nimprovements. However, majority of existing methods roughly use all context\nsentences in a fixed scope. They neglect the fact that different source\nsentences need different sizes of context. To address this problem, we propose\nan effective approach to select dynamic context so that the document-level\ntranslation model can utilize the more useful selected context sentences to\nproduce better translations. Specifically, we introduce a selection module that\nis independent of the translation module to score each candidate context\nsentence. Then, we propose two strategies to explicitly select a variable\nnumber of context sentences and feed them into the translation module. We train\nthe two modules end-to-end via reinforcement learning. A novel reward is\nproposed to encourage the selection and utilization of dynamic context\nsentences. Experiments demonstrate that our approach can select adaptive\ncontext sentences for different source sentences, and significantly improves\nthe performance of document-level translation methods.</p>\n", "tags": ["Agentic","EMNLP","Reinforcement Learning"] },
{"key": "kang2020improved", "citations": "76", "year": "2020", "title":"Improved Natural Language Generation Via Loss Truncation", "abstract": "<p>Neural language models are usually trained to match the distributional\nproperties of a large-scale corpus by minimizing the log loss. While\nstraightforward to optimize, this approach forces the model to reproduce all\nvariations in the dataset, including noisy and invalid references (e.g.,\nmisannotation and hallucinated facts). Worse, the commonly used log loss is\noverly sensitive to such phenomena and even a small fraction of noisy data can\ndegrade performance. In this work, we show that the distinguishability of the\nmodels and reference serves as a principled and robust alternative for handling\ninvalid references. To optimize distinguishability, we propose loss truncation,\nwhich adaptively removes high loss examples during training. We show this is as\neasy to optimize as log loss and tightly bounds distinguishability under noise.\nEmpirically, we demonstrate that loss truncation outperforms existing baselines\non distinguishability on a summarization task, and show that samples generated\nby the loss truncation model have factual accuracy ratings that exceed those of\nbaselines and match human references.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "kang2022large", "citations": "85", "year": "2023", "title":"Large Language Models Are Few-shot Testers: Exploring Llm-based General Bug Reproduction", "abstract": "<p>Many automated test generation techniques have been developed to aid\ndevelopers with writing tests. To facilitate full automation, most existing\ntechniques aim to either increase coverage, or generate exploratory inputs.\nHowever, existing test generation techniques largely fall short of achieving\nmore semantic objectives, such as generating tests to reproduce a given bug\nreport. Reproducing bugs is nonetheless important, as our empirical study shows\nthat the number of tests added in open source repositories due to issues was\nabout 28% of the corresponding project test suite size. Meanwhile, due to the\ndifficulties of transforming the expected program semantics in bug reports into\ntest oracles, existing failure reproduction techniques tend to deal exclusively\nwith program crashes, a small subset of all bug reports. To automate test\ngeneration from general bug reports, we propose LIBRO, a framework that uses\nLarge Language Models (LLMs), which have been shown to be capable of performing\ncode-related tasks. Since LLMs themselves cannot execute the target buggy code,\nwe focus on post-processing steps that help us discern when LLMs are effective,\nand rank the produced tests according to their validity. Our evaluation of\nLIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate\nfailure reproducing test cases for 33% of all studied cases (251 out of 750),\nwhile suggesting a bug reproducing test in first place for 149 bugs. To\nmitigate data contamination, we also evaluate LIBRO against 31 bug reports\nsubmitted after the collection of the LLM training data terminated: LIBRO\nproduces bug reproducing tests for 32% of the studied bug reports. Overall, our\nresults show LIBRO has the potential to significantly enhance developer\nefficiency by automatically generating tests from bug reports.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Llm For Code","Tools","Training Techniques"] },
{"key": "kang2023scaling", "citations": "235", "year": "2023", "title":"Scaling Up Gans For Text-to-image Synthesis", "abstract": "<p>The recent success of text-to-image synthesis has taken the world by storm\nand captured the general public’s imagination. From a technical standpoint, it\nalso marked a drastic change in the favored architecture to design generative\nimage models. GANs used to be the de facto choice, with techniques like\nStyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new\nstandard for large-scale generative models overnight. This rapid shift raises a\nfundamental question: can we scale up GANs to benefit from large datasets like\nLAION? We find that na\"Ively increasing the capacity of the StyleGAN\narchitecture quickly becomes unstable. We introduce GigaGAN, a new GAN\narchitecture that far exceeds this limit, demonstrating GANs as a viable option\nfor text-to-image synthesis. GigaGAN offers three major advantages. First, it\nis orders of magnitude faster at inference time, taking only 0.13 seconds to\nsynthesize a 512px image. Second, it can synthesize high-resolution images, for\nexample, 16-megapixel pixels in 3.66 seconds. Finally, GigaGAN supports various\nlatent space editing applications such as latent interpolation, style mixing,\nand vector arithmetic operations.</p>\n", "tags": ["Applications","CVPR","Datasets","Model Architecture"] },
{"key": "kannan2016smart", "citations": "152", "year": "2016", "title":"Smart Reply: Automated Response Suggestion For Email", "abstract": "<p>In this paper we propose and investigate a novel end-to-end method for\nautomatically generating short email responses, called Smart Reply. It\ngenerates semantically diverse suggestions that can be used as complete email\nresponses with just one tap on mobile. The system is currently used in Inbox by\nGmail and is responsible for assisting with 10% of all mobile responses. It is\ndesigned to work at very high throughput and process hundreds of millions of\nmessages daily. The system exploits state-of-the-art, large-scale deep\nlearning.\n  We describe the architecture of the system as well as the challenges that we\nfaced while building it, like response diversity and scalability. We also\nintroduce a new method for semantic clustering of user-generated content that\nrequires only a modest amount of explicitly labeled data.</p>\n", "tags": ["Model Architecture"] },
{"key": "kannan2017adversarial", "citations": "72", "year": "2016", "title":"Adversarial Evaluation Of Dialogue Models", "abstract": "<p>The recent application of RNN encoder-decoder models has resulted in\nsubstantial progress in fully data-driven dialogue systems, but evaluation\nremains a challenge. An adversarial loss could be a way to directly evaluate\nthe extent to which generated dialogue responses sound like they came from a\nhuman. This could reduce the need for human evaluation, while more directly\nevaluating on a generative task. In this work, we investigate this idea by\ntraining an RNN to discriminate a dialogue model’s samples from human-generated\nsamples. Although we find some evidence this setup could be viable, we also\nnote that many issues remain in its practical application. We discuss both\naspects and conclude that future work is warranted.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","Security","Training Techniques"] },
{"key": "kannan2017analysis", "citations": "254", "year": "2018", "title":"An Analysis Of Incorporating An External Language Model Into A Sequence-to-sequence Model", "abstract": "<p>Attention-based sequence-to-sequence models for automatic speech recognition\njointly train an acoustic model, language model, and alignment mechanism. Thus,\nthe language model component is only trained on transcribed audio-text pairs.\nThis leads to the use of shallow fusion with an external language model at\ninference time. Shallow fusion refers to log-linear interpolation with a\nseparately trained language model at each step of the beam search. In this\nwork, we investigate the behavior of shallow fusion across a range of\nconditions: different types of language models, different decoding units, and\ndifferent tasks. On Google Voice Search, we demonstrate that the use of shallow\nfusion with a neural LM with wordpieces yields a 9.1% relative word error rate\nreduction (WERR) over our competitive attention-based sequence-to-sequence\nmodel, obviating the need for second-pass rescoring.</p>\n", "tags": ["ICASSP","Model Architecture"] },
{"key": "kannan2019large", "citations": "151", "year": "2019", "title":"Large-scale Multilingual Speech Recognition With A Streaming End-to-end Model", "abstract": "<p>Multilingual end-to-end (E2E) models have shown great promise in expansion of\nautomatic speech recognition (ASR) coverage of the world’s languages. They have\nshown improvement over monolingual systems, and have simplified training and\nserving by eliminating language-specific acoustic, pronunciation, and language\nmodels. This work presents an E2E multilingual system which is equipped to\noperate in low-latency interactive applications, as well as handle a key\nchallenge of real world data: the imbalance in training data across languages.\nUsing nine Indic languages, we compare a variety of techniques, and find that a\ncombination of conditioning on a language vector and training language-specific\nadapter layers produces the best model. The resulting E2E multilingual model\nachieves a lower word error rate (WER) than both monolingual E2E models (eight\nof nine languages) and monolingual conventional systems (all nine languages).</p>\n", "tags": ["Applications","INTERSPEECH","Training Techniques"] },
{"key": "kant2020spatially", "citations": "79", "year": "2020", "title":"Spatially Aware Multimodal Transformers For Textvqa", "abstract": "<p>Textual cues are essential for everyday tasks like buying groceries and using\npublic transport. To develop this assistive technology, we study the TextVQA\ntask, i.e., reasoning about text in images to answer a question. Existing\napproaches are limited in their use of spatial relations and rely on\nfully-connected transformer-like architectures to implicitly learn the spatial\nstructure of a scene. In contrast, we propose a novel spatially aware\nself-attention layer such that each visual entity only looks at neighboring\nentities defined by a spatial graph. Further, each head in our multi-head\nself-attention layer focuses on a different subset of relations. Our approach\nhas two advantages: (1) each head considers local context instead of dispersing\nthe attention amongst all visual entities; (2) we avoid learning redundant\nfeatures. We show that our model improves the absolute accuracy of current\nstate-of-the-art methods on TextVQA by 2.2% overall over an improved baseline,\nand 4.62% on questions that involve spatial reasoning and can be answered\ncorrectly using OCR tokens. Similarly on ST-VQA, we improve the absolute\naccuracy by 4.2%. We further show that spatially aware self-attention improves\nvisual grounding.</p>\n", "tags": ["Model Architecture"] },
{"key": "kapanipathi2020leveraging", "citations": "74", "year": "2021", "title":"Leveraging Abstract Meaning Representation For Knowledge Base Question Answering", "abstract": "<p>Knowledge base question answering (KBQA)is an important task in Natural\nLanguage Processing. Existing approaches face significant challenges including\ncomplex question understanding, necessity for reasoning, and lack of large\nend-to-end training datasets. In this work, we propose Neuro-Symbolic Question\nAnswering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning\nRepresentation (AMR) parses for task-independent question understanding; (2) a\nsimple yet effective graph transformation approach to convert AMR parses into\ncandidate logical queries that are aligned to the KB; (3) a pipeline-based\napproach which integrates multiple, reusable modules that are trained\nspecifically for their individual tasks (semantic parser, entity\nandrelationship linkers, and neuro-symbolic reasoner) and do not require\nend-to-end training data. NSQA achieves state-of-the-art performance on two\nprominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore,\nour analysis emphasizes that AMR is a powerful tool for KBQA systems.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "kar2019meta", "citations": "213", "year": "2019", "title":"Meta-sim: Learning To Generate Synthetic Datasets", "abstract": "<p>Training models to high-end performance requires availability of large\nlabeled datasets, which are expensive to get. The goal of our work is to\nautomatically synthesize labeled datasets that are relevant for a downstream\ntask. We propose Meta-Sim, which learns a generative model of synthetic scenes,\nand obtain images as well as its corresponding ground-truth via a graphics\nengine. We parametrize our dataset generator with a neural network, which\nlearns to modify attributes of scene graphs obtained from probabilistic scene\ngrammars, so as to minimize the distribution gap between its rendered outputs\nand target data. If the real dataset comes with a small labeled validation set,\nwe additionally aim to optimize a meta-objective, i.e. downstream task\nperformance. Experiments show that the proposed method can greatly improve\ncontent generation quality over a human-engineered probabilistic scene grammar,\nboth qualitatively and quantitatively as measured by performance on a\ndownstream task.</p>\n", "tags": ["Datasets","ICCV","Training Techniques"] },
{"key": "karita2019comparative", "citations": "461", "year": "2019", "title":"A Comparative Study On Transformer Vs RNN In Speech Applications", "abstract": "<p>Sequence-to-sequence models have been widely used in end-to-end speech\nprocessing, for example, automatic speech recognition (ASR), speech translation\n(ST), and text-to-speech (TTS). This paper focuses on an emergent\nsequence-to-sequence model called Transformer, which achieves state-of-the-art\nperformance in neural machine translation and other natural language processing\napplications. We undertook intensive studies in which we experimentally\ncompared and analyzed Transformer and conventional recurrent neural networks\n(RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS\nbenchmarks. Our experiments revealed various training tips and significant\nperformance benefits obtained with Transformer for each task including the\nsurprising superiority of Transformer in 13/15 ASR benchmarks in comparison\nwith RNN. We are preparing to release Kaldi-style reproducible recipes using\nopen source and publicly available datasets for all the ASR, ST, and TTS tasks\nfor the community to succeed our exciting outcomes.</p>\n", "tags": ["ASRU","Applications","Datasets","Model Architecture","Training Techniques"] },
{"key": "karpukhin2019training", "citations": "99", "year": "2019", "title":"Training On Synthetic Noise Improves Robustness To Natural Noise In Machine Translation", "abstract": "<p>We consider the problem of making machine translation more robust to\ncharacter-level variation at the source side, such as typos. Existing methods\nachieve greater coverage by applying subword models such as byte-pair encoding\n(BPE) and character-level encoders, but these methods are highly sensitive to\nspelling mistakes. We show how training on a mild amount of random synthetic\nnoise can dramatically improve robustness to these variations, without\ndiminishing performance on clean text. We focus on translation performance on\nnatural noise, as captured by frequent corrections in Wikipedia edit logs, and\nshow that robustness to such noise can be achieved using a balanced diet of\nsimple synthetic noises at training time, without access to the natural noise\ndata or distribution.</p>\n", "tags": ["Training Techniques"] },
{"key": "karpukhin2020dense", "citations": "1965", "year": "2020", "title":"Dense Passage Retrieval For Open-domain Question Answering", "abstract": "<p>Open-domain question answering relies on efficient passage retrieval to\nselect candidate contexts, where traditional sparse vector space models, such\nas TF-IDF or BM25, are the de facto method. In this work, we show that\nretrieval can be practically implemented using dense representations alone,\nwhere embeddings are learned from a small number of questions and passages by a\nsimple dual-encoder framework. When evaluated on a wide range of open-domain QA\ndatasets, our dense retriever outperforms a strong Lucene-BM25 system largely\nby 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our\nend-to-end QA system establish new state-of-the-art on multiple open-domain QA\nbenchmarks.</p>\n", "tags": ["Datasets","EMNLP","Retrieval Systems","Tools"] },
{"key": "kasirzadeh2022conversation", "citations": "73", "year": "2023", "title":"In Conversation With Artificial Intelligence: Aligning Language Models With Human Values", "abstract": "<p>Large-scale language technologies are increasingly used in various forms of\ncommunication with humans across different contexts. One particular use case\nfor these technologies is conversational agents, which output natural language\ntext in response to prompts and queries. This mode of engagement raises a\nnumber of social and ethical questions. For example, what does it mean to align\nconversational agents with human norms or values? Which norms or values should\nthey be aligned with? And how can this be accomplished? In this paper, we\npropose a number of steps that help answer these questions. We start by\ndeveloping a philosophical analysis of the building blocks of linguistic\ncommunication between conversational agents and human interlocutors. We then\nuse this analysis to identify and formulate ideal norms of conversation that\ncan govern successful linguistic communication between humans and\nconversational agents. Furthermore, we explore how these norms can be used to\nalign conversational agents with human values across a range of different\ndiscursive domains. We conclude by discussing the practical implications of our\nproposal for the design of conversational agents that are aligned with these\nnorms and values.</p>\n", "tags": [] },
{"key": "kassner2021multilingual", "citations": "73", "year": "2021", "title":"Multilingual LAMA: Investigating Knowledge In Multilingual Pretrained Language Models", "abstract": "<p>Recently, it has been found that monolingual English language models can be\nused as knowledge bases. Instead of structural knowledge base queries, masked\nsentences such as “Paris is the capital of [MASK]” are used as probes. We\ntranslate the established benchmarks TREx and GoogleRE into 53 languages.\nWorking with mBERT, we investigate three questions. (i) Can mBERT be used as a\nmultilingual knowledge base? Most prior work only considers English. Extending\nresearch to multiple languages is important for diversity and accessibility.\n(ii) Is mBERT’s performance as knowledge base language-independent or does it\nvary from language to language? (iii) A multilingual model is trained on more\ntext, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for\nbetter performance? We find that using mBERT as a knowledge base yields varying\nperformance across languages and pooling predictions across languages improves\nperformance. Conversely, mBERT exhibits a language bias; e.g., when queried in\nItalian, it tends to predict Italy as the country of origin.</p>\n", "tags": ["EACL","NAACL"] },
{"key": "kaushik2018how", "citations": "276", "year": "2018", "title":"How Much Reading Does Reading Comprehension Require? A Critical Investigation Of Popular Benchmarks", "abstract": "<p>Many recent papers address reading comprehension, where examples consist of\n(question, passage, answer) tuples. Presumably, a model must combine\ninformation from both questions and passages to predict corresponding answers.\nHowever, despite intense interest in the topic, with hundreds of published\npapers vying for leaderboard dominance, basic questions about the difficulty of\nmany popular benchmarks remain unanswered. In this paper, we establish sensible\nbaselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding\nthat question- and passage-only models often perform surprisingly well. On \\(14\\)\nout of \\(20\\) bAbI tasks, passage-only models achieve greater than \\(50%\\)\naccuracy, sometimes matching the full model. Interestingly, while CBT provides\n\\(20\\)-sentence stories only the last is needed for comparably accurate\nprediction. By comparison, SQuAD and CNN appear better-constructed.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "kawar2022imagic", "citations": "446", "year": "2023", "title":"Imagic: Text-based Real Image Editing With Diffusion Models", "abstract": "<p>Text-conditioned image editing has recently attracted considerable interest.\nHowever, most methods are currently either limited to specific editing types\n(e.g., object overlay, style transfer), or apply to synthetically generated\nimages, or require multiple input images of a common object. In this paper we\ndemonstrate, for the very first time, the ability to apply complex (e.g.,\nnon-rigid) text-guided semantic edits to a single real image. For example, we\ncan change the posture and composition of one or multiple objects inside an\nimage, while preserving its original characteristics. Our method can make a\nstanding dog sit down or jump, cause a bird to spread its wings, etc. – each\nwithin its single high-resolution natural image provided by the user. Contrary\nto previous work, our proposed method requires only a single input image and a\ntarget text (the desired edit). It operates on real images, and does not\nrequire any additional inputs (such as image masks or additional views of the\nobject). Our method, which we call “Imagic”, leverages a pre-trained\ntext-to-image diffusion model for this task. It produces a text embedding that\naligns with both the input image and the target text, while fine-tuning the\ndiffusion model to capture the image-specific appearance. We demonstrate the\nquality and versatility of our method on numerous inputs from various domains,\nshowcasing a plethora of high quality complex semantic image edits, all within\na single unified framework.</p>\n", "tags": ["CVPR","Fine-Tuning","Tools","Training Techniques"] },
{"key": "kazemitabaar2023studying", "citations": "175", "year": "2023", "title":"Studying The Effect Of AI Code Generators On Supporting Novice Learners In Introductory Programming", "abstract": "<p>AI code generators like OpenAI Codex have the potential to assist novice\nprogrammers by generating code from natural language descriptions, however,\nover-reliance might negatively impact learning and retention. To explore the\nimplications that AI code generators have on introductory programming, we\nconducted a controlled experiment with 69 novices (ages 10-17). Learners worked\non 45 Python code-authoring tasks, for which half of the learners had access to\nCodex, each followed by a code-modification task. Our results show that using\nCodex significantly increased code-authoring performance (1.15x increased\ncompletion rate and 1.8x higher scores) while not decreasing performance on\nmanual code-modification tasks. Additionally, learners with access to Codex\nduring the training phase performed slightly better on the evaluation\npost-tests conducted one week later, although this difference did not reach\nstatistical significance. Of interest, learners with higher Scratch pre-test\nscores performed significantly better on retention post-tests, if they had\nprior access to Codex.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "kazemitabaar2024codeaid", "citations": "73", "year": "2024", "title":"Codeaid: Evaluating A Classroom Deployment Of An Llm-based Programming Assistant That Balances Student And Educator Needs", "abstract": "<p>Timely, personalized feedback is essential for students learning programming.\nLLM-powered tools like ChatGPT offer instant support, but reveal direct answers\nwith code, which may hinder deep conceptual engagement. We developed CodeAid,\nan LLM-powered programming assistant delivering helpful, technically correct\nresponses, without revealing code solutions. CodeAid answers conceptual\nquestions, generates pseudo-code with line-by-line explanations, and annotates\nstudent’s incorrect code with fix suggestions. We deployed CodeAid in a\nprogramming class of 700 students for a 12-week semester. A thematic analysis\nof 8,000 usages of CodeAid was performed, further enriched by weekly surveys,\nand 22 student interviews. We then interviewed eight programming educators to\ngain further insights. Our findings reveal four design considerations for\nfuture educational AI assistants: D1) exploiting AI’s unique benefits; D2)\nsimplifying query formulation while promoting cognitive engagement; D3)\navoiding direct responses while encouraging motivated learning; and D4)\nmaintaining transparency and control for students to asses and steer AI\nresponses.</p>\n", "tags": ["Ethics & Fairness","Evaluation","Tools"] },
{"key": "ke2019reflective", "citations": "94", "year": "2019", "title":"Reflective Decoding Network For Image Captioning", "abstract": "<p>State-of-the-art image captioning methods mostly focus on improving visual\nfeatures, less attention has been paid to utilizing the inherent properties of\nlanguage to boost captioning performance. In this paper, we show that\nvocabulary coherence between words and syntactic paradigm of sentences are also\nimportant to generate high-quality image caption. Following the conventional\nencoder-decoder framework, we propose the Reflective Decoding Network (RDN) for\nimage captioning, which enhances both the long-sequence dependency and position\nperception of words in a caption decoder. Our model learns to collaboratively\nattend on both visual and textual features and meanwhile perceive each word’s\nrelative position in the sentence to maximize the information delivered in the\ngenerated caption. We evaluate the effectiveness of our RDN on the COCO image\ncaptioning datasets and achieve superior performance over the previous methods.\nFurther experiments reveal that our approach is particularly advantageous for\nhard cases with complex scenes to describe by captions.</p>\n", "tags": ["Datasets","ICCV","Model Architecture","Tools"] },
{"key": "ke2020rethinking", "citations": "131", "year": "2021", "title":"Rethinking Positional Encoding In Language Pre-training", "abstract": "<p>In this work, we investigate the positional encoding methods used in language\npre-training (e.g., BERT) and identify several problems in the existing\nformulations. First, we show that in the absolute positional encoding, the\naddition operation applied on positional embeddings and word embeddings brings\nmixed correlations between the two heterogeneous information resources. It may\nbring unnecessary randomness in the attention and further limit the\nexpressiveness of the model. Second, we question whether treating the position\nof the symbol \\texttt{[CLS]} the same as other words is a reasonable design,\nconsidering its special role (the representation of the entire sentence) in the\ndownstream tasks. Motivated from above analysis, we propose a new positional\nencoding method called \\textbf{T}ransformer with \\textbf{U}ntied\n\\textbf{P}ositional \\textbf{E}ncoding (TUPE). In the self-attention module,\nTUPE computes the word contextual correlation and positional correlation\nseparately with different parameterizations and then adds them together. This\ndesign removes the mixed and noisy correlations over heterogeneous embeddings\nand offers more expressiveness by using different projection matrices.\nFurthermore, TUPE unties the \\texttt{[CLS]} symbol from other positions, making\nit easier to capture information from all positions. Extensive experiments and\nablation studies on GLUE benchmark demonstrate the effectiveness of the\nproposed method. Codes and models are released at\nhttps://github.com/guolinke/TUPE.</p>\n", "tags": ["Evaluation","Has Code","ICLR","Model Architecture","Training Techniques"] },
{"key": "ke2021jointgt", "citations": "62", "year": "2021", "title":"Jointgt: Graph-text Joint Representation Learning For Text Generation From Knowledge Graphs", "abstract": "<p>Existing pre-trained models for knowledge-graph-to-text (KG-to-text)\ngeneration simply fine-tune text-to-text pre-trained models such as BART or T5\non KG-to-text datasets, which largely ignore the graph structure during\nencoding and lack elaborate pre-training tasks to explicitly model graph-text\nalignments. To tackle these problems, we propose a graph-text joint\nrepresentation learning model called JointGT. During encoding, we devise a\nstructure-aware semantic aggregation module which is plugged into each\nTransformer layer to preserve the graph structure. Furthermore, we propose\nthree new pre-training tasks to explicitly enhance the graph-text alignment\nincluding respective text / graph reconstruction, and graph-text alignment in\nthe embedding space via Optimal Transport. Experiments show that JointGT\nobtains new state-of-the-art performance on various KG-to-text datasets.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "keneshloo2018deep", "citations": "198", "year": "2019", "title":"Deep Reinforcement Learning For Sequence To Sequence Models", "abstract": "<p>In recent times, sequence-to-sequence (seq2seq) models have gained a lot of\npopularity and provide state-of-the-art performance in a wide variety of tasks\nsuch as machine translation, headline generation, text summarization, speech to\ntext conversion, and image caption generation. The underlying framework for all\nthese models is usually a deep neural network comprising an encoder and a\ndecoder. Although simple encoder-decoder models produce competitive results,\nmany researchers have proposed additional improvements over these\nsequence-to-sequence models, e.g., using an attention-based model over the\ninput, pointer-generation models, and self-attention models. However, such\nseq2seq models suffer from two common problems: 1) exposure bias and 2)\ninconsistency between train/test measurement. Recently, a completely novel\npoint of view has emerged in addressing these two problems in seq2seq models,\nleveraging methods from reinforcement learning (RL). In this survey, we\nconsider seq2seq problems from the RL point of view and provide a formulation\ncombining the power of RL methods in decision-making with sequence-to-sequence\nmodels that enable remembering long-term memories. We present some of the most\nrecent frameworks that combine concepts from RL and deep neural networks and\nexplain how these two areas could benefit from each other in solving complex\nseq2seq tasks. Our work aims to provide insights into some of the problems that\ninherently arise with current approaches and how we can address them with\nbetter RL models. We also provide the source code for implementing most of the\nRL models discussed in this paper to support the complex task of abstractive\ntext summarization.</p>\n", "tags": ["Agentic","Model Architecture","Reinforcement Learning","Survey Paper","Tools"] },
{"key": "kepler2019openkiwi", "citations": "107", "year": "2019", "title":"Openkiwi: An Open Source Framework For Quality Estimation", "abstract": "<p>We introduce OpenKiwi, a PyTorch-based open source framework for translation\nquality estimation. OpenKiwi supports training and testing of word-level and\nsentence-level quality estimation systems, implementing the winning systems of\nthe WMT 2015-18 quality estimation campaigns. We benchmark OpenKiwi on two\ndatasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art\nperformance on the word-level tasks and near state-of-the-art in the\nsentence-level tasks.</p>\n", "tags": ["Datasets","Evaluation","Tools","Training Techniques"] },
{"key": "kerr2023lerf", "citations": "122", "year": "2023", "title":"LERF: Language Embedded Radiance Fields", "abstract": "<p>Humans describe the physical world using natural language to refer to\nspecific 3D locations based on a vast range of properties: visual appearance,\nsemantics, abstract associations, or actionable affordances. In this work we\npropose Language Embedded Radiance Fields (LERFs), a method for grounding\nlanguage embeddings from off-the-shelf models like CLIP into NeRF, which enable\nthese types of open-ended language queries in 3D. LERF learns a dense,\nmulti-scale language field inside NeRF by volume rendering CLIP embeddings\nalong training rays, supervising these embeddings across training views to\nprovide multi-view consistency and smooth the underlying language field. After\noptimization, LERF can extract 3D relevancy maps for a broad range of language\nprompts interactively in real-time, which has potential use cases in robotics,\nunderstanding vision-language models, and interacting with 3D scenes. LERF\nenables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings\nwithout relying on region proposals or masks, supporting long-tail\nopen-vocabulary queries hierarchically across the volume. The project website\ncan be found at https://lerf.io .</p>\n", "tags": ["Applications","ICCV","Training Techniques"] },
{"key": "keskar2019ctrl", "citations": "824", "year": "2019", "title":"CTRL: A Conditional Transformer Language Model For Controllable Generation", "abstract": "<p>Large-scale language models show promising text generation capabilities, but\nusers cannot easily control particular aspects of the generated text. We\nrelease CTRL, a 1.63 billion-parameter conditional transformer language model,\ntrained to condition on control codes that govern style, content, and\ntask-specific behavior. Control codes were derived from structure that\nnaturally co-occurs with raw text, preserving the advantages of unsupervised\nlearning while providing more explicit control over text generation. These\ncodes also allow CTRL to predict which parts of the training data are most\nlikely given a sequence. This provides a potential method for analyzing large\namounts of data via model-based source attribution. We have released multiple\nfull-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "keung2019adversarial", "citations": "76", "year": "2019", "title":"Adversarial Learning With Contextual Embeddings For Zero-resource Cross-lingual Classification And NER", "abstract": "<p>Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated\nstate-of-the-art performance on various NLP tasks. Recent work with the\nmultilingual version of BERT has shown that the model performs very well in\nzero-shot and zero-resource cross-lingual settings, where only labeled English\ndata is used to finetune the model. We improve upon multilingual BERT’s\nzero-resource cross-lingual performance via adversarial learning. We report the\nmagnitude of the improvement on the multilingual MLDoc text classification and\nCoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that\nlanguage-adversarial training encourages BERT to align the embeddings of\nEnglish documents and their translations, which may be the cause of the\nobserved performance gains.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "khachatryan2023text2video", "citations": "165", "year": "2023", "title":"Text2video-zero: Text-to-image Diffusion Models Are Zero-shot Video Generators", "abstract": "<p>Recent text-to-video generation approaches rely on computationally heavy\ntraining and require large-scale video datasets. In this paper, we introduce a\nnew task of zero-shot text-to-video generation and propose a low-cost approach\n(without any training or optimization) by leveraging the power of existing\ntext-to-image synthesis methods (e.g., Stable Diffusion), making them suitable\nfor the video domain.\n  Our key modifications include (i) enriching the latent codes of the generated\nframes with motion dynamics to keep the global scene and the background time\nconsistent; and (ii) reprogramming frame-level self-attention using a new\ncross-frame attention of each frame on the first frame, to preserve the\ncontext, appearance, and identity of the foreground object.\n  Experiments show that this leads to low overhead, yet high-quality and\nremarkably consistent video generation. Moreover, our approach is not limited\nto text-to-video synthesis but is also applicable to other tasks such as\nconditional and content-specialized video generation, and Video\nInstruct-Pix2Pix, i.e., instruction-guided video editing.\n  As experiments show, our method performs comparably or sometimes better than\nrecent approaches, despite not being trained on additional video data. Our code\nwill be open sourced at: https://github.com/Picsart-AI-Research/Text2Video-Zero .</p>\n", "tags": ["Datasets","Efficiency","Has Code","ICCV","Model Architecture","Training Techniques"] },
{"key": "khalid2022clip", "citations": "127", "year": "2022", "title":"Clip-mesh: Generating Textured Meshes From Text Using Pretrained Image-text Models", "abstract": "<p>We present a technique for zero-shot generation of a 3D model using only a\ntarget text prompt. Without any 3D supervision our method deforms the control\nshape of a limit subdivided surface along with its texture map and normal map\nto obtain a 3D asset that corresponds to the input text prompt and can be\neasily deployed into games or modeling applications. We rely only on a\npre-trained CLIP model that compares the input text prompt with differentiably\nrendered images of our 3D model. While previous works have focused on\nstylization or required training of generative models we perform optimization\non mesh parameters directly to generate shape, texture or both. To constrain\nthe optimization to produce plausible meshes and textures we introduce a number\nof techniques using image augmentations and the use of a pretrained prior that\ngenerates CLIP image embeddings given a text embedding.</p>\n", "tags": ["Applications","Prompting","Training Techniques"] },
{"key": "khalil2023will", "citations": "192", "year": "2023", "title":"Will Chatgpt Get You Caught? Rethinking Of Plagiarism Detection", "abstract": "<p>The rise of Artificial Intelligence (AI) technology and its impact on\neducation has been a topic of growing concern in recent years. The new\ngeneration AI systems such as chatbots have become more accessible on the\nInternet and stronger in terms of capabilities. The use of chatbots,\nparticularly ChatGPT, for generating academic essays at schools and colleges\nhas sparked fears among scholars. This study aims to explore the originality of\ncontents produced by one of the most popular AI chatbots, ChatGPT. To this end,\ntwo popular plagiarism detection tools were used to evaluate the originality of\n50 essays generated by ChatGPT on various topics. Our results manifest that\nChatGPT has a great potential to generate sophisticated text outputs without\nbeing well caught by the plagiarism check software. In other words, ChatGPT can\ncreate content on many topics with high originality as if they were written by\nsomeone. These findings align with the recent concerns about students using\nchatbots for an easy shortcut to success with minimal or no effort. Moreover,\nChatGPT was asked to verify if the essays were generated by itself, as an\nadditional measure of plagiarism check, and it showed superior performance\ncompared to the traditional plagiarism-detection tools. The paper discusses the\nneed for institutions to consider appropriate measures to mitigate potential\nplagiarism issues and advise on the ongoing debate surrounding the impact of AI\ntechnology on education. Further implications are discussed in the paper.</p>\n", "tags": ["Tools"] },
{"key": "khan2021exploiting", "citations": "111", "year": "2021", "title":"Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation", "abstract": "<p>Multimodal target/aspect sentiment classification combines multimodal\nsentiment analysis and aspect/target sentiment classification. The goal of the\ntask is to combine vision and language to understand the sentiment towards a\ntarget entity in a sentence. Twitter is an ideal setting for the task because\nit is inherently multimodal, highly emotional, and affects real world events.\nHowever, multimodal tweets are short and accompanied by complex, possibly\nirrelevant images. We introduce a two-stream model that translates images in\ninput space using an object-aware transformer followed by a single-pass\nnon-autoregressive text generation approach. We then leverage the translation\nto construct an auxiliary sentence that provides multimodal information to a\nlanguage model. Our approach increases the amount of text available to the\nlanguage model and distills the object-level information in complex images. We\nachieve state-of-the-art performance on two multimodal Twitter datasets without\nmodifying the internals of the language model to accept multimodal data,\ndemonstrating the effectiveness of our translation. In addition, we explain a\nfailure mode of a popular approach for aspect sentiment analysis when applied\nto tweets. Our code is available at\n\\textcolor{blue}{https://github.com/codezakh/exploiting-BERT-thru-translation}.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "khan2022automatic", "citations": "60", "year": "2022", "title":"Automatic Code Documentation Generation Using GPT-3", "abstract": "<p>Source code documentation is an important artifact for efficient software\ndevelopment. Code documentation could greatly benefit from automation since\nmanual documentation is often labouring, resource and time-intensive. In this\npaper, we employed Codex for automatic code documentation creation. Codex is a\nGPT-3 based model pre-trained on both natural and programming languages. We\nfind that Codex outperforms existing techniques even with basic settings like\none-shot learning (i.e., providing only one example for training). Codex\nachieves an overall BLEU score of 20.6 for six different programming languages\n(11.2% improvement over earlier state-of-the-art techniques). Thus, Codex shows\npromise and warrants in-depth future studies for automatic code documentation\ngeneration to support diverse development tasks.</p>\n", "tags": ["Llm For Code","Model Architecture","Training Techniques"] },
{"key": "khandelwal2019generalization", "citations": "214", "year": "2019", "title":"Generalization Through Memorization: Nearest Neighbor Language Models", "abstract": "<p>We introduce \\(k\\)NN-LMs, which extend a pre-trained neural language model (LM)\nby linearly interpolating it with a \\(k\\)-nearest neighbors (\\(k\\)NN) model. The\nnearest neighbors are computed according to distance in the pre-trained LM\nembedding space, and can be drawn from any text collection, including the\noriginal LM training data. Applying this augmentation to a strong Wikitext-103\nLM, with neighbors drawn from the original training set, our \\(k\\)NN-LM achieves\na new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no\nadditional training. We also show that this approach has implications for\nefficiently scaling up to larger training sets and allows for effective domain\nadaptation, by simply varying the nearest neighbor datastore, again without\nfurther training. Qualitatively, the model is particularly helpful in\npredicting rare patterns, such as factual knowledge. Together, these results\nstrongly suggest that learning similarity between sequences of text is easier\nthan predicting the next word, and that nearest neighbor search is an effective\napproach for language modeling in the long tail.</p>\n", "tags": ["Memory & Context","Training Techniques"] },
{"key": "khandelwal2019sample", "citations": "67", "year": "2019", "title":"Sample Efficient Text Summarization Using A Single Pre-trained Transformer", "abstract": "<p>Language model (LM) pre-training has resulted in impressive performance and\nsample efficiency on a variety of language understanding tasks. However, it\nremains unclear how to best use pre-trained LMs for generation tasks such as\nabstractive summarization, particularly to enhance sample efficiency. In these\nsequence-to-sequence settings, prior work has experimented with loading\npre-trained weights into the encoder and/or decoder networks, but used\nnon-pre-trained encoder-decoder attention weights. We instead use a pre-trained\ndecoder-only network, where the same Transformer LM both encodes the source and\ngenerates the summary. This ensures that all parameters in the network,\nincluding those governing attention over source states, have been pre-trained\nbefore the fine-tuning step. Experiments on the CNN/Daily Mail dataset show\nthat our pre-trained Transformer LM substantially improves over pre-trained\nTransformer encoder-decoder networks in limited-data settings. For instance, it\nachieves 13.1 ROUGE-2 using only 1% of the training data (~3000 examples),\nwhile pre-trained encoder-decoder models score 2.3 ROUGE-2.</p>\n", "tags": ["Datasets","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "khandelwal2020nearest", "citations": "130", "year": "2020", "title":"Nearest Neighbor Machine Translation", "abstract": "<p>We introduce \\(k\\)-nearest-neighbor machine translation (\\(k\\)NN-MT), which\npredicts tokens with a nearest neighbor classifier over a large datastore of\ncached examples, using representations from a neural translation model for\nsimilarity search. This approach requires no additional training and scales to\ngive the decoder direct access to billions of examples at test time, resulting\nin a highly expressive model that consistently improves performance across many\nsettings. Simply adding nearest neighbor search improves a state-of-the-art\nGerman-English translation model by 1.5 BLEU. \\(k\\)NN-MT allows a single model to\nbe adapted to diverse domains by using a domain-specific datastore, improving\nresults by an average of 9.2 BLEU over zero-shot transfer, and achieving new\nstate-of-the-art results – without training on these domains. A massively\nmultilingual model can also be specialized for particular language pairs, with\nimprovements of 3 BLEU for translating from English into German and Chinese.\nQualitatively, \\(k\\)NN-MT is easily interpretable; it combines source and target\ncontext to retrieve highly relevant examples.</p>\n", "tags": ["Training Techniques"] },
{"key": "khanuja2020gluecos", "citations": "101", "year": "2020", "title":"Gluecos : An Evaluation Benchmark For Code-switched NLP", "abstract": "<p>Code-switching is the use of more than one language in the same conversation\nor utterance. Recently, multilingual contextual embedding models, trained on\nmultiple monolingual corpora, have shown promising results on cross-lingual and\nmultilingual tasks. We present an evaluation benchmark, GLUECoS, for\ncode-switched languages, that spans several NLP tasks in English-Hindi and\nEnglish-Spanish. Specifically, our evaluation benchmark includes Language\nIdentification from text, POS tagging, Named Entity Recognition, Sentiment\nAnalysis, Question Answering and a new task for code-switching, Natural\nLanguage Inference. We present results on all these tasks using cross-lingual\nword embedding models and multilingual models. In addition, we fine-tune\nmultilingual models on artificially generated code-switched data. Although\nmultilingual models perform significantly better than cross-lingual models, our\nresults show that in most tasks, across both language pairs, multilingual\nmodels fine-tuned on code-switched data perform best, showing that multilingual\nmodels can be further optimized for code-switching tasks.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "khanuja2021muril", "citations": "138", "year": "2021", "title":"Muril: Multilingual Representations For Indian Languages", "abstract": "<p>India is a multilingual society with 1369 rationalized languages and dialects\nbeing spoken across the country (INDIA, 2011). Of these, the 22 scheduled\nlanguages have a staggering total of 1.17 billion speakers and 121 languages\nhave more than 10,000 speakers (INDIA, 2011). India also has the second largest\n(and an ever growing) digital footprint (Statista, 2020). Despite this, today’s\nstate-of-the-art multilingual systems perform suboptimally on Indian (IN)\nlanguages. This can be explained by the fact that multilingual language models\n(LMs) are often trained on 100+ languages together, leading to a small\nrepresentation of IN languages in their vocabulary and training data.\nMultilingual LMs are substantially less effective in resource-lean scenarios\n(Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn’t help\ncapture the various nuances of a language. One also commonly observes IN\nlanguage text transliterated to Latin or code-mixed with English, especially in\ninformal settings (for example, on social media platforms) (Rijhwani et al.,\n2017). This phenomenon is not adequately handled by current state-of-the-art\nmultilingual LMs. To address the aforementioned gaps, we propose MuRIL, a\nmultilingual LM specifically built for IN languages. MuRIL is trained on\nsignificantly large amounts of IN text corpora only. We explicitly augment\nmonolingual text corpora with both translated and transliterated document\npairs, that serve as supervised cross-lingual signals in training. MuRIL\nsignificantly outperforms multilingual BERT (mBERT) on all tasks in the\nchallenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present\nresults on transliterated (native to Latin script) test sets of the chosen\ndatasets and demonstrate the efficacy of MuRIL in handling transliterated data.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "khare2021mmbert", "citations": "95", "year": "2021", "title":"MMBERT: Multimodal BERT Pretraining For Improved Medical VQA", "abstract": "<p>Images in the medical domain are fundamentally different from the general\ndomain images. Consequently, it is infeasible to directly employ general domain\nVisual Question Answering (VQA) models for the medical domain. Additionally,\nmedical images annotation is a costly and time-consuming process. To overcome\nthese limitations, we propose a solution inspired by self-supervised\npretraining of Transformer-style architectures for NLP, Vision and Language\ntasks. Our method involves learning richer medical image and text semantic\nrepresentations using Masked Language Modeling (MLM) with image features as the\npretext task on a large medical image+caption dataset. The proposed solution\nachieves new state-of-the-art performance on two VQA datasets for radiology\nimages – VQA-Med 2019 and VQA-RAD, outperforming even the ensemble models of\nprevious best solutions. Moreover, our solution provides attention maps which\nhelp in model interpretability. The code is available at\nhttps://github.com/VirajBagal/MMBERT</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "khashabi2019question", "citations": "60", "year": "2019", "title":"Question Answering As Global Reasoning Over Semantic Abstractions", "abstract": "<p>We propose a novel method for exploiting the semantic structure of text to\nanswer multiple-choice questions. The approach is especially suitable for\ndomains that require reasoning over a diverse set of linguistic constructs but\nhave limited training data. To address these challenges, we present the first\nsystem, to the best of our knowledge, that reasons over a wide range of\nsemantic abstractions of the text, which are derived using off-the-shelf,\ngeneral-purpose, pre-trained natural language modules such as semantic role\nlabelers, coreference resolvers, and dependency parsers. Representing multiple\nabstractions as a family of graphs, we translate question answering (QA) into a\nsearch for an optimal subgraph that satisfies certain global and local\nproperties. This formulation generalizes several prior structured QA systems.\nOur system, SEMANTICILP, demonstrates strong performance on two domains\nsimultaneously. In particular, on a collection of challenging science QA\ndatasets, it outperforms various state-of-the-art approaches, including neural\nmodels, broad coverage information retrieval, and specialized techniques using\nstructured knowledge bases, by 2%-6%.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "khashabi2020more", "citations": "63", "year": "2020", "title":"More Bang For Your Buck: Natural Perturbation For Robust Question Answering", "abstract": "<p>While recent models have achieved human-level scores on many NLP datasets, we\nobserve that they are considerably sensitive to small changes in input. As an\nalternative to the standard approach of addressing this issue by constructing\ntraining sets of completely new examples, we propose doing so via minimal\nperturbation of examples. Specifically, our approach involves first collecting\na set of seed examples and then applying human-driven natural perturbations (as\nopposed to rule-based machine perturbations), which often change the gold label\nas well. Local perturbations have the advantage of being relatively easier (and\nhence cheaper) to create than writing out completely new examples. To evaluate\nthe impact of this phenomenon, we consider a recent question-answering dataset\n(BoolQ) and study the benefit of our approach as a function of the perturbation\ncost ratio, the relative cost of perturbing an existing question vs. creating a\nnew one from scratch. We find that when natural perturbations are moderately\ncheaper to create, it is more effective to train models using them: such models\nexhibit higher robustness and better generalization, while retaining\nperformance on the original BoolQ dataset.</p>\n", "tags": ["Datasets","EMNLP","Training Techniques"] },
{"key": "khashabi2020unifiedqa", "citations": "450", "year": "2020", "title":"Unifiedqa: Crossing Format Boundaries With A Single QA System", "abstract": "<p>Question answering (QA) tasks have been posed using a variety of formats,\nsuch as extractive span selection, multiple choice, etc. This has led to\nformat-specialized models, and even to an implicit division in the QA\ncommunity. We argue that such boundaries are artificial and perhaps\nunnecessary, given the reasoning abilities we seek to teach are not governed by\nthe format. As evidence, we use the latest advances in language modeling to\nbuild a single pre-trained QA model, UnifiedQA, that performs surprisingly well\nacross 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par\nwith 9 different models that were trained on individual datasets themselves.\nEven when faced with 12 unseen datasets of observed formats, UnifiedQA performs\nsurprisingly well, showing strong generalization from its out-of-format\ntraining data. Finally, simply fine-tuning this pre-trained QA model into\nspecialized models results in a new state of the art on 6 datasets,\nestablishing UnifiedQA as a strong starting point for building QA systems.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "khatri2018advancing", "citations": "62", "year": "2018", "title":"Advancing The State Of The Art In Open Domain Dialog Systems Through The Alexa Prize", "abstract": "<p>Building open domain conversational systems that allow users to have engaging\nconversations on topics of their choice is a challenging task. Alexa Prize was\nlaunched in 2016 to tackle the problem of achieving natural, sustained,\ncoherent and engaging open-domain dialogs. In the second iteration of the\ncompetition in 2018, university teams advanced the state of the art by using\ncontext in dialog models, leveraging knowledge graphs for language\nunderstanding, handling complex utterances, building statistical and\nhierarchical dialog managers, and leveraging model-driven signals from user\nresponses. The 2018 competition also included the provision of a suite of tools\nand models to the competitors including the CoBot (conversational bot) toolkit,\ntopic and dialog act detection models, conversation evaluators, and a sensitive\ncontent detection model so that the competing teams could focus on building\nknowledge-rich, coherent and engaging multi-turn dialog systems. This paper\noutlines the advances developed by the university teams as well as the Alexa\nPrize team to achieve the common goal of advancing the science of\nConversational AI. We address several key open-ended problems such as\nconversational speech recognition, open domain natural language understanding,\ncommonsense reasoning, statistical dialog management, and dialog evaluation.\nThese collaborative efforts have driven improved experiences by Alexa users to\nan average rating of 3.61, the median duration of 2 mins 18 seconds, and\naverage turns to 14.6, increases of 14%, 92%, 54% respectively since the launch\nof the 2018 competition. For conversational speech recognition, we have\nimproved our relative Word Error Rate by 55% and our relative Entity Error Rate\nby 34% since the launch of the Alexa Prize. Socialbots improved in quality\nsignificantly more rapidly in 2018, in part due to the release of the CoBot\ntoolkit.</p>\n", "tags": ["Evaluation","Tools"] },
{"key": "khattab2020colbert", "citations": "196", "year": "2020", "title":"Colbert: Efficient And Effective Passage Search Via Contextualized Late Interaction Over BERT", "abstract": "<p>Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT’s pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT’s effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.</p>\n", "tags": ["Datasets","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "khattak2022maple", "citations": "314", "year": "2023", "title":"Maple: Multi-modal Prompt Learning", "abstract": "<p>Pre-trained vision-language (V-L) models such as CLIP have shown excellent\ngeneralization ability to downstream tasks. However, they are sensitive to the\nchoice of input text prompts and require careful selection of prompt templates\nto perform well. Inspired by the Natural Language Processing (NLP) literature,\nrecent CLIP adaptation approaches learn prompts as the textual inputs to\nfine-tune CLIP for downstream tasks. We note that using prompting to adapt\nrepresentations in a single branch of CLIP (language or vision) is sub-optimal\nsince it does not allow the flexibility to dynamically adjust both\nrepresentation spaces on a downstream task. In this work, we propose\nMulti-modal Prompt Learning (MaPLe) for both vision and language branches to\nimprove alignment between the vision and language representations. Our design\npromotes strong coupling between the vision-language prompts to ensure mutual\nsynergy and discourages learning independent uni-modal solutions. Further, we\nlearn separate prompts across different early stages to progressively model the\nstage-wise feature relationships to allow rich context learning. We evaluate\nthe effectiveness of our approach on three representative tasks of\ngeneralization to novel classes, new target datasets and unseen domain shifts.\nCompared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable\nperformance and achieves an absolute gain of 3.45% on novel classes and 2.72%\non overall harmonic-mean, averaged over 11 diverse image recognition datasets.\nOur code and pre-trained models are available at\nhttps://github.com/muzairkhattak/multimodal-prompt-learning.</p>\n", "tags": ["CVPR","Datasets","Has Code","Prompting"] },
{"key": "khot2019qasc", "citations": "199", "year": "2020", "title":"QASC: A Dataset For Question Answering Via Sentence Composition", "abstract": "<p>Composing knowledge from multiple pieces of texts is a key challenge in\nmulti-hop question answering. We present a multi-hop reasoning dataset,\nQuestion Answering via Sentence Composition(QASC), that requires retrieving\nfacts from a large corpus and composing them to answer a multiple-choice\nquestion. QASC is the first dataset to offer two desirable properties: (a) the\nfacts to be composed are annotated in a large corpus, and (b) the decomposition\ninto these facts is not evident from the question itself. The latter makes\nretrieval challenging as the system must introduce new concepts or relations in\norder to discover potential decompositions. Further, the reasoning model must\nthen learn to identify valid compositions of these retrieved facts using\ncommon-sense reasoning. To help address these challenges, we provide annotation\nfor supporting facts as well as their composition. Guided by these annotations,\nwe present a two-step approach to mitigate the retrieval challenges. We use\nother multiple-choice datasets as additional training data to strengthen the\nreasoning model. Our proposed approach improves over current state-of-the-art\nlanguage models by 11% (absolute). The reasoning and retrieval problems,\nhowever, remain unsolved as this model still lags by 20% behind human\nperformance.</p>\n", "tags": ["AAAI","Datasets","Training Techniques"] },
{"key": "khot2022decomposed", "citations": "62", "year": "2022", "title":"Decomposed Prompting: A Modular Approach For Solving Complex Tasks", "abstract": "<p>Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks. Datasets, Code and\nPrompts available at https://github.com/allenai/DecomP.</p>\n", "tags": ["Datasets","Few-Shot","Has Code","In Context Learning","Prompting","Tools"] },
{"key": "khoury2023how", "citations": "60", "year": "2023", "title":"How Secure Is Code Generated By Chatgpt?", "abstract": "<p>In recent years, large language models have been responsible for great\nadvances in the field of artificial intelligence (AI). ChatGPT in particular,\nan AI chatbot developed and recently released by OpenAI, has taken the field to\nthe next level. The conversational model is able not only to process human-like\ntext, but also to translate natural language into code. However, the safety of\nprograms generated by ChatGPT should not be overlooked. In this paper, we\nperform an experiment to address this issue. Specifically, we ask ChatGPT to\ngenerate a number of program and evaluate the security of the resulting source\ncode. We further investigate whether ChatGPT can be prodded to improve the\nsecurity by appropriate prompts, and discuss the ethical aspects of using AI to\ngenerate code. Results suggest that ChatGPT is aware of potential\nvulnerabilities, but nonetheless often generates source code that are not\nrobust to certain attacks.</p>\n", "tags": ["Security"] },
{"key": "kiela2017learning", "citations": "76", "year": "2018", "title":"Learning Visually Grounded Sentence Representations", "abstract": "<p>We introduce a variety of models, trained on a supervised image captioning\ncorpus to predict the image features for a given caption, to perform sentence\nrepresentation grounding. We train a grounded sentence encoder that achieves\ngood performance on COCO caption and image retrieval and subsequently show that\nthis encoder can successfully be transferred to various NLP tasks, with\nimproved performance over text-only models. Lastly, we analyze the contribution\nof grounding, and show that word embeddings learned by this system outperform\nnon-grounded ones.</p>\n", "tags": ["Datasets","NAACL"] },
{"key": "kiela2019supervised", "citations": "161", "year": "2019", "title":"Supervised Multimodal Bitransformers For Classifying Images And Text", "abstract": "<p>Self-supervised bidirectional transformer models such as BERT have led to\ndramatic improvements in a wide variety of textual classification tasks. The\nmodern digital world is increasingly multimodal, however, and textual\ninformation is often accompanied by other modalities such as images. We\nintroduce a supervised multimodal bitransformer model that fuses information\nfrom text and image encoders, and obtain state-of-the-art performance on\nvarious multimodal classification benchmark tasks, outperforming strong\nbaselines, including on hard test sets specifically designed to measure\nmultimodal performance.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "kiela2021dynabench", "citations": "186", "year": "2021", "title":"Dynabench: Rethinking Benchmarking In NLP", "abstract": "<p>We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.</p>\n", "tags": ["Datasets","Evaluation","NAACL","Tools"] },
{"key": "kim2016multimodal", "citations": "218", "year": "2016", "title":"Multimodal Residual Learning For Visual QA", "abstract": "<p>Deep neural networks continue to advance the state-of-the-art of image\nrecognition tasks with various methods. However, applications of these methods\nto multimodality remain limited. We present Multimodal Residual Networks (MRN)\nfor the multimodal residual learning of visual question-answering, which\nextends the idea of the deep residual learning. Unlike the deep residual\nlearning, MRN effectively learns the joint representation from vision and\nlanguage information. The main idea is to use element-wise multiplication for\nthe joint residual mappings exploiting the residual learning of the attentional\nmodels in recent studies. Various alternative models introduced by\nmultimodality are explored based on our study. We achieve the state-of-the-art\nresults on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks.\nMoreover, we introduce a novel method to visualize the attention effect of the\njoint representations for each learning block using back-propagation algorithm,\neven though the visual features are collapsed without spatial information.</p>\n", "tags": ["Applications","Datasets","Model Architecture"] },
{"key": "kim2016sequence", "citations": "759", "year": "2016", "title":"Sequence-level Knowledge Distillation", "abstract": "<p>Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large. In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU.</p>\n", "tags": ["EMNLP","Efficiency"] },
{"key": "kim2017deepstory", "citations": "135", "year": "2017", "title":"Deepstory: Video Story QA By Deep Embedded Memory Networks", "abstract": "<p>Question-answering (QA) on video contents is a significant challenge for\nachieving human-level intelligence as it involves both vision and language in\nreal-world settings. Here we demonstrate the possibility of an AI agent\nperforming video story QA by learning from a large amount of cartoon videos. We\ndevelop a video-story learning model, i.e. Deep Embedded Memory Networks\n(DEMN), to reconstruct stories from a joint scene-dialogue video stream using a\nlatent embedding space of observed data. The video stories are stored in a\nlong-term memory component. For a given question, an LSTM-based attention model\nuses the long-term memory to recall the best question-story-answer triplet by\nfocusing on specific words containing key information. We trained the DEMN on a\nnovel QA dataset of children’s cartoon video series, Pororo. The dataset\ncontains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained\nsentences for scene description, and 8,913 story-related QA pairs. Our\nexperimental results show that the DEMN outperforms other QA models. This is\nmainly due to 1) the reconstruction of video stories in a scene-dialogue\ncombined form that utilize the latent embedding and 2) attention. DEMN also\nachieved state-of-the-art results on the MovieQA benchmark.</p>\n", "tags": ["Datasets","Evaluation","IJCAI","Model Architecture"] },
{"key": "kim2018bilinear", "citations": "581", "year": "2018", "title":"Bilinear Attention Networks", "abstract": "<p>Attention networks in multimodal learning provide an efficient way to utilize\ngiven visual information selectively. However, the computational cost to learn\nattention distributions for every pair of multimodal input channels is\nprohibitively expensive. To solve this problem, co-attention builds two\nseparate attention distributions for each modality neglecting the interaction\nbetween multimodal inputs. In this paper, we propose bilinear attention\nnetworks (BAN) that find bilinear attention distributions to utilize given\nvision-language information seamlessly. BAN considers bilinear interactions\namong two groups of input channels, while low-rank bilinear pooling extracts\nthe joint representations for each pair of channels. Furthermore, we propose a\nvariant of multimodal residual networks to exploit eight-attention maps of the\nBAN efficiently. We quantitatively and qualitatively evaluate our model on\nvisual question answering (VQA 2.0) and Flickr30k Entities datasets, showing\nthat BAN significantly outperforms previous methods and achieves new\nstate-of-the-arts on both datasets.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "kim2018improving", "citations": "171", "year": "2019", "title":"Improving Neural Question Generation Using Answer Separation", "abstract": "<p>Neural question generation (NQG) is the task of generating a question from a\ngiven passage with deep neural networks. Previous NQG models suffer from a\nproblem that a significant proportion of the generated questions include words\nin the question target, resulting in the generation of unintended questions. In\nthis paper, we propose answer-separated seq2seq, which better utilizes the\ninformation from both the passage and the target answer. By replacing the\ntarget answer in the original passage with a special token, our model learns to\nidentify which interrogative word should be used. We also propose a new module\ntermed keyword-net, which helps the model better capture the key information in\nthe target answer and generate an appropriate question. Experimental results\ndemonstrate that our answer separation method significantly reduces the number\nof improper questions which include answers. Consequently, our model\nsignificantly outperforms previous state-of-the-art NQG models.</p>\n", "tags": ["AAAI"] },
{"key": "kim2018multimodal", "citations": "78", "year": "2018", "title":"Multimodal Dual Attention Memory For Video Story Question Answering", "abstract": "<p>We propose a video story question-answering (QA) architecture, Multimodal\nDual Attention Memory (MDAM). The key idea is to use a dual attention mechanism\nwith late fusion. MDAM uses self-attention to learn the latent concepts in\nscene frames and captions. Given a question, MDAM uses the second attention\nover these latent concepts. Multimodal fusion is performed after the dual\nattention processes (late fusion). Using this processing pipeline, MDAM learns\nto infer a high-level vision-language joint representation from an abstraction\nof the full video content. We evaluate MDAM on PororoQA and MovieQA datasets\nwhich have large-scale QA annotations on cartoon videos and movies,\nrespectively. For both datasets, MDAM achieves new state-of-the-art results\nwith significant margins compared to the runner-up models. We confirm the best\nperformance of the dual attention mechanism combined with late fusion by\nablation studies. We also perform qualitative analysis by visualizing the\ninference mechanisms of MDAM.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "kim2018semantic", "citations": "181", "year": "2019", "title":"Semantic Sentence Matching With Densely-connected Recurrent And Co-attentive Information", "abstract": "<p>Sentence matching is widely used in various natural language tasks such as\nnatural language inference, paraphrase identification, and question answering.\nFor these tasks, understanding logical and semantic relationship between two\nsentences is required but it is yet challenging. Although attention mechanism\nis useful to capture the semantic relationship and to properly align the\nelements of two sentences, previous methods of attention mechanism simply use a\nsummation operation which does not retain original features enough. Inspired by\nDenseNet, a densely connected convolutional network, we propose a\ndensely-connected co-attentive recurrent neural network, each layer of which\nuses concatenated information of attentive features as well as hidden features\nof all the preceding recurrent layers. It enables preserving the original and\nthe co-attentive feature information from the bottommost word embedding layer\nto the uppermost recurrent layer. To alleviate the problem of an\never-increasing size of feature vectors due to dense concatenation operations,\nwe also propose to use an autoencoder after dense concatenation. We evaluate\nour proposed architecture on highly competitive benchmark datasets related to\nsentence matching. Experimental results show that our architecture, which\nretains recurrent and attentive features, achieves state-of-the-art\nperformances for most of the tasks.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture"] },
{"key": "kim2018textual", "citations": "246", "year": "2018", "title":"Textual Explanations For Self-driving Vehicles", "abstract": "<p>Deep neural perception and control networks have become key components of\nself-driving vehicles. User acceptance is likely to benefit from\neasy-to-interpret textual explanations which allow end-users to understand what\ntriggered a particular behavior. Explanations may be triggered by the neural\ncontroller, namely introspective explanations, or informed by the neural\ncontroller’s output, namely rationalizations. We propose a new approach to\nintrospective explanations which consists of two parts. First, we use a visual\n(spatial) attention model to train a convolutional network end-to-end from\nimages to the vehicle control commands, i.e., acceleration and change of\ncourse. The controller’s attention identifies image regions that potentially\ninfluence the network’s output. Second, we use an attention-based video-to-text\nmodel to produce textual explanations of model actions. The attention maps of\ncontroller and explanation model are aligned so that explanations are grounded\nin the parts of the scene that mattered to the controller. We explore two\napproaches to attention alignment, strong- and weak-alignment. Finally, we\nexplore a version of our model that generates rationalizations, and compare\nwith introspective explanations on the same video segments. We evaluate these\nmodels on a novel driving dataset with ground-truth human explanations, the\nBerkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at\nhttps://github.com/JinkyuKimUCB/explainable-deep-driving.</p>\n", "tags": ["Has Code"] },
{"key": "kim2019dense", "citations": "94", "year": "2019", "title":"Dense Relational Captioning: Triple-stream Networks For Relationship-based Captioning", "abstract": "<p>Our goal in this work is to train an image captioning model that generates\nmore dense and informative captions. We introduce “relational captioning,” a\nnovel image captioning task which aims to generate multiple captions with\nrespect to relational information between objects in an image. Relational\ncaptioning is a framework that is advantageous in both diversity and amount of\ninformation, leading to image understanding based on relationships. Part-of\nspeech (POS, i.e. subject-object-predicate categories) tags can be assigned to\nevery English word. We leverage the POS as a prior to guide the correct\nsequence of words in a caption. To this end, we propose a multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units for the\nrespective POS and jointly performs POS prediction and captioning. We\ndemonstrate more diverse and richer representations generated by the proposed\nmodel against several baselines and competing methods.</p>\n", "tags": ["CVPR","Tools"] },
{"key": "kim2019effective", "citations": "83", "year": "2019", "title":"Effective Cross-lingual Transfer Of Neural Machine Translation Models Without Shared Vocabularies", "abstract": "<p>Transfer learning or multilingual model is essential for low-resource neural\nmachine translation (NMT), but the applicability is limited to cognate\nlanguages by sharing their vocabularies. This paper shows effective techniques\nto transfer a pre-trained NMT model to a new, unrelated language without shared\nvocabularies. We relieve the vocabulary mismatch by using cross-lingual word\nembedding, train a more language-agnostic encoder by injecting artificial\nnoises, and generate synthetic data easily from the pre-training data without\nback-translation. Our methods do not require restructuring the vocabulary or\nretraining the model. We improve plain NMT transfer by up to +5.1% BLEU in five\nlow-resource translation tasks, outperforming multilingual joint training by a\nlarge margin. We also provide extensive ablation studies on pre-trained\nembedding, synthetic data, vocabulary size, and parameter freezing for a better\nunderstanding of NMT transfer.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "kim2019efficient", "citations": "193", "year": "2020", "title":"Efficient Dialogue State Tracking By Selectively Overwriting Memory", "abstract": "<p>Recent works in dialogue state tracking (DST) focus on an open\nvocabulary-based setting to resolve scalability and generalization issues of\nthe predefined ontology-based approaches. However, they are inefficient in that\nthey predict the dialogue state at every turn from scratch. Here, we consider\ndialogue state as an explicit fixed-sized memory and propose a selectively\noverwriting mechanism for more efficient DST. This mechanism consists of two\nsteps: (1) predicting state operation on each of the memory slots, and (2)\noverwriting the memory with new values, of which only a few are generated\naccording to the predicted state operations. Our method decomposes DST into two\nsub-tasks and guides the decoder to focus only on one of the tasks, thus\nreducing the burden of the decoder. This enhances the effectiveness of training\nand DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue\nState Tracking) model achieves state-of-the-art joint goal accuracy with 51.72%\nin MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST\nsetting. In addition, we analyze the accuracy gaps between the current and the\nground truth-given situations and suggest that it is a promising direction to\nimprove state operation prediction to boost the DST performance.</p>\n", "tags": ["Dialogue & Multi Turn","Efficiency","Memory & Context","Training Techniques"] },
{"key": "kim2019eighth", "citations": "60", "year": "2019", "title":"The Eighth Dialog System Technology Challenge", "abstract": "<p>This paper introduces the Eighth Dialog System Technology Challenge. In line\nwith recent challenges, the eighth edition focuses on applying end-to-end\ndialog technologies in a pragmatic way for multi-domain task-completion, noetic\nresponse selection, audio visual scene-aware dialog, and schema-guided dialog\nstate tracking tasks. This paper describes the task definition, provided\ndatasets, and evaluation set-up for each track. We also summarize the results\nof the submitted systems to highlight the overall trends of the\nstate-of-the-art technologies for the tasks.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "kim2019grounding", "citations": "86", "year": "2019", "title":"Grounding Human-to-vehicle Advice For Self-driving Vehicles", "abstract": "<p>Recent success suggests that deep neural control networks are likely to be a\nkey component of self-driving vehicles. These networks are trained on large\ndatasets to imitate human actions, but they lack semantic understanding of\nimage contents. This makes them brittle and potentially unsafe in situations\nthat do not match training data. Here, we propose to address this issue by\naugmenting training data with natural language advice from a human. Advice\nincludes guidance about what to do and where to attend. We present the first\nstep toward advice giving, where we train an end-to-end vehicle controller that\naccepts advice. The controller adapts the way it attends to the scene (visual\nattention) and the control (steering and speed). Attention mechanisms tie\ncontroller behavior to salient objects in the advice. We evaluate our model on\na novel advisable driving dataset with manually annotated human-to-vehicle\nadvice called Honda Research Institute-Advice Dataset (HAD). We show that\ntaking advice improves the performance of the end-to-end network, while the\nnetwork cues on a variety of visual features that are provided by advice. The\ndataset is available at https://usa.honda-ri.com/HAD.</p>\n", "tags": ["CVPR","Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "kim2019pivot", "citations": "65", "year": "2019", "title":"Pivot-based Transfer Learning For Neural Machine Translation Between Non-english Languages", "abstract": "<p>We present effective pre-training strategies for neural machine translation\n(NMT) using parallel corpora involving a pivot language, i.e., source-pivot and\npivot-target, leading to a significant improvement in source-target\ntranslation. We propose three methods to increase the relation among source,\npivot, and target languages in the pre-training: 1) step-wise training of a\nsingle model for different language pairs, 2) additional adapter component to\nsmoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder\ntraining via autoencoding of the pivot language. Our methods greatly outperform\nmultilingual models up to +2.6% BLEU in WMT 2019 French-German and German-Czech\ntasks. We show that our improvements are valid also in zero-shot/zero-resource\nscenarios.</p>\n", "tags": ["EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "kim2019probing", "citations": "110", "year": "2019", "title":"Probing What Different NLP Tasks Teach Machines About Function Word Comprehension", "abstract": "<p>We introduce a set of nine challenge tasks that test for the understanding of\nfunction words. These tasks are created by structurally mutating sentences from\nexisting datasets to target the comprehension of specific types of function\nwords (e.g., prepositions, wh-words). Using these probing tasks, we explore the\neffects of various pretraining objectives for sentence encoders (e.g., language\nmodeling, CCG supertagging and natural language inference (NLI)) on the learned\nrepresentations. Our results show that pretraining on language modeling\nperforms the best on average across our probing tasks, supporting its\nwidespread use for pretraining state-of-the-art NLP models, and CCG\nsupertagging and NLI pretraining perform comparably. Overall, no pretraining\nobjective dominates across the board, and our function word probing tasks\nhighlight several intuitive differences between pretraining objectives, e.g.,\nthat NLI helps the comprehension of negation.</p>\n", "tags": ["Datasets"] },
{"key": "kim2019progressive", "citations": "91", "year": "2019", "title":"Progressive Attention Memory Network For Movie Story Question Answering", "abstract": "<p>This paper proposes the progressive attention memory network (PAMN) for movie\nstory question answering (QA). Movie story QA is challenging compared to VQA in\ntwo aspects: (1) pinpointing the temporal parts relevant to answer the question\nis difficult as the movies are typically longer than an hour, (2) it has both\nvideo and subtitle where different questions require different modality to\ninfer the answer. To overcome these challenges, PAMN involves three main\nfeatures: (1) progressive attention mechanism that utilizes cues from both\nquestion and answer to progressively prune out irrelevant temporal parts in\nmemory, (2) dynamic modality fusion that adaptively determines the contribution\nof each modality for answering the current question, and (3) belief correction\nanswering scheme that successively corrects the prediction score on each\ncandidate answer. Experiments on publicly available benchmark datasets, MovieQA\nand TVQA, demonstrate that each feature contributes to our movie story QA\narchitecture, PAMN, and improves performance to achieve the state-of-the-art\nresult. Qualitative analysis by visualizing the inference mechanism of PAMN is\nalso provided.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture"] },
{"key": "kim2019when", "citations": "68", "year": "2019", "title":"When And Why Is Document-level Context Useful In Neural Machine Translation?", "abstract": "<p>Document-level context has received lots of attention for compensating neural\nmachine translation (NMT) of isolated sentences. However, recent advances in\ndocument-level NMT focus on sophisticated integration of the context,\nexplaining its improvement with only a few selected examples or targeted test\nsets. We extensively quantify the causes of improvements by a document-level\nmodel in general test sets, clarifying the limit of the usefulness of\ndocument-level context in NMT. We show that most of the improvements are not\ninterpretable as utilizing the context. We also show that a minimal encoding is\nsufficient for the context modeling and very long context is not helpful for\nNMT.</p>\n", "tags": ["Evaluation","Memory & Context","Model Architecture"] },
{"key": "kim2020code", "citations": "162", "year": "2021", "title":"Code Prediction By Feeding Trees To Transformers", "abstract": "<p>We advance the state-of-the-art in the accuracy of code prediction (next\ntoken prediction) used in autocomplete systems. First, we report that using the\nrecently proposed Transformer architecture even out-of-the-box outperforms\nprevious neural and non-neural systems for code prediction. We then show that\nby making the Transformer architecture aware of the syntactic structure of\ncode, we further increase the margin by which a Transformer-based system\noutperforms previous systems. With this, it outperforms the accuracy of an\nRNN-based system (similar to Hellendoorn et al. 2018) by 18.3%, the Deep3\nsystem (Raychev et al 2016) by 14.1%, and an adaptation of Code2Seq (Alon et\nal., 2018) for code prediction by 14.4%.\n  We present in the paper several ways of communicating the code structure to\nthe Transformer, which is fundamentally built for processing sequence data. We\nprovide a comprehensive experimental evaluation of our proposal, along with\nalternative design choices, on a standard Python dataset, as well as on a\nFacebook internal Python corpus. Our code and data preparation pipeline will be\navailable in open source.</p>\n", "tags": ["Datasets","Evaluation","Llm For Code","Model Architecture"] },
{"key": "kim2020sequential", "citations": "110", "year": "2020", "title":"Sequential Latent Knowledge Selection For Knowledge-grounded Dialogue", "abstract": "<p>Knowledge-grounded dialogue is a task of generating an informative response\nbased on both discourse context and external knowledge. As we focus on better\nmodeling the knowledge selection in the multi-turn knowledge-grounded dialogue,\nwe propose a sequential latent variable model as the first approach to this\nmatter. The model named sequential knowledge transformer (SKT) can keep track\nof the prior and posterior distribution over knowledge; as a result, it can not\nonly reduce the ambiguity caused from the diversity in knowledge selection of\nconversation but also better leverage the response information for proper\nchoice of knowledge. Our experimental results show that the proposed model\nimproves the knowledge selection accuracy and subsequently the performance of\nutterance generation. We achieve the new state-of-the-art performance on Wizard\nof Wikipedia (Dinan et al., 2019) as one of the most large-scale and\nchallenging benchmarks. We further validate the effectiveness of our model over\nexisting conversation methods in another knowledge-based dialogue Holl-E\ndataset (Moghe et al., 2018).</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "kim2021i", "citations": "90", "year": "2021", "title":"I-BERT: Integer-only BERT Quantization", "abstract": "<p>Transformer based models, like BERT and RoBERTa, have achieved\nstate-of-the-art results in many Natural Language Processing tasks. However,\ntheir memory footprint, inference latency, and power consumption are\nprohibitive efficient inference at the edge, and even at the data center. While\nquantization can be a viable solution for this, previous work on quantizing\nTransformer based models use floating-point arithmetic during inference, which\ncannot efficiently utilize integer-only logical units such as the recent Turing\nTensor Cores, or traditional integer-only ARM processors. In this work, we\npropose I-BERT, a novel quantization scheme for Transformer based models that\nquantizes the entire inference with integer-only arithmetic. Based on\nlightweight integer-only approximation methods for nonlinear operations, e.g.,\nGELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end\ninteger-only BERT inference without any floating point calculation. We evaluate\nour approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that\nfor both cases, I-BERT achieves similar (and slightly higher) accuracy as\ncompared to the full-precision baseline. Furthermore, our preliminary\nimplementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4\nGPU system as compared to FP32 inference. The framework has been developed in\nPyTorch and has been open-sourced.</p>\n", "tags": ["Efficiency","Model Architecture","Tools"] },
{"key": "kim2021self", "citations": "153", "year": "2021", "title":"Self-guided Contrastive Learning For BERT Sentence Representations", "abstract": "<p>Although BERT and its variants have reshaped the NLP landscape, it still\nremains unclear how best to derive sentence embeddings from such pre-trained\nTransformers. In this work, we propose a contrastive learning method that\nutilizes self-guidance for improving the quality of BERT sentence\nrepresentations. Our method fine-tunes BERT in a self-supervised fashion, does\nnot rely on data augmentation, and enables the usual [CLS] token embeddings to\nfunction as sentence vectors. Moreover, we redesign the contrastive learning\nobjective (NT-Xent) and apply it to sentence representation learning. We\ndemonstrate with extensive experiments that our approach is more effective than\ncompetitive baselines on diverse sentence-related tasks. We also show it is\nefficient at inference and robust to domain shifts.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "kim2021vilt", "citations": "421", "year": "2021", "title":"Vilt: Vision-and-language Transformer Without Convolution Or Region Supervision", "abstract": "<p>Vision-and-Language Pre-training (VLP) has improved performance on various\njoint vision-and-language downstream tasks. Current approaches to VLP heavily\nrely on image feature extraction processes, most of which involve region\nsupervision (e.g., object detection) and the convolutional architecture (e.g.,\nResNet). Although disregarded in the literature, we find it problematic in\nterms of both (1) efficiency/speed, that simply extracting input features\nrequires much more computation than the multimodal interaction steps; and (2)\nexpressive power, as it is upper bounded to the expressive power of the visual\nembedder and its predefined visual vocabulary. In this paper, we present a\nminimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the\nsense that the processing of visual inputs is drastically simplified to just\nthe same convolution-free manner that we process textual inputs. We show that\nViLT is up to tens of times faster than previous VLP models, yet with\ncompetitive or better downstream task performance. Our code and pre-trained\nweights are available at https://github.com/dandelin/vilt.</p>\n", "tags": ["Efficiency","Has Code","Model Architecture","Training Techniques"] },
{"key": "kim2022flame", "citations": "66", "year": "2023", "title":"FLAME: Free-form Language-based Motion Synthesis & Editing", "abstract": "<p>Text-based motion generation models are drawing a surge of interest for their\npotential for automating the motion-making process in the game, animation, or\nrobot industries. In this paper, we propose a diffusion-based motion synthesis\nand editing model named FLAME. Inspired by the recent successes in diffusion\nmodels, we integrate diffusion-based generative models into the motion domain.\nFLAME can generate high-fidelity motions well aligned with the given text.\nAlso, it can edit the parts of the motion, both frame-wise and joint-wise,\nwithout any fine-tuning. FLAME involves a new transformer-based architecture we\ndevise to better handle motion data, which is found to be crucial to manage\nvariable-length motions and well attend to free-form text. In experiments, we\nshow that FLAME achieves state-of-the-art generation performances on three\ntext-motion datasets: HumanML3D, BABEL, and KIT. We also demonstrate that\nediting capability of FLAME can be extended to other tasks such as motion\nprediction or motion in-betweening, which have been previously covered by\ndedicated models.</p>\n", "tags": ["AAAI","Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "kim2022help", "citations": "105", "year": "2023", "title":"\"help Me Help The AI\": Understanding How Explainability Can Support Human-ai Interaction", "abstract": "<p>Despite the proliferation of explainable AI (XAI) methods, little is\nunderstood about end-users’ explainability needs and behaviors around XAI\nexplanations. To address this gap and contribute to understanding how\nexplainability can support human-AI interaction, we conducted a mixed-methods\nstudy with 20 end-users of a real-world AI application, the Merlin bird\nidentification app, and inquired about their XAI needs, uses, and perceptions.\nWe found that participants desire practically useful information that can\nimprove their collaboration with the AI, more so than technical system details.\nRelatedly, participants intended to use XAI explanations for various purposes\nbeyond understanding the AI’s outputs: calibrating trust, improving their task\nskills, changing their behavior to supply better inputs to the AI, and giving\nconstructive feedback to developers. Finally, among existing XAI approaches,\nparticipants preferred part-based explanations that resemble human reasoning\nand explanations. We discuss the implications of our findings and provide\nrecommendations for future XAI design.</p>\n", "tags": [] },
{"key": "kiperwasser2018scheduled", "citations": "81", "year": "2018", "title":"Scheduled Multi-task Learning: From Syntax To Translation", "abstract": "<p>Neural encoder-decoder models of machine translation have achieved impressive\nresults, while learning linguistic knowledge of both the source and target\nlanguages in an implicit end-to-end manner. We propose a framework in which our\nmodel begins learning syntax and translation interleaved, gradually putting\nmore focus on translation. Using this approach, we achieve considerable\nimprovements in terms of BLEU score on relatively large parallel corpus (WMT14\nEnglish to German) and a low-resource (WIT German to English) setup.</p>\n", "tags": ["Datasets","TACL","Tools"] },
{"key": "kirstain2021coreference", "citations": "60", "year": "2021", "title":"Coreference Resolution Without Span Representations", "abstract": "<p>The introduction of pretrained language models has reduced many complex\ntask-specific NLP models to simple lightweight layers. An exception to this\ntrend is coreference resolution, where a sophisticated task-specific model is\nappended to a pretrained transformer encoder. While highly effective, the model\nhas a very large memory footprint – primarily due to dynamically-constructed\nspan and span-pair representations – which hinders the processing of complete\ndocuments and the ability to train on multiple instances in a single batch. We\nintroduce a lightweight end-to-end coreference model that removes the\ndependency on span representations, handcrafted features, and heuristics. Our\nmodel performs competitively with the current standard model, while being\nsimpler and more efficient.</p>\n", "tags": ["Model Architecture"] },
{"key": "kitaev2018multilingual", "citations": "215", "year": "2019", "title":"Multilingual Constituency Parsing With Self-attention And Pre-training", "abstract": "<p>We show that constituency parsing benefits from unsupervised pre-training\nacross a variety of languages and a range of pre-training conditions. We first\ncompare the benefits of no pre-training, fastText, ELMo, and BERT for English\nand find that BERT outperforms ELMo, in large part due to increased model\ncapacity, whereas ELMo in turn outperforms the non-contextual fastText\nembeddings. We also find that pre-training is beneficial across all 11\nlanguages tested; however, large model sizes (more than 100 million parameters)\nmake it computationally expensive to train separate models for each language.\nTo address this shortcoming, we show that joint multilingual pre-training and\nfine-tuning allows sharing all but a small number of parameters between ten\nlanguages in the final model. The 10x reduction in model size compared to\nfine-tuning one model per language causes only a 3.2% relative error increase\nin aggregate. We further explore the idea of joint fine-tuning and show that it\ngives low-resource languages a way to benefit from the larger datasets of other\nlanguages. Finally, we demonstrate new state-of-the-art results for 11\nlanguages, including English (95.8 F1) and Chinese (91.8 F1).</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "kitaev2020reformer", "citations": "897", "year": "2020", "title":"Reformer: The Efficient Transformer", "abstract": "<p>Large Transformer models routinely achieve state-of-the-art results on a\nnumber of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve the\nefficiency of Transformers. For one, we replace dot-product attention by one\nthat uses locality-sensitive hashing, changing its complexity from O(\\(L^2\\)) to\nO(\\(Llog L\\)), where \\(L\\) is the length of the sequence. Furthermore, we use\nreversible residual layers instead of the standard residuals, which allows\nstoring activations only once in the training process instead of \\(N\\) times,\nwhere \\(N\\) is the number of layers. The resulting model, the Reformer, performs\non par with Transformer models while being much more memory-efficient and much\nfaster on long sequences.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "klein2017opennmt", "citations": "1819", "year": "2017", "title":"Opennmt: Open-source Toolkit For Neural Machine Translation", "abstract": "<p>We introduce an open-source toolkit for neural machine translation (NMT) to\nsupport research into model architectures, feature representations, and source\nmodalities, while maintaining competitive performance, modularity and\nreasonable training requirements.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "klein2018opennmt", "citations": "67", "year": "2018", "title":"Opennmt: Neural Machine Translation Toolkit", "abstract": "<p>OpenNMT is an open-source toolkit for neural machine translation (NMT). The\nsystem prioritizes efficiency, modularity, and extensibility with the goal of\nsupporting NMT research into model architectures, feature representations, and\nsource modalities, while maintaining competitive performance and reasonable\ntraining requirements. The toolkit consists of modeling and translation\nsupport, as well as detailed pedagogical documentation about the underlying\ntechniques. OpenNMT has been used in several production MT systems, modified\nfor numerous research papers, and is implemented across several deep learning\nframeworks.</p>\n", "tags": ["Efficiency","Model Architecture","Tools","Training Techniques"] },
{"key": "klerke2016improving", "citations": "101", "year": "2016", "title":"Improving Sentence Compression By Learning To Predict Gaze", "abstract": "<p>We show how eye-tracking corpora can be used to improve sentence compression\nmodels, presenting a novel multi-task learning algorithm based on multi-layer\nLSTMs. We obtain performance competitive with or better than state-of-the-art\napproaches.</p>\n", "tags": ["NAACL"] },
{"key": "klubička2017fine", "citations": "84", "year": "2017", "title":"Fine-grained Human Evaluation Of Neural Versus Phrase-based Machine Translation", "abstract": "<p>We compare three approaches to statistical machine translation (pure\nphrase-based, factored phrase-based and neural) by performing a fine-grained\nmanual evaluation via error annotation of the systems’ outputs. The error types\nin our annotation are compliant with the multidimensional quality metrics\n(MQM), and the annotation is performed by two annotators. Inter-annotator\nagreement is high for such a task, and results show that the best performing\nsystem (neural) reduces the errors produced by the worst system (phrase-based)\nby 54%.</p>\n", "tags": ["Evaluation","RAG"] },
{"key": "kochkina2017turing", "citations": "116", "year": "2017", "title":"Turing At Semeval-2017 Task 8: Sequential Approach To Rumour Stance Classification With Branch-lstm", "abstract": "<p>This paper describes team Turing’s submission to SemEval 2017 RumourEval:\nDetermining rumour veracity and support for rumours (SemEval 2017 Task 8,\nSubtask A). Subtask A addresses the challenge of rumour stance classification,\nwhich involves identifying the attitude of Twitter users towards the\ntruthfulness of the rumour they are discussing. Stance classification is\nconsidered to be an important step towards rumour verification, therefore\nperforming well in this task is expected to be useful in debunking false\nrumours. In this work we classify a set of Twitter posts discussing rumours\ninto either supporting, denying, questioning or commenting on the underlying\nrumours. We propose a LSTM-based sequential model that, through modelling the\nconversational structure of tweets, which achieves an accuracy of 0.784 on the\nRumourEval test set outperforming all other systems in Subtask A.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "kocijan2019surprisingly", "citations": "109", "year": "2019", "title":"A Surprisingly Robust Trick For Winograd Schema Challenge", "abstract": "<p>The Winograd Schema Challenge (WSC) dataset WSC273 and its inference\ncounterpart WNLI are popular benchmarks for natural language understanding and\ncommonsense reasoning. In this paper, we show that the performance of three\nlanguage models on WSC273 strongly improves when fine-tuned on a similar\npronoun disambiguation problem dataset (denoted WSCR). We additionally generate\na large unsupervised WSC-like dataset. By fine-tuning the BERT language model\nboth on the introduced and on the WSCR dataset, we achieve overall accuracies\nof 72.5% and 74.7% on WSC273 and WNLI, improving the previous state-of-the-art\nsolutions by 8.8% and 9.6%, respectively. Furthermore, our fine-tuned models\nare also consistently more robust on the “complex” subsets of WSC273,\nintroduced by Trichelair et al. (2018).</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "kocmi2017curriculum", "citations": "97", "year": "2017", "title":"Curriculum Learning And Minibatch Bucketing In Neural Machine Translation", "abstract": "<p>We examine the effects of particular orderings of sentence pairs on the\non-line training of neural machine translation (NMT). We focus on two types of\nsuch orderings: (1) ensuring that each minibatch contains sentences similar in\nsome aspect and (2) gradual inclusion of some sentence types as the training\nprogresses (so called “curriculum learning”). In our English-to-Czech\nexperiments, the internal homogeneity of minibatches has no effect on the\ntraining but some of our “curricula” achieve a small improvement over the\nbaseline.</p>\n", "tags": ["Training Techniques"] },
{"key": "kocmi2018trivial", "citations": "157", "year": "2018", "title":"Trivial Transfer Learning For Low-resource Neural Machine Translation", "abstract": "<p>Transfer learning has been proven as an effective technique for neural\nmachine translation under low-resource conditions. Existing methods require a\ncommon target language, language relatedness, or specific training tricks and\nregimes. We present a simple transfer learning method, where we first train a\n“parent” model for a high-resource language pair and then continue the training\non a lowresource pair only by replacing the training corpus. This “child” model\nperforms significantly better than the baseline trained for lowresource pair\nonly. We are the first to show this for targeting different languages, and we\nobserve the improvements even for unrelated languages with different alphabets.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "kocmi2023large", "citations": "85", "year": "2023", "title":"Large Language Models Are State-of-the-art Evaluators Of Translation Quality", "abstract": "<p>We describe GEMBA, a GPT-based metric for assessment of translation quality,\nwhich works both with a reference translation and without. In our evaluation,\nwe focus on zero-shot prompting, comparing four prompt variants in two modes,\nbased on the availability of the reference. We investigate nine versions of GPT\nmodels, including ChatGPT and GPT-4. We show that our method for translation\nquality assessment only works with GPT~3.5 and larger models. Comparing to\nresults from WMT22’s Metrics shared task, our method achieves state-of-the-art\naccuracy in both modes when compared to MQM-based human labels. Our results are\nvalid on the system level for all three WMT22 Metrics shared task language\npairs, namely English into German, English into Russian, and Chinese into\nEnglish. This provides a first glimpse into the usefulness of pre-trained,\ngenerative large language models for quality assessment of translations. We\npublicly release all our code and prompt templates used for the experiments\ndescribed in this work, as well as all corresponding scoring results, to allow\nfor external validation and reproducibility.</p>\n", "tags": ["Evaluation","Model Architecture","Prompting"] },
{"key": "kojima2022large", "citations": "907", "year": "2022", "title":"Large Language Models Are Zero-shot Reasoners", "abstract": "<p>Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs’ ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding “Let’s think step by step”\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Prompting"] },
{"key": "kolluru2020imojie", "citations": "68", "year": "2020", "title":"Imojie: Iterative Memory-based Joint Open Information Extraction", "abstract": "<p>While traditional systems for Open Information Extraction were statistical\nand rule-based, recently neural models have been introduced for the task. Our\nwork builds upon CopyAttention, a sequence generation OpenIE model (Cui et.\nal., 2018). Our analysis reveals that CopyAttention produces a constant number\nof extractions per sentence, and its extracted tuples often express redundant\ninformation.\n  We present IMoJIE, an extension to CopyAttention, which produces the next\nextraction conditioned on all previously extracted tuples. This approach\novercomes both shortcomings of CopyAttention, resulting in a variable number of\ndiverse extractions per sentence. We train IMoJIE on training data bootstrapped\nfrom extractions of several non-neural systems, which have been automatically\nfiltered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by\nabout 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a\nnew state of the art for the task.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "komeili2021internet", "citations": "143", "year": "2022", "title":"Internet-augmented Dialogue Generation", "abstract": "<p>The largest store of continually updating knowledge on our planet can be\naccessed via internet search. In this work we study giving access to this\ninformation to conversational agents. Large language models, even though they\nstore an impressive amount of knowledge within their weights, are known to\nhallucinate facts when generating dialogue (Shuster et al., 2021); moreover,\nthose facts are frozen in time at the point of model training. In contrast, we\npropose an approach that learns to generate an internet search query based on\nthe context, and then conditions on the search results to finally generate a\nresponse, a method that can employ up-to-the-minute relevant information. We\ntrain and evaluate such models on a newly collected dataset of human-human\nconversations whereby one of the speakers is given access to internet search\nduring knowledgedriven discussions in order to ground their responses. We find\nthat search-query based access of the internet in conversation provides\nsuperior performance compared to existing approaches that either use no\naugmentation or FAISS-based retrieval (Lewis et al., 2020).</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Training Techniques"] },
{"key": "konstas2017neural", "citations": "271", "year": "2017", "title":"Neural AMR: Sequence-to-sequence Models For Parsing And Generation", "abstract": "<p>Sequence-to-sequence models have shown strong performance across a broad\nrange of applications. However, their application to parsing and generating\ntext usingAbstract Meaning Representation (AMR)has been limited, due to the\nrelatively limited amount of labeled data and the non-sequential nature of the\nAMR graphs. We present a novel training procedure that can lift this limitation\nusing millions of unlabeled sentences and careful preprocessing of the AMR\ngraphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH,\nthe current best score reported without significant use of external semantic\nresources. For AMR generation, our model establishes a new state-of-the-art\nperformance of BLEU 33.8. We present extensive ablative and qualitative\nanalysis including strong evidence that sequence-based AMR models are robust\nagainst ordering variations of graph-to-sequence conversions.</p>\n", "tags": ["Applications","Training Techniques"] },
{"key": "kotek2023gender", "citations": "155", "year": "2023", "title":"Gender Bias And Stereotypes In Large Language Models", "abstract": "<p>Large Language Models (LLMs) have made substantial progress in the past\nseveral months, shattering state-of-the-art benchmarks in many domains. This\npaper investigates LLMs’ behavior with respect to gender stereotypes, a known\nissue for prior models. We use a simple paradigm to test the presence of gender\nbias, building on but differing from WinoBias, a commonly used gender bias\ndataset, which is likely to be included in the training data of current LLMs.\nWe test four recently published LLMs and demonstrate that they express biased\nassumptions about men and women’s occupations. Our contributions in this paper\nare as follows: (a) LLMs are 3-6 times more likely to choose an occupation that\nstereotypically aligns with a person’s gender; (b) these choices align with\npeople’s perceptions better than with the ground truth as reflected in official\njob statistics; (c) LLMs in fact amplify the bias beyond what is reflected in\nperceptions or the ground truth; (d) LLMs ignore crucial ambiguities in\nsentence structure 95% of the time in our study items, but when explicitly\nprompted, they recognize the ambiguity; (e) LLMs provide explanations for their\nchoices that are factually inaccurate and likely obscure the true reason behind\ntheir predictions. That is, they provide rationalizations of their biased\nbehavior. This highlights a key property of these models: LLMs are trained on\nimbalanced datasets; as such, even with the recent successes of reinforcement\nlearning with human feedback, they tend to reflect those imbalances back at us.\nAs with other types of societal biases, we suggest that LLMs must be carefully\ntested to ensure that they treat minoritized individuals and communities\nequitably.</p>\n", "tags": ["Datasets","Ethics & Fairness","Training Techniques"] },
{"key": "koto2020indolem", "citations": "168", "year": "2020", "title":"Indolem And Indobert: A Benchmark Dataset And Pre-trained Language Model For Indonesian NLP", "abstract": "<p>Although the Indonesian language is spoken by almost 200 million people and\nthe 10th most spoken language in the world, it is under-represented in NLP\nresearch. Previous work on Indonesian has been hampered by a lack of annotated\ndatasets, a sparsity of language resources, and a lack of resource\nstandardization. In this work, we release the IndoLEM dataset comprising seven\ntasks for the Indonesian language, spanning morpho-syntax, semantics, and\ndiscourse. We additionally release IndoBERT, a new pre-trained language model\nfor Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it\nagainst existing resources. Our experiments show that IndoBERT achieves\nstate-of-the-art performance over most of the tasks in IndoLEM.</p>\n", "tags": ["COLING","Datasets","Evaluation"] },
{"key": "kottur2018visual", "citations": "185", "year": "2018", "title":"Visual Coreference Resolution In Visual Dialog Using Neural Module Networks", "abstract": "<p>Visual dialog entails answering a series of questions grounded in an image,\nusing dialog history as context. In addition to the challenges found in visual\nquestion answering (VQA), which can be seen as one-round dialog, visual dialog\nencompasses several more. We focus on one such problem called visual\ncoreference resolution that involves determining which words, typically noun\nphrases and pronouns, co-refer to the same entity/object instance in an image.\nThis is crucial, especially for pronouns (e.g., <code class=\"language-plaintext highlighter-rouge\">it'), as the dialog agent must\nfirst link it to a previous coreference (e.g., </code>boat’), and only then can rely\non the visual grounding of the coreference <code class=\"language-plaintext highlighter-rouge\">boat' to reason about the pronoun\n</code>it’. Prior work (in visual dialog) models visual coreference resolution either\n(a) implicitly via a memory network over history, or (b) at a coarse level for\nthe entire question; and not explicitly at a phrase level of granularity. In\nthis work, we propose a neural module network architecture for visual dialog by\nintroducing two novel modules - Refer and Exclude - that perform explicit,\ngrounded, coreference resolution at a finer word level. We demonstrate the\neffectiveness of our model on MNIST Dialog, a visually simple yet\ncoreference-wise complex dataset, by achieving near perfect accuracy, and on\nVisDial, a large and challenging visual dialog dataset on real images, where\nour model outperforms other approaches, and is more interpretable, grounded,\nand consistent qualitatively.</p>\n", "tags": ["Agentic","Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "kovaleva2019revealing", "citations": "495", "year": "2019", "title":"Revealing The Dark Secrets Of BERT", "abstract": "<p>BERT-based architectures currently give state-of-the-art performance on many\nNLP tasks, but little is known about the exact mechanisms that contribute to\nits success. In the current work, we focus on the interpretation of\nself-attention, which is one of the fundamental underlying components of BERT.\nUsing a subset of GLUE tasks and a set of handcrafted features-of-interest, we\npropose the methodology and carry out a qualitative and quantitative analysis\nof the information encoded by the individual BERT’s heads. Our findings suggest\nthat there is a limited set of attention patterns that are repeated across\ndifferent heads, indicating the overall model overparametrization. While\ndifferent heads consistently use the same attention patterns, they have varying\nimpact on performance across different tasks. We show that manually disabling\nattention in certain heads leads to a performance improvement over the regular\nfine-tuned BERT models.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "kočiský2017narrativeqa", "citations": "526", "year": "2018", "title":"The Narrativeqa Reading Comprehension Challenge", "abstract": "<p>Reading comprehension (RC)—in contrast to information retrieval—requires\nintegrating information and reasoning about events, entities, and their\nrelations across a full document. Question answering is conventionally used to\nassess RC ability, in both artificial agents and children learning to read.\nHowever, existing RC datasets and tasks are dominated by questions that can be\nsolved by selecting answers using superficial information (e.g., local context\nsimilarity or global term frequency); they thus fail to test for the essential\nintegrative aspect of RC. To encourage progress on deeper comprehension of\nlanguage, we present a new dataset and set of tasks in which the reader must\nanswer questions about stories by reading entire books or movie scripts. These\ntasks are designed so that successfully answering their questions requires\nunderstanding the underlying narrative rather than relying on shallow pattern\nmatching or salience. We show that although humans solve the tasks easily,\nstandard RC models struggle on the tasks presented here. We provide an analysis\nof the dataset and the challenges it presents.</p>\n", "tags": ["Datasets","TACL"] },
{"key": "kratzwald2018adaptive", "citations": "60", "year": "2018", "title":"Adaptive Document Retrieval For Deep Question Answering", "abstract": "<p>State-of-the-art systems in deep question answering proceed as follows: (1)\nan initial document retrieval selects relevant documents, which (2) are then\nprocessed by a neural network in order to extract the final answer. Yet the\nexact interplay between both components is poorly understood, especially\nconcerning the number of candidate documents that should be retrieved. We show\nthat choosing a static number of documents – as used in prior research –\nsuffers from a noise-information trade-off and yields suboptimal results. As a\nremedy, we propose an adaptive document retrieval model. This learns the\noptimal candidate number for document retrieval, conditional on the size of the\ncorpus and the query. We report extensive experimental results showing that our\nadaptive approach outperforms state-of-the-art methods on multiple benchmark\ndatasets, as well as in the context of corpora with variable sizes.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Retrieval Systems"] },
{"key": "krause2016hierarchical", "citations": "375", "year": "2017", "title":"A Hierarchical Approach For Generating Descriptive Image Paragraphs", "abstract": "<p>Recent progress on image captioning has made it possible to generate novel\nsentences describing images in natural language, but compressing an image into\na single sentence can describe visual content in only coarse detail. While one\nnew captioning approach, dense captioning, can potentially describe images in\nfiner levels of detail by captioning many regions within an image, it in turn\nis unable to produce a coherent story for an image. In this paper we overcome\nthese limitations by generating entire paragraphs for describing images, which\ncan tell detailed, unified stories. We develop a model that decomposes both\nimages and paragraphs into their constituent parts, detecting semantic regions\nin images and using a hierarchical recurrent neural network to reason about\nlanguage. Linguistic analysis confirms the complexity of the paragraph\ngeneration task, and thorough experiments on a new dataset of image and\nparagraph pairs demonstrate the effectiveness of our approach.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "krause2016multiplicative", "citations": "97", "year": "2016", "title":"Multiplicative LSTM For Sequence Modelling", "abstract": "<p>We introduce multiplicative LSTM (mLSTM), a recurrent neural network\narchitecture for sequence modelling that combines the long short-term memory\n(LSTM) and multiplicative recurrent neural network architectures. mLSTM is\ncharacterised by its ability to have different recurrent transition functions\nfor each possible input, which we argue makes it more expressive for\nautoregressive density estimation. We demonstrate empirically that mLSTM\noutperforms standard LSTM and its deep variants for a range of character level\nlanguage modelling tasks. In this version of the paper, we regularise mLSTM to\nachieve 1.27 bits/char on text8 and 1.24 bits/char on Hutter Prize. We also\napply a purely byte-level mLSTM on the WikiText-2 dataset to achieve a\ncharacter level entropy of 1.26 bits/char, corresponding to a word level\nperplexity of 88.8, which is comparable to word level LSTMs regularised in\nsimilar ways on the same task.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "krause2020gedi", "citations": "194", "year": "2021", "title":"Gedi: Generative Discriminator Guided Sequence Generation", "abstract": "<p>While large-scale language models (LMs) are able to imitate the distribution\nof natural language well enough to generate realistic text, it is difficult to\ncontrol which regions of the distribution they generate. This is especially\nproblematic because datasets used for training large LMs usually contain\nsignificant toxicity, hate, bias, and negativity. We propose GeDi as an\nefficient method for using smaller LMs as generative discriminators to guide\ngeneration from large LMs to make them safer and more controllable. GeDi guides\ngeneration at each step by computing classification probabilities for all\npossible next tokens via Bayes rule by normalizing over two class-conditional\ndistributions; one conditioned on the desired attribute, or control code, and\nanother conditioned on the undesired attribute, or anti control code. We find\nthat GeDi gives stronger controllability than the state of the art method while\nalso achieving generation speeds more than 30 times faster. Additionally,\ntraining GeDi on only four topics allows us to controllably generate new topics\nzero-shot from just a keyword, unlocking a new capability that previous\ncontrollable generation methods do not have. Lastly, we show that GeDi can make\nGPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic\nquality, making it by far the most practical existing method for detoxifying\nlarge language models while maintaining a fast generation speed.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "kreutzer2019joey", "citations": "90", "year": "2019", "title":"Joey NMT: A Minimalist NMT Toolkit For Novices", "abstract": "<p>We present Joey NMT, a minimalist neural machine translation toolkit based on\nPyTorch that is specifically designed for novices. Joey NMT provides many\npopular NMT features in a small and simple code base, so that novices can\neasily and quickly learn to use it and adapt it to their needs. Despite its\nfocus on simplicity, Joey NMT supports classic architectures (RNNs,\ntransformers), fast beam search, weight tying, and more, and achieves\nperformance comparable to more complex toolkits on standard benchmarks. We\nevaluate the accessibility of our toolkit in a user study where novices with\ngeneral knowledge about Pytorch and NMT and experts work through a\nself-contained Joey NMT tutorial, showing that novices perform almost as well\nas experts in a subsequent code quiz. Joey NMT is available at\nhttps://github.com/joeynmt/joeynmt .</p>\n", "tags": ["EMNLP","Has Code"] },
{"key": "kreutzer2021quality", "citations": "148", "year": "2022", "title":"Quality At A Glance: An Audit Of Web-crawled Multilingual Datasets", "abstract": "<p>With the success of large-scale pre-training and multilingual modeling in\nNatural Language Processing (NLP), recent years have seen a proliferation of\nlarge, web-mined text datasets covering hundreds of languages. We manually\naudit the quality of 205 language-specific corpora released with five major\npublic datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource\ncorpora have systematic issues: At least 15 corpora have no usable text, and a\nsignificant fraction contains less than 50% sentences of acceptable quality. In\naddition, many are mislabeled or use nonstandard/ambiguous language codes. We\ndemonstrate that these issues are easy to detect even for non-proficient\nspeakers, and supplement the human audit with automatic analyses. Finally, we\nrecommend techniques to evaluate and improve multilingual corpora and discuss\npotential risks that come with low-quality data releases.</p>\n", "tags": ["Datasets","TACL","Training Techniques"] },
{"key": "krishna2019information", "citations": "89", "year": "2019", "title":"Information Maximizing Visual Question Generation", "abstract": "<p>Though image-to-sequence generation models have become overwhelmingly popular\nin human-computer communications, they suffer from strongly favoring safe\ngeneric questions (“What is in this picture?”). Generating uninformative but\nrelevant questions is not sufficient or useful. We argue that a good question\nis one that has a tightly focused purpose — one that is aimed at expecting a\nspecific type of response. We build a model that maximizes mutual information\nbetween the image, the expected answer and the generated question. To overcome\nthe non-differentiability of discrete natural language tokens, we introduce a\nvariational continuous latent space onto which the expected answers project. We\nregularize this latent space with a second latent space that ensures clustering\nof similar answers. Even when we don’t know the expected answer, this second\nlatent space can generate goal-driven questions specifically aimed at\nextracting objects (“what is the person throwing”), attributes, (“What kind of\nshirt is the person wearing?”), color (“what color is the frisbee?”), material\n(“What material is the frisbee?”), etc. We quantitatively show that our model\nis able to retain information about an expected answer category, resulting in\nmore diverse, goal-driven questions. We launch our model on a set of real world\nimages and extract previously unseen visual concepts.</p>\n", "tags": ["CVPR"] },
{"key": "krishna2021hurdles", "citations": "103", "year": "2021", "title":"Hurdles To Progress In Long-form Question Answering", "abstract": "<p>The task of long-form question answering (LFQA) involves retrieving documents\nrelevant to a given question and using them to generate a paragraph-length\nanswer. While many models have recently been proposed for LFQA, we show in this\npaper that the task formulation raises fundamental challenges regarding\nevaluation and dataset creation that currently preclude meaningful modeling\nprogress. To demonstrate these challenges, we first design a new system that\nrelies on sparse attention and contrastive retriever learning to achieve\nstate-of-the-art performance on the ELI5 LFQA dataset. While our system tops\nthe public leaderboard, a detailed analysis reveals several troubling trends:\n(1) our system’s generated answers are not actually grounded in the documents\nthat it retrieves; (2) ELI5 contains significant train / validation overlap, as\nat least 81% of ELI5 validation questions occur in paraphrased form in the\ntraining set; (3) ROUGE-L is not an informative metric of generated answer\nquality and can be easily gamed; and (4) human evaluations used for other text\ngeneration tasks are unreliable for LFQA. We offer suggestions to mitigate each\nof these issues, which we hope will lead to more rigorous LFQA research and\nmeaningful progress in the future.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","NAACL","Retrieval Systems","Training Techniques"] },
{"key": "kriz2019complexity", "citations": "62", "year": "2019", "title":"Complexity-weighted Loss And Diverse Reranking For Sentence Simplification", "abstract": "<p>Sentence simplification is the task of rewriting texts so they are easier to\nunderstand. Recent research has applied sequence-to-sequence (Seq2Seq) models\nto this task, focusing largely on training-time improvements via reinforcement\nlearning and memory augmentation. One of the main problems with applying\ngeneric Seq2Seq models for simplification is that these models tend to copy\ndirectly from the original sentence, resulting in outputs that are relatively\nlong and complex. We aim to alleviate this issue through the use of two main\ntechniques. First, we incorporate content word complexities, as predicted with\na leveled word complexity model, into our loss function during training.\nSecond, we generate a large set of diverse candidate simplifications at test\ntime, and rerank these to promote fluency, adequacy, and simplicity. Here, we\nmeasure simplicity through a novel sentence complexity model. These extensions\nallow our models to perform competitively with state-of-the-art systems while\ngenerating simpler sentences. We report standard automatic and human evaluation\nmetrics.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "kruk2019integrating", "citations": "93", "year": "2019", "title":"Integrating Text And Image: Determining Multimodal Document Intent In Instagram Posts", "abstract": "<p>Computing author intent from multimodal data like Instagram posts requires\nmodeling a complex relationship between text and image. For example, a caption\nmight evoke an ironic contrast with the image, so neither caption nor image is\na mere transcript of the other. Instead they combine – via what has been\ncalled meaning multiplication – to create a new meaning that has a more\ncomplex relation to the literal meanings of text and image. Here we introduce a\nmultimodal dataset of 1299 Instagram posts labeled for three orthogonal\ntaxonomies: the authorial intent behind the image-caption pair, the contextual\nrelationship between the literal meanings of the image and caption, and the\nsemiotic relationship between the signified meanings of the image and caption.\nWe build a baseline deep multimodal classifier to validate the taxonomy,\nshowing that employing both text and image improves intent detection by 9.6%\ncompared to using only the image modality, demonstrating the commonality of\nnon-intersective meaning multiplication. The gain with multimodality is\ngreatest when the image and caption diverge semiotically. Our dataset offers a\nnew resource for the study of the rich meanings that result from pairing text\nand image.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "kryściński2018improving", "citations": "151", "year": "2018", "title":"Improving Abstraction In Text Summarization", "abstract": "<p>Abstractive text summarization aims to shorten long text documents into a\nhuman readable form that contains the most important facts from the original\ndocument. However, the level of actual abstraction as measured by novel phrases\nthat do not appear in the source document remains low in existing approaches.\nWe propose two techniques to improve the level of abstraction of generated\nsummaries. First, we decompose the decoder into a contextual network that\nretrieves relevant parts of the source document, and a pretrained language\nmodel that incorporates prior knowledge about language generation. Second, we\npropose a novelty metric that is optimized directly through policy learning to\nencourage the generation of novel phrases. Our model achieves results\ncomparable to state-of-the-art models, as determined by ROUGE scores and human\nevaluations, while achieving a significantly higher level of abstraction as\nmeasured by n-gram overlap with the source document.</p>\n", "tags": ["EMNLP"] },
{"key": "kryściński2019neural", "citations": "344", "year": "2019", "title":"Neural Text Summarization: A Critical Evaluation", "abstract": "<p>Text summarization aims at compressing long documents into a shorter form\nthat conveys the most important parts of the original document. Despite\nincreased interest in the community and notable research effort, progress on\nbenchmark datasets has stagnated. We critically evaluate key ingredients of the\ncurrent research setup: datasets, evaluation metrics, and models, and highlight\nthree primary shortcomings: 1) automatically collected datasets leave the task\nunderconstrained and may contain noise detrimental to training and evaluation,\n2) current evaluation protocol is weakly correlated with human judgment and\ndoes not account for important characteristics such as factual correctness, 3)\nmodels overfit to layout biases of current datasets and offer limited diversity\nin their outputs.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "ku2020room", "citations": "157", "year": "2020", "title":"Room-across-room: Multilingual Vision-and-language Navigation With Dense Spatiotemporal Grounding", "abstract": "<p>We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation\n(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger\n(more paths and instructions) than other VLN datasets. It emphasizes the role\nof language in VLN by addressing known biases in paths and eliciting more\nreferences to visible entities. Furthermore, each word in an instruction is\ntime-aligned to the virtual poses of instruction creators and validators. We\nestablish baseline scores for monolingual and multilingual settings and\nmultitask learning when including Room-to-Room annotations. We also provide\nresults for a model that learns from synchronized pose traces by focusing only\non portions of the panorama attended to in human demonstrations. The size,\nscope and detail of RxR dramatically expands the frontier for research on\nembodied language agents in simulated, photo-realistic environments.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "kuang2017modeling", "citations": "87", "year": "2017", "title":"Modeling Coherence For Neural Machine Translation With Dynamic And Topic Caches", "abstract": "<p>Sentences in a well-formed text are connected to each other via various links\nto form the cohesive structure of the text. Current neural machine translation\n(NMT) systems translate a text in a conventional sentence-by-sentence fashion,\nignoring such cross-sentence links and dependencies. This may lead to generate\nan incoherent target text for a coherent source text. In order to handle this\nissue, we propose a cache-based approach to modeling coherence for neural\nmachine translation by capturing contextual information either from recently\ntranslated sentences or the entire document. Particularly, we explore two types\nof caches: a dynamic cache, which stores words from the best translation\nhypotheses of preceding sentences, and a topic cache, which maintains a set of\ntarget-side topical words that are semantically related to the document to be\ntranslated. On this basis, we build a new layer to score target words in these\ntwo caches with a cache-based neural model. Here the estimated probabilities\nfrom the cache-based neural model are combined with NMT probabilities into the\nfinal word prediction probabilities via a gating mechanism. Finally, the\nproposed cache-based neural model is trained jointly with NMT system in an\nend-to-end manner. Experiments and analysis presented in this paper demonstrate\nthat the proposed cache-based model achieves substantial improvements over\nseveral state-of-the-art SMT and NMT baselines.</p>\n", "tags": ["Memory & Context"] },
{"key": "kuchaiev2019nemo", "citations": "164", "year": "2019", "title":"Nemo: A Toolkit For Building AI Applications Using Neural Modules", "abstract": "<p>NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI\napplications through re-usability, abstraction, and composition. NeMo is built\naround neural modules, conceptual blocks of neural networks that take typed\ninputs and produce typed outputs. Such modules typically represent data layers,\nencoders, decoders, language models, loss functions, or methods of combining\nactivations. NeMo makes it easy to combine and re-use these building blocks\nwhile providing a level of semantic correctness checking via its neural type\nsystem. The toolkit comes with extendable collections of pre-built modules for\nautomatic speech recognition and natural language processing. Furthermore, NeMo\nprovides built-in support for distributed training and mixed precision on\nlatest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo</p>\n", "tags": ["Applications","Has Code","Tools","Training Techniques"] },
{"key": "kudo2018sentencepiece", "citations": "2639", "year": "2018", "title":"Sentencepiece: A Simple And Language Independent Subword Tokenizer And Detokenizer For Neural Text Processing", "abstract": "<p>This paper describes SentencePiece, a language-independent subword tokenizer\nand detokenizer designed for Neural-based text processing, including Neural\nMachine Translation. It provides open-source C++ and Python implementations for\nsubword units. While existing subword segmentation tools assume that the input\nis pre-tokenized into word sequences, SentencePiece can train subword models\ndirectly from raw sentences, which allows us to make a purely end-to-end and\nlanguage independent system. We perform a validation experiment of NMT on\nEnglish-Japanese machine translation, and find that it is possible to achieve\ncomparable accuracy to direct subword training from raw sentences. We also\ncompare the performance of subword training and segmentation with various\nconfigurations. SentencePiece is available under the Apache 2 license at\nhttps://github.com/google/sentencepiece.</p>\n", "tags": ["EMNLP","Has Code","Tools","Training Techniques"] },
{"key": "kudugunta2019investigating", "citations": "108", "year": "2019", "title":"Investigating Multilingual NMT Representations At Scale", "abstract": "<p>Multilingual Neural Machine Translation (NMT) models have yielded large\nempirical success in transfer learning settings. However, these black-box\nrepresentations are poorly understood, and their mode of transfer remains\nelusive. In this work, we attempt to understand massively multilingual NMT\nrepresentations (with 103 languages) using Singular Value Canonical Correlation\nAnalysis (SVCCA), a representation similarity framework that allows us to\ncompare representations across different languages, layers and models. Our\nanalysis validates several empirical results and long-standing intuitions, and\nunveils new observations regarding how representations evolve in a multilingual\ntranslation model. We draw three major conclusions from our analysis, with\nimplications on cross-lingual transfer learning: (i) Encoder representations of\ndifferent languages cluster based on linguistic similarity, (ii)\nRepresentations of a source language learned by the encoder are dependent on\nthe target language, and vice-versa, and (iii) Representations of high resource\nand/or linguistically similar languages are more robust when fine-tuning on an\narbitrary language pair, which is critical to determining how much\ncross-lingual transfer can be expected in a zero or few-shot setting. We\nfurther connect our findings with existing empirical observations in\nmultilingual NMT and transfer learning.</p>\n", "tags": ["EMNLP","Few-Shot","Fine-Tuning","Tools","Training Techniques"] },
{"key": "kulkarni2018annotated", "citations": "62", "year": "2018", "title":"An Annotated Corpus For Machine Reading Of Instructions In Wet Lab Protocols", "abstract": "<p>We describe an effort to annotate a corpus of natural language instructions\nconsisting of 622 wet lab protocols to facilitate automatic or semi-automatic\nconversion of protocols into a machine-readable format and benefit biological\nresearch. Experimental results demonstrate the utility of our corpus for\ndeveloping machine learning approaches to shallow semantic parsing of\ninstructional texts. We make our annotated Wet Lab Protocol Corpus available to\nthe research community.</p>\n", "tags": ["Datasets","NAACL"] },
{"key": "kumar2019closer", "citations": "84", "year": "2019", "title":"A Closer Look At Feature Space Data Augmentation For Few-shot Intent Classification", "abstract": "<p>New conversation topics and functionalities are constantly being added to\nconversational AI agents like Amazon Alexa and Apple Siri. As data collection\nand annotation is not scalable and is often costly, only a handful of examples\nfor the new functionalities are available, which results in poor generalization\nperformance. We formulate it as a Few-Shot Integration (FSI) problem where a\nfew examples are used to introduce a new intent. In this paper, we study six\nfeature space data augmentation methods to improve classification performance\nin FSI setting in combination with both supervised and unsupervised\nrepresentation learning methods such as BERT. Through realistic experiments on\ntwo public conversational datasets, SNIPS, and the Facebook Dialog corpus, we\nshow that data augmentation in feature space provides an effective way to\nimprove intent classification performance in few-shot setting beyond\ntraditional transfer learning approaches. In particular, we show that (a)\nupsampling in latent space is a competitive baseline for feature space\naugmentation (b) adding the difference between two examples to a new example is\na simple yet effective data augmentation method.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Model Architecture"] },
{"key": "kumar2019reinforcement", "citations": "67", "year": "2019", "title":"Reinforcement Learning Based Curriculum Optimization For Neural Machine Translation", "abstract": "<p>We consider the problem of making efficient use of heterogeneous training\ndata in neural machine translation (NMT). Specifically, given a training\ndataset with a sentence-level feature such as noise, we seek an optimal\ncurriculum, or order for presenting examples to the system during training. Our\ncurriculum framework allows examples to appear an arbitrary number of times,\nand thus generalizes data weighting, filtering, and fine-tuning schemes. Rather\nthan relying on prior knowledge to design a curriculum, we use reinforcement\nlearning to learn one automatically, jointly with the NMT system, in the course\nof a single training run. We show that this approach can beat uniform and\nfiltering baselines on Paracrawl and WMT English-to-French datasets by up to\n+3.4 BLEU, and match the performance of a hand-designed, state-of-the-art\ncurriculum.</p>\n", "tags": ["Agentic","Datasets","Efficiency","Fine-Tuning","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "kumar2020data", "citations": "128", "year": "2020", "title":"Data Augmentation Using Pre-trained Transformer Models", "abstract": "<p>Language model based pre-trained models such as BERT have provided\nsignificant gains across different NLP tasks. In this paper, we study different\ntypes of transformer based pre-trained models such as auto-regressive models\n(GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional\ndata augmentation. We show that prepending the class labels to text sequences\nprovides a simple yet effective way to condition the pre-trained models for\ndata augmentation. Additionally, on three classification benchmarks,\npre-trained Seq2Seq model outperforms other data augmentation methods in a\nlow-resource setting. Further, we explore how different pre-trained model based\ndata augmentation differs in-terms of data diversity, and how well such methods\npreserve the class-label information.</p>\n", "tags": ["Model Architecture"] },
{"key": "kumar2020nile", "citations": "109", "year": "2020", "title":"NILE : Natural Language Inference With Faithful Natural Language Explanations", "abstract": "<p>The recent growth in the popularity and success of deep learning models on\nNLP classification tasks has accompanied the need for generating some form of\nnatural language explanation of the predicted labels. Such generated natural\nlanguage (NL) explanations are expected to be faithful, i.e., they should\ncorrelate well with the model’s internal decision making. In this work, we\nfocus on the task of natural language inference (NLI) and address the following\nquestion: can we build NLI systems which produce labels with high accuracy,\nwhile also generating faithful explanations of its decisions? We propose\nNatural-language Inference over Label-specific Explanations (NILE), a novel NLI\nmethod which utilizes auto-generated label-specific NL explanations to produce\nlabels along with its faithful explanation. We demonstrate NILE’s effectiveness\nover previously reported methods through automated and human evaluation of the\nproduced labels and explanations. Our evaluation of NILE also supports the\nclaim that accurate systems capable of providing testable explanations of their\ndecisions can be designed. We discuss the faithfulness of NILE’s explanations\nin terms of sensitivity of the decisions to the corresponding explanations. We\nargue that explicit evaluation of faithfulness, in addition to label and\nexplanation accuracy, is an important step in evaluating model’s explanations.\nFurther, we demonstrate that task-specific probes are necessary to establish\nsuch sensitivity.</p>\n", "tags": ["Evaluation"] },
{"key": "kumar2020syntax", "citations": "71", "year": "2020", "title":"Syntax-guided Controlled Generation Of Paraphrases", "abstract": "<p>Given a sentence (e.g., “I like mangoes”) and a constraint (e.g., sentiment\nflip), the goal of controlled text generation is to produce a sentence that\nadapts the input sentence to meet the requirements of the constraint (e.g., “I\nhate mangoes”). Going beyond such simple constraints, recent works have started\nexploring the incorporation of complex syntactic-guidance as constraints in the\ntask of controlled paraphrase generation. In these methods, syntactic-guidance\nis sourced from a separate exemplar sentence. However, these prior works have\nonly utilized limited syntactic information available in the parse tree of the\nexemplar sentence. We address this limitation in the paper and propose Syntax\nGuided Controlled Paraphraser (SGCP), an end-to-end framework for syntactic\nparaphrase generation. We find that SGCP can generate syntax conforming\nsentences while not compromising on relevance. We perform extensive automated\nand human evaluations over multiple real-world English language datasets to\ndemonstrate the efficacy of SGCP over state-of-the-art baselines. To drive\nfuture research, we have made SGCP’s source code available</p>\n", "tags": ["Datasets","Has Code","TACL","Tools"] },
{"key": "kuo2022beyond", "citations": "63", "year": "2022", "title":"Beyond A Pre-trained Object Detector: Cross-modal Textual And Visual Context For Image Captioning", "abstract": "<p>Significant progress has been made on visual captioning, largely relying on\npre-trained features and later fixed object detectors that serve as rich inputs\nto auto-regressive models. A key limitation of such methods, however, is that\nthe output of the model is conditioned only on the object detector’s outputs.\nThe assumption that such outputs can represent all necessary information is\nunrealistic, especially when the detector is transferred across datasets. In\nthis work, we reason about the graphical model induced by this assumption, and\npropose to add an auxiliary input to represent missing information such as\nobject relationships. We specifically propose to mine attributes and\nrelationships from the Visual Genome dataset and condition the captioning model\non them. Crucially, we propose (and show to be important) the use of a\nmulti-modal pre-trained model (CLIP) to retrieve such contextual descriptions.\nFurther, object detector models are frozen and do not have sufficient richness\nto allow the captioning model to properly ground them. As a result, we propose\nto condition both the detector and description outputs on the image, and show\nqualitatively and quantitatively that this can improve grounding. We validate\nour method on image captioning, perform thorough analyses of each component and\nimportance of the pre-trained multi-modal model, and demonstrate significant\nimprovements over the current state of the art, specifically +7.5% in CIDEr and\n+1.3% in BLEU-4 metrics.</p>\n", "tags": ["CVPR","Datasets","Evaluation"] },
{"key": "kuratov2019adaptation", "citations": "252", "year": "2019", "title":"Adaptation Of Deep Bidirectional Multilingual Transformers For Russian Language", "abstract": "<p>The paper introduces methods of adaptation of multilingual masked language\nmodels for a specific language. Pre-trained bidirectional language models show\nstate-of-the-art performance on a wide range of tasks including reading\ncomprehension, natural language inference, and sentiment analysis. At the\nmoment there are two alternative approaches to train such models: monolingual\nand multilingual. While language specific models show superior performance,\nmultilingual models allow to perform a transfer from one language to another\nand solve tasks for different languages simultaneously. This work shows that\ntransfer learning from a multilingual model to monolingual model results in\nsignificant growth of performance on such tasks as reading comprehension,\nparaphrase detection, and sentiment analysis. Furthermore, multilingual\ninitialization of monolingual model substantially reduces training time.\nPre-trained models for the Russian language are open sourced.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "kwon2023efficient", "citations": "259", "year": "2023", "title":"Efficient Memory Management For Large Language Model Serving With Pagedattention", "abstract": "<p>High throughput serving of large language models (LLMs) requires batching\nsufficiently many requests at a time. However, existing systems struggle\nbecause the key-value cache (KV cache) memory for each request is huge and\ngrows and shrinks dynamically. When managed inefficiently, this memory can be\nsignificantly wasted by fragmentation and redundant duplication, limiting the\nbatch size. To address this problem, we propose PagedAttention, an attention\nalgorithm inspired by the classical virtual memory and paging techniques in\noperating systems. On top of it, we build vLLM, an LLM serving system that\nachieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce memory usage. Our\nevaluations show that vLLM improves the throughput of popular LLMs by\n2-4\\(\\times\\) with the same level of latency compared to the state-of-the-art\nsystems, such as FasterTransformer and Orca. The improvement is more pronounced\nwith longer sequences, larger models, and more complex decoding algorithms.\nvLLM’s source code is publicly available at\nhttps://github.com/vllm-project/vllm</p>\n", "tags": ["Efficiency","Has Code","Memory & Context","Model Architecture","Tools"] },
{"key": "köbis2020artificial", "citations": "287", "year": "2020", "title":"Artificial Intelligence Versus Maya Angelou: Experimental Evidence That People Cannot Differentiate Ai-generated From Human-written Poetry", "abstract": "<p>The release of openly available, robust natural language generation\nalgorithms (NLG) has spurred much public attention and debate. One reason lies\nin the algorithms’ purported ability to generate human-like text across various\ndomains. Empirical evidence using incentivized tasks to assess whether people\n(a) can distinguish and (b) prefer algorithm-generated versus human-written\ntext is lacking. We conducted two experiments assessing behavioral reactions to\nthe state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal =\n830). Using the identical starting lines of human poems, GPT-2 produced samples\nof poems. From these samples, either a random poem was chosen\n(Human-out-of-the-loop) or the best one was selected (Human-in-the-loop) and in\nturn matched with a human-written poem. In a new incentivized version of the\nTuring Test, participants failed to reliably detect the\nalgorithmically-generated poems in the Human-in-the-loop treatment, yet\nsucceeded in the Human-out-of-the-loop treatment. Further, people reveal a\nslight aversion to algorithm-generated poetry, independent on whether\nparticipants were informed about the algorithmic origin of the poem\n(Transparency) or not (Opacity). We discuss what these results convey about the\nperformance of NLG algorithms to produce human-like text and propose\nmethodologies to study such learning algorithms in human-agent experimental\nsettings.</p>\n", "tags": ["Agentic","Ethics & Fairness","Model Architecture"] },
{"key": "köpf2023openassistant", "citations": "83", "year": "2023", "title":"Openassistant Conversations -- Democratizing Large Language Model Alignment", "abstract": "<p>Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.</p>\n", "tags": ["Agentic","Datasets","Fine-Tuning","Reinforcement Learning","Training Techniques"] },
{"key": "laban2021summac", "citations": "102", "year": "2022", "title":"Summac: Re-visiting Nli-based Models For Inconsistency Detection In Summarization", "abstract": "<p>In the summarization domain, a key requirement for summaries is to be\nfactually consistent with the input document. Previous work has found that\nnatural language inference (NLI) models do not perform competitively when\napplied to inconsistency detection. In this work, we revisit the use of NLI for\ninconsistency detection, finding that past work suffered from a mismatch in\ninput granularity between NLI datasets (sentence-level), and inconsistency\ndetection (document level). We provide a highly effective and light-weight\nmethod called SummaCConv that enables NLI models to be successfully used for\nthis task by segmenting documents into sentence units and aggregating scores\nbetween pairs of sentences. On our newly introduced benchmark called SummaC\n(Summary Consistency) consisting of six large inconsistency detection datasets,\nSummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%,\na 5% point improvement compared to prior work. We make the models and datasets\navailable: https://github.com/tingofurro/summac</p>\n", "tags": ["Datasets","Evaluation","Has Code","TACL"] },
{"key": "labrak2024biomistral", "citations": "67", "year": "2024", "title":"Biomistral: A Collection Of Open-source Pretrained Large Language Models For Medical Domains", "abstract": "<p>Large Language Models (LLMs) have demonstrated remarkable versatility in\nrecent years, offering potential applications across specialized domains such\nas healthcare and medicine. Despite the availability of various open-source\nLLMs tailored for health contexts, adapting general-purpose LLMs to the medical\ndomain presents significant challenges. In this paper, we introduce BioMistral,\nan open-source LLM tailored for the biomedical domain, utilizing Mistral as its\nfoundation model and further pre-trained on PubMed Central. We conduct a\ncomprehensive evaluation of BioMistral on a benchmark comprising 10 established\nmedical question-answering (QA) tasks in English. We also explore lightweight\nmodels obtained through quantization and model merging approaches. Our results\ndemonstrate BioMistral’s superior performance compared to existing open-source\nmedical models and its competitive edge against proprietary counterparts.\nFinally, to address the limited availability of data beyond English and to\nassess the multilingual generalization of medical LLMs, we automatically\ntranslated and evaluated this benchmark into 7 other languages. This marks the\nfirst large-scale multilingual evaluation of LLMs in the medical domain.\nDatasets, multilingual evaluation benchmarks, scripts, and all the models\nobtained during our experiments are freely released.</p>\n", "tags": ["Applications","Datasets","Evaluation"] },
{"key": "lachaux2020unsupervised", "citations": "162", "year": "2020", "title":"Unsupervised Translation Of Programming Languages", "abstract": "<p>A transcompiler, also known as source-to-source translator, is a system that\nconverts source code from a high-level programming language (such as C++ or\nPython) to another. Transcompilers are primarily used for interoperability, and\nto port codebases written in an obsolete or deprecated language (e.g. COBOL,\nPython 2) to a modern one. They typically rely on handcrafted rewrite rules,\napplied to the source code abstract syntax tree. Unfortunately, the resulting\ntranslations often lack readability, fail to respect the target language\nconventions, and require manual modifications in order to work properly. The\noverall translation process is timeconsuming and requires expertise in both the\nsource and target languages, making code-translation projects expensive.\nAlthough neural models significantly outperform their rule-based counterparts\nin the context of natural language translation, their applications to\ntranscompilation have been limited due to the scarcity of parallel data in this\ndomain. In this paper, we propose to leverage recent approaches in unsupervised\nmachine translation to train a fully unsupervised neural transcompiler. We\ntrain our model on source code from open source GitHub projects, and show that\nit can translate functions between C++, Java, and Python with high accuracy.\nOur method relies exclusively on monolingual source code, requires no expertise\nin the source or target languages, and can easily be generalized to other\nprogramming languages. We also build and release a test set composed of 852\nparallel functions, along with unit tests to check the correctness of\ntranslations. We show that our model outperforms rule-based commercial\nbaselines by a significant margin.</p>\n", "tags": ["Applications","Evaluation","Has Code"] },
{"key": "lage2019evaluation", "citations": "125", "year": "2019", "title":"An Evaluation Of The Human-interpretability Of Explanation", "abstract": "<p>Recent years have seen a boom in interest in machine learning systems that\ncan provide a human-understandable rationale for their predictions or\ndecisions. However, exactly what kinds of explanation are truly\nhuman-interpretable remains poorly understood. This work advances our\nunderstanding of what makes explanations interpretable under three specific\ntasks that users may perform with machine learning systems: simulation of the\nresponse, verification of a suggested response, and determining whether the\ncorrectness of a suggested response changes under a change to the inputs.\nThrough carefully controlled human-subject experiments, we identify\nregularizers that can be used to optimize for the interpretability of machine\nlearning systems. Our results show that the type of complexity matters:\ncognitive chunks (newly defined concepts) affect performance more than variable\nrepetitions, and these trends are consistent across tasks and domains. This\nsuggests that there may exist some common design principles for explanation\nsystems.</p>\n", "tags": ["Evaluation"] },
{"key": "lagunas2021block", "citations": "84", "year": "2021", "title":"Block Pruning For Faster Transformers", "abstract": "<p>Pre-training has improved model accuracy for both classification and\ngeneration tasks at the cost of introducing much larger and slower models.\nPruning methods have proven to be an effective way of reducing model size,\nwhereas distillation methods are proven for speeding up inference. We introduce\na block pruning approach targeting both small and fast models. Our approach\nextends structured methods by considering blocks of any size and integrates\nthis structure into the movement pruning paradigm for fine-tuning. We find that\nthis approach learns to prune out full components of the underlying model, such\nas attention heads. Experiments consider classification and generation tasks,\nyielding among other results a pruned model that is a 2.4x faster, 74% smaller\nBERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models\nin speed and pruned models in size.</p>\n", "tags": ["EMNLP","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "lai2017race", "citations": "955", "year": "2017", "title":"RACE: Large-scale Reading Comprehension Dataset From Examinations", "abstract": "<p>We present RACE, a new dataset for benchmark evaluation of methods in the\nreading comprehension task. Collected from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, RACE consists\nof near 28,000 passages and near 100,000 questions generated by human experts\n(English instructors), and covers a variety of topics which are carefully\ndesigned for evaluating the students’ ability in understanding and reasoning.\nIn particular, the proportion of questions that requires reasoning is much\nlarger in RACE than that in other benchmark datasets for reading comprehension,\nand there is a significant gap between the performance of the state-of-the-art\nmodels (43%) and the ceiling human performance (95%). We hope this new dataset\ncan serve as a valuable resource for research and evaluation in machine\ncomprehension. The dataset is freely available at\nhttp://www.cs.cmu.edu/~glai1/data/race/ and the code is available at\nhttps://github.com/qizhex/RACE_AR_baselines.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code"] },
{"key": "lai2020why", "citations": "100", "year": "2020", "title":"\"why Is 'chicago' Deceptive?\" Towards Building Model-driven Tutorials For Humans", "abstract": "<p>To support human decision making with machine learning models, we often need\nto elucidate patterns embedded in the models that are unsalient, unknown, or\ncounterintuitive to humans. While existing approaches focus on explaining\nmachine predictions with real-time assistance, we explore model-driven\ntutorials to help humans understand these patterns in a training phase. We\nconsider both tutorials with guidelines from scientific papers, analogous to\ncurrent practices of science communication, and automatically selected examples\nfrom training data with explanations. We use deceptive review detection as a\ntestbed and conduct large-scale, randomized human-subject experiments to\nexamine the effectiveness of such tutorials. We find that tutorials indeed\nimprove human performance, with and without real-time assistance. In\nparticular, although deep learning provides superior predictive performance\nthan simple models, tutorials and explanations from simple models are more\nuseful to humans. Our work suggests future directions for human-centered\ntutorials and explanations towards a synergy between humans and AI.</p>\n", "tags": ["Training Techniques"] },
{"key": "lai2023chatgpt", "citations": "116", "year": "2023", "title":"Chatgpt Beyond English: Towards A Comprehensive Evaluation Of Large Language Models In Multilingual Learning", "abstract": "<p>Over the last few years, large language models (LLMs) have emerged as the\nmost important breakthroughs in natural language processing (NLP) that\nfundamentally transform research and developments in the field. ChatGPT\nrepresents one of the most exciting LLM systems developed recently to showcase\nimpressive skills for language generation and highly attract public attention.\nAmong various exciting applications discovered for ChatGPT in English, the\nmodel can process and generate texts for multiple languages due to its\nmultilingual training data. Given the broad adoption of ChatGPT for English in\ndifferent problems and areas, a natural question is whether ChatGPT can also be\napplied effectively for other languages or it is necessary to develop more\nlanguage-specific technologies. The answer to this question requires a thorough\nevaluation of ChatGPT over multiple tasks with diverse languages and large\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\nin current research. Our work aims to fill this gap for the evaluation of\nChatGPT and similar LLMs to provide more comprehensive information for\nmultilingual NLP applications. While this work will be an ongoing effort to\ninclude additional experiments in the future, our current paper evaluates\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\nlow, and extremely low resources. We also focus on the zero-shot learning\nsetting for ChatGPT to improve reproducibility and better simulate the\ninteractions of general users. Compared to the performance of previous models,\nour extensive experimental results demonstrate a worse performance of ChatGPT\nfor different NLP tasks and languages, calling for further research to develop\nbetter models and understanding for multilingual learning.</p>\n", "tags": ["Applications","Datasets","EMNLP","Evaluation","Model Architecture","Training Techniques"] },
{"key": "lai2023lisa", "citations": "72", "year": "2024", "title":"LISA: Reasoning Segmentation Via Large Language Model", "abstract": "<p>Although perception systems have made remarkable advancements in recent\nyears, they still rely on explicit human instruction or pre-defined categories\nto identify the target objects before executing visual recognition tasks. Such\nsystems cannot actively reason and comprehend implicit user intention. In this\nwork, we propose a new segmentation task – reasoning segmentation. The task is\ndesigned to output a segmentation mask given a complex and implicit query text.\nFurthermore, we establish a benchmark comprising over one thousand\nimage-instruction-mask data samples, incorporating intricate reasoning and\nworld knowledge for evaluation purposes. Finally, we present LISA: large\nLanguage Instructed Segmentation Assistant, which inherits the language\ngeneration capabilities of multimodal Large Language Models (LLMs) while also\npossessing the ability to produce segmentation masks. We expand the original\nvocabulary with a <SEG> token and propose the embedding-as-mask paradigm to\nunlock the segmentation capability. Remarkably, LISA can handle cases involving\ncomplex reasoning and world knowledge. Also, it demonstrates robust zero-shot\ncapability when trained exclusively on reasoning-free datasets. In addition,\nfine-tuning the model with merely 239 reasoning segmentation data samples\nresults in further performance enhancement. Both quantitative and qualitative\nexperiments show our method effectively unlocks new reasoning segmentation\ncapabilities for multimodal LLMs. Code, models, and data are available at\nhttps://github.com/dvlab-research/LISA.</SEG></p>\n", "tags": ["CVPR","Datasets","Evaluation","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "lake2017generalization", "citations": "357", "year": "2018", "title":"Generalization Without Systematicity: On The Compositional Skills Of Sequence-to-sequence Recurrent Networks", "abstract": "<p>Humans can understand and produce new utterances effortlessly, thanks to\ntheir compositional skills. Once a person learns the meaning of a new verb\n“dax,” he or she can immediately understand the meaning of “dax twice” or “sing\nand dax.” In this paper, we introduce the SCAN domain, consisting of a set of\nsimple compositional navigation commands paired with the corresponding action\nsequences. We then test the zero-shot generalization capabilities of a variety\nof recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence\nmethods. We find that RNNs can make successful zero-shot generalizations when\nthe differences between training and test commands are small, so that they can\napply “mix-and-match” strategies to solve the task. However, when\ngeneralization requires systematic compositional skills (as in the “dax”\nexample above), RNNs fail spectacularly. We conclude with a proof-of-concept\nexperiment in neural machine translation, suggesting that lack of systematicity\nmight be partially responsible for neural networks’ notorious training data\nthirst.</p>\n", "tags": ["Training Techniques"] },
{"key": "lalor2016building", "citations": "63", "year": "2016", "title":"Building An Evaluation Scale Using Item Response Theory", "abstract": "<p>Evaluation of NLP methods requires testing against a previously vetted\ngold-standard test set and reporting standard metrics\n(accuracy/precision/recall/F1). The current assumption is that all items in a\ngiven test set are equal with regards to difficulty and discriminating power.\nWe propose Item Response Theory (IRT) from psychometrics as an alternative\nmeans for gold-standard test-set generation and NLP system evaluation. IRT is\nable to describe characteristics of individual items - their difficulty and\ndiscriminating power - and can account for these characteristics in its\nestimation of human intelligence or ability for an NLP task. In this paper, we\ndemonstrate IRT by generating a gold-standard test set for Recognizing Textual\nEntailment. By collecting a large number of human responses and fitting our IRT\nmodel, we show that our IRT model compares NLP systems with the performance in\na human population and is able to provide more insight into system performance\nthan standard evaluation metrics. We show that a high accuracy score does not\nalways imply a high IRT score, which depends on the item characteristics and\nthe response pattern.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "lampinen2022can", "citations": "90", "year": "2022", "title":"Can Language Models Handle Recursively Nested Grammatical Structures? A Case Study On Comparing Models And Humans", "abstract": "<p>How should we compare the capabilities of language models (LMs) and humans? I\ndraw inspiration from comparative psychology to highlight some challenges. In\nparticular, I consider a case study: processing of recursively nested\ngrammatical structures. Prior work suggests that LMs cannot handle these\nstructures as reliably as humans can. However, the humans were provided with\ninstructions and training, while the LMs were evaluated zero-shot. I therefore\nmatch the evaluation more closely. Providing large LMs with a simple prompt –\nsubstantially less content than the human training – allows the LMs to\nconsistently outperform the human results, and even to extrapolate to more\ndeeply nested conditions than were tested with humans. Further, reanalyzing the\nprior human data suggests that the humans may not perform above chance at the\ndifficult structures initially. Thus, large LMs may indeed process recursively\nnested grammatical structures as reliably as humans. This case study highlights\nhow discrepancies in the evaluation can confound comparisons of language models\nand humans. I therefore reflect on the broader challenge of comparing human and\nmodel capabilities, and highlight an important difference between evaluating\ncognitive models and foundation models.</p>\n", "tags": ["EMNLP","Evaluation","Prompting","Training Techniques"] },
{"key": "lample2017unsupervised", "citations": "564", "year": "2018", "title":"Unsupervised Machine Translation Using Monolingual Corpora Only", "abstract": "<p>Machine translation has recently achieved impressive performance thanks to\nrecent advances in deep learning and the availability of large-scale parallel\ncorpora. There have been numerous attempts to extend these successes to\nlow-resource language pairs, yet requiring tens of thousands of parallel\nsentences. In this work, we take this research direction to the extreme and\ninvestigate whether it is possible to learn to translate even without any\nparallel data. We propose a model that takes sentences from monolingual corpora\nin two different languages and maps them into the same latent space. By\nlearning to reconstruct in both languages from this shared feature space, the\nmodel effectively learns to translate without using any labeled data. We\ndemonstrate our model on two widely used datasets and two language pairs,\nreporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French\ndatasets, without using even a single parallel sentence at training time.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "lample2018phrase", "citations": "260", "year": "2018", "title":"Phrase-based & Neural Unsupervised Machine Translation", "abstract": "<p>Machine translation systems achieve near human-level performance on some\nlanguages, yet their effectiveness strongly relies on the availability of large\namounts of parallel sentences, which hinders their applicability to the\nmajority of language pairs. This work investigates how to learn to translate\nwhen having access to only large monolingual corpora in each language. We\npropose two model variants, a neural and a phrase-based model. Both versions\nleverage a careful initialization of the parameters, the denoising effect of\nlanguage models and automatic generation of parallel data by iterative\nback-translation. These models are significantly better than methods from the\nliterature, while being simpler and having fewer hyper-parameters. On the\nwidely used WMT’14 English-French and WMT’16 German-English benchmarks, our\nmodels respectively obtain 28.1 and 25.2 BLEU points without using a single\nparallel sentence, outperforming the state of the art by more than 11 BLEU\npoints. On low-resource languages like English-Urdu and English-Romanian, our\nmethods achieve even better results than semi-supervised and supervised\napproaches leveraging the paucity of available bitexts. Our code for NMT and\nPBSMT is publicly available.</p>\n", "tags": ["Training Techniques"] },
{"key": "lample2019cross", "citations": "1745", "year": "2019", "title":"Cross-lingual Language Model Pretraining", "abstract": "<p>Recent studies have demonstrated the efficiency of generative pretraining for\nEnglish natural language understanding. In this work, we extend this approach\nto multiple languages and show the effectiveness of cross-lingual pretraining.\nWe propose two methods to learn cross-lingual language models (XLMs): one\nunsupervised that only relies on monolingual data, and one supervised that\nleverages parallel data with a new cross-lingual language model objective. We\nobtain state-of-the-art results on cross-lingual classification, unsupervised\nand supervised machine translation. On XNLI, our approach pushes the state of\nthe art by an absolute gain of 4.9% accuracy. On unsupervised machine\ntranslation, we obtain 34.3 BLEU on WMT’16 German-English, improving the\nprevious state of the art by more than 9 BLEU. On supervised machine\ntranslation, we obtain a new state of the art of 38.5 BLEU on WMT’16\nRomanian-English, outperforming the previous best approach by more than 4 BLEU.\nOur code and pretrained models will be made publicly available.</p>\n", "tags": [] },
{"key": "lan2019albert", "citations": "4206", "year": "2019", "title":"ALBERT: A Lite BERT For Self-supervised Learning Of Language Representations", "abstract": "<p>Increasing model size when pretraining natural language representations often\nresults in improved performance on downstream tasks. However, at some point\nfurther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two\nparameter-reduction techniques to lower memory consumption and increase the\ntraining speed of BERT. Comprehensive empirical evidence shows that our\nproposed methods lead to models that scale much better compared to the original\nBERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new\nstate-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having\nfewer parameters compared to BERT-large. The code and the pretrained models are\navailable at https://github.com/google-research/ALBERT.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "lan2020empirical", "citations": "62", "year": "2020", "title":"An Empirical Study Of Pre-trained Transformers For Arabic Information Extraction", "abstract": "<p>Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019)\nand XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable the\neffective cross-lingual zero-shot transfer. However, their performance on\nArabic information extraction (IE) tasks is not very well studied. In this\npaper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is\ndesigned specifically for Arabic NLP and English-to-Arabic zero-shot transfer\nlearning. We study GigaBERT’s effectiveness on zero-short transfer across four\nIE tasks: named entity recognition, part-of-speech tagging, argument role\nlabeling, and relation extraction. Our best model significantly outperforms\nmBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised\nand zero-shot transfer settings. We have made our pre-trained models publicly\navailable at https://github.com/lanwuwei/GigaBERT.</p>\n", "tags": ["EMNLP","Has Code","Model Architecture"] },
{"key": "larson2019evaluation", "citations": "333", "year": "2019", "title":"An Evaluation Dataset For Intent Classification And Out-of-scope Prediction", "abstract": "<p>Task-oriented dialog systems need to know when a query falls outside their\nrange of supported intents, but current text classification corpora only define\nlabel sets that cover every example. We introduce a new dataset that includes\nqueries that are out-of-scope—i.e., queries that do not fall into any of the\nsystem’s supported intents. This poses a new challenge because models cannot\nassume that every query at inference time belongs to a system-supported intent\nclass. Our dataset also covers 150 intent classes over 10 domains, capturing\nthe breadth that a production task-oriented agent must handle. We evaluate a\nrange of benchmark classifiers on our dataset along with several different\nout-of-scope identification schemes. We find that while the classifiers perform\nwell on in-scope intent classification, they struggle to identify out-of-scope\nqueries. Our dataset and evaluation fill an important gap in the field,\noffering a way of more rigorously and realistically benchmarking text\nclassification in task-driven dialog systems.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "latif2023fine", "citations": "73", "year": "2024", "title":"Fine-tuning Chatgpt For Automatic Scoring", "abstract": "<p>This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for\nautomatically scoring student written constructed responses using example\nassessment tasks in science education. Recent studies on OpenAI’s generative\nmodel GPT-3.5 proved its superiority in predicting the natural language with\nhigh accuracy and human-like responses. GPT-3.5 has been trained over enormous\nonline language materials such as journals and Wikipedia; therefore, more than\ndirect usage of pre-trained GPT-3.5 is required for automatic scoring as\nstudents utilize a different language than trained material. These imply that a\ndomain-specific model, fine-tuned over data for specific tasks, can enhance\nmodel performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks\nwith a diverse dataset of middle-school and high-school student responses and\nexpert scoring. The six tasks comprise two multi-label and four multi-class\nassessment tasks. We compare the performance of fine-tuned GPT-3.5 with the\nfine-tuned state-of-the-art Google’s generated language model, BERT. The\nresults show that in-domain training corpora constructed from science questions\nand responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5\nshows a remarkable average increase (9.1%) in automatic scoring accuracy (mean\n= 9.15, SD = 0.042) for the six tasks, p =0.001 &lt; 0.05. Specifically, for\nmulti-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5\nachieved significantly higher scoring accuracy than BERT across all the labels,\nwith the second item achieving a 7.1% increase. The average scoring increase\nfor the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our\nstudy confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring\nof student responses on domain-specific data in education with high accuracy.\nWe have released fine-tuned models for public use and community engagement.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "lau2016empirical", "citations": "560", "year": "2016", "title":"An Empirical Evaluation Of Doc2vec With Practical Insights Into Document Embedding Generation", "abstract": "<p>Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec\n(Mikolov et al., 2013a) to learn document-level embeddings. Despite promising\nresults in the original paper, others have struggled to reproduce those\nresults. This paper presents a rigorous empirical evaluation of doc2vec over\ntwo tasks. We compare doc2vec to two baselines and two state-of-the-art\ndocument embedding methodologies. We found that doc2vec performs robustly when\nusing models trained on large external corpora, and can be further improved by\nusing pre-trained word embeddings. We also provide recommendations on\nhyper-parameter settings for general purpose applications, and release source\ncode to induce document embeddings using our trained doc2vec models.</p>\n", "tags": ["Applications","Evaluation"] },
{"key": "lau2017topically", "citations": "63", "year": "2017", "title":"Topically Driven Neural Language Model", "abstract": "<p>Language models are typically applied at the sentence level, without access\nto the broader document context. We present a neural language model that\nincorporates document context in the form of a topic model-like architecture,\nthus providing a succinct representation of the broader document context\noutside of the current sentence. Experiments over a range of datasets\ndemonstrate that our model outperforms a pure sentence-based model in terms of\nlanguage model perplexity, and leads to topics that are potentially more\ncoherent than those produced by a standard LDA topic model. Our model also has\nthe ability to generate related sentences for a topic, providing another way to\ninterpret topics.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "lauscher2021sustainable", "citations": "64", "year": "2021", "title":"Sustainable Modular Debiasing Of Language Models", "abstract": "<p>Unfair stereotypical biases (e.g., gender, racial, or religious biases)\nencoded in modern pretrained language models (PLMs) have negative ethical\nimplications for widespread adoption of state-of-the-art language technology.\nTo remedy for this, a wide range of debiasing techniques have recently been\nintroduced to remove such stereotypical biases from PLMs. Existing debiasing\nmethods, however, directly modify all of the PLMs parameters, which – besides\nbeing computationally expensive – comes with the inherent risk of\n(catastrophic) forgetting of useful language knowledge acquired in pretraining.\nIn this work, we propose a more sustainable modular debiasing approach based on\ndedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter\nmodules into the original PLM layers and (2) update only the adapters (i.e., we\nkeep the original PLM parameters frozen) via language modeling training on a\ncounterfactually augmented corpus. We showcase ADELE, in gender debiasing of\nBERT: our extensive evaluation, encompassing three intrinsic and two extrinsic\nbias measures, renders ADELE, very effective in bias mitigation. We further\nshow that – due to its modular nature – ADELE, coupled with task adapters,\nretains fairness even after large-scale downstream training. Finally, by means\nof multilingual BERT, we successfully transfer ADELE, to six target languages.</p>\n", "tags": ["Datasets","EMNLP","Ethics & Fairness","Evaluation","Model Architecture","Training Techniques"] },
{"key": "lazaridou2021mind", "citations": "70", "year": "2021", "title":"Mind The Gap: Assessing Temporal Generalization In Neural Language Models", "abstract": "<p>Our world is open-ended, non-stationary, and constantly evolving; thus what\nwe talk about and how we talk about it change over time. This inherent dynamic\nnature of language contrasts with the current static language modelling\nparadigm, which trains and evaluates models on utterances from overlapping time\nperiods. Despite impressive recent progress, we demonstrate that Transformer-XL\nlanguage models perform worse in the realistic setup of predicting future\nutterances from beyond their training period, and that model performance\nbecomes increasingly worse with time. We find that, while increasing model size\nalone – a key driver behind recent progress – does not solve this problem,\nhaving models that continually update their knowledge with new information can\nindeed mitigate this performance degradation over time. Hence, given the\ncompilation of ever-larger language modelling datasets, combined with the\ngrowing list of language-model-based NLP applications that require up-to-date\nfactual knowledge about the world, we argue that now is the right time to\nrethink the static way in which we currently train and evaluate our language\nmodels, and develop adaptive language models that can remain up-to-date with\nrespect to our ever-changing and non-stationary world. We publicly release our\ndynamic, streaming language modelling benchmarks for WMT and arXiv to\nfacilitate language model evaluation that takes temporal dynamics into account.</p>\n", "tags": ["Applications","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "le2019flaubert", "citations": "125", "year": "2019", "title":"Flaubert: Unsupervised Language Model Pre-training For French", "abstract": "<p>Language models have become a key step to achieve state-of-the art results in\nmany different Natural Language Processing (NLP) tasks. Leveraging the huge\namount of unlabeled texts nowadays available, they provide an efficient way to\npre-train continuous word representations that can be fine-tuned for a\ndownstream task, along with their contextualization at the sentence level. This\nhas been widely demonstrated for English using contextualized representations\n(Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al.,\n2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and\nshare FlauBERT, a model learned on a very large and heterogeneous French\ncorpus. Models of different sizes are trained using the new CNRS (French\nNational Centre for Scientific Research) Jean Zay supercomputer. We apply our\nFrench language models to diverse NLP tasks (text classification, paraphrasing,\nnatural language inference, parsing, word sense disambiguation) and show that\nmost of the time they outperform other pre-training approaches. Different\nversions of FlauBERT as well as a unified evaluation protocol for the\ndownstream tasks, called FLUE (French Language Understanding Evaluation), are\nshared to the research community for further reproducible experiments in French\nNLP.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "le2019multimodal", "citations": "95", "year": "2019", "title":"Multimodal Transformer Networks For End-to-end Video-grounded Dialogue Systems", "abstract": "<p>Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is\nconducted based on visual and audio aspects of a given video, is significantly\nmore challenging than traditional image or text-grounded dialogue systems\nbecause (1) feature space of videos span across multiple picture frames, making\nit difficult to obtain semantic information; and (2) a dialogue agent must\nperceive and process information from different modalities (audio, video,\ncaption, etc.) to obtain a comprehensive understanding. Most existing work is\nbased on RNNs and sequence-to-sequence architectures, which are not very\neffective for capturing complex long-term dependencies (like in videos). To\novercome this, we propose Multimodal Transformer Networks (MTN) to encode\nvideos and incorporate information from different modalities. We also propose\nquery-aware attention through an auto-encoder to extract query-aware features\nfrom non-text modalities. We develop a training procedure to simulate\ntoken-level decoding to improve the quality of generated responses during\ninference. We get state of the art performance on Dialogue System Technology\nChallenge 7 (DSTC7). Our model also generalizes to another multimodal\nvisual-grounded dialogue task, and obtains promising performance. We\nimplemented our models using PyTorch and the code is released at\nhttps://github.com/henryhungle/MTN.</p>\n", "tags": ["Dialogue & Multi Turn","Has Code","Model Architecture","Training Techniques"] },
{"key": "le2022coderl", "citations": "76", "year": "2022", "title":"Coderl: Mastering Code Generation Through Pretrained Models And Deep Reinforcement Learning", "abstract": "<p>Program synthesis or code generation aims to generate a program that\nsatisfies a problem specification. Recent approaches using large-scale\npretrained language models (LMs) have shown promising results, yet they have\nsome critical limitations. In particular, they often follow a standard\nsupervised fine-tuning procedure to train a code generation model only from the\npairs of natural-language problem descriptions and ground-truth programs. Such\nparadigm largely ignores some important but potentially useful signals in the\nproblem specification such as unit tests, which thus often results in poor\nperformance when solving complex unseen coding tasks. To address the\nlimitations, we propose “CodeRL”, a new framework for program synthesis tasks\nthrough pretrained LMs and deep reinforcement learning (RL). Specifically,\nduring training, we treat the code-generating LM as an actor network, and\nintroduce a critic network that is trained to predict the functional\ncorrectness of generated programs and provide dense feedback signals to the\nactor. During inference, we introduce a new generation procedure with a\ncritical sampling strategy that allows a model to automatically regenerate\nprograms based on feedback from example unit tests and critic scores. For the\nmodel backbones, we extended the encoder-decoder architecture of CodeT5 with\nenhanced learning objectives, larger model sizes, and better pretraining data.\nOur method not only achieves new SOTA results on the challenging APPS\nbenchmark, but also shows strong zero-shot transfer capability with new SOTA\nresults on the simpler MBPP benchmark.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Fine-Tuning","Llm For Code","Model Architecture","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "lebret2016neural", "citations": "458", "year": "2016", "title":"Neural Text Generation From Structured Data With Application To The Biography Domain", "abstract": "<p>This paper introduces a neural model for concept-to-text generation that\nscales to large, rich domains. We experiment with a new dataset of biographies\nfrom Wikipedia that is an order of magnitude larger than existing resources\nwith over 700k samples. The dataset is also vastly more diverse with a 400k\nvocabulary, compared to a few hundred words for Weathergov or Robocup. Our\nmodel builds upon recent work on conditional neural language model for text\ngeneration. To deal with the large vocabulary, we extend these models to mix a\nfixed vocabulary with copy actions that transfer sample-specific words from the\ninput database to the generated output sentence. Our neural model significantly\nout-performs a classical Kneser-Ney language model adapted to this task by\nnearly 15 BLEU.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "leclair2019neural", "citations": "280", "year": "2019", "title":"A Neural Model For Generating Natural Language Summaries Of Program Subroutines", "abstract": "<p>Source code summarization – creating natural language descriptions of source\ncode behavior – is a rapidly-growing research topic with applications to\nautomatic documentation generation, program comprehension, and software\nmaintenance. Traditional techniques relied on heuristics and templates built\nmanually by human experts. Recently, data-driven approaches based on neural\nmachine translation have largely overtaken template-based systems. But nearly\nall of these techniques rely almost entirely on programs having good internal\ndocumentation; without clear identifier names, the models fail to create good\nsummaries. In this paper, we present a neural model that combines words from\ncode with code structure from an AST. Unlike previous approaches, our model\nprocesses each data source as a separate input, which allows the model to learn\ncode structure independent of the text in code. This process helps our approach\nprovide coherent summaries in many cases even when zero internal documentation\nis provided. We evaluate our technique with a dataset we created from 2.1m Java\nmethods. We find improvement over two baseline techniques from SE literature\nand one from NLP literature.</p>\n", "tags": ["Applications","Llm For Code"] },
{"key": "lee2016fully", "citations": "426", "year": "2017", "title":"Fully Character-level Neural Machine Translation Without Explicit Segmentation", "abstract": "<p>Most existing machine translation systems operate at the level of words,\nrelying on explicit segmentation to extract tokens. We introduce a neural\nmachine translation (NMT) model that maps a source character sequence to a\ntarget character sequence without any segmentation. We employ a character-level\nconvolutional network with max-pooling at the encoder to reduce the length of\nsource representation, allowing the model to be trained at a speed comparable\nto subword-level models while capturing local regularities. Our\ncharacter-to-character model outperforms a recently proposed baseline with a\nsubword-level encoder on WMT’15 DE-EN and CS-EN, and gives comparable\nperformance on FI-EN and RU-EN. We then demonstrate that it is possible to\nshare a single character-level encoder across multiple languages by training a\nmodel on a many-to-one translation task. In this multilingual setting, the\ncharacter-level encoder significantly outperforms the subword-level encoder on\nall the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality\nof the multilingual character-level translation even surpasses the models\nspecifically trained on that language pair alone, both in terms of BLEU score\nand human judgment.</p>\n", "tags": ["TACL","Training Techniques"] },
{"key": "lee2016learning", "citations": "147", "year": "2016", "title":"Learning Recurrent Span Representations For Extractive Question Answering", "abstract": "<p>The reading comprehension task, that asks questions about a given evidence\ndocument, is a central problem in natural language understanding. Recent\nformulations of this task have typically focused on answer selection from a set\nof candidates pre-defined manually or through the use of an external NLP\npipeline. However, Rajpurkar et al. (2016) recently released the SQuAD dataset\nin which the answers can be arbitrary strings from the supplied text. In this\npaper, we focus on this answer extraction task, presenting a novel model\narchitecture that efficiently builds fixed length representations of all spans\nin the evidence document with a recurrent network. We show that scoring\nexplicit span representations significantly improves performance over other\napproaches that factor the prediction into separate predictions about words or\nstart and end markers. Our approach improves upon the best published results of\nWang &amp; Jiang (2016) by 5% and decreases the error of Rajpurkar et al.’s\nbaseline by &gt; 50%.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "lee2016recursive", "citations": "454", "year": "2016", "title":"Recursive Recurrent Nets With Attention Modeling For OCR In The Wild", "abstract": "<p>We present recursive recurrent neural networks with attention modeling\n(R\\(^2\\)AM) for lexicon-free optical character recognition in natural scene\nimages. The primary advantages of the proposed method are: (1) use of recursive\nconvolutional neural networks (CNNs), which allow for parametrically efficient\nand effective image feature extraction; (2) an implicitly learned\ncharacter-level language model, embodied in a recurrent neural network which\navoids the need to use N-grams; and (3) the use of a soft-attention mechanism,\nallowing the model to selectively exploit image features in a coordinated way,\nand allowing for end-to-end training within a standard backpropagation\nframework. We validate our method with state-of-the-art performance on\nchallenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "lee2017emotional", "citations": "61", "year": "2017", "title":"Emotional End-to-end Neural Speech Synthesizer", "abstract": "<p>In this paper, we introduce an emotional speech synthesizer based on the\nrecent end-to-end neural model, named Tacotron. Despite its benefits, we found\nthat the original Tacotron suffers from the exposure bias problem and\nirregularity of the attention alignment. Later, we address the problem by\nutilization of context vector and residual connection at recurrent neural\nnetworks (RNNs). Our experiments showed that the model could successfully train\nand generate speech for given emotion labels.</p>\n", "tags": ["Ethics & Fairness","Model Architecture"] },
{"key": "lee2018ranking", "citations": "97", "year": "2018", "title":"Ranking Paragraphs For Improving Answer Recall In Open-domain Question Answering", "abstract": "<p>Recently, open-domain question answering (QA) has been combined with machine\ncomprehension models to find answers in a large knowledge source. As\nopen-domain QA requires retrieving relevant documents from text corpora to\nanswer questions, its performance largely depends on the performance of\ndocument retrievers. However, since traditional information retrieval systems\nare not effective in obtaining documents with a high probability of containing\nanswers, they lower the performance of QA systems. Simply extracting more\ndocuments increases the number of irrelevant documents, which also degrades the\nperformance of QA systems. In this paper, we introduce Paragraph Ranker which\nranks paragraphs of retrieved documents for a higher answer recall with less\nnoise. We show that ranking paragraphs and aggregating answers using Paragraph\nRanker improves performance of open-domain QA pipeline on the four open-domain\nQA datasets by 7.8% on average.</p>\n", "tags": ["Datasets","EMNLP","Retrieval Systems"] },
{"key": "lee2018zero", "citations": "61", "year": "2019", "title":"Zero-shot Adaptive Transfer For Conversational Language Understanding", "abstract": "<p>Conversational agents such as Alexa and Google Assistant constantly need to\nincrease their language understanding capabilities by adding new domains. A\nmassive amount of labeled data is required for training each new domain. While\ndomain adaptation approaches alleviate the annotation cost, prior approaches\nsuffer from increased training time and suboptimal concept alignments. To\ntackle this, we introduce a novel Zero-Shot Adaptive Transfer method for slot\ntagging that utilizes the slot description for transferring reusable concepts\nacross domains, and enjoys efficient training without any explicit concept\nalignments. Extensive experimentation over a dataset of 10 domains relevant to\nour commercial personal digital assistant shows that our model outperforms\nprevious state-of-the-art systems by a large margin, and achieves an even\nhigher improvement in the low data regime.</p>\n", "tags": ["AAAI","Datasets","Fine-Tuning","Training Techniques"] },
{"key": "lee2019convlab", "citations": "90", "year": "2019", "title":"Convlab: Multi-domain End-to-end Dialog System Platform", "abstract": "<p>We present ConvLab, an open-source multi-domain end-to-end dialog system\nplatform, that enables researchers to quickly set up experiments with reusable\ncomponents and compare a large set of different approaches, ranging from\nconventional pipeline systems to end-to-end neural models, in common\nenvironments. ConvLab offers a set of fully annotated datasets and associated\npre-trained reference models. As a showcase, we extend the MultiWOZ dataset\nwith user dialog act annotations to train all component models and demonstrate\nhow ConvLab makes it easy and effortless to conduct complicated experiments in\nmulti-domain end-to-end dialog settings.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Tools"] },
{"key": "lee2019latent", "citations": "709", "year": "2019", "title":"Latent Retrieval For Weakly Supervised Open Domain Question Answering", "abstract": "<p>Recent work on open domain question answering (QA) assumes strong supervision\nof the supporting evidence and/or assumes a blackbox information retrieval (IR)\nsystem to retrieve evidence candidates. We argue that both are suboptimal,\nsince gold evidence is not always available, and QA is fundamentally different\nfrom IR. We show for the first time that it is possible to jointly learn the\nretriever and reader from question-answer string pairs and without any IR\nsystem. In this setting, evidence retrieval from all of Wikipedia is treated as\na latent variable. Since this is impractical to learn from scratch, we\npre-train the retriever with an Inverse Cloze Task. We evaluate on open\nversions of five QA datasets. On datasets where the questioner already knows\nthe answer, a traditional IR system such as BM25 is sufficient. On datasets\nwhere a user is genuinely seeking an answer, we show that learned retrieval is\ncrucial, outperforming BM25 by up to 19 points in exact match.</p>\n", "tags": ["Datasets","Retrieval Systems"] },
{"key": "lee2019mixout", "citations": "103", "year": "2019", "title":"Mixout: Effective Regularization To Finetune Large-scale Pretrained Language Models", "abstract": "<p>In natural language processing, it has been observed recently that\ngeneralization could be greatly improved by finetuning a large-scale language\nmodel pretrained on a large unlabeled corpus. Despite its recent success and\nwide adoption, finetuning a large pretrained language model on a downstream\ntask is prone to degenerate performance when there are only a small number of\ntraining instances available. In this paper, we introduce a new regularization\ntechnique, to which we refer as “mixout”, motivated by dropout. Mixout\nstochastically mixes the parameters of two models. We show that our mixout\ntechnique regularizes learning to minimize the deviation from one of the two\nmodels and that the strength of regularization adapts along the optimization\ntrajectory. We empirically evaluate the proposed mixout and its variants on\nfinetuning a pretrained language model on downstream tasks. More specifically,\nwe demonstrate that the stability of finetuning and the average accuracy\ngreatly increase when we use the proposed approach to regularize finetuning of\nBERT on downstream tasks in GLUE.</p>\n", "tags": ["Datasets","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "lee2019patent", "citations": "127", "year": "2020", "title":"Patent Claim Generation By Fine-tuning Openai GPT-2", "abstract": "<p>In this work, we focus on fine-tuning an OpenAI GPT-2 pre-trained model for\ngenerating patent claims. GPT-2 has demonstrated impressive efficacy of\npre-trained language models on various tasks, particularly coherent text\ngeneration. Patent claim language itself has rarely been explored in the past\nand poses a unique challenge. We are motivated to generate coherent patent\nclaims automatically so that augmented inventing might be viable someday. In\nour implementation, we identified a unique language structure in patent claims\nand leveraged its implicit human annotations. We investigated the fine-tuning\nprocess by probing the first 100 steps and observing the generated text at each\nstep. Based on both conditional and unconditional random sampling, we analyze\nthe overall quality of generated patent claims. Our contributions include: (1)\nbeing the first to generate patent claims by machines and being the first to\napply GPT-2 to patent claim generation, (2) providing various experiment\nresults for qualitative analysis and future research, (3) proposing a new\nsampling approach for text generation, and (4) building an e-mail bot for\nfuture researchers to explore the fine-tuned GPT-2 model further.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "lee2019what", "citations": "69", "year": "2019", "title":"What Would Elsa Do? Freezing Layers During Transformer Fine-tuning", "abstract": "<p>Pretrained transformer-based language models have achieved state of the art\nacross countless tasks in natural language processing. These models are highly\nexpressive, comprising at least a hundred million parameters and a dozen\nlayers. Recent evidence suggests that only a few of the final layers need to be\nfine-tuned for high quality on downstream tasks. Naturally, a subsequent\nresearch question is, “how many of the last layers do we need to fine-tune?” In\nthis paper, we precisely answer this question. We examine two recent pretrained\nlanguage models, BERT and RoBERTa, across standard tasks in textual entailment,\nsemantic similarity, sentiment analysis, and linguistic acceptability. We vary\nthe number of final layers that are fine-tuned, then study the resulting change\nin task-specific effectiveness. We show that only a fourth of the final layers\nneed to be fine-tuned to achieve 90% of the original quality. Surprisingly, we\nalso find that fine-tuning all layers does not always help.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "lee2020learning", "citations": "73", "year": "2021", "title":"Learning Dense Representations Of Phrases At Scale", "abstract": "<p>Open-domain question answering can be reformulated as a phrase retrieval\nproblem, without the need for processing documents on-demand during inference\n(Seo et al., 2019). However, current phrase retrieval models heavily depend on\nsparse representations and still underperform retriever-reader approaches. In\nthis work, we show for the first time that we can learn dense representations\nof phrases alone that achieve much stronger performance in open-domain QA. We\npresent an effective method to learn phrase representations from the\nsupervision of reading comprehension tasks, coupled with novel negative\nsampling methods. We also propose a query-side fine-tuning strategy, which can\nsupport transfer learning and reduce the discrepancy between training and\ninference. On five popular open-domain QA datasets, our model DensePhrases\nimproves over previous phrase retrieval models by 15%-25% absolute accuracy and\nmatches the performance of state-of-the-art retriever-reader models. Our model\nis easy to parallelize due to pure dense representations and processes more\nthan 10 questions per second on CPUs. Finally, we directly use our pre-indexed\ndense phrase representations for two slot filling tasks, showing the promise of\nutilizing DensePhrases as a dense knowledge base for downstream tasks.</p>\n", "tags": ["Datasets","Fine-Tuning","Retrieval Systems","Training Techniques"] },
{"key": "lee2021dialogue", "citations": "78", "year": "2021", "title":"Dialogue State Tracking With A Language Model Using Schema-driven Prompting", "abstract": "<p>Task-oriented conversational systems often use dialogue state tracking to\nrepresent the user’s intentions, which involves filling in values of\npre-defined slots. Many approaches have been proposed, often using\ntask-specific architectures with special-purpose classifiers. Recently, good\nresults have been obtained using more general architectures based on pretrained\nlanguage models. Here, we introduce a new variation of the language modeling\napproach that uses schema-driven prompting to provide task-aware history\nencoding that is used for both categorical and non-categorical slots. We\nfurther improve performance by augmenting the prompting with schema\ndescriptions, a naturally occurring source of in-domain knowledge. Our purely\ngenerative system achieves state-of-the-art performance on MultiWOZ 2.2 and\nachieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M.\nThe data and code will be available at\nhttps://github.com/chiahsuan156/DST-as-Prompting.</p>\n", "tags": ["EMNLP","Has Code","Prompting"] },
{"key": "lee2021direct", "citations": "72", "year": "2022", "title":"Direct Speech-to-speech Translation With Discrete Units", "abstract": "<p>We present a direct speech-to-speech translation (S2ST) model that translates\nspeech from one language to speech in another language without relying on\nintermediate text generation. We tackle the problem by first applying a\nself-supervised discrete speech encoder on the target speech and then training\na sequence-to-sequence speech-to-unit translation (S2UT) model to predict the\ndiscrete representations of the target speech. When target text transcripts are\navailable, we design a joint speech and text training framework that enables\nthe model to generate dual modality output (speech and text) simultaneously in\nthe same inference pass. Experiments on the Fisher Spanish-English dataset show\nthat the proposed framework yields improvement of 6.7 BLEU compared with a\nbaseline direct S2ST model that predicts spectrogram features. When trained\nwithout any text transcripts, our model performance is comparable to models\nthat predict spectrograms and are trained with text supervision, showing the\npotential of our system for translation between unwritten languages. Audio\nsamples are available at\nhttps://facebookresearch.github.io/speech_translation/direct_s2st_units/index.html .</p>\n", "tags": ["Datasets","Has Code","Tools","Training Techniques"] },
{"key": "lee2021towards", "citations": "63", "year": "2021", "title":"Towards Few-shot Fact-checking Via Perplexity", "abstract": "<p>Few-shot learning has drawn researchers’ attention to overcome the problem of\ndata scarcity. Recently, large pre-trained language models have shown great\nperformance in few-shot learning for various downstream tasks, such as question\nanswering and machine translation. Nevertheless, little exploration has been\nmade to achieve few-shot learning for the fact-checking task. However,\nfact-checking is an important problem, especially when the amount of\ninformation online is growing exponentially every day. In this paper, we\npropose a new way of utilizing the powerful transfer learning ability of a\nlanguage model via a perplexity score. The most notable strength of our\nmethodology lies in its capability in few-shot learning. With only two training\nsamples, our methodology can already outperform the Major Class baseline by\nmore than absolute 10% on the F1-Macro metric across multiple datasets. Through\nexperiments, we empirically verify the plausibility of the rather surprising\nusage of the perplexity score in the context of fact-checking and highlight the\nstrength of our few-shot methodology by comparing it to strong\nfine-tuning-based baseline models. Moreover, we construct and publicly release\ntwo new fact-checking datasets related to COVID-19.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Model Architecture","NAACL","Training Techniques"] },
{"key": "lee2022coauthor", "citations": "236", "year": "2022", "title":"Coauthor: Designing A Human-ai Collaborative Writing Dataset For Exploring Language Model Capabilities", "abstract": "<p>Large language models (LMs) offer unprecedented language generation\ncapabilities and exciting opportunities for interaction design. However, their\nhighly context-dependent capabilities are difficult to grasp and are often\nsubjectively interpreted. In this paper, we argue that by curating and\nanalyzing large interaction datasets, the HCI community can foster more\nincisive examinations of LMs’ generative capabilities. Exemplifying this\napproach, we present CoAuthor, a dataset designed for revealing GPT-3’s\ncapabilities in assisting creative and argumentative writing. CoAuthor captures\nrich interactions between 63 writers and four instances of GPT-3 across 1445\nwriting sessions. We demonstrate that CoAuthor can address questions about\nGPT-3’s language, ideation, and collaboration capabilities, and reveal its\ncontribution as a writing “collaborator” under various definitions of good\ncollaboration. Finally, we discuss how this work may facilitate a more\nprincipled discussion around LMs’ promises and pitfalls in relation to\ninteraction design. The dataset and an interface for replaying the writing\nsessions are publicly available at https://coauthor.stanford.edu.</p>\n", "tags": ["Datasets","Has Code"] },
{"key": "lee2023multimodal", "citations": "64", "year": "2023", "title":"Multimodal Prompting With Missing Modalities For Visual Recognition", "abstract": "<p>In this paper, we tackle two challenges in multimodal learning for visual\nrecognition: 1) when missing-modality occurs either during training or testing\nin real-world situations; and 2) when the computation resources are not\navailable to finetune on heavy transformer models. To this end, we propose to\nutilize prompt learning and mitigate the above two challenges together.\nSpecifically, our modality-missing-aware prompts can be plugged into multimodal\ntransformers to handle general missing-modality cases, while only requiring\nless than 1% learnable parameters compared to training the entire model. We\nfurther explore the effect of different prompt configurations and analyze the\nrobustness to missing modality. Extensive experiments are conducted to show the\neffectiveness of our prompt learning framework that improves the performance\nunder various missing-modality cases, while alleviating the requirement of\nheavy model re-training. Code is available.</p>\n", "tags": ["CVPR","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "lees2022new", "citations": "92", "year": "2022", "title":"A New Generation Of Perspective API: Efficient Multilingual Character-level Transformers", "abstract": "<p>On the world wide web, toxic content detectors are a crucial line of defense\nagainst potentially hateful and offensive messages. As such, building highly\neffective classifiers that enable a safer internet is an important research\narea. Moreover, the web is a highly multilingual, cross-cultural community that\ndevelops its own lingo over time. As such, it is crucial to develop models that\nare effective across a diverse range of languages, usages, and styles. In this\npaper, we present the fundamentals behind the next version of the Perspective\nAPI from Google Jigsaw. At the heart of the approach is a single multilingual\ntoken-free Charformer model that is applicable across a range of languages,\ndomains, and tasks. We demonstrate that by forgoing static vocabularies, we\ngain flexibility across a variety of settings. We additionally outline the\ntechniques employed to make such a byte-level model efficient and feasible for\nproductionization. Through extensive experiments on multilingual toxic comment\nclassification benchmarks derived from real API traffic and evaluation on an\narray of code-switching, covert toxicity, emoji-based hate, human-readable\nobfuscation, distribution shift, and bias evaluation settings, we show that our\nproposed approach outperforms strong baselines. Finally, we present our\nfindings from deploying this system in production.</p>\n", "tags": ["Ethics & Fairness","Evaluation","KDD","Tools"] },
{"key": "leethorp2021fnet", "citations": "363", "year": "2022", "title":"Fnet: Mixing Tokens With Fourier Transforms", "abstract": "<p>We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that “mix” input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n“efficient” Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","NAACL","Tools"] },
{"key": "lehman2019inferring", "citations": "105", "year": "2019", "title":"Inferring Which Medical Treatments Work From Reports Of Clinical Trials", "abstract": "<p>How do we know if a particular medical treatment actually works? Ideally one\nwould consult all available evidence from relevant clinical trials.\nUnfortunately, such results are primarily disseminated in natural language\nscientific articles, imposing substantial burden on those trying to make sense\nof them. In this paper, we present a new task and corpus for making this\nunstructured evidence actionable. The task entails inferring reported findings\nfrom a full-text article describing a randomized controlled trial (RCT) with\nrespect to a given intervention, comparator, and outcome of interest, e.g.,\ninferring if an article provides evidence supporting the use of aspirin to\nreduce risk of stroke, as compared to placebo.\n  We present a new corpus for this task comprising 10,000+ prompts coupled with\nfull-text articles describing RCTs. Results using a suite of models — ranging\nfrom heuristic (rule-based) approaches to attentive neural architectures —\ndemonstrate the difficulty of the task, which we believe largely owes to the\nlengthy, technical input texts. To facilitate further work on this important,\nchallenging problem we make the corpus, documentation, a website and\nleaderboard, and code for baselines and evaluation available at\nhttp://evidence-inference.ebm-nlp.com/.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "lei2016rationalizing", "citations": "613", "year": "2016", "title":"Rationalizing Neural Predictions", "abstract": "<p>Prediction without justification has limited applicability. As a remedy, we\nlearn to extract pieces of input text as justifications – rationales – that\nare tailored to be short and coherent, yet sufficient for making the same\nprediction. Our approach combines two modular components, generator and\nencoder, which are trained to operate well together. The generator specifies a\ndistribution over text fragments as candidate rationales and these are passed\nthrough the encoder for prediction. Rationales are never given during training.\nInstead, the model is regularized by desiderata for rationales. We evaluate the\napproach on multi-aspect sentiment analysis against manually annotated test\ncases. Our approach outperforms attention-based baseline by a significant\nmargin. We also successfully illustrate the method on the question retrieval\ntask.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "lei2017simple", "citations": "246", "year": "2018", "title":"Simple Recurrent Units For Highly Parallelizable Recurrence", "abstract": "<p>Common recurrent neural architectures scale poorly due to the intrinsic\ndifficulty in parallelizing their state computations. In this work, we propose\nthe Simple Recurrent Unit (SRU), a light recurrent unit that balances model\ncapacity and scalability. SRU is designed to provide expressive recurrence,\nenable highly parallelized implementation, and comes with careful\ninitialization to facilitate training of deep models. We demonstrate the\neffectiveness of SRU on multiple NLP tasks. SRU achieves 5–9x speed-up over\ncuDNN-optimized LSTM on classification and question answering datasets, and\ndelivers stronger results than LSTM and convolutional models. We also obtain an\naverage of 0.7 BLEU improvement over the Transformer model on translation by\nincorporating SRU into the architecture.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "lei2020estimation", "citations": "102", "year": "2020", "title":"Estimation-action-reflection: Towards Deep Interaction Between Conversational And Recommender Systems", "abstract": "<p>Recommender systems are embracing conversational technologies to obtain user\npreferences dynamically, and to overcome inherent limitations of their static\nmodels. A successful Conversational Recommender System (CRS) requires proper\nhandling of interactions between conversation and recommendation. We argue that\nthree fundamental problems need to be solved: 1) what questions to ask\nregarding item attributes, 2) when to recommend items, and 3) how to adapt to\nthe users’ online feedback. To the best of our knowledge, there lacks a unified\nframework that addresses these problems.\n  In this work, we fill this missing interaction framework gap by proposing a\nnew CRS framework named Estimation-Action-Reflection, or EAR, which consists of\nthree stages to better converse with users. (1) Estimation, which builds\npredictive models to estimate user preference on both items and item\nattributes; (2) Action, which learns a dialogue policy to determine whether to\nask attributes or recommend items, based on Estimation stage and conversation\nhistory; and (3) Reflection, which updates the recommender model when a user\nrejects the recommendations made by the Action stage. We present two\nconversation scenarios on binary and enumerated questions, and conduct\nextensive experiments on two datasets from Yelp and LastFM, for each scenario,\nrespectively. Our experiments demonstrate significant improvements over the\nstate-of-the-art method CRM [32], corresponding to fewer conversation turns and\na higher level of recommendation hits.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "lei2020mart", "citations": "156", "year": "2020", "title":"MART: Memory-augmented Recurrent Transformer For Coherent Video Paragraph Captioning", "abstract": "<p>Generating multi-sentence descriptions for videos is one of the most\nchallenging captioning tasks due to its high requirements for not only visual\nrelevance but also discourse-based coherence across the sentences in the\nparagraph. Towards this goal, we propose a new approach called Memory-Augmented\nRecurrent Transformer (MART), which uses a memory module to augment the\ntransformer architecture. The memory module generates a highly summarized\nmemory state from the video segments and the sentence history so as to help\nbetter prediction of the next sentence (w.r.t. coreference and repetition\naspects), thus encouraging coherent paragraph generation. Extensive\nexperiments, human evaluations, and qualitative analyses on two popular\ndatasets ActivityNet Captions and YouCookII show that MART generates more\ncoherent and less repetitive paragraph captions than baseline methods, while\nmaintaining relevance to the input video events. All code is available\nopen-source at: https://github.com/jayleicn/recurrent-transformer</p>\n", "tags": ["Has Code","Model Architecture"] },
{"key": "lei2021less", "citations": "448", "year": "2021", "title":"Less Is More: Clipbert For Video-and-language Learning Via Sparse Sampling", "abstract": "<p>The canonical approach to video-and-language learning (e.g., video question\nanswering) dictates a neural model to learn from offline-extracted dense video\nfeatures from vision models and text features from language models. These\nfeature extractors are trained independently and usually on tasks different\nfrom the target domains, rendering these fixed features sub-optimal for\ndownstream tasks. Moreover, due to the high computational overload of dense\nvideo features, it is often difficult (or infeasible) to plug feature\nextractors directly into existing approaches for easy finetuning. To provide a\nremedy to this dilemma, we propose a generic framework ClipBERT that enables\naffordable end-to-end learning for video-and-language tasks, by employing\nsparse sampling, where only a single or a few sparsely sampled short clips from\na video are used at each training step. Experiments on text-to-video retrieval\nand video question answering on six datasets demonstrate that ClipBERT\noutperforms (or is on par with) existing methods that exploit full-length\nvideos, suggesting that end-to-end learning with just a few sparsely sampled\nclips is often more accurate than using densely extracted offline features from\nfull-length videos, proving the proverbial less-is-more principle. Videos in\nthe datasets are from considerably different domains and lengths, ranging from\n3-second generic domain GIF videos to 180-second YouTube human activity videos,\nshowing the generalization ability of our approach. Comprehensive ablation\nstudies and thorough analyses are provided to dissect what factors lead to this\nsuccess. Our code is publicly available at https://github.com/jayleicn/ClipBERT</p>\n", "tags": ["CVPR","Datasets","Has Code","Tools","Training Techniques"] },
{"key": "leiter2023chatgpt", "citations": "85", "year": "2024", "title":"Chatgpt: A Meta-analysis After 2.5 Months", "abstract": "<p>ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and\nmedia attention since its release in November 2022. However, little hard\nevidence is available regarding its perception in various sources. In this\npaper, we analyze over 300,000 tweets and more than 150 scientific papers to\ninvestigate how ChatGPT is perceived and discussed. Our findings show that\nChatGPT is generally viewed as of high quality, with positive sentiment and\nemotions of joy dominating in social media. Its perception has slightly\ndecreased since its debut, however, with joy decreasing and (negative) surprise\non the rise, and it is perceived more negatively in languages other than\nEnglish. In recent scientific papers, ChatGPT is characterized as a great\nopportunity across various fields including the medical domain, but also as a\nthreat concerning ethics and receives mixed assessments for education. Our\ncomprehensive meta-analysis of ChatGPT’s current perception after 2.5 months\nsince its release can contribute to shaping the public debate and informing its\nfuture development. We make our data available.</p>\n", "tags": ["Applications","Ethics & Fairness","Model Architecture","Survey Paper"] },
{"key": "lertvittayakumjorn2019human", "citations": "63", "year": "2019", "title":"Human-grounded Evaluations Of Explanation Methods For Text Classification", "abstract": "<p>Due to the black-box nature of deep learning models, methods for explaining\nthe models’ results are crucial to gain trust from humans and support\ncollaboration between AIs and humans. In this paper, we consider several\nmodel-agnostic and model-specific explanation methods for CNNs for text\nclassification and conduct three human-grounded evaluations, focusing on\ndifferent purposes of explanations: (1) revealing model behavior, (2)\njustifying model predictions, and (3) helping humans investigate uncertain\npredictions. The results highlight dissimilar qualities of the various\nexplanation methods we consider and show the degree to which these methods\ncould serve for each purpose.</p>\n", "tags": ["EMNLP"] },
{"key": "lewis2017deal", "citations": "332", "year": "2017", "title":"Deal Or No Deal? End-to-end Learning For Negotiation Dialogues", "abstract": "<p>Much of human dialogue occurs in semi-cooperative settings, where agents with\ndifferent goals attempt to agree on common decisions. Negotiations require\ncomplex communication and reasoning skills, but success is easy to measure,\nmaking this an interesting task for AI. We gather a large dataset of\nhuman-human negotiations on a multi-issue bargaining task, where agents who\ncannot observe each other’s reward functions must reach an agreement (or a\ndeal) via natural language dialogue. For the first time, we show it is possible\nto train end-to-end models for negotiation, which must learn both linguistic\nand reasoning skills with no annotated dialogue states. We also introduce\ndialogue rollouts, in which the model plans ahead by simulating possible\ncomplete continuations of the conversation, and find that this technique\ndramatically improves performance. Our code and dataset are publicly available\n(https://github.com/facebookresearch/end-to-end-negotiator).</p>\n", "tags": ["Datasets","EMNLP","Has Code","Reinforcement Learning"] },
{"key": "lewis2019mlqa", "citations": "347", "year": "2020", "title":"MLQA: Evaluating Cross-lingual Extractive Question Answering", "abstract": "<p>Question answering (QA) models have shown rapid progress enabled by the\navailability of large, high-quality benchmark datasets. Such annotated datasets\nare difficult and costly to collect, and rarely exist in languages other than\nEnglish, making training QA systems in other languages challenging. An\nalternative to building large monolingual training datasets is to develop\ncross-lingual systems which can transfer to a target language without requiring\ntraining data in that language. In order to develop such systems, it is crucial\nto invest in high quality multilingual evaluation benchmarks to measure\nprogress. We present MLQA, a multi-way aligned extractive QA evaluation\nbenchmark intended to spur research in this area. MLQA contains QA instances in\n7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and\nSimplified Chinese. It consists of over 12K QA instances in English and 5K in\neach other language, with each QA instance being parallel between 4 languages\non average. MLQA is built using a novel alignment context strategy on Wikipedia\narticles, and serves as a cross-lingual extension to existing extractive QA\ndatasets. We evaluate current state-of-the-art cross-lingual representations on\nMLQA, and also provide machine-translation-based baselines. In all cases,\ntransfer results are shown to be significantly behind training-language\nperformance.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "lewis2019unsupervised", "citations": "110", "year": "2019", "title":"Unsupervised Question Answering By Cloze Translation", "abstract": "<p>Obtaining training data for Question Answering (QA) is time-consuming and\nresource-intensive, and existing QA datasets are only available for limited\ndomains and languages. In this work, we explore to what extent high quality\ntraining data is actually required for Extractive QA, and investigate the\npossibility of unsupervised Extractive QA. We approach this problem by first\nlearning to generate context, question and answer triples in an unsupervised\nmanner, which we then use to synthesize Extractive QA training data\nautomatically. To generate such triples, we first sample random context\nparagraphs from a large corpus of documents and then random noun phrases or\nnamed entity mentions from these paragraphs as answers. Next we convert answers\nin context to “fill-in-the-blank” cloze questions and finally translate them\ninto natural questions. We propose and compare various unsupervised ways to\nperform cloze-to-natural question translation, including training an\nunsupervised NMT model using non-aligned corpora of natural questions and cloze\nquestions as well as a rule-based approach. We find that modern QA models can\nlearn to answer human questions surprisingly well using only synthetic training\ndata. We demonstrate that, without using the SQuAD training data at all, our\napproach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named\nentity mention), outperforming early supervised models.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "lewis2020pre", "citations": "104", "year": "2020", "title":"Pre-training Via Paraphrasing", "abstract": "<p>We introduce MARGE, a pre-trained sequence-to-sequence model learned with an\nunsupervised multi-lingual multi-document paraphrasing objective. MARGE\nprovides an alternative to the dominant masked language modeling paradigm,\nwhere we self-supervise the reconstruction of target text by retrieving a set\nof related texts (in many languages) and conditioning on them to maximize the\nlikelihood of generating the original. We show it is possible to jointly learn\nto do retrieval and reconstruction, given only a random initialization. The\nobjective noisily captures aspects of paraphrase, translation, multi-document\nsummarization, and information retrieval, allowing for strong zero-shot\nperformance on several tasks. For example, with no additional task-specific\ntraining we achieve BLEU scores of up to 35.8 for document translation. We\nfurther show that fine-tuning gives strong performance on a range of\ndiscriminative and generative tasks in many languages, making MARGE the most\ngenerally applicable pre-training method to date.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "lewis2020question", "citations": "151", "year": "2021", "title":"Question And Answer Test-train Overlap In Open-domain Question Answering Datasets", "abstract": "<p>Ideally Open-Domain Question Answering models should exhibit a number of\ncompetencies, ranging from simply memorizing questions seen at training time,\nto answering novel question formulations with answers seen during training, to\ngeneralizing to completely novel questions with novel answers. However, single\naggregated test set scores do not show the full picture of what capabilities\nmodels truly have. In this work, we perform a detailed study of the test sets\nof three popular open-domain benchmark datasets with respect to these\ncompetencies. We find that 60-70% of test-time answers are also present\nsomewhere in the training sets. We also find that 30% of test-set questions\nhave a near-duplicate paraphrase in their corresponding training sets. Using\nthese findings, we evaluate a variety of popular open-domain models to obtain\ngreater insight into what extent they can actually generalize, and what drives\ntheir overall performance. We find that all models perform dramatically worse\non questions that cannot be memorized from training sets, with a mean absolute\nperformance difference of 63% between repeated and non-repeated data. Finally\nwe show that simple nearest-neighbor models out-perform a BART closed-book QA\nmodel, further highlighting the role that training set memorization plays in\nthese benchmarks</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "lewis2020retrieval", "citations": "1726", "year": "2020", "title":"Retrieval-augmented Generation For Knowledge-intensive NLP Tasks", "abstract": "<p>Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) – models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.</p>\n", "tags": ["Fine-Tuning","RAG","Retrieval Systems","Training Techniques"] },
{"key": "lewis2021paq", "citations": "125", "year": "2021", "title":"PAQ: 65 Million Probably-asked Questions And What You Can Do With Them", "abstract": "<p>Open-domain Question Answering models which directly leverage question-answer\n(QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show\npromise in terms of speed and memory compared to conventional models which\nretrieve and read from text corpora. QA-pair retrievers also offer\ninterpretable answers, a high degree of control, and are trivial to update at\ntest time with new knowledge. However, these models lack the accuracy of\nretrieve-and-read systems, as substantially less knowledge is covered by the\navailable QA-pairs relative to text corpora like Wikipedia. To facilitate\nimproved QA-pair models, we introduce Probably Asked Questions (PAQ), a very\nlarge resource of 65M automatically-generated QA-pairs. We introduce a new\nQA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and\ncaches test questions, enabling RePAQ to match the accuracy of recent\nretrieve-and-read models, whilst being significantly faster. Using PAQ, we\ntrain CBQA models which outperform comparable baselines by 5%, but trail RePAQ\nby over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be\nconfigured for size (under 500MB) or speed (over 1K questions per second)\nwhilst retaining high accuracy. Lastly, we demonstrate RePAQ’s strength at\nselective QA, abstaining from answering when it is likely to be incorrect. This\nenables RePAQ to ``back-off” to a more expensive state-of-the-art model,\nleading to a combined system which is both more accurate and 2x faster than the\nstate-of-the-art model alone.</p>\n", "tags": ["Retrieval Systems","TACL"] },
{"key": "li2016dataset", "citations": "78", "year": "2016", "title":"Dataset And Neural Recurrent Sequence Labeling Model For Open-domain Factoid Question Answering", "abstract": "<p>While question answering (QA) with neural network, i.e. neural QA, has\nachieved promising results in recent years, lacking of large scale real-word QA\ndataset is still a challenge for developing and evaluating neural QA system. To\nalleviate this problem, we propose a large scale human annotated real-world QA\ndataset WebQA with more than 42k questions and 556k evidences. As existing\nneural QA methods resolve QA either as sequence generation or\nclassification/ranking problem, they face challenges of expensive softmax\ncomputation, unseen answers handling or separate candidate answer generation\ncomponent. In this work, we cast neural QA as a sequence labeling problem and\npropose an end-to-end sequence labeling model, which overcomes all the above\nchallenges. Experimental results on WebQA show that our model outperforms the\nbaselines significantly with an F1 score of 74.69% with word-based input, and\nthe performance drops only 3.72 F1 points with more challenging character-based\ninput.</p>\n", "tags": ["Datasets"] },
{"key": "li2016deep", "citations": "1065", "year": "2016", "title":"Deep Reinforcement Learning For Dialogue Generation", "abstract": "<p>Recent neural models of dialogue generation offer great promise for\ngenerating responses for conversational agents, but tend to be shortsighted,\npredicting utterances one at a time while ignoring their influence on future\noutcomes. Modeling the future direction of a dialogue is crucial to generating\ncoherent, interesting dialogues, a need which led traditional NLP models of\ndialogue to draw on reinforcement learning. In this paper, we show how to\nintegrate these goals, applying deep reinforcement learning to model future\nreward in chatbot dialogue. The model simulates dialogues between two virtual\nagents, using policy gradient methods to reward sequences that display three\nuseful conversational properties: informativity (non-repetitive turns),\ncoherence, and ease of answering (related to forward-looking function). We\nevaluate our model on diversity, length as well as with human judges, showing\nthat the proposed algorithm generates more interactive responses and manages to\nfoster a more sustained conversation in dialogue simulation. This work marks a\nfirst step towards learning a neural conversational model based on the\nlong-term success of dialogues.</p>\n", "tags": ["Agentic","EMNLP","Reinforcement Learning"] },
{"key": "li2016dialogue", "citations": "70", "year": "2016", "title":"Dialogue Learning With Human-in-the-loop", "abstract": "<p>An important aspect of developing conversational agents is to give a bot the\nability to improve through communicating with humans and to learn from the\nmistakes that it makes. Most research has focused on learning from fixed\ntraining sets of labeled data rather than interacting with a dialogue partner\nin an online fashion. In this paper we explore this direction in a\nreinforcement learning setting where the bot improves its question-answering\nability from feedback a teacher gives following its generated responses. We\nbuild a simulator that tests various aspects of such learning in a synthetic\nenvironment, and introduce models that work in this regime. Finally, real\nexperiments with Mechanical Turk validate the approach.</p>\n", "tags": ["Agentic","Reinforcement Learning","Training Techniques"] },
{"key": "li2016learning", "citations": "78", "year": "2016", "title":"Learning Through Dialogue Interactions By Asking Questions", "abstract": "<p>A good dialogue agent should have the ability to interact with users by both\nresponding to questions and by asking questions, and importantly to learn from\nboth types of interaction. In this work, we explore this direction by designing\na simulator and a set of synthetic tasks in the movie domain that allow such\ninteractions between a learner and a teacher. We investigate how a learner can\nbenefit from asking questions in both offline and online reinforcement learning\nsettings, and demonstrate that the learner improves when asking questions.\nFinally, real experiments with Mechanical Turk validate the approach. Our work\nrepresents a first step in developing such end-to-end learned interactive\ndialogue agents.</p>\n", "tags": ["Agentic","Dialogue & Multi Turn","Reinforcement Learning"] },
{"key": "li2016mutual", "citations": "99", "year": "2016", "title":"Mutual Information And Diverse Decoding Improve Neural Machine Translation", "abstract": "<p>Sequence-to-sequence neural translation models learn semantic and syntactic\nrelations between sentence pairs by optimizing the likelihood of the target\ngiven the source, i.e., \\(p(y|x)\\), an objective that ignores other potentially\nuseful sources of information. We introduce an alternative objective function\nfor neural MT that maximizes the mutual information between the source and\ntarget sentences, modeling the bi-directional dependency of sources and\ntargets. We implement the model with a simple re-ranking method, and also\nintroduce a decoding algorithm that increases diversity in the N-best list\nproduced by the first pass. Applied to the WMT German/English and\nFrench/English tasks, the proposed models offers a consistent performance boost\non both standard LSTM and attention-based neural MT architectures.</p>\n", "tags": ["Model Architecture"] },
{"key": "li2016persona", "citations": "930", "year": "2016", "title":"A Persona-based Neural Conversation Model", "abstract": "<p>We present persona-based models for handling the issue of speaker consistency\nin neural response generation. A speaker model encodes personas in distributed\nembeddings that capture individual characteristics such as background\ninformation and speaking style. A dyadic speaker-addressee model captures\nproperties of interactions between two interlocutors. Our models yield\nqualitative performance improvements in both perplexity and BLEU scores over\nbaseline sequence-to-sequence models, with similar gains in speaker consistency\nas measured by human judges.</p>\n", "tags": [] },
{"key": "li2016user", "citations": "145", "year": "2016", "title":"A User Simulator For Task-completion Dialogues", "abstract": "<p>Despite widespread interests in reinforcement-learning for task-oriented\ndialogue systems, several obstacles can frustrate research and development\nprogress. First, reinforcement learners typically require interaction with the\nenvironment, so conventional dialogue corpora cannot be used directly. Second,\neach task presents specific challenges, requiring separate corpus of\ntask-specific annotated data. Third, collecting and annotating human-machine or\nhuman-human conversations for task-oriented dialogues requires extensive domain\nknowledge. Because building an appropriate dataset can be both financially\ncostly and time-consuming, one popular approach is to build a user simulator\nbased upon a corpus of example dialogues. Then, one can train reinforcement\nlearning agents in an online fashion as they interact with the simulator.\nDialogue agents trained on these simulators can serve as an effective starting\npoint. Once agents master the simulator, they may be deployed in a real\nenvironment to interact with humans, and continue to be trained online. To ease\nempirical algorithmic comparisons in dialogues, this paper introduces a new,\npublicly available simulation framework, where our simulator, designed for the\nmovie-booking domain, leverages both rules and collected data. The simulator\nsupports two tasks: movie ticket booking and movie seeking. Finally, we\ndemonstrate several agents and detail the procedure to add and test your own\nagent in the proposed framework.</p>\n", "tags": ["Agentic","Datasets","Dialogue & Multi Turn","Tools"] },
{"key": "li2017adversarial", "citations": "796", "year": "2017", "title":"Adversarial Learning For Neural Dialogue Generation", "abstract": "<p>In this paper, drawing intuition from the Turing test, we propose using\nadversarial training for open-domain dialogue generation: the system is trained\nto produce sequences that are indistinguishable from human-generated dialogue\nutterances. We cast the task as a reinforcement learning (RL) problem where we\njointly train two systems, a generative model to produce response sequences,\nand a discriminator—analagous to the human evaluator in the Turing test— to\ndistinguish between the human-generated dialogues and the machine-generated\nones. The outputs from the discriminator are then used as rewards for the\ngenerative model, pushing the system to generate dialogues that mostly resemble\nhuman dialogues.\n  In addition to adversarial training we describe a model for adversarial {\\em\nevaluation} that uses success in fooling an adversary as a dialogue evaluation\nmetric, while avoiding a number of potential pitfalls. Experimental results on\nseveral metrics, including adversarial evaluation, demonstrate that the\nadversarially-trained system generates higher-quality responses than previous\nbaselines.</p>\n", "tags": ["EMNLP","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "li2017code", "citations": "195", "year": "2018", "title":"Code Completion With Neural Attention And Pointer Networks", "abstract": "<p>Intelligent code completion has become an essential research task to\naccelerate modern software development. To facilitate effective code completion\nfor dynamically-typed programming languages, we apply neural language models by\nlearning from large codebases, and develop a tailored attention mechanism for\ncode completion. However, standard neural language models even with attention\nmechanism cannot correctly predict the out-of-vocabulary (OoV) words that\nrestrict the code completion performance. In this paper, inspired by the\nprevalence of locally repeated terms in program source code, and the recently\nproposed pointer copy mechanism, we propose a pointer mixture network for\nbetter predicting OoV words in code completion. Based on the context, the\npointer mixture network learns to either generate a within-vocabulary word\nthrough an RNN component, or regenerate an OoV word from local context through\na pointer component. Experiments on two benchmarked datasets demonstrate the\neffectiveness of our attention mechanism and pointer mixture network on the\ncode completion task.</p>\n", "tags": ["Datasets","IJCAI","Llm For Code","Model Architecture"] },
{"key": "li2017end", "citations": "250", "year": "2017", "title":"End-to-end Task-completion Neural Dialogue Systems", "abstract": "<p>One of the major drawbacks of modularized task-completion dialogue systems is\nthat each module is trained individually, which presents several challenges.\nFor example, downstream modules are affected by earlier modules, and the\nperformance of the entire system is not robust to the accumulated errors. This\npaper presents a novel end-to-end learning framework for task-completion\ndialogue systems to tackle such issues. Our neural dialogue system can directly\ninteract with a structured database to assist users in accessing information\nand accomplishing certain tasks. The reinforcement learning based dialogue\nmanager offers robust capabilities to handle noises caused by other components\nof the dialogue system. Our experiments in a movie-ticket booking domain show\nthat our end-to-end system not only outperforms modularized dialogue system\nbaselines for both objective and subjective evaluation, but also is robust to\nnoises as demonstrated by several systematic experiments with different error\ngranularity and rates specific to the language understanding module.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","Reinforcement Learning","Tools"] },
{"key": "li2017modeling", "citations": "133", "year": "2017", "title":"Modeling Source Syntax For Neural Machine Translation", "abstract": "<p>Even though a linguistics-free sequence to sequence model in neural machine\ntranslation (NMT) has certain capability of implicitly learning syntactic\ninformation of source sentences, this paper shows that source syntax can be\nexplicitly incorporated into NMT effectively to provide further improvements.\nSpecifically, we linearize parse trees of source sentences to obtain structural\nlabel sequences. On the basis, we propose three different sorts of encoders to\nincorporate source syntax into NMT: 1) Parallel RNN encoder that learns word\nand label annotation vectors parallelly; 2) Hierarchical RNN encoder that\nlearns word and label annotation vectors in a two-level hierarchy; and 3) Mixed\nRNN encoder that stitchingly learns word and label annotation vectors over\nsequences where words and labels are mixed. Experimentation on\nChinese-to-English translation demonstrates that all the three proposed\nsyntactic encoders are able to improve translation accuracy. It is interesting\nto note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best\nperformance with an significant improvement of 1.4 BLEU points. Moreover, an\nin-depth analysis from several perspectives is provided to reveal how source\nsyntax benefits NMT.</p>\n", "tags": [] },
{"key": "li2017neural", "citations": "285", "year": "2017", "title":"Neural Attentive Session-based Recommendation", "abstract": "<p>Given e-commerce scenarios that user profiles are invisible, session-based\nrecommendation is proposed to generate recommendation results from short\nsessions. Previous work only considers the user’s sequential behavior in the\ncurrent session, whereas the user’s main purpose in the current session is not\nemphasized. In this paper, we propose a novel neural networks framework, i.e.,\nNeural Attentive Recommendation Machine (NARM), to tackle this problem.\nSpecifically, we explore a hybrid encoder with an attention mechanism to model\nthe user’s sequential behavior and capture the user’s main purpose in the\ncurrent session, which are combined as a unified session representation later.\nWe then compute the recommendation scores for each candidate item with a\nbi-linear matching scheme based on this unified session representation. We\ntrain NARM by jointly learning the item and session representations as well as\ntheir matchings. We carried out extensive experiments on two benchmark\ndatasets. Our experimental results show that NARM outperforms state-of-the-art\nbaselines on both datasets. Furthermore, we also find that NARM achieves a\nsignificant improvement on long sessions, which demonstrates its advantages in\nmodeling the user’s sequential behavior and main purpose simultaneously.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","SIGIR","Tools"] },
{"key": "li2017paraphrase", "citations": "204", "year": "2018", "title":"Paraphrase Generation With Deep Reinforcement Learning", "abstract": "<p>Automatic generation of paraphrases from a given sentence is an important yet\nchallenging task in natural language processing (NLP), and plays a key role in\na number of applications such as question answering, search, and dialogue. In\nthis paper, we present a deep reinforcement learning approach to paraphrase\ngeneration. Specifically, we propose a new framework for the task, which\nconsists of a \\textit{generator} and an \\textit{evaluator}, both of which are\nlearned from data. The generator, built as a sequence-to-sequence learning\nmodel, can produce paraphrases given a sentence. The evaluator, constructed as\na deep matching model, can judge whether two sentences are paraphrases of each\nother. The generator is first trained by deep learning and then further\nfine-tuned by reinforcement learning in which the reward is given by the\nevaluator. For the learning of the evaluator, we propose two methods based on\nsupervised learning and inverse reinforcement learning respectively, depending\non the type of available training data. Empirical study shows that the learned\nevaluator can guide the generator to produce more accurate paraphrases.\nExperimental results demonstrate the proposed models (the generators)\noutperform the state-of-the-art methods in paraphrase generation in both\nautomatic evaluation and human evaluation.</p>\n", "tags": ["Applications","EMNLP","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "li2017visual", "citations": "177", "year": "2018", "title":"Visual Question Generation As Dual Task Of Visual Question Answering", "abstract": "<p>Recently visual question answering (VQA) and visual question generation (VQG)\nare two trending topics in the computer vision, which have been explored\nseparately. In this work, we propose an end-to-end unified framework, the\nInvertible Question Answering Network (iQAN), to leverage the complementary\nrelations between questions and answers in images by jointly training the model\non VQA and VQG tasks. Corresponding parameter sharing scheme and regular terms\nare proposed as constraints to explicitly leverage Q,A’s dependencies to guide\nthe training process. After training, iQAN can take either question or answer\nas input, then output the counterpart. Evaluated on the large-scale visual\nquestion answering datasets CLEVR and VQA2, our iQAN improves the VQA accuracy\nover the baselines. We also show the dual learning framework of iQAN can be\ngeneralized to other VQA architectures and consistently improve the results\nover both the VQA and VQG tasks.</p>\n", "tags": ["CVPR","Datasets","Tools","Training Techniques"] },
{"key": "li2018bytes", "citations": "133", "year": "2019", "title":"Bytes Are All You Need: End-to-end Multilingual Speech Recognition And Synthesis With Bytes", "abstract": "<p>We present two end-to-end models: Audio-to-Byte (A2B) and Byte-to-Audio\n(B2A), for multilingual speech recognition and synthesis. Prior work has\npredominantly used characters, sub-words or words as the unit of choice to\nmodel text. These units are difficult to scale to languages with large\nvocabularies, particularly in the case of multilingual processing. In this\nwork, we model text via a sequence of Unicode bytes, specifically, the UTF-8\nvariable length byte sequence for each character. Bytes allow us to avoid large\nsoftmaxes in languages with large vocabularies, and share representations in\nmultilingual models. We show that bytes are superior to grapheme characters\nover a wide variety of languages in monolingual end-to-end speech recognition.\nAdditionally, our multilingual byte model outperform each respective single\nlanguage baseline on average by 4.4% relatively. In Japanese-English\ncode-switching speech, our multilingual byte model outperform our monolingual\nbaseline by 38.6% relatively. Finally, we present an end-to-end multilingual\nspeech synthesis model using byte representations which matches the performance\nof our monolingual baselines.</p>\n", "tags": ["ICASSP"] },
{"key": "li2018end", "citations": "74", "year": "2019", "title":"End-to-end Video Captioning With Multitask Reinforcement Learning", "abstract": "<p>Although end-to-end (E2E) learning has led to impressive progress on a\nvariety of visual understanding tasks, it is often impeded by hardware\nconstraints (e.g., GPU memory) and is prone to overfitting. When it comes to\nvideo captioning, one of the most challenging benchmark tasks in computer\nvision, those limitations of E2E learning are especially amplified by the fact\nthat both the input videos and output captions are lengthy sequences. Indeed,\nstate-of-the-art methods for video captioning process video frames by\nconvolutional neural networks and generate captions by unrolling recurrent\nneural networks. If we connect them in an E2E manner, the resulting model is\nboth memory-consuming and data-hungry, making it extremely hard to train. In\nthis paper, we propose a multitask reinforcement learning approach to training\nan E2E video captioning model. The main idea is to mine and construct as many\neffective tasks (e.g., attributes, rewards, and the captions) as possible from\nthe human captioned videos such that they can jointly regulate the search space\nof the E2E neural network, from which an E2E video captioning model can be\nfound and generalized to the testing phase. To the best of our knowledge, this\nis the first video captioning model that is trained end-to-end from the raw\nvideo input to the caption output. Experimental results show that such a model\noutperforms existing ones to a large margin on two benchmark video captioning\ndatasets.</p>\n", "tags": ["Applications","Datasets","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "li2018hybrid", "citations": "134", "year": "2018", "title":"Hybrid Retrieval-generation Reinforced Agent For Medical Image Report Generation", "abstract": "<p>Generating long and coherent reports to describe medical images poses\nchallenges to bridging visual patterns with informative human linguistic\ndescriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent\n(HRGR-Agent) which reconciles traditional retrieval-based approaches populated\nwith human prior knowledge, with modern learning-based approaches to achieve\nstructured, robust, and diverse report generation. HRGR-Agent employs a\nhierarchical decision-making procedure. For each sentence, a high-level\nretrieval policy module chooses to either retrieve a template sentence from an\noff-the-shelf template database, or invoke a low-level generation module to\ngenerate a new sentence. HRGR-Agent is updated via reinforcement learning,\nguided by sentence-level and word-level rewards. Experiments show that our\napproach achieves the state-of-the-art results on two medical report datasets,\ngenerating well-balanced structured sentences with robust coverage of\nheterogeneous medical report contents. In addition, our model achieves the\nhighest detection accuracy of medical terminologies, and improved human\nevaluation performance.</p>\n", "tags": ["Agentic","Datasets","Evaluation","RAG","Reinforcement Learning"] },
{"key": "li2018microsoft", "citations": "67", "year": "2018", "title":"Microsoft Dialogue Challenge: Building End-to-end Task-completion Dialogue Systems", "abstract": "<p>This proposal introduces a Dialogue Challenge for building end-to-end\ntask-completion dialogue systems, with the goal of encouraging the dialogue\nresearch community to collaborate and benchmark on standard datasets and\nunified experimental environment. In this special session, we will release\nhuman-annotated conversational data in three domains (movie-ticket booking,\nrestaurant reservation, and taxi booking), as well as an experiment platform\nwith built-in simulators in each domain, for training and evaluation purposes.\nThe final submitted systems will be evaluated both in simulated setting and by\nhuman judges.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Tools","Training Techniques"] },
{"key": "li2018multi", "citations": "176", "year": "2018", "title":"Multi-head Attention With Disagreement Regularization", "abstract": "<p>Multi-head attention is appealing for the ability to jointly attend to\ninformation from different representation subspaces at different positions. In\nthis work, we introduce a disagreement regularization to explicitly encourage\nthe diversity among multiple attention heads. Specifically, we propose three\ntypes of disagreement regularization, which respectively encourage the\nsubspace, the attended positions, and the output representation associated with\neach attention head to be different from other heads. Experimental results on\nwidely-used WMT14 English-German and WMT17 Chinese-English translation tasks\ndemonstrate the effectiveness and universality of the proposed approach.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "li2018neural", "citations": "632", "year": "2019", "title":"Neural Speech Synthesis With Transformer Network", "abstract": "<p>Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2)\nare proposed and achieve state-of-the-art performance, they still suffer from\ntwo problems: 1) low efficiency during training and inference; 2) hard to model\nlong dependency using current recurrent neural networks (RNNs). Inspired by the\nsuccess of Transformer network in neural machine translation (NMT), in this\npaper, we introduce and adapt the multi-head attention mechanism to replace the\nRNN structures and also the original attention mechanism in Tacotron2. With the\nhelp of multi-head self-attention, the hidden states in the encoder and decoder\nare constructed in parallel, which improves the training efficiency. Meanwhile,\nany two inputs at different times are connected directly by self-attention\nmechanism, which solves the long range dependency problem effectively. Using\nphoneme sequences as input, our Transformer TTS network generates mel\nspectrograms, followed by a WaveNet vocoder to output the final audio results.\nExperiments are conducted to test the efficiency and performance of our new\nnetwork. For the efficiency, our Transformer TTS network can speed up the\ntraining about 4.25 times faster compared with Tacotron2. For the performance,\nrigorous human tests show that our proposed model achieves state-of-the-art\nperformance (outperforms Tacotron2 with a gap of 0.048) and is very close to\nhuman quality (4.39 vs 4.44 in MOS).</p>\n", "tags": ["AAAI","Model Architecture","Training Techniques"] },
{"key": "li2018textbugger", "citations": "571", "year": "2019", "title":"Textbugger: Generating Adversarial Text Against Real-world Applications", "abstract": "<p>Deep Learning-based Text Understanding (DLTU) is the backbone technique\nbehind various applications, including question answering, machine translation,\nand text classification. Despite its tremendous popularity, the security\nvulnerabilities of DLTU are still largely unknown, which is highly concerning\ngiven its increasing use in security-sensitive applications such as sentiment\nanalysis and toxic content detection. In this paper, we show that DLTU is\ninherently vulnerable to adversarial text attacks, in which maliciously crafted\ntexts trigger target DLTU systems and services to misbehave. Specifically, we\npresent TextBugger, a general attack framework for generating adversarial\ntexts. In contrast to prior works, TextBugger differs in significant ways: (i)\neffective – it outperforms state-of-the-art attacks in terms of attack success\nrate; (ii) evasive – it preserves the utility of benign text, with 94.9% of\nthe adversarial text correctly recognized by human readers; and (iii) efficient\n– it generates adversarial text with computational complexity sub-linear to\nthe text length. We empirically evaluate TextBugger on a set of real-world DLTU\nsystems and services used for sentiment analysis and toxic content detection,\ndemonstrating its effectiveness, evasiveness, and efficiency. For instance,\nTextBugger achieves 100% success rate on the IMDB dataset based on Amazon AWS\nComprehend within 4.61 seconds and preserves 97% semantic similarity. We\nfurther discuss possible defense mechanisms to mitigate such attack and the\nadversary’s potential countermeasures, which leads to promising directions for\nfurther research.</p>\n", "tags": ["Applications","Datasets","Security","Tools"] },
{"key": "li2018towards", "citations": "126", "year": "2018", "title":"Towards Deep Conversational Recommendations", "abstract": "<p>There has been growing interest in using neural networks and deep learning\ntechniques to create dialogue systems. Conversational recommendation is an\ninteresting setting for the scientific exploration of dialogue with natural\nlanguage as the associated discourse involves goal-driven dialogue that often\ntransforms naturally into more free-form chat. This paper provides two\ncontributions. First, until now there has been no publicly available\nlarge-scale dataset consisting of real-world dialogues centered around\nrecommendations. To address this issue and to facilitate our exploration here,\nwe have collected ReDial, a dataset consisting of over 10,000 conversations\ncentered around the theme of providing movie recommendations. We make this data\navailable to the community for further research. Second, we use this dataset to\nexplore multiple facets of conversational recommendations. In particular we\nexplore new neural architectures, mechanisms, and methods suitable for\ncomposing conversational recommendation systems. Our dataset allows us to\nsystematically probe model sub-components addressing different parts of the\noverall problem domain ranging from: sentiment analysis and cold-start\nrecommendation generation to detailed aspects of how natural language is used\nin this setting in the real world. We combine such sub-components into a\nfull-blown dialogue system and examine its behavior.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn"] },
{"key": "li2019acute", "citations": "84", "year": "2019", "title":"ACUTE-EVAL: Improved Dialogue Evaluation With Optimized Questions And Multi-turn Comparisons", "abstract": "<p>While dialogue remains an important end-goal of natural language research,\nthe difficulty of evaluation is an oft-quoted reason why it remains troublesome\nto make real progress towards its solution. Evaluation difficulties are\nactually two-fold: not only do automatic metrics not correlate well with human\njudgments, but also human judgments themselves are in fact difficult to\nmeasure. The two most used human judgment tests, single-turn pairwise\nevaluation and multi-turn Likert scores, both have serious flaws as we discuss\nin this work.\n  We instead provide a novel procedure involving comparing two full dialogues,\nwhere a human judge is asked to pay attention to only one speaker within each,\nand make a pairwise judgment. The questions themselves are optimized to\nmaximize the robustness of judgments across different annotators, resulting in\nbetter tests. We also show how these tests work in self-play model chat setups,\nresulting in faster, cheaper tests. We hope these tests become the de facto\nstandard, and will release open-source code to that end.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","Model Architecture"] },
{"key": "li2019augmenting", "citations": "70", "year": "2019", "title":"Augmenting Neural Networks With First-order Logic", "abstract": "<p>Today, the dominant paradigm for training neural networks involves minimizing\ntask loss on a large dataset. Using world knowledge to inform a model, and yet\nretain the ability to perform end-to-end training remains an open question. In\nthis paper, we present a novel framework for introducing declarative knowledge\nto neural network architectures in order to guide training and prediction. Our\nframework systematically compiles logical statements into computation graphs\nthat augment a neural network without extra learnable parameters or manual\nredesign. We evaluate our modeling strategy on three tasks: machine\ncomprehension, natural language inference, and text chunking. Our experiments\nshow that knowledge-augmented networks can strongly improve over baselines,\nespecially in low-data regimes.</p>\n", "tags": ["Datasets","RAG","Tools","Training Techniques"] },
{"key": "li2019controllable", "citations": "143", "year": "2019", "title":"Controllable Text-to-image Generation", "abstract": "<p>In this paper, we propose a novel controllable text-to-image generative\nadversarial network (ControlGAN), which can effectively synthesise high-quality\nimages and also control parts of the image generation according to natural\nlanguage descriptions. To achieve this, we introduce a word-level spatial and\nchannel-wise attention-driven generator that can disentangle different visual\nattributes, and allow the model to focus on generating and manipulating\nsubregions corresponding to the most relevant words. Also, a word-level\ndiscriminator is proposed to provide fine-grained supervisory feedback by\ncorrelating words with image regions, facilitating training an effective\ngenerator which is able to manipulate specific visual attributes without\naffecting the generation of other content. Furthermore, perceptual loss is\nadopted to reduce the randomness involved in the image generation, and to\nencourage the generator to manipulate specific attributes required in the\nmodified text. Extensive experiments on benchmark datasets demonstrate that our\nmethod outperforms existing state of the art, and is able to effectively\nmanipulate synthetic images using natural language descriptions. Code is\navailable at https://github.com/mrlibw/ControlGAN.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "li2019decomposable", "citations": "86", "year": "2019", "title":"Decomposable Neural Paraphrase Generation", "abstract": "<p>Paraphrasing exists at different granularity levels, such as lexical level,\nphrasal level and sentential level. This paper presents Decomposable Neural\nParaphrase Generator (DNPG), a Transformer-based model that can learn and\ngenerate paraphrases of a sentence at different levels of granularity in a\ndisentangled way. Specifically, the model is composed of multiple encoders and\ndecoders with different structures, each of which corresponds to a specific\ngranularity. The empirical study shows that the decomposition mechanism of DNPG\nmakes paraphrase generation more interpretable and controllable. Based on DNPG,\nwe further develop an unsupervised domain adaptation method for paraphrase\ngeneration. Experimental results show that the proposed model achieves\ncompetitive in-domain performance compared to the state-of-the-art neural\nmodels, and significantly better performance when adapting to a new domain.</p>\n", "tags": ["Fine-Tuning","Model Architecture"] },
{"key": "li2019entity", "citations": "327", "year": "2019", "title":"Entity-relation Extraction As Multi-turn Question Answering", "abstract": "<p>In this paper, we propose a new paradigm for the task of entity-relation\nextraction. We cast the task as a multi-turn question answering problem, i.e.,\nthe extraction of entities and relations is transformed to the task of\nidentifying answer spans from the context. This multi-turn QA formalization\ncomes with several key advantages: firstly, the question query encodes\nimportant information for the entity/relation class we want to identify;\nsecondly, QA provides a natural way of jointly modeling entity and relation;\nand thirdly, it allows us to exploit the well developed machine reading\ncomprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora\ndemonstrate that the proposed paradigm significantly outperforms previous best\nmodels. We are able to obtain the state-of-the-art results on all of the ACE04,\nACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets\nto 49.4 (+1.0), 60.2 (+0.6) and 68.9 (+2.1), respectively. Additionally, we\nconstruct a newly developed dataset RESUME in Chinese, which requires\nmulti-step reasoning to construct entity dependencies, as opposed to the\nsingle-step dependency extraction in the triplet exaction in previous datasets.\nThe proposed multi-turn QA model also achieves the best performance on the\nRESUME dataset.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn"] },
{"key": "li2019hint", "citations": "65", "year": "2019", "title":"Hint-based Training For Non-autoregressive Machine Translation", "abstract": "<p>Due to the unparallelizable nature of the autoregressive factorization,\nAutoRegressive Translation (ART) models have to generate tokens sequentially\nduring decoding and thus suffer from high inference latency. Non-AutoRegressive\nTranslation (NART) models were proposed to reduce the inference time, but could\nonly achieve inferior translation accuracy. In this paper, we proposed a novel\napproach to leveraging the hints from hidden states and word alignments to help\nthe training of NART models. The results achieve significant improvement over\nprevious NART models for the WMT14 En-De and De-En datasets and are even\ncomparable to a strong LSTM-based ART baseline but one order of magnitude\nfaster in inference.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "li2019incremental", "citations": "105", "year": "2019", "title":"Incremental Transformer With Deliberation Decoder For Document Grounded Conversations", "abstract": "<p>Document Grounded Conversations is a task to generate dialogue responses when\nchatting about the content of a given document. Obviously, document knowledge\nplays a critical role in Document Grounded Conversations, while existing\ndialogue models do not exploit this kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-based architecture for multi-turn\ndocument grounded conversations. In particular, we devise an Incremental\nTransformer to encode multi-turn utterances along with knowledge in related\ndocuments. Motivated by the human cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve context coherence and knowledge\ncorrectness. Our empirical study on a real-world Document Grounded Dataset\nproves that responses generated by our model significantly outperform\ncompetitive baselines on both context coherence and knowledge relevance.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "li2019jasper", "citations": "212", "year": "2019", "title":"Jasper: An End-to-end Convolutional Neural Acoustic Model", "abstract": "<p>In this paper, we report state-of-the-art results on LibriSpeech among\nend-to-end speech recognition models without any external training data. Our\nmodel, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout,\nand residual connections. To improve training, we further introduce a new\nlayer-wise optimizer called NovoGrad. Through experiments, we demonstrate that\nthe proposed deep architecture performs as well or better than more complex\nchoices. Our deepest Jasper variant uses 54 convolutional layers. With this\narchitecture, we achieve 2.95% WER using a beam-search decoder with an external\nneural language model and 3.86% WER with a greedy decoder on LibriSpeech\ntest-clean. We also report competitive results on the Wall Street Journal and\nthe Hub5’00 conversational evaluation datasets.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "li2019object", "citations": "304", "year": "2019", "title":"Object-driven Text-to-image Synthesis Via Adversarial Training", "abstract": "<p>In this paper, we propose Object-driven Attentive Generative Adversarial\nNewtorks (Obj-GANs) that allow object-centered text-to-image synthesis for\ncomplex scenes. Following the two-step (layout-image) generation process, a\nnovel object-driven attentive image generator is proposed to synthesize salient\nobjects by paying attention to the most relevant words in the text description\nand the pre-generated semantic layout. In addition, a new Fast R-CNN based\nobject-wise discriminator is proposed to provide rich object-wise\ndiscrimination signals on whether the synthesized object matches the text\ndescription and the pre-generated layout. The proposed Obj-GAN significantly\noutperforms the previous state of the art in various metrics on the large-scale\nCOCO benchmark, increasing the Inception score by 27% and decreasing the FID\nscore by 11%. A thorough comparison between the traditional grid attention and\nthe new object-driven attention is provided through analyzing their mechanisms\nand visualizing their attention layers, showing insights of how the proposed\nmodel generates complex scenes in high quality.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "li2019pointing", "citations": "80", "year": "2019", "title":"Pointing Novel Objects In Image Captioning", "abstract": "<p>Image captioning has received significant attention with remarkable\nimprovements in recent advances. Nevertheless, images in the wild encapsulate\nrich knowledge and cannot be sufficiently described with models built on\nimage-caption pairs containing only in-domain objects. In this paper, we\npropose to address the problem by augmenting standard deep captioning\narchitectures with object learners. Specifically, we present Long Short-Term\nMemory with Pointing (LSTM-P) — a new architecture that facilitates\nvocabulary expansion and produces novel objects via pointing mechanism.\nTechnically, object learners are initially pre-trained on available object\nrecognition data. Pointing in LSTM-P then balances the probability between\ngenerating a word through LSTM and copying a word from the recognized objects\nat each time step in decoder stage. Furthermore, our captioning encourages\nglobal coverage of objects in the sentence. Extensive experiments are conducted\non both held-out COCO image captioning and ImageNet datasets for describing\nnovel objects, and superior results are reported when comparing to\nstate-of-the-art approaches. More remarkably, we obtain an average of 60.9% in\nF1 score on held-out COCO~dataset.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "li2019relation", "citations": "357", "year": "2019", "title":"Relation-aware Graph Attention Network For Visual Question Answering", "abstract": "<p>In order to answer semantically-complicated questions about an image, a\nVisual Question Answering (VQA) model needs to fully understand the visual\nscene in the image, especially the interactive dynamics between different\nobjects. We propose a Relation-aware Graph Attention Network (ReGAT), which\nencodes each image into a graph and models multi-type inter-object relations\nvia a graph attention mechanism, to learn question-adaptive relation\nrepresentations. Two types of visual object relations are explored: (i)\nExplicit Relations that represent geometric positions and semantic interactions\nbetween objects; and (ii) Implicit Relations that capture the hidden dynamics\nbetween image regions. Experiments demonstrate that ReGAT outperforms prior\nstate-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further\nshow that ReGAT is compatible to existing VQA architectures, and can be used as\na generic relation encoder to boost the model performance for VQA.</p>\n", "tags": ["Datasets","ICCV","Model Architecture"] },
{"key": "li2019robust", "citations": "85", "year": "2019", "title":"Robust Navigation With Language Pretraining And Stochastic Sampling", "abstract": "<p>Core to the vision-and-language navigation (VLN) challenge is building robust\ninstruction representations and action decoding schemes, which can generalize\nwell to previously unseen instructions and environments. In this paper, we\nreport two simple but highly effective methods to address these challenges and\nlead to a new state-of-the-art performance. First, we adapt large-scale\npretrained language models to learn text representations that generalize better\nto previously unseen instructions. Second, we propose a stochastic sampling\nscheme to reduce the considerable gap between the expert actions in training\nand sampled actions in test, so that the agent can learn to correct its own\nmistakes during long sequential action decoding. Combining the two techniques,\nwe achieve a new state of the art on the Room-to-Room benchmark with 6%\nabsolute gain over the previous best result (47% -&gt; 53%) on the Success Rate\nweighted by Path Length metric.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "li2019unicoder", "citations": "702", "year": "2020", "title":"Unicoder-vl: A Universal Encoder For Vision And Language By Cross-modal Pre-training", "abstract": "<p>We propose Unicoder-VL, a universal encoder that aims to learn joint\nrepresentations of vision and language in a pre-training manner. Borrow ideas\nfrom cross-lingual pre-trained models, such as XLM and Unicoder, both visual\nand linguistic contents are fed into a multi-layer Transformer for the\ncross-modal pre-training, where three pre-trained tasks are employed, including\nMasked Language Modeling (MLM), Masked Object Classification (MOC) and\nVisual-linguistic Matching (VLM). The first two tasks learn context-aware\nrepresentations for input tokens based on linguistic and visual contents\njointly. The last task tries to predict whether an image and a text describe\neach other. After pretraining on large-scale image-caption pairs, we transfer\nUnicoder-VL to caption-based image-text retrieval and visual commonsense\nreasoning, with just one additional output layer. We achieve state-of-the-art\nor comparable results on both two tasks and show the powerful ability of the\ncross-modal pre-training.</p>\n", "tags": ["AAAI","Model Architecture","Training Techniques"] },
{"key": "li2019visualbert", "citations": "1208", "year": "2019", "title":"Visualbert: A Simple And Performant Baseline For Vision And Language", "abstract": "<p>We propose VisualBERT, a simple and flexible framework for modeling a broad\nrange of vision-and-language tasks. VisualBERT consists of a stack of\nTransformer layers that implicitly align elements of an input text and regions\nin an associated input image with self-attention. We further propose two\nvisually-grounded language model objectives for pre-training VisualBERT on\nimage caption data. Experiments on four vision-and-language tasks including\nVQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with\nstate-of-the-art models while being significantly simpler. Further analysis\ndemonstrates that VisualBERT can ground elements of language to image regions\nwithout any explicit supervision and is even sensitive to syntactic\nrelationships, tracking, for example, associations between verbs and image\nregions corresponding to their arguments.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "li2020bert", "citations": "445", "year": "2020", "title":"BERT-ATTACK: Adversarial Attack Against BERT Using BERT", "abstract": "<p>Adversarial attacks for discrete data (such as texts) have been proved\nsignificantly more challenging than continuous data (such as images) since it\nis difficult to generate adversarial samples with gradient-based methods.\nCurrent successful attack methods for texts usually adopt heuristic replacement\nstrategies on the character or word level, which remains challenging to find\nthe optimal solution in the massive space of possible combinations of\nreplacements while preserving semantic consistency and language fluency. In\nthis paper, we propose \\textbf{BERT-Attack}, a high-quality and effective\nmethod to generate adversarial samples using pre-trained masked language models\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\nneural models in downstream tasks so that we can successfully mislead the\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\nattack strategies in both success rate and perturb percentage, while the\ngenerated adversarial samples are fluent and semantically preserved. Also, the\ncost of calculation is low, thus possible for large-scale generations. The code\nis available at https://github.com/LinyangLee/BERT-Attack.</p>\n", "tags": ["EMNLP","Has Code","Model Architecture","Security"] },
{"key": "li2020bridging", "citations": "61", "year": "2021", "title":"Bridging Text And Video: A Universal Multimodal Transformer For Video-audio Scene-aware Dialog", "abstract": "<p>Audio-Visual Scene-Aware Dialog (AVSD) is a task to generate responses when\nchatting about a given video, which is organized as a track of the 8th Dialog\nSystem Technology Challenge (DSTC8). To solve the task, we propose a universal\nmultimodal transformer and introduce the multi-task learning method to learn\njoint representations among different modalities as well as generate\ninformative and fluent responses. Our method extends the natural language\ngeneration pre-trained model to multimodal dialogue generation task. Our system\nachieves the best performance in both objective and subjective evaluations in\nthe challenge.</p>\n", "tags": ["Model Architecture"] },
{"key": "li2020comparison", "citations": "69", "year": "2020", "title":"A Comparison Of Pre-trained Vision-and-language Models For Multimodal Representation Learning Across Medical Images And Reports", "abstract": "<p>Joint image-text embedding extracted from medical images and associated\ncontextual reports is the bedrock for most biomedical vision-and-language (V+L)\ntasks, including medical visual question answering, clinical image-text\nretrieval, clinical report auto-generation. In this study, we adopt four\npre-trained V+L models: LXMERT, VisualBERT, UNIER and PixelBERT to learn\nmultimodal representation from MIMIC-CXR radiographs and associated reports.\nThe extrinsic evaluation on OpenI dataset shows that in comparison to the\npioneering CNN-RNN model, the joint embedding learned by pre-trained V+L models\ndemonstrate performance improvement in the thoracic findings classification\ntask. We conduct an ablation study to analyze the contribution of certain model\ncomponents and validate the advantage of joint embedding over text-only\nembedding. We also visualize attention maps to illustrate the attention\nmechanism of V+L models.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "li2020contextualized", "citations": "162", "year": "2021", "title":"Contextualized Perturbation For Textual Adversarial Attack", "abstract": "<p>Adversarial examples expose the vulnerabilities of natural language\nprocessing (NLP) models, and can be used to evaluate and improve their\nrobustness. Existing techniques of generating such examples are typically\ndriven by local heuristic rules that are agnostic to the context, often\nresulting in unnatural and ungrammatical outputs. This paper presents CLARE, a\nContextuaLized AdversaRial Example generation model that produces fluent and\ngrammatical outputs through a mask-then-infill procedure. CLARE builds on a\npre-trained masked language model and modifies the inputs in a context-aware\nmanner. We propose three contextualized perturbations, Replace, Insert and\nMerge, allowing for generating outputs of varied lengths. With a richer range\nof available strategies, CLARE is able to attack a victim model more\nefficiently with fewer edits. Extensive experiments and human evaluation\ndemonstrate that CLARE outperforms the baselines in terms of attack success\nrate, textual similarity, fluency and grammaticality.</p>\n", "tags": ["Evaluation","NAACL","Security"] },
{"key": "li2020developing", "citations": "98", "year": "2020", "title":"Developing RNN-T Models Surpassing High-performance Hybrid Models With Customization Capability", "abstract": "<p>Because of its streaming nature, recurrent neural network transducer (RNN-T)\nis a very promising end-to-end (E2E) model that may replace the popular hybrid\nmodel for automatic speech recognition. In this paper, we describe our recent\ndevelopment of RNN-T models with reduced GPU memory consumption during\ntraining, better initialization strategy, and advanced encoder modeling with\nfuture lookahead. When trained with Microsoft’s 65 thousand hours of anonymized\ntraining data, the developed RNN-T model surpasses a very well trained hybrid\nmodel with both better recognition accuracy and lower latency. We further study\nhow to customize RNN-T models to a new domain, which is important for deploying\nE2E models to practical scenarios. By comparing several methods leveraging\ntext-only data in the new domain, we found that updating RNN-T’s prediction and\njoint networks using text-to-speech generated from domain-specific text is the\nmost effective.</p>\n", "tags": ["INTERSPEECH","Training Techniques"] },
{"key": "li2020hero", "citations": "342", "year": "2020", "title":"HERO: Hierarchical Encoder For Video+language Omni-representation Pre-training", "abstract": "<p>We present HERO, a novel framework for large-scale video+language\nomni-representation learning. HERO encodes multimodal inputs in a hierarchical\nstructure, where local context of a video frame is captured by a Cross-modal\nTransformer via multimodal fusion, and global video context is captured by a\nTemporal Transformer. In addition to standard Masked Language Modeling (MLM)\nand Masked Frame Modeling (MFM) objectives, we design two new pre-training\ntasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global\nand local temporal alignment; and (ii) Frame Order Modeling (FOM), where the\nmodel predicts the right order of shuffled video frames. HERO is jointly\ntrained on HowTo100M and large-scale TV datasets to gain deep understanding of\ncomplex social dynamics with multi-character interactions. Comprehensive\nexperiments demonstrate that HERO achieves new state of the art on multiple\nbenchmarks over Text-based Video/Video-moment Retrieval, Video Question\nAnswering (QA), Video-and-language Inference and Video Captioning tasks across\ndifferent domains. We also introduce two new challenging benchmarks How2QA and\nHow2R for Video QA and Retrieval, collected from diverse video content over\nmultimodalities.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Tools","Training Techniques"] },
{"key": "li2020knowledge", "citations": "89", "year": "2022", "title":"Knowledge Bridging For Empathetic Dialogue Generation", "abstract": "<p>Lack of external knowledge makes empathetic dialogue systems difficult to\nperceive implicit emotions and learn emotional interactions from limited\ndialogue history. To address the above problems, we propose to leverage\nexternal knowledge, including commonsense knowledge and emotional lexical\nknowledge, to explicitly understand and express emotions in empathetic dialogue\ngeneration. We first enrich the dialogue history by jointly interacting with\nexternal knowledge and construct an emotional context graph. Then we learn\nemotional context representations from the knowledge-enriched emotional context\ngraph and distill emotional signals, which are the prerequisites to predicate\nemotions expressed in responses. Finally, to generate the empathetic response,\nwe propose an emotional cross-attention mechanism to learn the emotional\ndependencies from the emotional context graph. Extensive experiments conducted\non a benchmark dataset verify the effectiveness of the proposed method. In\naddition, we find the performance of our method can be further improved by\nintegrating with a pre-trained model that works orthogonally.</p>\n", "tags": ["AAAI","Datasets","Dialogue & Multi Turn","Evaluation","Model Architecture"] },
{"key": "li2020mapping", "citations": "65", "year": "2020", "title":"Mapping Natural Language Instructions To Mobile UI Action Sequences", "abstract": "<p>We present a new problem: grounding natural language instructions to mobile\nuser interface actions, and create three new datasets for it. For full task\nevaluation, we create PIXELHELP, a corpus that pairs English instructions with\nactions performed by people on a mobile UI emulator. To scale training, we\ndecouple the language and action data by (a) annotating action phrase spans in\nHowTo instructions and (b) synthesizing grounded descriptions of actions for\nmobile user interfaces. We use a Transformer to extract action phrase tuples\nfrom long-range natural language instructions. A grounding Transformer then\ncontextually represents UI objects using both their content and screen position\nand connects them to object descriptions. Given a starting screen and\ninstruction, our model achieves 70.59% accuracy on predicting complete\nground-truth action sequences in PIXELHELP.</p>\n", "tags": ["Datasets","Evaluation","Instruction Following","Model Architecture","Training Techniques"] },
{"key": "li2020molweni", "citations": "73", "year": "2020", "title":"Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset With Discourse Structure", "abstract": "<p>Research into the area of multiparty dialog has grown considerably over\nrecent years. We present the Molweni dataset, a machine reading comprehension\n(MRC) dataset with discourse structure built over multiparty dialog. Molweni’s\nsource samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising\n88,303 utterances. We annotate 30,066 questions on this corpus, including both\nanswerable and unanswerable questions. Molweni also uniquely contributes\ndiscourse dependency annotations in a modified Segmented Discourse\nRepresentation Theory (SDRT; Asher et al., 2016) style for all of its\nmultiparty dialogs, contributing large-scale (78,245 annotated discourse\nrelations) data to bear on the task of multiparty dialog discourse parsing. Our\nexperiments show that Molweni is a challenging dataset for current MRC models:\nBERT-wwm, a current, strong SQuAD 2.0 performer, achieves only 67.7% F1 on\nMolweni’s questions, a 20+% significant drop as compared against its SQuAD 2.0\nperformance.</p>\n", "tags": ["COLING","Datasets","Model Architecture"] },
{"key": "li2020mtop", "citations": "114", "year": "2021", "title":"MTOP: A Comprehensive Multilingual Task-oriented Semantic Parsing Benchmark", "abstract": "<p>Scaling semantic parsing models for task-oriented dialog systems to new\nlanguages is often expensive and time-consuming due to the lack of available\ndatasets. Available datasets suffer from several shortcomings: a) they contain\nfew languages b) they contain small amounts of labeled examples per language c)\nthey are based on the simple intent and slot detection paradigm for\nnon-compositional queries. In this paper, we present a new multilingual\ndataset, called MTOP, comprising of 100k annotated utterances in 6 languages\nacross 11 domains. We use this dataset and other publicly available datasets to\nconduct a comprehensive benchmarking study on using various state-of-the-art\nmultilingual pre-trained models for task-oriented semantic parsing. We achieve\nan average improvement of +6.3 points on Slot F1 for the two existing\nmultilingual datasets, over best results reported in their experiments.\nFurthermore, we demonstrate strong zero-shot performance using pre-trained\nmodels combined with automatic translation and alignment, and a proposed\ndistant supervision method to reduce the noise in slot label projection.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "li2020multilingual", "citations": "96", "year": "2021", "title":"Multilingual Speech Translation With Efficient Finetuning Of Pretrained Models", "abstract": "<p>We present a simple yet effective approach to build multilingual\nspeech-to-text (ST) translation by efficient transfer learning from pretrained\nspeech encoder and text decoder. Our key finding is that a minimalistic LNA\n(LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and\ncross-modality transfer ability by only finetuning less than 10% of the\npretrained parameters. This enables effectively leveraging large pretrained\nmodels with low training cost. Using wav2vec 2.0 for acoustic modeling, and\nmBART for multilingual text generation, our approach advanced the new\nstate-of-the-art for 34 translation directions (and surpassing cascaded ST for\n23 of them) on large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on\naverage across 15 En-X directions and +5.1 BLEU on average across 19 X-En\ndirections). Our approach demonstrates strong zero-shot performance in a\nmany-to-many multilingual model (+5.7 BLEU on average across 18 non-English\ndirections), making it an appealing approach for attaining high-quality speech\ntranslation with improved parameter and data efficiency.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "li2020optimus", "citations": "125", "year": "2020", "title":"Optimus: Organizing Sentences Via Pre-trained Modeling Of A Latent Space", "abstract": "<p>When trained effectively, the Variational Autoencoder (VAE) can be both a\npowerful generative model and an effective representation learning framework\nfor natural language. In this paper, we propose the first large-scale language\nVAE model, Optimus. A universal latent embedding space for sentences is first\npre-trained on large text corpus, and then fine-tuned for various language\ngeneration and understanding tasks. Compared with GPT-2, Optimus enables guided\nlanguage generation from an abstract level using the latent vectors. Compared\nwith BERT, Optimus can generalize better on low-resource language understanding\ntasks due to the smooth latent space structure. Extensive experimental results\non a wide range of language tasks demonstrate the effectiveness of Optimus. It\nachieves new state-of-the-art on VAE language modeling benchmarks. We hope that\nour first pre-trained big VAE language model itself and results can help the\nNLP community renew the interests of deep generative models in the era of\nlarge-scale pre-training, and make these principled methods more practical.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Tools","Training Techniques"] },
{"key": "li2020oscar", "citations": "1293", "year": "2020", "title":"Oscar: Object-semantics Aligned Pre-training For Vision-language Tasks", "abstract": "<p>Large-scale pre-training methods of learning cross-modal representations on\nimage-text pairs are becoming popular for vision-language tasks. While existing\nmethods simply concatenate image region features and text features as input to\nthe model to be pre-trained and use self-attention to learn image-text semantic\nalignments in a brute force manner, in this paper, we propose a new learning\nmethod Oscar (Object-Semantics Aligned Pre-training), which uses object tags\ndetected in images as anchor points to significantly ease the learning of\nalignments. Our method is motivated by the observation that the salient objects\nin an image can be accurately detected, and are often mentioned in the paired\ntext. We pre-train an Oscar model on the public corpus of 6.5 million\ntext-image pairs, and fine-tune it on downstream tasks, creating new\nstate-of-the-arts on six well-established vision-language understanding and\ngeneration tasks.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "li2020unimo", "citations": "210", "year": "2021", "title":"UNIMO: Towards Unified-modal Understanding And Generation Via Cross-modal Contrastive Learning", "abstract": "<p>Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "li2021align", "citations": "683", "year": "2021", "title":"Align And Prompt: Video-and-language Pre-training With Entity Prompts", "abstract": "<p>Video-and-language pre-training has shown promising improvements on various\ndownstream tasks. Most previous methods capture cross-modal interactions with a\ntransformer-based multimodal encoder, not fully addressing the misalignment\nbetween unimodal video and text features. Besides, learning fine-grained\nvisual-language alignment usually requires off-the-shelf object detectors to\nprovide object information, which is bottlenecked by the detector’s limited\nvocabulary and expensive computation cost.\n  We propose Align and Prompt: an efficient and effective video-and-language\npre-training framework with better cross-modal alignment. First, we introduce a\nvideo-text contrastive (VTC) loss to align unimodal video-text features at the\ninstance level, which eases the modeling of cross-modal interactions. Then, we\npropose a new visually-grounded pre-training task, prompting entity modeling\n(PEM), which aims to learn fine-grained region-entity alignment. To achieve\nthis, we first introduce an entity prompter module, which is trained with VTC\nto produce the similarity between a video crop and text prompts instantiated\nwith entity names. The PEM task then asks the model to predict the entity\npseudo-labels (i.e~normalized similarity scores) for randomly-selected video\ncrops. The resulting pre-trained model achieves state-of-the-art performance on\nboth text-video retrieval and videoQA, outperforming prior work by a\nsubstantial margin. Our code and pre-trained models are available at\nhttps://github.com/salesforce/ALPRO.</p>\n", "tags": ["Has Code","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "li2021contrast", "citations": "71", "year": "2022", "title":"Contrast And Generation Make BART A Good Dialogue Emotion Recognizer", "abstract": "<p>In dialogue systems, utterances with similar semantics may have distinctive\nemotions under different contexts. Therefore, modeling long-range contextual\nemotional relationships with speaker dependency plays a crucial part in\ndialogue emotion recognition. Meanwhile, distinguishing the different emotion\ncategories is non-trivial since they usually have semantically similar\nsentiments. To this end, we adopt supervised contrastive learning to make\ndifferent emotions mutually exclusive to identify similar emotions better.\nMeanwhile, we utilize an auxiliary response generation task to enhance the\nmodel’s ability of handling context information, thereby forcing the model to\nrecognize emotions with similar semantics in diverse contexts. To achieve these\nobjectives, we use the pre-trained encoder-decoder model BART as our backbone\nmodel since it is very suitable for both understanding and generation tasks.\nThe experiments on four datasets demonstrate that our proposed model obtains\nsignificantly more favorable results than the state-of-the-art model in\ndialogue emotion recognition. The ablation study further demonstrates the\neffectiveness of supervised contrastive loss and generative loss.</p>\n", "tags": ["AAAI","Datasets","Dialogue & Multi Turn"] },
{"key": "li2021document", "citations": "200", "year": "2021", "title":"Document-level Event Argument Extraction By Conditional Generation", "abstract": "<p>Event extraction has long been treated as a sentence-level task in the IE\ncommunity. We argue that this setting does not match human information-seeking\nbehavior and leads to incomplete and uninformative extraction results. We\npropose a document-level neural event argument extraction model by formulating\nthe task as conditional generation following event templates. We also compile a\nnew document-level event extraction benchmark dataset WikiEvents which includes\ncomplete event and coreference annotation. On the task of argument extraction,\nwe achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on\nthe RAMS and WikiEvents datasets respectively. On the more challenging task of\ninformative argument extraction, which requires implicit coreference reasoning,\nwe achieve a 9.3% F1 gain over the best baseline. To demonstrate the\nportability of our model, we also create the first end-to-end zero-shot event\nextraction framework and achieve 97% of fully supervised model’s trigger\nextraction performance and 82% of the argument extraction performance given\nonly access to 10 out of the 33 types on ACE.</p>\n", "tags": ["Datasets","Evaluation","NAACL","Tools"] },
{"key": "li2021grounded", "citations": "501", "year": "2022", "title":"Grounded Language-image Pre-training", "abstract": "<p>This paper presents a grounded language-image pre-training (GLIP) model for\nlearning object-level, language-aware, and semantic-rich visual\nrepresentations. GLIP unifies object detection and phrase grounding for\npre-training. The unification brings two benefits: 1) it allows GLIP to learn\nfrom both detection and grounding data to improve both tasks and bootstrap a\ngood grounding model; 2) GLIP can leverage massive image-text pairs by\ngenerating grounding boxes in a self-training fashion, making the learned\nrepresentation semantic-rich. In our experiments, we pre-train GLIP on 27M\ngrounding data, including 3M human-annotated and 24M web-crawled image-text\npairs. The learned representations demonstrate strong zero-shot and few-shot\ntransferability to various object-level recognition tasks. 1) When directly\nevaluated on COCO and LVIS (without seeing any images in COCO during\npre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many\nsupervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val\nand 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13\ndownstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised\nDynamic Head. Code is released at https://github.com/microsoft/GLIP.</p>\n", "tags": ["CVPR","Few-Shot","Has Code","Training Techniques"] },
{"key": "li2021hidden", "citations": "85", "year": "2021", "title":"Hidden Backdoors In Human-centric Language Models", "abstract": "<p>Natural language processing (NLP) systems have been proven to be vulnerable\nto backdoor attacks, whereby hidden features (backdoors) are trained into a\nlanguage model and may only be activated by specific inputs (called triggers),\nto trick the model into producing unexpected behaviors. In this paper, we\ncreate covert and natural triggers for textual backdoor attacks, \\textit{hidden\nbackdoors}, where triggers can fool both modern language models and human\ninspection. We deploy our hidden backdoors through two state-of-the-art trigger\nembedding methods. The first approach via homograph replacement, embeds the\ntrigger into deep neural networks through the visual spoofing of lookalike\ncharacter replacement. The second approach uses subtle differences between text\ngenerated by language models and real natural text to produce trigger sentences\nwith correct grammar and high fluency. We demonstrate that the proposed hidden\nbackdoors can be effective across three downstream security-critical NLP tasks,\nrepresentative of modern human-centric NLP systems, including toxic comment\ndetection, neural machine translation (NMT), and question answering (QA). Our\ntwo hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at\nleast \\(97%\\) with an injection rate of only \\(3%\\) in toxic comment detection,\n\\(95.1%\\) ASR in NMT with less than \\(0.5%\\) injected data, and finally \\(91.12%\\)\nASR against QA updated with only 27 poisoning data samples on a model\npreviously trained with 92,024 samples (0.029%). We are able to demonstrate\nthe adversary’s high success rate of attacks, while maintaining functionality\nfor regular users, with triggers inconspicuous by the human administrators.</p>\n", "tags": ["Security"] },
{"key": "li2021implicit", "citations": "65", "year": "2021", "title":"Implicit Representations Of Meaning In Neural Language Models", "abstract": "<p>Does the effectiveness of neural language models derive entirely from\naccurate modeling of surface word co-occurrence statistics, or do these models\nrepresent and reason about the world they describe? In BART and T5 transformer\nlanguage models, we identify contextual word representations that function as\nmodels of entities and situations as they evolve throughout a discourse. These\nneural representations have functional similarities to linguistic models of\ndynamic semantics: they support a linear readout of each entity’s current\nproperties and relations, and can be manipulated with predictable effects on\nlanguage generation. Our results indicate that prediction in pretrained neural\nlanguage models is supported, at least in part, by dynamic representations of\nmeaning and implicit simulation of entity state, and that this behavior can be\nlearned with only text as training data. Code and data are available at\nhttps://github.com/belindal/state-probes .</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "li2021lightweight", "citations": "84", "year": "2021", "title":"Lightweight Self-attentive Sequential Recommendation", "abstract": "<p>Modern deep neural networks (DNNs) have greatly facilitated the development\nof sequential recommender systems by achieving state-of-the-art recommendation\nperformance on various sequential recommendation tasks. Given a sequence of\ninteracted items, existing DNN-based sequential recommenders commonly embed\neach item into a unique vector to support subsequent computations of the user\ninterest. However, due to the potentially large number of items, the\nover-parameterised item embedding matrix of a sequential recommender has become\na memory bottleneck for efficient deployment in resource-constrained\nenvironments, e.g., smartphones and other edge devices. Furthermore, we observe\nthat the widely-used multi-head self-attention, though being effective in\nmodelling sequential dependencies among items, heavily relies on redundant\nattention units to fully capture both global and local item-item transition\npatterns within a sequence.\n  In this paper, we introduce a novel lightweight self-attentive network (LSAN)\nfor sequential recommendation. To aggressively compress the original embedding\nmatrix, LSAN leverages the notion of compositional embeddings, where each item\nembedding is composed by merging a group of selected base embedding vectors\nderived from substantially smaller embedding matrices. Meanwhile, to account\nfor the intrinsic dynamics of each item, we further propose a temporal\ncontext-aware embedding composition scheme. Besides, we develop an innovative\ntwin-attention network that alleviates the redundancy of the traditional\nmulti-head self-attention while retaining full capacity for capturing long- and\nshort-term (i.e., global and local) item dependencies. Comprehensive\nexperiments demonstrate that LSAN significantly advances the accuracy and\nmemory efficiency of existing sequential recommenders.</p>\n", "tags": ["CIKM","Efficiency","Model Architecture"] },
{"key": "li2021personalized", "citations": "91", "year": "2021", "title":"Personalized Transformer For Explainable Recommendation", "abstract": "<p>Personalization of natural language generation plays a vital role in a large\nspectrum of tasks, such as explainable recommendation, review summarization and\ndialog systems. In these tasks, user and item IDs are important identifiers for\npersonalization. Transformer, which is demonstrated with strong language\nmodeling capability, however, is not personalized and fails to make use of the\nuser and item IDs since the ID tokens are not even in the same semantic space\nas the words. To address this problem, we present a PErsonalized Transformer\nfor Explainable Recommendation (PETER), on which we design a simple and\neffective learning objective that utilizes the IDs to predict the words in the\ntarget explanation, so as to endow the IDs with linguistic meanings and to\nachieve personalized Transformer. Besides generating explanations, PETER can\nalso make recommendations, which makes it a unified model for the whole\nrecommendation-explanation pipeline. Extensive experiments show that our small\nunpretrained model outperforms fine-tuned BERT on the generation task, in terms\nof both effectiveness and efficiency, which highlights the importance and the\nnice utility of our design.</p>\n", "tags": ["Model Architecture"] },
{"key": "li2021prefix", "citations": "1656", "year": "2021", "title":"Prefix-tuning: Optimizing Continuous Prompts For Generation", "abstract": "<p>Fine-tuning is the de facto way to leverage large pretrained language models\nto perform downstream tasks. However, it modifies all the language model\nparameters and therefore necessitates storing a full copy for each task. In\nthis paper, we propose prefix-tuning, a lightweight alternative to fine-tuning\nfor natural language generation tasks, which keeps language model parameters\nfrozen, but optimizes a small continuous task-specific vector (called the\nprefix). Prefix-tuning draws inspiration from prompting, allowing subsequent\ntokens to attend to this prefix as if it were “virtual tokens”. We apply\nprefix-tuning to GPT-2 for table-to-text generation and to BART for\nsummarization. We find that by learning only 0.1% of the parameters,\nprefix-tuning obtains comparable performance in the full data setting,\noutperforms fine-tuning in low-data settings, and extrapolates better to\nexamples with topics unseen during training.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "li2021pretrained", "citations": "102", "year": "2021", "title":"Pretrained Language Models For Text Generation: A Survey", "abstract": "<p>Text generation has become one of the most important yet challenging tasks in\nnatural language processing (NLP). The resurgence of deep learning has greatly\nadvanced this field by neural generation models, especially the paradigm of\npretrained language models (PLMs). In this paper, we present an overview of the\nmajor advances achieved in the topic of PLMs for text generation. As the\npreliminaries, we present the general task definition and briefly describe the\nmainstream architectures of PLMs for text generation. As the core content, we\ndiscuss how to adapt existing PLMs to model different input data and satisfy\nspecial properties in the generated text. We further summarize several\nimportant fine-tuning strategies for text generation. Finally, we present\nseveral future directions and conclude this paper. Our survey aims to provide\ntext generation researchers a synthesis and pointer to related research.</p>\n", "tags": ["Fine-Tuning","IJCAI","Survey Paper","Training Techniques"] },
{"key": "li2021referring", "citations": "66", "year": "2021", "title":"Referring Transformer: A One-step Approach To Multi-task Visual Grounding", "abstract": "<p>As an important step towards visual reasoning, visual grounding (e.g., phrase\nlocalization, referring expression comprehension/segmentation) has been widely\nexplored Previous approaches to referring expression comprehension (REC) or\nsegmentation (RES) either suffer from limited performance, due to a two-stage\nsetup, or require the designing of complex task-specific one-stage\narchitectures. In this paper, we propose a simple one-stage multi-task\nframework for visual grounding tasks. Specifically, we leverage a transformer\narchitecture, where two modalities are fused in a visual-lingual encoder. In\nthe decoder, the model learns to generate contextualized lingual queries which\nare then decoded and used to directly regress the bounding box and produce a\nsegmentation mask for the corresponding referred regions. With this simple but\nhighly contextualized model, we outperform state-of-the-arts methods by a large\nmargin on both REC and RES tasks. We also show that a simple pre-training\nschedule (on an external dataset) further improves the performance. Extensive\nexperiments and ablations illustrate that our model benefits greatly from\ncontextualized information and multi-task training.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "li2021selfdoc", "citations": "90", "year": "2021", "title":"Selfdoc: Self-supervised Document Representation Learning", "abstract": "<p>We propose SelfDoc, a task-agnostic pre-training framework for document image\nunderstanding. Because documents are multimodal and are intended for sequential\nreading, our framework exploits the positional, textual, and visual information\nof every semantically meaningful component in a document, and it models the\ncontextualization between each block of content. Unlike existing document\npre-training models, our model is coarse-grained instead of treating individual\nwords as input, therefore avoiding an overly fine-grained with excessive\ncontextualization. Beyond that, we introduce cross-modal learning in the model\npre-training phase to fully leverage multimodal information from unlabeled\ndocuments. For downstream usage, we propose a novel modality-adaptive attention\nmechanism for multimodal feature fusion by adaptively emphasizing language and\nvision signals. Our framework benefits from self-supervised pre-training on\ndocuments without requiring annotations by a feature masking training strategy.\nIt achieves superior performance on multiple downstream tasks with\nsignificantly fewer document images used in the pre-training stage compared to\nprevious works.</p>\n", "tags": ["CVPR","Model Architecture","Tools","Training Techniques"] },
{"key": "li2021structext", "citations": "94", "year": "2021", "title":"Structext: Structured Text Understanding With Multi-modal Transformers", "abstract": "<p>Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "li2021structurallm", "citations": "75", "year": "2021", "title":"Structurallm: Structural Pre-training For Form Understanding", "abstract": "<p>Large pre-trained language models achieve state-of-the-art results when\nfine-tuned on downstream NLP tasks. However, they almost exclusively focus on\ntext-only representation, while neglecting cell-level layout information that\nis important for form image understanding. In this paper, we propose a new\npre-training approach, StructuralLM, to jointly leverage cell and layout\ninformation from scanned documents. Specifically, we pre-train StructuralLM\nwith two new designs to make the most of the interactions of cell and layout\ninformation: 1) each cell as a semantic unit; 2) classification of cell\npositions. The pre-trained StructuralLM achieves new state-of-the-art results\nin different types of downstream tasks, including form understanding (from\n78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and\ndocument image classification (from 94.43 to 96.08).</p>\n", "tags": ["Training Techniques"] },
{"key": "li2022blip", "citations": "547", "year": "2022", "title":"BLIP: Bootstrapping Language-image Pre-training For Unified Vision-language Understanding And Generation", "abstract": "<p>Vision-Language Pre-training (VLP) has advanced the performance for many\nvision-language tasks. However, most existing pre-trained models only excel in\neither understanding-based tasks or generation-based tasks. Furthermore,\nperformance improvement has been largely achieved by scaling up the dataset\nwith noisy image-text pairs collected from the web, which is a suboptimal\nsource of supervision. In this paper, we propose BLIP, a new VLP framework\nwhich transfers flexibly to both vision-language understanding and generation\ntasks. BLIP effectively utilizes the noisy web data by bootstrapping the\ncaptions, where a captioner generates synthetic captions and a filter removes\nthe noisy ones. We achieve state-of-the-art results on a wide range of\nvision-language tasks, such as image-text retrieval (+2.7% in average\nrecall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).\nBLIP also demonstrates strong generalization ability when directly transferred\nto video-language tasks in a zero-shot manner. Code, models, and datasets are\nreleased at https://github.com/salesforce/BLIP.</p>\n", "tags": ["Datasets","Has Code","Tools","Training Techniques"] },
{"key": "li2022competition", "citations": "511", "year": "2022", "title":"Competition-level Code Generation With Alphacode", "abstract": "<p>Programming is a powerful and ubiquitous problem-solving tool. Developing\nsystems that can assist programmers or even generate programs independently\ncould make programming more productive and accessible, yet so far incorporating\ninnovations in AI has proven challenging. Recent large-scale language models\nhave demonstrated an impressive ability to generate code, and are now able to\ncomplete simple programming tasks. However, these models still perform poorly\nwhen evaluated on more complex, unseen problems that require problem-solving\nskills beyond simply translating instructions into code. For example,\ncompetitive programming problems which require an understanding of algorithms\nand complex natural language remain extremely challenging. To address this gap,\nwe introduce AlphaCode, a system for code generation that can create novel\nsolutions to these problems that require deeper reasoning. In simulated\nevaluations on recent programming competitions on the Codeforces platform,\nAlphaCode achieved on average a ranking of top 54.3% in competitions with more\nthan 5,000 participants. We found that three key components were critical to\nachieve good and reliable performance: (1) an extensive and clean competitive\nprogramming dataset for training and evaluation, (2) large and\nefficient-to-sample transformer-based architectures, and (3) large-scale model\nsampling to explore the search space, followed by filtering based on program\nbehavior to a small set of submissions.</p>\n", "tags": ["Datasets","Evaluation","Llm For Code","Model Architecture","Tools","Training Techniques"] },
{"key": "li2022comprehending", "citations": "85", "year": "2022", "title":"Comprehending And Ordering Semantics For Image Captioning", "abstract": "<p>Comprehending the rich semantics in an image and ordering them in linguistic\norder are essential to compose a visually-grounded and linguistically coherent\ndescription for image captioning. Modern techniques commonly capitalize on a\npre-trained object detector/classifier to mine the semantics in an image, while\nleaving the inherent linguistic ordering of semantics under-exploited. In this\npaper, we propose a new recipe of Transformer-style structure, namely\nComprehending and Ordering Semantics Networks (COS-Net), that novelly unifies\nan enriched semantic comprehending and a learnable semantic ordering processes\ninto a single architecture. Technically, we initially utilize a cross-modal\nretrieval model to search the relevant sentences of each image, and all words\nin the searched sentences are taken as primary semantic cues. Next, a novel\nsemantic comprehender is devised to filter out the irrelevant semantic words in\nprimary semantic cues, and meanwhile infer the missing relevant semantic words\nvisually grounded in the image. After that, we feed all the screened and\nenriched semantic words into a semantic ranker, which learns to allocate all\nsemantic words in linguistic order as humans. Such sequence of ordered semantic\nwords are further integrated with visual tokens of images to trigger sentence\ngeneration. Empirical evidences show that COS-Net clearly surpasses the\nstate-of-the-art approaches on COCO and achieves to-date the best CIDEr score\nof 141.1% on Karpathy test split. Source code is available at\nhttps://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet.</p>\n", "tags": ["CVPR","Has Code","Model Architecture"] },
{"key": "li2022diffusion", "citations": "203", "year": "2022", "title":"Diffusion-lm Improves Controllable Text Generation", "abstract": "<p>Controlling the behavior of language models (LMs) without re-training is a\nmajor open problem in natural language generation. While recent works have\ndemonstrated successes on controlling simple sentence attributes (e.g.,\nsentiment), there has been little progress on complex, fine-grained controls\n(e.g., syntactic structure). To address this challenge, we develop a new\nnon-autoregressive language model based on continuous diffusions that we call\nDiffusion-LM. Building upon the recent successes of diffusion models in\ncontinuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian\nvectors into word vectors, yielding a sequence of intermediate latent\nvariables. The continuous, hierarchical nature of these intermediate variables\nenables a simple gradient-based algorithm to perform complex, controllable\ngeneration tasks. We demonstrate successful control of Diffusion-LM for six\nchallenging fine-grained control tasks, significantly outperforming prior work.</p>\n", "tags": ["Training Techniques"] },
{"key": "li2022dit", "citations": "109", "year": "2022", "title":"Dit: Self-supervised Pre-training For Document Image Transformer", "abstract": "<p>Image Transformer has recently achieved significant progress for natural\nimage understanding, either using supervised (ViT, DeiT, etc.) or\nself-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we\npropose \\textbf{DiT}, a self-supervised pre-trained \\textbf{D}ocument\n\\textbf{I}mage \\textbf{T}ransformer model using large-scale unlabeled text\nimages for Document AI tasks, which is essential since no supervised\ncounterparts ever exist due to the lack of human-labeled document images. We\nleverage DiT as the backbone network in a variety of vision-based Document AI\ntasks, including document image classification, document layout analysis, table\ndetection as well as text detection for OCR. Experiment results have\nillustrated that the self-supervised pre-trained DiT model achieves new\nstate-of-the-art results on these downstream tasks, e.g. document image\nclassification (91.11 \\(\\rightarrow\\) 92.69), document layout analysis (91.0\n\\(\\rightarrow\\) 94.9), table detection (94.23 \\(\\rightarrow\\) 96.55) and text\ndetection for OCR (93.07 \\(\\rightarrow\\) 94.29). The code and pre-trained models\nare publicly available at https://aka.ms/msdit.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "li2022invariant", "citations": "81", "year": "2022", "title":"Invariant Grounding For Video Question Answering", "abstract": "<p>Video Question Answering (VideoQA) is the task of answering questions about a\nvideo. At its core is understanding the alignments between visual scenes in\nvideo and linguistic semantics in question to yield the answer. In leading\nVideoQA models, the typical learning objective, empirical risk minimization\n(ERM), latches on superficial correlations between video-question pairs and\nanswers as the alignments. However, ERM can be problematic, because it tends to\nover-exploit the spurious correlations between question-irrelevant scenes and\nanswers, instead of inspecting the causal effect of question-critical scenes.\nAs a result, the VideoQA models suffer from unreliable reasoning. In this work,\nwe first take a causal look at VideoQA and argue that invariant grounding is\nthe key to ruling out the spurious correlations. Towards this end, we propose a\nnew learning framework, Invariant Grounding for VideoQA (IGV), to ground the\nquestion-critical scene, whose causal relations with answers are invariant\nacross different interventions on the complement. With IGV, the VideoQA models\nare forced to shield the answering process from the negative influence of\nspurious correlations, which significantly improves the reasoning ability.\nExperiments on three benchmark datasets validate the superiority of IGV in\nterms of accuracy, visual explainability, and generalization ability over the\nleading baselines.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Tools"] },
{"key": "li2022knowledge", "citations": "89", "year": "2022", "title":"Knowledge-enriched Attention Network With Group-wise Semantic For Visual Storytelling", "abstract": "<p>As a technically challenging topic, visual storytelling aims at generating an\nimaginary and coherent story with narrative multi-sentences from a group of\nrelevant images. Existing methods often generate direct and rigid descriptions\nof apparent image-based contents, because they are not capable of exploring\nimplicit information beyond images. Hence, these schemes could not capture\nconsistent dependencies from holistic representation, impairing the generation\nof reasonable and fluent story. To address these problems, a novel\nknowledge-enriched attention network with group-wise semantic model is\nproposed. Three main novel components are designed and supported by substantial\nexperiments to reveal practical advantages. First, a knowledge-enriched\nattention network is designed to extract implicit concepts from external\nknowledge system, and these concepts are followed by a cascade cross-modal\nattention mechanism to characterize imaginative and concrete representations.\nSecond, a group-wise semantic module with second-order pooling is developed to\nexplore the globally consistent guidance. Third, a unified one-stage story\ngeneration model with encoder-decoder structure is proposed to simultaneously\ntrain and infer the knowledge-enriched attention network, group-wise semantic\nmodule and multi-modal story generation decoder in an end-to-end fashion.\nSubstantial experiments on the popular Visual Storytelling dataset with both\nobjective and subjective evaluation metrics demonstrate the superior\nperformance of the proposed scheme as compared with other state-of-the-art\nmethods.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture"] },
{"key": "li2022mplug", "citations": "102", "year": "2022", "title":"Mplug: Effective And Efficient Vision-language Learning By Cross-modal Skip-connections", "abstract": "<p>Large-scale pretrained foundation models have been an emerging paradigm for\nbuilding artificial intelligence (AI) systems, which can be quickly adapted to\na wide range of downstream tasks. This paper presents mPLUG, a new\nvision-language foundation model for both cross-modal understanding and\ngeneration. Most existing pre-trained models suffer from the problems of low\ncomputational efficiency and information asymmetry brought by the long visual\nsequence in cross-modal alignment. To address these problems, mPLUG introduces\nan effective and efficient vision-language architecture with novel cross-modal\nskip-connections, which creates inter-layer shortcuts that skip a certain\nnumber of layers for time-consuming full self-attention on the vision side.\nmPLUG is pre-trained end-to-end on large-scale image-text pairs with both\ndiscriminative and generative objectives. It achieves state-of-the-art results\non a wide range of vision-language downstream tasks, such as image captioning,\nimage-text retrieval, visual grounding and visual question answering. mPLUG\nalso demonstrates strong zero-shot transferability when directly transferred to\nmultiple video-language tasks.</p>\n", "tags": ["EMNLP","Efficiency","Model Architecture"] },
{"key": "li2022personalized", "citations": "101", "year": "2023", "title":"Personalized Prompt Learning For Explainable Recommendation", "abstract": "<p>Providing user-understandable explanations to justify recommendations could\nhelp users better understand the recommended items, increase the system’s ease\nof use, and gain users’ trust. A typical approach to realize it is natural\nlanguage generation. However, previous works mostly adopt recurrent neural\nnetworks to meet the ends, leaving the potentially more effective pre-trained\nTransformer models under-explored. In fact, user and item IDs, as important\nidentifiers in recommender systems, are inherently in different semantic space\nas words that pre-trained models were already trained on. Thus, how to\neffectively fuse IDs into such models becomes a critical issue. Inspired by\nrecent advancement in prompt learning, we come up with two solutions: find\nalternative words to represent IDs (called discrete prompt learning), and\ndirectly input ID vectors to a pre-trained model (termed continuous prompt\nlearning). In the latter case, ID vectors are randomly initialized but the\nmodel is trained in advance on large corpora, so they are actually in different\nlearning stages. To bridge the gap, we further propose two training strategies:\nsequential tuning and recommendation as regularization. Extensive experiments\nshow that our continuous prompt learning approach equipped with the training\nstrategies consistently outperforms strong baselines on three datasets of\nexplainable recommendation.</p>\n", "tags": ["Datasets","Model Architecture","Prompting","Training Techniques"] },
{"key": "li2022scaling", "citations": "141", "year": "2023", "title":"Scaling Language-image Pre-training Via Masking", "abstract": "<p>We present Fast Language-Image Pre-training (FLIP), a simple and more\nefficient method for training CLIP. Our method randomly masks out and removes a\nlarge portion of image patches during training. Masking allows us to learn from\nmore image-text pairs given the same wall-clock time and contrast more samples\nper iteration with similar memory footprint. It leads to a favorable trade-off\nbetween accuracy and training time. In our experiments on 400 million\nimage-text pairs, FLIP improves both accuracy and speed over the no-masking\nbaseline. On a large diversity of downstream tasks, FLIP dominantly outperforms\nthe CLIP counterparts trained on the same data. Facilitated by the speedup, we\nexplore the scaling behavior of increasing the model size, data size, or\ntraining length, and report encouraging results and comparisons. We hope that\nour work will foster future research on scaling vision-language learning.</p>\n", "tags": ["CVPR","Training Techniques"] },
{"key": "li2023blip", "citations": "630", "year": "2023", "title":"Blip-diffusion: Pre-trained Subject Representation For Controllable Text-to-image Generation And Editing", "abstract": "<p>Subject-driven text-to-image generation models create novel renditions of an\ninput subject based on text prompts. Existing models suffer from lengthy\nfine-tuning and difficulties preserving the subject fidelity. To overcome these\nlimitations, we introduce BLIP-Diffusion, a new subject-driven image generation\nmodel that supports multimodal control which consumes inputs of subject images\nand text prompts. Unlike other subject-driven generation models, BLIP-Diffusion\nintroduces a new multimodal encoder which is pre-trained to provide subject\nrepresentation. We first pre-train the multimodal encoder following BLIP-2 to\nproduce visual representation aligned with the text. Then we design a subject\nrepresentation learning task which enables a diffusion model to leverage such\nvisual representation and generates new subject renditions. Compared with\nprevious methods such as DreamBooth, our model enables zero-shot subject-driven\ngeneration, and efficient fine-tuning for customized subject with up to 20x\nspeedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with\nexisting techniques such as ControlNet and prompt-to-prompt to enable novel\nsubject-driven generation and editing applications. Code and models will be\nreleased at\nhttps://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion. Project\npage at https://dxli94.github.io/BLIP-Diffusion-website/.</p>\n", "tags": ["Applications","Fine-Tuning","Has Code","Prompting","Training Techniques"] },
{"key": "li2023chatdoctor", "citations": "259", "year": "2023", "title":"Chatdoctor: A Medical Chat Model Fine-tuned On A Large Language Model Meta-ai (llama) Using Medical Domain Knowledge", "abstract": "<p>The primary aim of this research was to address the limitations observed in\nthe medical knowledge of prevalent large language models (LLMs) such as\nChatGPT, by creating a specialized language model with enhanced accuracy in\nmedical advice. We achieved this by adapting and refining the large language\nmodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues\nsourced from a widely used online medical consultation platform. These\nconversations were cleaned and anonymized to respect privacy concerns. In\naddition to the model refinement, we incorporated a self-directed information\nretrieval mechanism, allowing the model to access and utilize real-time\ninformation from online sources like Wikipedia and data from curated offline\nmedical databases. The fine-tuning of the model with real-world patient-doctor\ninteractions significantly improved the model’s ability to understand patient\nneeds and provide informed advice. By equipping the model with self-directed\ninformation retrieval from reliable online and offline sources, we observed\nsubstantial improvements in the accuracy of its responses. Our proposed\nChatDoctor, represents a significant advancement in medical LLMs, demonstrating\na significant improvement in understanding patient inquiries and providing\naccurate advice. Given the high stakes and low error tolerance in the medical\nfield, such enhancements in providing accurate and reliable information are not\nonly beneficial but essential.</p>\n", "tags": ["Datasets","Fine-Tuning","Privacy","Tools","Training Techniques"] },
{"key": "li2023comparative", "citations": "81", "year": "2022", "title":"A Comparative Study Of Pretrained Language Models For Long Clinical Text", "abstract": "<p>Objective: Clinical knowledge enriched transformer models (e.g.,\nClinicalBERT) have state-of-the-art results on clinical NLP (natural language\nprocessing) tasks. One of the core limitations of these transformer models is\nthe substantial memory consumption due to their full self-attention mechanism,\nwhich leads to the performance degradation in long clinical texts. To overcome\nthis, we propose to leverage long-sequence transformer models (e.g., Longformer\nand BigBird), which extend the maximum input sequence length from 512 to 4096,\nto enhance the ability to model long-term dependencies in long clinical texts.\n  Materials and Methods: Inspired by the success of long sequence transformer\nmodels and the fact that clinical notes are mostly long, we introduce two\ndomain enriched language models, Clinical-Longformer and Clinical-BigBird,\nwhich are pre-trained on a large-scale clinical corpus. We evaluate both\nlanguage models using 10 baseline tasks including named entity recognition,\nquestion answering, natural language inference, and document classification\ntasks.\n  Results: The results demonstrate that Clinical-Longformer and\nClinical-BigBird consistently and significantly outperform ClinicalBERT and\nother short-sequence transformers in all 10 downstream tasks and achieve new\nstate-of-the-art results.\n  Discussion: Our pre-trained language models provide the bedrock for clinical\nNLP using long texts. We have made our source code available at\nhttps://github.com/luoyuanlab/Clinical-Longformer, and the pre-trained models\navailable for public download at:\nhttps://huggingface.co/yikuan8/Clinical-Longformer.\n  Conclusion: This study demonstrates that clinical knowledge enriched\nlong-sequence transformers are able to learn long-term dependencies in long\nclinical text. Our methods can also inspire the development of other\ndomain-enriched long-sequence transformers.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "li2023gligen", "citations": "249", "year": "2023", "title":"GLIGEN: Open-set Grounded Text-to-image Generation", "abstract": "<p>Large-scale text-to-image diffusion models have made amazing advances.\nHowever, the status quo is to use text input alone, which can impede\ncontrollability. In this work, we propose GLIGEN, Grounded-Language-to-Image\nGeneration, a novel approach that builds upon and extends the functionality of\nexisting pre-trained text-to-image diffusion models by enabling them to also be\nconditioned on grounding inputs. To preserve the vast concept knowledge of the\npre-trained model, we freeze all of its weights and inject the grounding\ninformation into new trainable layers via a gated mechanism. Our model achieves\nopen-world grounded text2img generation with caption and bounding box condition\ninputs, and the grounding ability generalizes well to novel spatial\nconfigurations and concepts. GLIGEN’s zero-shot performance on COCO and LVIS\noutperforms that of existing supervised layout-to-image baselines by a large\nmargin.</p>\n", "tags": ["CVPR"] },
{"key": "li2023halueval", "citations": "112", "year": "2023", "title":"Halueval: A Large-scale Hallucination Evaluation Benchmark For Large Language Models", "abstract": "<p>Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, i.e., content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation benchmark for Large Language Models (HaluEval), a large collection\nof generated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing hallucination. To generate these samples, we\npropose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.\nBesides, we also hire some human labelers to annotate the hallucinations in\nChatGPT responses. The empirical results suggest that ChatGPT is likely to\ngenerate hallucinated content in specific topics by fabricating unverifiable\ninformation (i.e., about \\(19.5%\\) responses). Moreover, existing LLMs face\ngreat challenges in recognizing the hallucinations in texts. However, our\nexperiments also prove that providing external knowledge or adding reasoning\nsteps can help LLMs recognize hallucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code","Tools"] },
{"key": "li2023llava", "citations": "113", "year": "2023", "title":"Llava-med: Training A Large Language-and-vision Assistant For Biomedicine In One Day", "abstract": "<p>Conversational generative AI has demonstrated remarkable promise for\nempowering biomedical practitioners, but current investigations focus on\nunimodal text. Multimodal conversational AI has seen rapid progress by\nleveraging billions of image-text pairs from the public web, but such\ngeneral-domain vision-language models still lack sophistication in\nunderstanding and conversing about biomedical images. In this paper, we propose\na cost-efficient approach for training a vision-language conversational\nassistant that can answer open-ended research questions of biomedical images.\nThe key idea is to leverage a large-scale, broad-coverage biomedical\nfigure-caption dataset extracted from PubMed Central, use GPT-4 to\nself-instruct open-ended instruction-following data from the captions, and then\nfine-tune a large general-domain vision-language model using a novel curriculum\nlearning method. Specifically, the model first learns to align biomedical\nvocabulary using the figure-caption pairs as is, then learns to master\nopen-ended conversational semantics using GPT-4 generated instruction-following\ndata, broadly mimicking how a layperson gradually acquires biomedical\nknowledge. This enables us to train a Large Language and Vision Assistant for\nBioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med\nexhibits excellent multimodal conversational capability and can follow\nopen-ended instruction to assist with inquiries about a biomedical image. On\nthree standard biomedical visual question answering datasets, LLaVA-Med\noutperforms previous supervised state-of-the-art on certain metrics. To\nfacilitate biomedical multimodal research, we will release our\ninstruction-following data and the LLaVA-Med model.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "li2023making", "citations": "82", "year": "2023", "title":"Making Large Language Models A Better Foundation For Dense Retrieval", "abstract": "<p>Dense retrieval needs to learn discriminative text embeddings to represent\nthe semantic relationship between query and document. It may benefit from the\nusing of large language models (LLMs), given LLMs’ strong capability on\nsemantic understanding. However, the LLMs are pre-trained by text generation\ntasks, whose working pattern is completely different from representing texts as\nembeddings. As a result, it is imperative to study how to adapt LLMs properly\nso that they can be effectively initialized as the backbone encoder for dense\nretrieval.\n  In this paper, we propose a novel approach, called LLaRA (LLM adapted for\ndense RetrievAl), which works as a post-hoc adaptation of LLM for the dense\nretrieval application. LLaRA consists of two pretext tasks: EBAE\n(Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression),\nwhere the text embeddings from LLM are used to reconstruct the tokens for the\ninput sentence and predict the tokens for the next sentence, respectively.\nLLaRA turns out to be simple, lightweight, and highly effective. It is applied\nto adapt LLaMA-2-7B (base) on the Wikipedia corpus, where it substantially\nimproves the model’s fine-tuned performances on a variety of dense retrieval\nbenchmarks, like MSMARCO and BEIR. Our model and code will be made publicly\navailable at BGE repository.</p>\n", "tags": ["Datasets","Has Code"] },
{"key": "li2023multi", "citations": "84", "year": "2023", "title":"Multi-modality Is All You Need For Transferable Recommender Systems", "abstract": "<p>ID-based Recommender Systems (RecSys), where each item is assigned a unique\nidentifier and subsequently converted into an embedding vector, have dominated\nthe designing of RecSys. Though prevalent, such ID-based paradigm is not\nsuitable for developing transferable RecSys and is also susceptible to the\ncold-start issue. In this paper, we unleash the boundaries of the ID-based\nparadigm and propose a Pure Multi-Modality based Recommender system (PMMRec),\nwhich relies solely on the multi-modal contents of the items (e.g., texts and\nimages) and learns transition patterns general enough to transfer across\ndomains and platforms. Specifically, we design a plug-and-play framework\narchitecture consisting of multi-modal item encoders, a fusion module, and a\nuser encoder. To align the cross-modal item representations, we propose a novel\nnext-item enhanced cross-modal contrastive learning objective, which is\nequipped with both inter- and intra-modality negative samples and explicitly\nincorporates the transition patterns of user behaviors into the item encoders.\nTo ensure the robustness of user representations, we propose a novel noised\nitem detection objective and a robustness-aware contrastive learning objective,\nwhich work together to denoise user sequences in a self-supervised manner.\nPMMRec is designed to be loosely coupled, so after being pre-trained on the\nsource data, each component can be transferred alone, or in conjunction with\nother components, allowing PMMRec to achieve versatility under both\nmulti-modality and single-modality transfer learning settings. Extensive\nexperiments on 4 sources and 10 target datasets demonstrate that PMMRec\nsurpasses the state-of-the-art recommenders in both recommendation performance\nand transferability. Our code and dataset is available at:\nhttps://github.com/ICDE24/PMMRec.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "li2023otter", "citations": "78", "year": "2023", "title":"Otter: A Multi-modal Model With In-context Instruction Tuning", "abstract": "<p>Large language models (LLMs) have demonstrated significant universal\ncapabilities as few/zero-shot learners in various tasks due to their\npre-training on vast amounts of text data, as exemplified by GPT-3, which\nboosted to InstrctGPT and ChatGPT, effectively following natural language\ninstructions to accomplish real-world tasks. In this paper, we propose to\nintroduce instruction tuning into multi-modal models, motivated by the Flamingo\nmodel’s upstream interleaved format pretraining dataset. We adopt a similar\napproach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT)\ndataset. We then introduce Otter, a multi-modal model based on OpenFlamingo\n(open-sourced version of DeepMind’s Flamingo), trained on MIMIC-IT and\nshowcasing improved instruction-following ability and in-context learning. We\nalso optimize OpenFlamingo’s implementation for researchers, democratizing the\nrequired training resources from 1\\(\\times\\) A100 GPU to 4\\(\\times\\) RTX-3090 GPUs,\nand integrate both OpenFlamingo and Otter into Huggingface Transformers for\nmore researchers to incorporate the models into their customized training and\ninference pipelines.</p>\n", "tags": ["Datasets","In Context Learning","Model Architecture","Training Techniques"] },
{"key": "lian2019learning", "citations": "185", "year": "2019", "title":"Learning To Select Knowledge For Response Generation In Dialog Systems", "abstract": "<p>End-to-end neural models for intelligent dialogue systems suffer from the\nproblem of generating uninformative responses. Various methods were proposed to\ngenerate more informative responses by leveraging external knowledge. However,\nfew previous work has focused on selecting appropriate knowledge in the\nlearning process. The inappropriate selection of knowledge could prohibit the\nmodel from learning to make full use of the knowledge. Motivated by this, we\npropose an end-to-end neural model which employs a novel knowledge selection\nmechanism where both prior and posterior distributions over knowledge are used\nto facilitate knowledge selection. Specifically, a posterior distribution over\nknowledge is inferred from both utterances and responses, and it ensures the\nappropriate selection of knowledge during the training process. Meanwhile, a\nprior distribution, which is inferred from utterances only, is used to\napproximate the posterior distribution so that appropriate knowledge can be\nselected even without responses during the inference process. Compared with the\nprevious work, our model can better incorporate appropriate knowledge in\nresponse generation. Experiments on both automatic and human evaluation verify\nthe superiority of our model over previous baselines.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","IJCAI","Training Techniques"] },
{"key": "liang2016neural", "citations": "382", "year": "2017", "title":"Neural Symbolic Machines: Learning Semantic Parsers On Freebase With Weak Supervision", "abstract": "<p>Harnessing the statistical power of neural networks to perform language\nunderstanding and symbolic reasoning is difficult, when it requires executing\nefficient discrete operations against a large knowledge-base. In this work, we\nintroduce a Neural Symbolic Machine, which contains (a) a neural “programmer”,\ni.e., a sequence-to-sequence model that maps language utterances to programs\nand utilizes a key-variable memory to handle compositionality (b) a symbolic\n“computer”, i.e., a Lisp interpreter that performs program execution, and helps\nfind good programs by pruning the search space. We apply REINFORCE to directly\noptimize the task reward of this structured prediction problem. To train with\nweak supervision and improve the stability of REINFORCE, we augment it with an\niterative maximum-likelihood training process. NSM outperforms the\nstate-of-the-art on the WebQuestionsSP dataset when trained from\nquestion-answer pairs only, without requiring any feature engineering or\ndomain-specific knowledge.</p>\n", "tags": ["Datasets","Nesy","Reinforcement Learning","Training Techniques"] },
{"key": "liang2017recurrent", "citations": "166", "year": "2017", "title":"Recurrent Topic-transition GAN For Visual Paragraph Generation", "abstract": "<p>A natural image usually conveys rich semantic content and can be viewed from\ndifferent angles. Existing image description methods are largely restricted by\nsmall sets of biased visual paragraph annotations, and fail to cover rich\nunderlying semantics. In this paper, we investigate a semi-supervised paragraph\ngenerative framework that is able to synthesize diverse and semantically\ncoherent paragraph descriptions by reasoning over local semantic regions and\nexploiting linguistic knowledge. The proposed Recurrent Topic-Transition\nGenerative Adversarial Network (RTT-GAN) builds an adversarial framework\nbetween a structured paragraph generator and multi-level paragraph\ndiscriminators. The paragraph generator generates sentences recurrently by\nincorporating region-based visual and language attention mechanisms at each\nstep. The quality of generated paragraph sentences is assessed by multi-level\nadversarial discriminators from two aspects, namely, plausibility at sentence\nlevel and topic-transition coherence at paragraph level. The joint adversarial\ntraining of RTT-GAN drives the model to generate realistic paragraphs with\nsmooth logical transition between sentence topics. Extensive quantitative\nexperiments on image and video paragraph datasets demonstrate the effectiveness\nof our RTT-GAN in both supervised and semi-supervised settings. Qualitative\nresults on telling diverse stories for an image also verify the\ninterpretability of RTT-GAN.</p>\n", "tags": ["Datasets","ICCV","Model Architecture","Tools","Training Techniques"] },
{"key": "liang2018focal", "citations": "115", "year": "2018", "title":"Focal Visual-text Attention For Visual Question Answering", "abstract": "<p>Recent insights on language and vision with neural networks have been\nsuccessfully applied to simple single-image visual question answering. However,\nto tackle real-life question answering problems on multimedia collections such\nas personal photos, we have to look at whole collections with sequences of\nphotos or videos. When answering questions from a large collection, a natural\nproblem is to identify snippets to support the answer. In this paper, we\ndescribe a novel neural network called Focal Visual-Text Attention network\n(FVTA) for collective reasoning in visual question answering, where both visual\nand text sequence information such as images and text metadata are presented.\nFVTA introduces an end-to-end approach that makes use of a hierarchical process\nto dynamically determine what media and what time to focus on in the sequential\ndata to answer the question. FVTA can not only answer the questions well but\nalso provides the justifications which the system results are based upon to get\nthe answers. FVTA achieves state-of-the-art performance on the MemexQA dataset\nand competitive results on the MovieQA dataset.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "liang2018memory", "citations": "105", "year": "2018", "title":"Memory Augmented Policy Optimization For Program Synthesis And Semantic Parsing", "abstract": "<p>We present Memory Augmented Policy Optimization (MAPO), a simple and novel\nway to leverage a memory buffer of promising trajectories to reduce the\nvariance of policy gradient estimate. MAPO is applicable to deterministic\nenvironments with discrete actions, such as structured prediction and\ncombinatorial optimization tasks. We express the expected return objective as a\nweighted sum of two terms: an expectation over the high-reward trajectories\ninside the memory buffer, and a separate expectation over trajectories outside\nthe buffer. To make an efficient algorithm of MAPO, we propose: (1) memory\nweight clipping to accelerate and stabilize training; (2) systematic\nexploration to discover high-reward trajectories; (3) distributed sampling from\ninside and outside of the memory buffer to scale up training. MAPO improves the\nsample efficiency and robustness of policy gradient, especially on tasks with\nsparse rewards. We evaluate MAPO on weakly supervised program synthesis from\nnatural language (semantic parsing). On the WikiTableQuestions benchmark, we\nimprove the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the\nWikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak\nsupervision, outperforming several strong baselines with full supervision. Our\nsource code is available at\nhttps://github.com/crazydonkey200/neural-symbolic-machines</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Has Code","Reinforcement Learning","Training Techniques"] },
{"key": "liang2021r", "citations": "231", "year": "2021", "title":"R-drop: Regularized Dropout For Neural Networks", "abstract": "<p>Dropout is a powerful and widely used technique to regularize the training of\ndeep neural networks. In this paper, we introduce a simple regularization\nstrategy upon dropout in model training, namely R-Drop, which forces the output\ndistributions of different sub models generated by dropout to be consistent\nwith each other. Specifically, for each training sample, R-Drop minimizes the\nbidirectional KL-divergence between the output distributions of two sub models\nsampled by dropout. Theoretical analysis reveals that R-Drop reduces the\nfreedom of the model parameters and complements dropout. Experiments on\n\\(\\bf{5}\\) widely used deep learning tasks (\\(\\bf{18}\\) datasets in total),\nincluding neural machine translation, abstractive summarization, language\nunderstanding, language modeling, and image classification, show that R-Drop is\nuniversally effective. In particular, it yields substantial improvements when\napplied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large,\nand BART, and achieves state-of-the-art (SOTA) performances with the vanilla\nTransformer model on WMT14 English\\(\\to\\)German translation (\\(\\bf{30.91}\\) BLEU)\nand WMT14 English\\(\\to\\)French translation (\\(\\bf{43.95}\\) BLEU), even surpassing\nmodels trained with extra large-scale data and expert-designed advanced\nvariants of Transformer models. Our code is available at\nGitHub{https://github.com/dropreg/R-Drop}.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "liang2022code", "citations": "213", "year": "2023", "title":"Code As Policies: Language Model Programs For Embodied Control", "abstract": "<p>Large language models (LLMs) trained on code completion have been shown to be\ncapable of synthesizing simple Python programs from docstrings [1]. We find\nthat these code-writing LLMs can be re-purposed to write robot policy code,\ngiven natural language commands. Specifically, policy code can express\nfunctions or feedback loops that process perception outputs (e.g.,from object\ndetectors [2], [3]) and parameterize control primitive APIs. When provided as\ninput several example language commands (formatted as comments) followed by\ncorresponding policy code (via few-shot prompting), LLMs can take in new\ncommands and autonomously re-compose API calls to generate new policy code\nrespectively. By chaining classic logic structures and referencing third-party\nlibraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way\ncan write robot policies that (i) exhibit spatial-geometric reasoning, (ii)\ngeneralize to new instructions, and (iii) prescribe precise values (e.g.,\nvelocities) to ambiguous descriptions (“faster”) depending on context (i.e.,\nbehavioral commonsense). This paper presents code as policies: a robot-centric\nformulation of language model generated programs (LMPs) that can represent\nreactive policies (e.g., impedance controllers), as well as waypoint-based\npolicies (vision-based pick and place, trajectory-based control), demonstrated\nacross multiple real robot platforms. Central to our approach is prompting\nhierarchical code-gen (recursively defining undefined functions), which can\nwrite more complex code and also improves state-of-the-art to solve 39.8% of\nproblems on the HumanEval [1] benchmark. Code and videos are available at\nhttps://code-as-policies.github.io</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Has Code","ICRA","Llm For Code","Prompting","Tools"] },
{"key": "liang2022holistic", "citations": "314", "year": "2023", "title":"Holistic Evaluation Of Language Models", "abstract": "<p>Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what’s missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don’t fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.</p>\n", "tags": ["Datasets","Efficiency","Ethics & Fairness","Evaluation Frameworks","Evaluation"] },
{"key": "liang2023can", "citations": "67", "year": "2024", "title":"Can Large Language Models Provide Useful Feedback On Research Papers? A Large-scale Empirical Analysis", "abstract": "<p>Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4’s feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4’s generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.</p>\n", "tags": ["Model Architecture"] },
{"key": "liang2023gpt", "citations": "236", "year": "2023", "title":"GPT Detectors Are Biased Against Non-native English Writers", "abstract": "<p>The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse. The published\nversion of this study can be accessed at:\nwww.cell.com/patterns/fulltext/S2666-3899(23)00130-7</p>\n", "tags": ["Ethics & Fairness","Model Architecture","Prompting","Security"] },
{"key": "liao2020questioning", "citations": "617", "year": "2020", "title":"Questioning The AI: Informing Design Practices For Explainable AI User Experiences", "abstract": "<p>A surge of interest in explainable AI (XAI) has led to a vast collection of\nalgorithmic work on the topic. While many recognize the necessity to\nincorporate explainability features in AI systems, how to address real-world\nuser needs for understanding AI remains an open question. By interviewing 20 UX\nand design practitioners working on various AI products, we seek to identify\ngaps between the current XAI algorithmic work and practices to create\nexplainable AI products. To do so, we develop an algorithm-informed XAI\nquestion bank in which user needs for explainability are represented as\nprototypical questions users might ask about the AI, and use it as a study\nprobe. Our work contributes insights into the design space of XAI, informs\nefforts to support design practices in this space, and identifies opportunities\nfor future XAI work. We also provide an extended XAI question bank and discuss\nhow it can be used for creating user-centered XAI.</p>\n", "tags": [] },
{"key": "liao2022designing", "citations": "80", "year": "2022", "title":"Designing For Responsible Trust In AI Systems: A Communication Perspective", "abstract": "<p>Current literature and public discourse on “trust in AI” are often focused on\nthe principles underlying trustworthy AI, with insufficient attention paid to\nhow people develop trust. Given that AI systems differ in their level of\ntrustworthiness, two open questions come to the fore: how should AI\ntrustworthiness be responsibly communicated to ensure appropriate and equitable\ntrust judgments by different users, and how can we protect users from deceptive\nattempts to earn their trust? We draw from communication theories and\nliterature on trust in technologies to develop a conceptual model called MATCH,\nwhich describes how trustworthiness is communicated in AI systems through\ntrustworthiness cues and how those cues are processed by people to make trust\njudgments. Besides AI-generated content, we highlight transparency and\ninteraction as AI systems’ affordances that present a wide range of\ntrustworthiness cues to users. By bringing to light the variety of users’\ncognitive processes to make trust judgments and their potential limitations, we\nurge technology creators to make conscious decisions in choosing reliable\ntrustworthiness cues for target users and, as an industry, to regulate this\nspace and prevent malicious use. Towards these goals, we define the concepts of\nwarranted trustworthiness cues and expensive trustworthiness cues, and propose\na checklist of requirements to help technology creators identify appropriate\ncues to use. We present a hypothetical use case to illustrate how practitioners\ncan use MATCH to design AI systems responsibly, and discuss future directions\nfor research and industry efforts aimed at promoting responsible trust in AI.</p>\n", "tags": ["Ethics & Fairness","Model Architecture"] },
{"key": "liao2023ai", "citations": "78", "year": "2024", "title":"AI Transparency In The Age Of Llms: A Human-centered Research Roadmap", "abstract": "<p>The rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI – transparency – is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency – model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty – and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.</p>\n", "tags": ["Applications","Ethics & Fairness","Evaluation","Llm For Code"] },
{"key": "libovický2017attention", "citations": "175", "year": "2017", "title":"Attention Strategies For Multi-source Sequence-to-sequence Learning", "abstract": "<p>Modeling attention in neural multi-source sequence-to-sequence learning\nremains a relatively unexplored area, despite its usefulness in tasks that\nincorporate multiple source languages or modalities. We propose two novel\napproaches to combine the outputs of attention mechanisms over each source\nsequence, flat and hierarchical. We compare the proposed methods with existing\ntechniques and present results of systematic evaluation of those methods on the\nWMT16 Multimodal Translation and Automatic Post-editing tasks. We show that the\nproposed methods achieve competitive results on both tasks.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "libovický2018end", "citations": "130", "year": "2018", "title":"End-to-end Non-autoregressive Neural Machine Translation With Connectionist Temporal Classification", "abstract": "<p>Autoregressive decoding is the only part of sequence-to-sequence models that\nprevents them from massive parallelization at inference time.\nNon-autoregressive models enable the decoder to generate all output symbols\nindependently in parallel. We present a novel non-autoregressive architecture\nbased on connectionist temporal classification and evaluate it on the task of\nneural machine translation. Unlike other non-autoregressive methods which\noperate in several steps, our model can be trained end-to-end. We conduct\nexperiments on the WMT English-Romanian and English-German datasets. Our models\nachieve a significant speedup over the autoregressive models, keeping the\ntranslation quality comparable to other non-autoregressive models.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "libovický2018input", "citations": "65", "year": "2018", "title":"Input Combination Strategies For Multi-source Transformer Decoder", "abstract": "<p>In multi-source sequence-to-sequence tasks, the attention mechanism can be\nmodeled in several ways. This topic has been thoroughly studied on recurrent\narchitectures. In this paper, we extend the previous work to the\nencoder-decoder attention in the Transformer architecture. We propose four\ndifferent input combination strategies for the encoder-decoder attention:\nserial, parallel, flat, and hierarchical. We evaluate our methods on tasks of\nmultimodal translation and translation with multiple source languages. The\nexperiments show that the models are able to use multiple sources and improve\nover single source baselines.</p>\n", "tags": ["Model Architecture"] },
{"key": "lichtarge2019corpora", "citations": "145", "year": "2019", "title":"Corpora Generation For Grammatical Error Correction", "abstract": "<p>Grammatical Error Correction (GEC) has been recently modeled using the\nsequence-to-sequence framework. However, unlike sequence transduction problems\nsuch as machine translation, GEC suffers from the lack of plentiful parallel\ndata. We describe two approaches for generating large parallel datasets for GEC\nusing publicly available Wikipedia data. The first method extracts\nsource-target pairs from Wikipedia edit histories with minimal filtration\nheuristics, while the second method introduces noise into Wikipedia sentences\nvia round-trip translation through bridge languages. Both strategies yield\nsimilar sized parallel corpora containing around 4B tokens. We employ an\niterative decoding strategy that is tailored to the loosely supervised nature\nof our constructed corpora. We demonstrate that neural GEC models trained using\neither type of corpora give similar performance. Fine-tuning these models on\nthe Lang-8 corpus and ensembling allows us to surpass the state of the art on\nboth the CoNLL-2014 benchmark and the JFLEG task. We provide systematic\nanalysis that compares the two approaches to data generation and highlights the\neffectiveness of ensembling.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Tools","Training Techniques"] },
{"key": "likang2023survey", "citations": "100", "year": "2024", "title":"A Survey On Large Language Models For Recommendation", "abstract": "<p>Large Language Models (LLMs) have emerged as powerful tools in the field of\nNatural Language Processing (NLP) and have recently gained significant\nattention in the domain of Recommendation Systems (RS). These models, trained\non massive amounts of data using self-supervised learning, have demonstrated\nremarkable success in learning universal representations and have the potential\nto enhance various aspects of recommendation systems by some effective transfer\ntechniques such as fine-tuning and prompt tuning, and so on. The crucial aspect\nof harnessing the power of language models in enhancing recommendation quality\nis the utilization of their high-quality representations of textual features\nand their extensive coverage of external knowledge to establish correlations\nbetween items and users. To provide a comprehensive understanding of the\nexisting LLM-based recommendation systems, this survey presents a taxonomy that\ncategorizes these models into two major paradigms, respectively Discriminative\nLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation\n(GLLM4Rec), with the latter being systematically sorted out for the first time.\nFurthermore, we systematically review and analyze existing LLM-based\nrecommendation systems within each paradigm, providing insights into their\nmethodologies, techniques, and performance. Additionally, we identify key\nchallenges and several valuable findings to provide researchers and\npractitioners with inspiration. We have also created a GitHub repository to\nindex relevant papers on LLMs for recommendation,\nhttps://github.com/WLiK/LLM4Rec.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Reinforcement Learning","Survey Paper","Tools"] },
{"key": "lin2016leveraging", "citations": "86", "year": "2016", "title":"Leveraging Visual Question Answering For Image-caption Ranking", "abstract": "<p>Visual Question Answering (VQA) is the task of taking as input an image and a\nfree-form natural language question about the image, and producing an accurate\nanswer. In this work we view VQA as a “feature extraction” module to extract\nimage and caption representations. We employ these representations for the task\nof image-caption ranking. Each feature dimension captures (imagines) whether a\nfact (question-answer pair) could plausibly be true for the image and caption.\nThis allows the model to interpret images and captions from a wide variety of\nperspectives. We propose score-level and representation-level fusion models to\nincorporate VQA knowledge in an existing state-of-the-art VQA-agnostic\nimage-caption ranking model. We find that incorporating and reasoning about\nconsistency between images and captions significantly improves performance.\nConcretely, our model improves state-of-the-art on caption retrieval by 7.1%\nand on image retrieval by 4.4% on the MSCOCO dataset.</p>\n", "tags": ["Datasets"] },
{"key": "lin2017adversarial", "citations": "192", "year": "2017", "title":"Adversarial Ranking For Language Generation", "abstract": "<p>Generative adversarial networks (GANs) have great successes on synthesizing\ndata. However, the existing GANs restrict the discriminator to be a binary\nclassifier, and thus limit their learning capacity for tasks that need to\nsynthesize output with rich structures such as natural language descriptions.\nIn this paper, we propose a novel generative adversarial network, RankGAN, for\ngenerating high-quality language descriptions. Rather than training the\ndiscriminator to learn and assign absolute binary predicate for individual data\nsample, the proposed RankGAN is able to analyze and rank a collection of\nhuman-written and machine-written sentences by giving a reference group. By\nviewing a set of data samples collectively and evaluating their quality through\nrelative ranking scores, the discriminator is able to make better assessment\nwhich in turn helps to learn a better generator. The proposed RankGAN is\noptimized through the policy gradient technique. Experimental results on\nmultiple public datasets clearly demonstrate the effectiveness of the proposed\napproach.</p>\n", "tags": ["Datasets","Reinforcement Learning","Security","Training Techniques"] },
{"key": "lin2018global", "citations": "153", "year": "2018", "title":"Global Encoding For Abstractive Summarization", "abstract": "<p>In neural abstractive summarization, the conventional sequence-to-sequence\n(seq2seq) model often suffers from repetition and semantic irrelevance. To\ntackle the problem, we propose a global encoding framework, which controls the\ninformation flow from the encoder to the decoder based on the global\ninformation of the source context. It consists of a convolutional gated unit to\nperform global encoding to improve the representations of the source-side\ninformation. Evaluations on the LCSTS and the English Gigaword both demonstrate\nthat our model outperforms the baseline models, and the analysis shows that our\nmodel is capable of reducing repetition.</p>\n", "tags": ["Tools"] },
{"key": "lin2019commongen", "citations": "215", "year": "2020", "title":"Commongen: A Constrained Text Generation Challenge For Generative Commonsense Reasoning", "abstract": "<p>Recently, large-scale pre-trained language models have demonstrated\nimpressive performance on several commonsense-reasoning benchmark datasets.\nHowever, building machines with commonsense to compose realistically plausible\nsentences remains challenging. In this paper, we present a constrained text\ngeneration task, CommonGen associated with a benchmark dataset, to explicitly\ntest machines for the ability of generative commonsense reasoning. Given a set\nof common concepts (e.g., {dog, frisbee, catch, throw}); the task is to\ngenerate a coherent sentence describing an everyday scenario using these\nconcepts (e.g., “a man throws a frisbee and his dog catches it”).\n  The CommonGen task is challenging because it inherently requires 1)\nrelational reasoning with background commonsense knowledge, and 2)\ncompositional generalization ability to work on unseen concept combinations.\nOur dataset, constructed through a combination of crowdsourced and existing\ncaption corpora, consists of 79k commonsense descriptions over 35k unique\nconcept-sets. Experiments show that there is a large gap between\nstate-of-the-art text generation models (e.g., T5) and human performance.\nFurthermore, we demonstrate that the learned generative commonsense reasoning\ncapability can be transferred to improve downstream tasks such as CommonsenseQA\nby generating additional context.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "lin2019deep", "citations": "138", "year": "2019", "title":"Deep Unknown Intent Detection With Margin Loss", "abstract": "<p>Identifying the unknown (novel) user intents that have never appeared in the\ntraining set is a challenging task in the dialogue system. In this paper, we\npresent a two-stage method for detecting unknown intents. We use bidirectional\nlong short-term memory (BiLSTM) network with the margin loss as the feature\nextractor. With margin loss, we can learn discriminative deep features by\nforcing the network to maximize inter-class variance and to minimize\nintra-class variance. Then, we feed the feature vectors to the density-based\nnovelty detection algorithm, local outlier factor (LOF), to detect unknown\nintents. Experiments on two benchmark datasets show that our method can yield\nconsistent improvements compared with the baseline methods.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Training Techniques"] },
{"key": "lin2019kagnet", "citations": "460", "year": "2019", "title":"Kagnet: Knowledge-aware Graph Networks For Commonsense Reasoning", "abstract": "<p>Commonsense reasoning aims to empower machines with the human ability to make\npresumptions about ordinary situations in our daily life. In this paper, we\npropose a textual inference framework for answering commonsense questions,\nwhich effectively utilizes external, structured commonsense knowledge graphs to\nperform explainable inferences. The framework first grounds a question-answer\npair from the semantic space to the knowledge-based symbolic space as a schema\ngraph, a related sub-graph of external knowledge graphs. It represents schema\ngraphs with a novel knowledge-aware graph network module named KagNet, and\nfinally scores answers with graph representations. Our model is based on graph\nconvolutional networks and LSTMs, with a hierarchical path-based attention\nmechanism. The intermediate attention scores make it transparent and\ninterpretable, which thus produce trustworthy inferences. Using ConceptNet as\nthe only external resource for Bert-based models, we achieved state-of-the-art\nperformance on the CommonsenseQA, a large-scale dataset for commonsense\nreasoning.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Tools"] },
{"key": "lin2019moel", "citations": "190", "year": "2019", "title":"Moel: Mixture Of Empathetic Listeners", "abstract": "<p>Previous research on empathetic dialogue systems has mostly focused on\ngenerating responses given certain emotions. However, being empathetic not only\nrequires the ability of generating emotional responses, but more importantly,\nrequires the understanding of user emotions and replying appropriately. In this\npaper, we propose a novel end-to-end approach for modeling empathy in dialogue\nsystems: Mixture of Empathetic Listeners (MoEL). Our model first captures the\nuser emotions and outputs an emotion distribution. Based on this, MoEL will\nsoftly combine the output states of the appropriate Listener(s), which are each\noptimized to react to certain emotions, and generate an empathetic response.\nHuman evaluations on empathetic-dialogues (Rashkin et al., 2018) dataset\nconfirm that MoEL outperforms multitask training baseline in terms of empathy,\nrelevance, and fluency. Furthermore, the case study on generated responses of\ndifferent Listeners shows high interpretability of our model.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","EMNLP","Training Techniques"] },
{"key": "lin2019personalizing", "citations": "183", "year": "2019", "title":"Personalizing Dialogue Agents Via Meta-learning", "abstract": "<p>Existing personalized dialogue models use human designed persona descriptions\nto improve dialogue consistency. Collecting such descriptions from existing\ndialogues is expensive and requires hand-crafted feature designs. In this\npaper, we propose to extend Model-Agnostic Meta-Learning (MAML)(Finn et al.,\n2017) to personalized dialogue learning without using any persona descriptions.\nOur model learns to quickly adapt to new personas by leveraging only a few\ndialogue samples collected from the same user, which is fundamentally different\nfrom conditioning the response on the persona descriptions. Empirical results\non Persona-chat dataset (Zhang et al., 2018) indicate that our solution\noutperforms non-meta-learning baselines using automatic evaluation metrics, and\nin terms of human-evaluated fluency and consistency.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "lin2019reasoning", "citations": "98", "year": "2019", "title":"Reasoning Over Paragraph Effects In Situations", "abstract": "<p>A key component of successfully reading a passage of text is the ability to\napply knowledge gained from the passage to a new situation. In order to\nfacilitate progress on this kind of reading, we present ROPES, a challenging\nbenchmark for reading comprehension targeting Reasoning Over Paragraph Effects\nin Situations. We target expository language describing causes and effects\n(e.g., “animal pollinators increase efficiency of fertilization in flowers”),\nas they have clear implications for new situations. A system is presented a\nbackground passage containing at least one of these relations, a novel\nsituation that uses this background, and questions that require reasoning about\neffects of the relationships in the background passage in the context of the\nsituation. We collect background passages from science textbooks and Wikipedia\nthat contain such phenomena, and ask crowd workers to author situations,\nquestions, and answers, resulting in a 14,322 question dataset. We analyze the\nchallenges of this task and evaluate the performance of state-of-the-art\nreading comprehension models. The best model performs only slightly better than\nrandomly guessing an answer of the correct type, at 61.6% F1, well below the\nhuman performance of 89.0%.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "lin2020birds", "citations": "118", "year": "2020", "title":"Birds Have Four Legs?! Numersense: Probing Numerical Commonsense Knowledge Of Pre-trained Language Models", "abstract": "<p>Recent works show that pre-trained language models (PTLMs), such as BERT,\npossess certain commonsense and factual knowledge. They suggest that it is\npromising to use PTLMs as “neural knowledge bases” via predicting masked words.\nSurprisingly, we find that this may not work for numerical commonsense\nknowledge (e.g., a bird usually has two legs). In this paper, we investigate\nwhether and to what extent we can induce numerical commonsense knowledge from\nPTLMs as well as the robustness of this process. To study this, we introduce a\nnovel probing task with a diagnostic dataset, NumerSense, containing 13.6k\nmasked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our\nanalysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly\non the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with\ndistant supervision brings some improvement; (3) the best supervised model\nstill performs poorly as compared to human performance (54.06% vs 96.3% in\naccuracy).</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "lin2020exploring", "citations": "73", "year": "2020", "title":"Exploring Versatile Generative Language Model Via Parameter-efficient Transfer Learning", "abstract": "<p>Fine-tuning pre-trained generative language models to down-stream language\ngeneration tasks has shown promising results. However, this comes with the cost\nof having a single, large model for each task, which is not ideal in\nlow-memory/power scenarios (e.g., mobile). In this paper, we propose an\neffective way to fine-tune multiple down-stream generation tasks simultaneously\nusing a single, large pre-trained model. The experiments on five diverse\nlanguage generation tasks show that by just using an additional 2-3% parameters\nfor each task, our model can maintain or even improve the performance of\nfine-tuning the whole model.</p>\n", "tags": ["EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "lin2020mintl", "citations": "141", "year": "2020", "title":"Mintl: Minimalist Transfer Learning For Task-oriented Dialogue Systems", "abstract": "<p>In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify\nthe system design process of task-oriented dialogue systems and alleviate the\nover-dependency on annotated data. MinTL is a simple yet effective transfer\nlearning framework, which allows us to plug-and-play pre-trained seq2seq\nmodels, and jointly learn dialogue state tracking and dialogue response\ngeneration. Unlike previous approaches, which use a copy mechanism to\n“carryover” the old dialogue states to the new one, we introduce Levenshtein\nbelief spans (Lev), that allows efficient dialogue state tracking with a\nminimal generation length. We instantiate our learning framework with two\npre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive\nexperiments demonstrate that: 1) our systems establish new state-of-the-art\nresults on end-to-end response generation, 2) MinTL-based systems are more\nrobust than baseline methods in the low resource setting, and they achieve\ncompetitive results with only 20% training data, and 3) Lev greatly improves\nthe inference efficiency.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP","Efficiency","Fine-Tuning","Tools","Training Techniques"] },
{"key": "lin2020pre", "citations": "81", "year": "2020", "title":"Pre-training Multilingual Neural Machine Translation By Leveraging Alignment Information", "abstract": "<p>We investigate the following question for machine translation (MT): can we\ndevelop a single universal MT model to serve as the common seed and obtain\nderivative and improved models on arbitrary language pairs? We propose mRASP,\nan approach to pre-train a universal multilingual neural machine translation\nmodel. Our key idea in mRASP is its novel technique of random aligned\nsubstitution, which brings words and phrases with similar meanings across\nmultiple languages closer in the representation space. We pre-train a mRASP\nmodel on 32 language pairs jointly with only public datasets. The model is then\nfine-tuned on downstream language pairs to obtain specialized MT models. We\ncarry out extensive experiments on 42 translation directions across a diverse\nsettings, including low, medium, rich resource, and as well as transferring to\nexotic language pairs. Experimental results demonstrate that mRASP achieves\nsignificant performance improvement compared to directly training on those\ntarget pairs. It is the first time to verify that multiple low-resource\nlanguage pairs can be utilized to improve rich resource MT. Surprisingly, mRASP\nis even able to improve the translation quality on exotic languages that never\noccur in the pre-training corpus. Code, data, and pre-trained models are\navailable at https://github.com/linzehui/mRASP.</p>\n", "tags": ["EMNLP","Has Code","Training Techniques"] },
{"key": "lin2021few", "citations": "72", "year": "2021", "title":"Few-shot Learning With Multilingual Language Models", "abstract": "<p>Large-scale generative language models such as GPT-3 are competitive few-shot\nlearners. While these models are known to be able to jointly represent many\ndifferent languages, their training data is dominated by English, potentially\nlimiting their cross-lingual generalization. In this work, we train\nmultilingual generative language models on a corpus covering a diverse set of\nlanguages, and study their few- and zero-shot learning capabilities in a wide\nrange of tasks. Our largest model with 7.5 billion parameters sets new state of\nthe art in few-shot learning in more than 20 representative languages,\noutperforming GPT-3 of comparable size in multilingual commonsense reasoning\n(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in\n4-shot settings) and natural language inference (+5.4% in each of 0-shot and\n4-shot settings). On the FLORES-101 machine translation benchmark, our model\noutperforms GPT-3 on 171 out of 182 directions with 32 training examples, while\nsurpassing the official supervised baseline in 45 directions. We conduct an\nin-depth analysis of different multilingual prompting approaches, showing in\nparticular that strong few-shot learning performance across languages can be\nachieved via cross-lingual transfer through both templates and demonstration\nexamples. Finally, we evaluate our models in social value tasks such as hate\nspeech detection in five languages and find it has limitations similar to\ncomparable sized GPT-3 models.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Model Architecture","Prompting","Training Techniques"] },
{"key": "lin2021swinbert", "citations": "161", "year": "2022", "title":"Swinbert: End-to-end Transformers With Sparse Attention For Video Captioning", "abstract": "<p>The canonical approach to video captioning dictates a caption generation\nmodel to learn from offline-extracted dense video features. These feature\nextractors usually operate on video frames sampled at a fixed frame rate and\nare often trained on image/video understanding tasks, without adaption to video\ncaptioning data. In this work, we present SwinBERT, an end-to-end\ntransformer-based model for video captioning, which takes video frame patches\ndirectly as inputs, and outputs a natural language description. Instead of\nleveraging multiple 2D/3D feature extractors, our method adopts a video\ntransformer to encode spatial-temporal representations that can adapt to\nvariable lengths of video input without dedicated design for different frame\nrates. Based on this model architecture, we show that video captioning can\nbenefit significantly from more densely sampled video frames as opposed to\nprevious successes with sparsely sampled video frames for video-and-language\nunderstanding tasks (e.g., video question answering). Moreover, to avoid the\ninherent redundancy in consecutive video frames, we propose adaptively learning\na sparse attention mask and optimizing it for task-specific performance\nimprovement through better long-range video sequence modeling. Through\nextensive experiments on 5 video captioning datasets, we show that SwinBERT\nachieves across-the-board performance improvements over previous methods, often\nby a large margin. The learned sparse attention masks in addition push the\nlimit to new state of the arts, and can be transferred between different video\nlengths and between different datasets. Code is available at\nhttps://github.com/microsoft/SwinBERT</p>\n", "tags": ["CVPR","Datasets","Has Code","Model Architecture"] },
{"key": "lin2021task", "citations": "60", "year": "2021", "title":"Task-adaptive Neural Process For User Cold-start Recommendation", "abstract": "<p>User cold-start recommendation is a long-standing challenge for recommender\nsystems due to the fact that only a few interactions of cold-start users can be\nexploited. Recent studies seek to address this challenge from the perspective\nof meta learning, and most of them follow a manner of parameter initialization,\nwhere the model parameters can be learned by a few steps of gradient updates.\nWhile these gradient-based meta-learning models achieve promising performances\nto some extent, a fundamental problem of them is how to adapt the global\nknowledge learned from previous tasks for the recommendations of cold-start\nusers more effectively. In this paper, we develop a novel meta-learning\nrecommender called task-adaptive neural process (TaNP). TaNP is a new member of\nthe neural process family, where making recommendations for each user is\nassociated with a corresponding stochastic process. TaNP directly maps the\nobserved interactions of each user to a predictive distribution, sidestepping\nsome training issues in gradient-based meta-learning models. More importantly,\nto balance the trade-off between model capacity and adaptation reliability, we\nintroduce a novel task-adaptive mechanism. It enables our model to learn the\nrelevance of different tasks and customize the global knowledge to the\ntask-related decoder parameters for estimating user preferences. We validate\nTaNP on multiple benchmark datasets in different experimental settings.\nEmpirical results demonstrate that TaNP yields consistent improvements over\nseveral state-of-the-art meta-learning recommenders.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "lin2021traceability", "citations": "88", "year": "2021", "title":"Traceability Transformed: Generating More Accurate Links With Pre-trained BERT Models", "abstract": "<p>Software traceability establishes and leverages associations between diverse\ndevelopment artifacts. Researchers have proposed the use of deep learning trace\nmodels to link natural language artifacts, such as requirements and issue\ndescriptions, to source code; however, their effectiveness has been restricted\nby availability of labeled data and efficiency at runtime. In this study, we\npropose a novel framework called Trace BERT (T-BERT) to generate trace links\nbetween source code and natural language artifacts. To address data sparsity,\nwe leverage a three-step training strategy to enable trace models to transfer\nknowledge from a closely related Software Engineering challenge, which has a\nrich dataset, to produce trace links with much higher accuracy than has\npreviously been achieved. We then apply the T-BERT framework to recover links\nbetween issues and commits in Open Source Projects. We comparatively evaluated\naccuracy and efficiency of three BERT architectures. Results show that a\nSingle-BERT architecture generated the most accurate links, while a\nSiamese-BERT architecture produced comparable results with significantly less\nexecution time. Furthermore, by learning and transferring knowledge, all three\nmodels in the framework outperform classical IR trace models. On the three\nevaluated real-word OSS projects, the best T-BERT stably outperformed the VSM\nmodel with average improvements of 60.31% measured using Mean Average Precision\n(MAP). RNN severely underperformed on these projects due to insufficient\ntraining data, while T-BERT overcame this problem by using pretrained language\nmodels and transfer learning.</p>\n", "tags": ["Datasets","Fine-Tuning","Llm For Code","Model Architecture","Tools","Training Techniques"] },
{"key": "lin2021truthfulqa", "citations": "223", "year": "2022", "title":"Truthfulqa: Measuring How Models Mimic Human Falsehoods", "abstract": "<p>We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. This contrasts with other NLP tasks, where\nperformance improves with model size. However, this result is expected if false\nanswers are learned from the training distribution. We suggest that scaling up\nmodels alone is less promising for improving truthfulness than fine-tuning\nusing training objectives other than imitation of text from the web.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "lin2022frozen", "citations": "106", "year": "2022", "title":"Frozen CLIP Models Are Efficient Video Learners", "abstract": "<p>Video recognition has been dominated by the end-to-end learning paradigm –\nfirst initializing a video recognition model with weights of a pretrained image\nmodel and then conducting end-to-end training on videos. This enables the video\nnetwork to benefit from the pretrained image model. However, this requires\nsubstantial computation and memory resources for finetuning on videos and the\nalternative of directly using pretrained image features without finetuning the\nimage backbone leads to subpar results. Fortunately, recent advances in\nContrastive Vision-Language Pre-training (CLIP) pave the way for a new route\nfor visual recognition tasks. Pretrained on large open-vocabulary image-text\npair data, these models learn powerful visual representations with rich\nsemantics. In this paper, we present Efficient Video Learning (EVL) – an\nefficient framework for directly training high-quality video recognition models\nwith frozen CLIP features. Specifically, we employ a lightweight Transformer\ndecoder and learn a query token to dynamically collect frame-level spatial\nfeatures from the CLIP image encoder. Furthermore, we adopt a local temporal\nmodule in each decoder layer to discover temporal clues from adjacent frames\nand their attention maps. We show that despite being efficient to train with a\nfrozen backbone, our models learn high quality video representations on a\nvariety of video recognition datasets. Code is available at\nhttps://github.com/OpenGVLab/efficient-video-recognition.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "lin2023text2motion", "citations": "82", "year": "2023", "title":"Text2motion: From Natural Language Instructions To Feasible Plans", "abstract": "<p>We propose Text2Motion, a language-based planning framework enabling robots\nto solve sequential manipulation tasks that require long-horizon reasoning.\nGiven a natural language instruction, our framework constructs both a task- and\nmotion-level plan that is verified to reach inferred symbolic goals.\nText2Motion uses feasibility heuristics encoded in Q-functions of a library of\nskills to guide task planning with Large Language Models. Whereas previous\nlanguage-based planners only consider the feasibility of individual skills,\nText2Motion actively resolves geometric dependencies spanning skill sequences\nby performing geometric feasibility planning during its search. We evaluate our\nmethod on a suite of problems that require long-horizon reasoning,\ninterpretation of abstract goals, and handling of partial affordance\nperception. Our experiments show that Text2Motion can solve these challenging\nproblems with a success rate of 82%, while prior state-of-the-art\nlanguage-based planning methods only achieve 13%. Text2Motion thus provides\npromising generalization characteristics to semantically diverse sequential\nmanipulation tasks with geometric dependencies between skills.</p>\n", "tags": ["Instruction Following","Tools"] },
{"key": "ling2016latent", "citations": "298", "year": "2016", "title":"Latent Predictor Networks For Code Generation", "abstract": "<p>Many language generation tasks require the production of text conditioned on\nboth structured and unstructured inputs. We present a novel neural network\narchitecture which generates an output sequence conditioned on an arbitrary\nnumber of input functions. Crucially, our approach allows both the choice of\nconditioning context and the granularity of generation, for example characters\nor tokens, to be marginalised, thus permitting scalable and effective training.\nUsing this framework, we address the problem of generating programming code\nfrom a mixed natural language and structured specification. We create two new\ndata sets for this paradigm derived from the collectible trading card games\nMagic the Gathering and Hearthstone. On these, and a third preexisting corpus,\nwe demonstrate that marginalising multiple predictors allows our model to\noutperform strong benchmarks.</p>\n", "tags": ["Datasets","Llm For Code","Model Architecture","Tools","Training Techniques"] },
{"key": "ling2017program", "citations": "261", "year": "2017", "title":"Program Induction By Rationale Generation : Learning To Solve And Explain Algebraic Word Problems", "abstract": "<p>Solving algebraic word problems requires executing a series of arithmetic\noperations—a program—to obtain a final answer. However, since programs can\nbe arbitrarily complicated, inducing them directly from question-answer pairs\nis a formidable challenge. To make this task more feasible, we solve these\nproblems by generating answer rationales, sequences of natural language and\nhuman-readable mathematical expressions that derive the final answer through a\nseries of small steps. Although rationales do not explicitly specify programs,\nthey provide a scaffolding for their structure via intermediate milestones. To\nevaluate our approach, we have created a new 100,000-sample dataset of\nquestions, answers and rationales. Experimental results show that indirect\nsupervision of program learning via answer rationales is a promising strategy\nfor inducing arithmetic programs.</p>\n", "tags": ["Datasets"] },
{"key": "ling2022vision", "citations": "74", "year": "2022", "title":"Vision-language Pre-training For Multimodal Aspect-based Sentiment Analysis", "abstract": "<p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.</p>\n", "tags": ["Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "linting2020mt5", "citations": "1300", "year": "2021", "title":"Mt5: A Massively Multilingual Pre-trained Text-to-text Transformer", "abstract": "<p>The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent “accidental\ntranslation” in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.</p>\n", "tags": ["Datasets","Model Architecture","NAACL","Training Techniques"] },
{"key": "linzen2016assessing", "citations": "843", "year": "2016", "title":"Assessing The Ability Of Lstms To Learn Syntax-sensitive Dependencies", "abstract": "<p>The success of long short-term memory (LSTM) neural networks in language\nprocessing is typically attributed to their ability to capture long-distance\nstatistical regularities. Linguistic regularities are often sensitive to\nsyntactic structure; can such dependencies be captured by LSTMs, which do not\nhave explicit structural representations? We begin addressing this question\nusing number agreement in English subject-verb dependencies. We probe the\narchitecture’s grammatical competence both using training objectives with an\nexplicit grammatical target (number prediction, grammaticality judgments) and\nusing language models. In the strongly supervised settings, the LSTM achieved\nvery high overall accuracy (less than 1% errors), but errors increased when\nsequential and structural information conflicted. The frequency of such errors\nrose sharply in the language-modeling setting. We conclude that LSTMs can\ncapture a non-trivial amount of grammatical structure given targeted\nsupervision, but stronger architectures may be required to further reduce\nerrors; furthermore, the language modeling signal is insufficient for capturing\nsyntax-sensitive dependencies, and should be supplemented with more direct\nsupervision if such dependencies need to be captured.</p>\n", "tags": ["Model Architecture","TACL","Training Techniques"] },
{"key": "linzen2020how", "citations": "143", "year": "2020", "title":"How Can We Accelerate Progress Towards Human-like Linguistic Generalization?", "abstract": "<p>This position paper describes and critiques the Pretraining-Agnostic\nIdentically Distributed (PAID) evaluation paradigm, which has become a central\ntool for measuring progress in natural language understanding. This paradigm\nconsists of three stages: (1) pre-training of a word prediction model on a\ncorpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set\nrepresenting a classification task; (3) evaluation on a test set drawn from the\nsame distribution as that training set. This paradigm favors simple, low-bias\narchitectures, which, first, can be scaled to process vast amounts of data, and\nsecond, can capture the fine-grained statistical properties of a particular\ndata set, regardless of whether those properties are likely to generalize to\nexamples of the task outside the data set. This contrasts with humans, who\nlearn language from several orders of magnitude less data than the systems\nfavored by this evaluation paradigm, and generalize to new tasks in a\nconsistent way. We advocate for supplementing or replacing PAID with paradigms\nthat reward architectures that generalize as quickly and robustly as humans.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Fine-Tuning","Reinforcement Learning","Training Techniques"] },
{"key": "linzen2020syntactic", "citations": "183", "year": "2020", "title":"Syntactic Structure From Deep Learning", "abstract": "<p>Modern deep neural networks achieve impressive performance in engineering\napplications that require extensive linguistic skills, such as machine\ntranslation. This success has sparked interest in probing whether these models\nare inducing human-like grammatical knowledge from the raw data they are\nexposed to, and, consequently, whether they can shed new light on long-standing\ndebates concerning the innate structure necessary for language acquisition. In\nthis article, we survey representative studies of the syntactic abilities of\ndeep networks, and discuss the broader implications that this work has for\ntheoretical linguistics.</p>\n", "tags": ["Applications","Survey Paper"] },
{"key": "lipton2016mythos", "citations": "2202", "year": "2018", "title":"The Mythos Of Model Interpretability", "abstract": "<p>Supervised machine learning models boast remarkable predictive capabilities.\nBut can you trust your model? Will it work in deployment? What else can it tell\nyou about the world? We want models to be not only good, but interpretable. And\nyet the task of interpretation appears underspecified. Papers provide diverse\nand sometimes non-overlapping motivations for interpretability, and offer\nmyriad notions of what attributes render models interpretable. Despite this\nambiguity, many papers proclaim interpretability axiomatically, absent further\nexplanation. In this paper, we seek to refine the discourse on\ninterpretability. First, we examine the motivations underlying interest in\ninterpretability, finding them to be diverse and occasionally discordant. Then,\nwe address model properties and techniques thought to confer interpretability,\nidentifying transparency to humans and post-hoc explanations as competing\nnotions. Throughout, we discuss the feasibility and desirability of different\nnotions, and question the oft-made assertions that linear models are\ninterpretable and that deep neural networks are not.</p>\n", "tags": ["Ethics & Fairness"] },
{"key": "liu2016attention", "citations": "720", "year": "2016", "title":"Attention-based Recurrent Neural Network Models For Joint Intent Detection And Slot Filling", "abstract": "<p>Attention-based encoder-decoder neural network models have recently shown\npromising results in machine translation and speech recognition. In this work,\nwe propose an attention-based neural network model for joint intent detection\nand slot filling, both of which are critical steps for many speech\nunderstanding and dialog systems. Unlike in machine translation and speech\nrecognition, alignment is explicit in slot filling. We explore different\nstrategies in incorporating this alignment information to the encoder-decoder\nframework. Learning from the attention mechanism in encoder-decoder model, we\nfurther propose introducing attention to the alignment-based RNN models. Such\nattentions provide additional information to the intent classification and slot\nlabel prediction. Our independent task models achieve state-of-the-art intent\ndetection error rate and slot filling F1 score on the benchmark ATIS task. Our\njoint training model further obtains 0.56% absolute (23.8% relative) error\nreduction on intent detection and 0.23% absolute gain on slot filling over the\nindependent task models.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH","Model Architecture","Tools","Training Techniques"] },
{"key": "liu2016how", "citations": "954", "year": "2016", "title":"How NOT To Evaluate Your Dialogue System: An Empirical Study Of Unsupervised Evaluation Metrics For Dialogue Response Generation", "abstract": "<p>We investigate evaluation metrics for dialogue response generation systems\nwhere supervised labels, such as task completion, are not available. Recent\nworks in response generation have adopted metrics from machine translation to\ncompare a model’s generated response to a single target response. We show that\nthese metrics correlate very weakly with human judgements in the non-technical\nTwitter domain, and not at all in the technical Ubuntu domain. We provide\nquantitative and qualitative results highlighting specific weaknesses in\nexisting metrics, and provide recommendations for future development of better\nautomatic evaluation metrics for dialogue systems.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP","Evaluation"] },
{"key": "liu2016improved", "citations": "322", "year": "2017", "title":"Improved Image Captioning Via Policy Gradient Optimization Of Spider", "abstract": "<p>Current image captioning methods are usually trained via (penalized) maximum\nlikelihood estimation. However, the log-likelihood score of a caption does not\ncorrelate well with human assessments of quality. Standard syntactic evaluation\nmetrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The\nnewer SPICE and CIDEr metrics are better correlated, but have traditionally\nbeen hard to optimize for. In this paper, we show how to use a policy gradient\n(PG) method to directly optimize a linear combination of SPICE and CIDEr (a\ncombination we call SPIDEr): the SPICE score ensures our captions are\nsemantically faithful to the image, while CIDEr score ensures our captions are\nsyntactically fluent. The PG method we propose improves on the prior MIXER\napproach, by using Monte Carlo rollouts instead of mixing MLE training with PG.\nWe show empirically that our algorithm leads to easier optimization and\nimproved results compared to MIXER. Finally, we show that using our PG method\nwe can optimize any of the metrics, including the proposed SPIDEr metric which\nresults in image captions that are strongly preferred by human raters compared\nto captions generated by the same model but trained to optimize MLE or the COCO\nmetrics.</p>\n", "tags": ["Evaluation","ICCV","Reinforcement Learning","Training Techniques"] },
{"key": "liu2016learning", "citations": "238", "year": "2016", "title":"Learning Natural Language Inference Using Bidirectional LSTM Model And Inner-attention", "abstract": "<p>In this paper, we proposed a sentence encoding-based model for recognizing\ntext entailment. In our approach, the encoding of sentence is a two-stage\nprocess. Firstly, average pooling was used over word-level bidirectional LSTM\n(biLSTM) to generate a first-stage sentence representation. Secondly, attention\nmechanism was employed to replace average pooling on the same sentence for\nbetter representations. Instead of using target sentence to attend words in\nsource sentence, we utilized the sentence’s first-stage representation to\nattend words appeared in itself, which is called “Inner-Attention” in our paper\n. Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus\nhas proved the effectiveness of “Inner-Attention” mechanism. With less number\nof parameters, our model outperformed the existing best sentence encoding-based\napproach by a large margin.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "liu2016neural", "citations": "135", "year": "2016", "title":"Neural Machine Translation With Supervised Attention", "abstract": "<p>The attention mechanisim is appealing for neural machine translation, since\nit is able to dynam- ically encode a source sentence by generating a alignment\nbetween a target word and source words. Unfortunately, it has been proved to be\nworse than conventional alignment models in aligment accuracy. In this paper,\nwe analyze and explain this issue from the point view of re- ordering, and\npropose a supervised attention which is learned with guidance from conventional\nalignment models. Experiments on two Chinese-to-English translation tasks show\nthat the super- vised attention mechanism yields better alignments leading to\nsubstantial gains over the standard attention based NMT.</p>\n", "tags": ["Model Architecture"] },
{"key": "liu2017end", "citations": "99", "year": "2017", "title":"End-to-end Optimization Of Task-oriented Dialogue Model With Deep Reinforcement Learning", "abstract": "<p>In this paper, we present a neural network based task-oriented dialogue\nsystem that can be optimized end-to-end with deep reinforcement learning (RL).\nThe system is able to track dialogue state, interface with knowledge bases, and\nincorporate query results into agent’s responses to successfully complete\ntask-oriented dialogues. Dialogue policy learning is conducted with a hybrid\nsupervised and deep RL methods. We first train the dialogue agent in a\nsupervised manner by learning directly from task-oriented dialogue corpora, and\nfurther optimize it with deep RL during its interaction with users. In the\nexperiments on two different dialogue task domains, our model demonstrates\nrobust performance in tracking dialogue state and producing reasonable system\nresponses. We show that deep RL based optimization leads to significant\nimprovement on task success rate and reduction in dialogue length comparing to\nsupervised training model. We further show benefits of training task-oriented\ndialogue model end-to-end comparing to component-wise optimization with\nexperiment results on dialogue simulations and human evaluations.</p>\n", "tags": ["Efficiency","INTERSPEECH","Reinforcement Learning","Training Techniques"] },
{"key": "liu2017iterative", "citations": "97", "year": "2017", "title":"Iterative Policy Learning In End-to-end Trainable Task-oriented Neural Dialog Models", "abstract": "<p>In this paper, we present a deep reinforcement learning (RL) framework for\niterative dialog policy optimization in end-to-end task-oriented dialog\nsystems. Popular approaches in learning dialog policy with RL include letting a\ndialog agent to learn against a user simulator. Building a reliable user\nsimulator, however, is not trivial, often as difficult as building a good\ndialog agent. We address this challenge by jointly optimizing the dialog agent\nand the user simulator with deep RL by simulating dialogs between the two\nagents. We first bootstrap a basic dialog agent and a basic user simulator by\nlearning directly from dialog corpora with supervised training. We then improve\nthem further by letting the two agents to conduct task-oriented dialogs and\niteratively optimizing their policies with deep RL. Both the dialog agent and\nthe user simulator are designed with neural network models that can be trained\nend-to-end. Our experiment results show that the proposed method leads to\npromising improvements on task success rate and total task reward comparing to\nsupervised training and single-agent RL training baseline models.</p>\n", "tags": ["ASRU","Dialogue & Multi Turn","Reinforcement Learning","Training Techniques"] },
{"key": "liu2017stochastic", "citations": "194", "year": "2018", "title":"Stochastic Answer Networks For Machine Reading Comprehension", "abstract": "<p>We propose a simple yet robust stochastic answer network (SAN) that simulates\nmulti-step reasoning in machine reading comprehension. Compared to previous\nwork such as ReasoNet which used reinforcement learning to determine the number\nof steps, the unique feature is the use of a kind of stochastic prediction\ndropout on the answer module (final layer) of the neural network during the\ntraining. We show that this simple trick improves robustness and achieves\nresults competitive to the state-of-the-art on the Stanford Question Answering\nDataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading\nCOmprehension Dataset (MS MARCO).</p>\n", "tags": ["Datasets","Reinforcement Learning","Training Techniques"] },
{"key": "liu2017table", "citations": "246", "year": "2018", "title":"Table-to-text Generation By Structure-aware Seq2seq Learning", "abstract": "<p>Table-to-text generation aims to generate a description for a factual table\nwhich can be viewed as a set of field-value records. To encode both the content\nand the structure of a table, we propose a novel structure-aware seq2seq\narchitecture which consists of field-gating encoder and description generator\nwith dual attention. In the encoding phase, we update the cell memory of the\nLSTM unit by a field gate and its corresponding field value in order to\nincorporate field information into table representation. In the decoding phase,\ndual attention mechanism which contains word level attention and field level\nattention is proposed to model the semantic relevance between the generated\ndescription and the table. We conduct experiments on the \\texttt{WIKIBIO}\ndataset which contains over 700k biographies and corresponding infoboxes from\nWikipedia. The attention visualizations and case studies show that our model is\ncapable of generating coherent and informative descriptions based on the\ncomprehensive understanding of both the content and the structure of a table.\nAutomatic evaluations also show our model outperforms the baselines by a great\nmargin. Code for this work is available on\nhttps://github.com/tyliupku/wiki2bio.</p>\n", "tags": ["AAAI","Datasets","Has Code","Model Architecture"] },
{"key": "liu2018dialogue", "citations": "156", "year": "2018", "title":"Dialogue Learning With Human Teaching And Feedback In End-to-end Trainable Task-oriented Dialogue Systems", "abstract": "<p>In this work, we present a hybrid learning method for training task-oriented\ndialogue systems through online user interactions. Popular methods for learning\ntask-oriented dialogues include applying reinforcement learning with user\nfeedback on supervised pre-training models. Efficiency of such learning method\nmay suffer from the mismatch of dialogue state distribution between offline\ntraining and online interactive learning stages. To address this challenge, we\npropose a hybrid imitation and reinforcement learning method, with which a\ndialogue agent can effectively learn from its interaction with users by\nlearning from human teaching and feedback. We design a neural network based\ntask-oriented dialogue agent that can be optimized end-to-end with the proposed\nlearning method. Experimental results show that our end-to-end dialogue agent\ncan learn effectively from the mistake it makes via imitation learning from\nuser teaching. Applying reinforcement learning with user feedback after the\nimitation learning stage further improves the agent’s capability in\nsuccessfully completing a task.</p>\n", "tags": ["Dialogue & Multi Turn","NAACL","Reinforcement Learning","Training Techniques"] },
{"key": "liu2018generating", "citations": "549", "year": "2018", "title":"Generating Wikipedia By Summarizing Long Sequences", "abstract": "<p>We show that generating English Wikipedia articles can be approached as a\nmulti- document summarization of source documents. We use extractive\nsummarization to coarsely identify salient information and a neural abstractive\nmodel to generate the article. For the abstractive model, we introduce a\ndecoder-only architecture that can scalably attend to very long sequences, much\nlonger than typical encoder- decoder architectures used in sequence\ntransduction. We show that this model can generate fluent, coherent\nmulti-sentence paragraphs and even whole Wikipedia articles. When given\nreference documents, we show it can extract relevant factual information as\nreflected in perplexity, ROUGE scores and human evaluations.</p>\n", "tags": ["Model Architecture"] },
{"key": "liu2018towards", "citations": "142", "year": "2019", "title":"Towards Explainable NLP: A Generative Explanation Framework For Text Classification", "abstract": "<p>Building explainable systems is a critical problem in the field of Natural\nLanguage Processing (NLP), since most machine learning models provide no\nexplanations for the predictions. Existing approaches for explainable machine\nlearning systems tend to focus on interpreting the outputs or the connections\nbetween inputs and outputs. However, the fine-grained information is often\nignored, and the systems do not explicitly generate the human-readable\nexplanations. To better alleviate this problem, we propose a novel generative\nexplanation framework that learns to make classification decisions and generate\nfine-grained explanations at the same time. More specifically, we introduce the\nexplainable factor and the minimum risk training approach that learn to\ngenerate more reasonable explanations. We construct two new datasets that\ncontain summaries, rating scores, and fine-grained reasons. We conduct\nexperiments on both datasets, comparing with several strong neural network\nbaseline systems. Experimental results show that our method surpasses all\nbaselines on both datasets, and is able to generate concise explanations at the\nsame time.</p>\n", "tags": ["Tools"] },
{"key": "liu2019attention", "citations": "93", "year": "2020", "title":"Attention-informed Mixed-language Training For Zero-shot Cross-lingual Task-oriented Dialogue Systems", "abstract": "<p>Recently, data-driven task-oriented dialogue systems have achieved promising\nperformance in English. However, developing dialogue systems that support\nlow-resource languages remains a long-standing challenge due to the absence of\nhigh-quality data. In order to circumvent the expensive and time-consuming data\ncollection, we introduce Attention-Informed Mixed-Language Training (MLT), a\nnovel zero-shot adaptation method for cross-lingual task-oriented dialogue\nsystems. It leverages very few task-related parallel word pairs to generate\ncode-switching sentences for learning the inter-lingual semantics across\nlanguages. Instead of manually selecting the word pairs, we propose to extract\nsource words based on the scores computed by the attention layer of a trained\nEnglish task-related model and then generate word pairs using existing\nbilingual dictionaries. Furthermore, intensive experiments with different\ncross-lingual embeddings demonstrate the effectiveness of our approach.\nFinally, with very few word pairs, our model achieves significant zero-shot\nadaptation performance improvements in both cross-lingual dialogue state\ntracking and natural language understanding (i.e., intent detection and slot\nfilling) tasks compared to the current state-of-the-art approaches, which\nutilize a much larger amount of bilingual data.</p>\n", "tags": ["AAAI","Model Architecture","Training Techniques"] },
{"key": "liu2019automatic", "citations": "97", "year": "2019", "title":"Automatic Generation Of Pull Request Descriptions", "abstract": "<p>Enabled by the pull-based development model, developers can easily contribute\nto a project through pull requests (PRs). When creating a PR, developers can\nadd a free-form description to describe what changes are made in this PR and/or\nwhy. Such a description is helpful for reviewers and other developers to gain a\nquick understanding of the PR without touching the details and may reduce the\npossibility of the PR being ignored or rejected. However, developers sometimes\nneglect to write descriptions for PRs. For example, in our collected dataset\nwith over 333K PRs, more than 34% of the PR descriptions are empty. To\nalleviate this problem, we propose an approach to automatically generate PR\ndescriptions based on the commit messages and the added source code comments in\nthe PRs. We regard this problem as a text summarization problem and solve it\nusing a novel sequence-to-sequence model. To cope with out-of-vocabulary words\nin software artifacts and bridge the gap between the training loss function of\nthe sequence-to-sequence model and the evaluation metric ROUGE, which has been\nshown to correspond to human evaluation, we integrate the pointer generator and\ndirectly optimize for ROUGE using reinforcement learning and a special loss\nfunction. We build a dataset with over 41K PRs and evaluate our approach on\nthis dataset through ROUGE and a human evaluation. Our evaluation results show\nthat our approach outperforms two baselines by significant margins.</p>\n", "tags": ["Evaluation","Llm For Code"] },
{"key": "liu2019benchmarking", "citations": "117", "year": "2021", "title":"Benchmarking Natural Language Understanding Services For Building Conversational Agents", "abstract": "<p>We have recently seen the emergence of several publicly available Natural\nLanguage Understanding (NLU) toolkits, which map user utterances to structured,\nbut more abstract, Dialogue Act (DA) or Intent specifications, while making\nthis process accessible to the lay developer. In this paper, we present the\nfirst wide coverage evaluation and comparison of some of the most popular NLU\nservices, on a large, multi-domain (21 domains) dataset of 25K user utterances\nthat we have collected and annotated with Intent and Entity Type specifications\nand which will be released as part of this submission. The results show that on\nIntent classification Watson significantly outperforms the other platforms,\nnamely, Dialogflow, LUIS and Rasa; though these also perform well.\nInterestingly, on Entity Type recognition, Watson performs significantly worse\ndue to its low Precision. Again, Dialogflow, LUIS and Rasa perform well on this\ntask.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "liu2019clevr", "citations": "95", "year": "2019", "title":"Clevr-ref+: Diagnosing Visual Reasoning With Referring Expressions", "abstract": "<p>Referring object detection and referring image segmentation are important\ntasks that require joint understanding of visual information and natural\nlanguage. Yet there has been evidence that current benchmark datasets suffer\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\ntheir intermediate reasoning process. To address these issues and complement\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\ndiagnostic dataset for referring expression comprehension. The precise\nlocations and attributes of the objects are readily available, and the\nreferring expressions are automatically associated with functional programs.\nThe synthetic nature allows control over dataset bias (through sampling\nstrategy), and the modular programs enable intermediate reasoning ground truth\nwithout human annotators.\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\nalso propose IEP-Ref, a module network approach that significantly outperforms\nother models on our dataset. In particular, we present two interesting and\nimportant findings using IEP-Ref: (1) the module trained to transform feature\nmaps into segmentation masks can be attached to any intermediate module to\nreveal the entire reasoning process step-by-step; (2) even if all training data\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\nwhen presented with false-premise referring expressions. To the best of our\nknowledge, this is the first direct and quantitative proof that neural modules\nbehave in the way they are intended.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Training Techniques"] },
{"key": "liu2019clinically", "citations": "108", "year": "2019", "title":"Clinically Accurate Chest X-ray Report Generation", "abstract": "<p>The automatic generation of radiology reports given medical radiographs has\nsignificant potential to operationally and improve clinical patient care. A\nnumber of prior works have focused on this problem, employing advanced methods\nfrom computer vision and natural language generation to produce readable\nreports. However, these works often fail to account for the particular nuances\nof the radiology domain, and, in particular, the critical importance of\nclinical accuracy in the resulting generated reports. In this work, we present\na domain-aware automatic chest X-ray radiology report generation system which\nfirst predicts what topics will be discussed in the report, then conditionally\ngenerates sentences corresponding to these topics. The resulting system is\nfine-tuned using reinforcement learning, considering both readability and\nclinical accuracy, as assessed by the proposed Clinically Coherent Reward. We\nverify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that\nour model offers marked improvements on both language generation metrics and\nCheXpert assessed accuracy over a variety of competitive baselines.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Reinforcement Learning"] },
{"key": "liu2019end", "citations": "146", "year": "2019", "title":"End-to-end Speech Translation With Knowledge Distillation", "abstract": "<p>End-to-end speech translation (ST), which directly translates from source\nlanguage speech into target language text, has attracted intensive attentions\nin recent years. Compared to conventional pipeline systems, end-to-end ST\nmodels have advantages of lower latency, smaller model size and less error\npropagation. However, the combination of speech recognition and text\ntranslation in one model is more difficult than each of these two tasks. In\nthis paper, we propose a knowledge distillation approach to improve ST model by\ntransferring the knowledge from text translation model. Specifically, we first\ntrain a text translation model, regarded as a teacher model, and then ST model\nis trained to learn output probabilities from teacher model through knowledge\ndistillation. Experiments on English- French Augmented LibriSpeech and\nEnglish-Chinese TED corpus show that end-to-end ST is possible to implement on\nboth similar and dissimilar language pairs. In addition, with the instruction\nof teacher model, end-to-end ST model can gain significant improvements by over\n3.5 BLEU points.</p>\n", "tags": ["Datasets","INTERSPEECH"] },
{"key": "liu2019fine", "citations": "358", "year": "2019", "title":"Fine-tune BERT For Extractive Summarization", "abstract": "<p>BERT, a pre-trained Transformer model, has achieved ground-breaking\nperformance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple\nvariant of BERT, for extractive summarization. Our system is the state of the\nart on the CNN/Dailymail dataset, outperforming the previous best-performed\nsystem by 1.65 on ROUGE-L. The codes to reproduce our results are available at\nhttps://github.com/nlpyang/BertSum</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Model Architecture"] },
{"key": "liu2019improving", "citations": "165", "year": "2019", "title":"Improving Multi-task Deep Neural Networks Via Knowledge Distillation For Natural Language Understanding", "abstract": "<p>This paper explores the use of knowledge distillation to improve a Multi-Task\nDeep Neural Network (MT-DNN) (Liu et al., 2019) for learning text\nrepresentations across multiple natural language understanding tasks. Although\nensemble learning can improve model performance, serving an ensemble of large\nDNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge\ndistillation method (Hinton et al., 2015) in the multi-task learning setting.\nFor each task, we train an ensemble of different MT-DNNs (teacher) that\noutperforms any single model, and then train a single MT-DNN (student) via\nmulti-task learning to <em>distill</em> knowledge from these ensemble teachers.\nWe show that the distilled MT-DNN significantly outperforms the original MT-DNN\non 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7%\n(1.5% absolute improvement\\footnote{ Based on the GLUE leaderboard at\nhttps://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and\npre-trained models will be made publicly available at\nhttps://github.com/namisan/mt-dnn.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Has Code"] },
{"key": "liu2019k", "citations": "676", "year": "2020", "title":"K-BERT: Enabling Language Representation With Knowledge Graph", "abstract": "<p>Pre-trained language representation models, such as BERT, capture a general\nlanguage representation from large-scale corpora, but lack domain-specific\nknowledge. When reading a domain text, experts make inferences with relevant\nknowledge. For machines to achieve this capability, we propose a\nknowledge-enabled language representation model (K-BERT) with knowledge graphs\n(KGs), in which triples are injected into the sentences as domain knowledge.\nHowever, too much knowledge incorporation may divert the sentence from its\ncorrect meaning, which is called knowledge noise (KN) issue. To overcome KN,\nK-BERT introduces soft-position and visible matrix to limit the impact of\nknowledge. K-BERT can easily inject domain knowledge into the models by\nequipped with a KG without pre-training by-self because it is capable of\nloading model parameters from the pre-trained BERT. Our investigation reveals\npromising results in twelve NLP tasks. Especially in domain-specific tasks\n(including finance, law, and medicine), K-BERT significantly outperforms BERT,\nwhich demonstrates that K-BERT is an excellent choice for solving the\nknowledge-driven problems that require experts.</p>\n", "tags": ["AAAI","Model Architecture","Training Techniques"] },
{"key": "liu2019knowledge", "citations": "99", "year": "2019", "title":"Knowledge Aware Conversation Generation With Explainable Reasoning Over Augmented Graphs", "abstract": "<p>Two types of knowledge, triples from knowledge graphs and texts from\ndocuments, have been studied for knowledge aware open-domain conversation\ngeneration, in which graph paths can narrow down vertex candidates for\nknowledge selection decision, and texts can provide rich information for\nresponse generation. Fusion of a knowledge graph and texts might yield mutually\nreinforcing advantages, but there is less study on that. To address this\nchallenge, we propose a knowledge aware chatting machine with three components,\nan augmented knowledge graph with both triples and texts, knowledge selector,\nand knowledge aware response generator. For knowledge selection on the graph,\nwe formulate it as a problem of multi-hop graph reasoning to effectively\ncapture conversation flow, which is more explainable and flexible in comparison\nwith previous work. To fully leverage long text information that differentiates\nour graph from others, we improve a state of the art reasoning algorithm with\nmachine reading comprehension technology. We demonstrate the effectiveness of\nour system on two datasets in comparison with state-of-the-art models.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "liu2019linguistic", "citations": "713", "year": "2019", "title":"Linguistic Knowledge And Transferability Of Contextual Representations", "abstract": "<p>Contextual word representations derived from large-scale neural language\nmodels are successful across a diverse set of NLP tasks, suggesting that they\nencode useful and transferable features of language. To shed light on the\nlinguistic knowledge they capture, we study the representations produced by\nseveral recent pretrained contextualizers (variants of ELMo, the OpenAI\ntransformer language model, and BERT) with a suite of seventeen diverse probing\ntasks. We find that linear models trained on top of frozen contextual\nrepresentations are competitive with state-of-the-art task-specific models in\nmany cases, but fail on tasks requiring fine-grained linguistic knowledge\n(e.g., conjunct identification). To investigate the transferability of\ncontextual word representations, we quantify differences in the transferability\nof individual layers within contextualizers, especially between recurrent\nneural networks (RNNs) and transformers. For instance, higher layers of RNNs\nare more task-specific, while transformer layers do not exhibit the same\nmonotonic trend. In addition, to better understand what makes contextual word\nrepresentations transferable, we compare language model pretraining with eleven\nsupervised pretraining tasks. For any given task, pretraining on a closely\nrelated task yields better performance than language model pretraining (which\nis better on average) when the pretraining dataset is fixed. However, language\nmodel pretraining on more data gives the best results.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "liu2019multi", "citations": "1049", "year": "2019", "title":"Multi-task Deep Neural Networks For Natural Language Understanding", "abstract": "<p>In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for\nlearning representations across multiple natural language understanding (NLU)\ntasks. MT-DNN not only leverages large amounts of cross-task data, but also\nbenefits from a regularization effect that leads to more general\nrepresentations in order to adapt to new tasks and domains. MT-DNN extends the\nmodel proposed in Liu et al. (2015) by incorporating a pre-trained\nbidirectional transformer language model, known as BERT (Devlin et al., 2018).\nMT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI,\nSciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7%\n(2.2% absolute improvement). We also demonstrate using the SNLI and SciTail\ndatasets that the representations learned by MT-DNN allow domain adaptation\nwith substantially fewer in-domain labels than the pre-trained BERT\nrepresentations. The code and pre-trained models are publicly available at\nhttps://github.com/namisan/mt-dnn.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture"] },
{"key": "liu2019neural", "citations": "152", "year": "2019", "title":"Neural Machine Reading Comprehension: Methods And Trends", "abstract": "<p>Machine reading comprehension (MRC), which requires a machine to answer\nquestions based on a given context, has attracted increasing attention with the\nincorporation of various deep-learning techniques over the past few years.\nAlthough research on MRC based on deep learning is flourishing, there remains a\nlack of a comprehensive survey summarizing existing approaches and recent\ntrends, which motivated the work presented in this article. Specifically, we\ngive a thorough review of this research field, covering different aspects\nincluding (1) typical MRC tasks: their definitions, differences, and\nrepresentative datasets; (2) the general architecture of neural MRC: the main\nmodules and prevalent approaches to each; and (3) new trends: some emerging\nareas in neural MRC as well as the corresponding challenges. Finally,\nconsidering what has been achieved so far, the survey also envisages what the\nfuture may hold by discussing the open issues left to be addressed.</p>\n", "tags": ["Datasets","Model Architecture","Survey Paper"] },
{"key": "liu2019roberta", "citations": "15934", "year": "2019", "title":"Roberta: A Robustly Optimized BERT Pretraining Approach", "abstract": "<p>Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "liu2019synchronous", "citations": "60", "year": "2020", "title":"Synchronous Speech Recognition And Speech-to-text Translation With Interactive Decoding", "abstract": "<p>Speech-to-text translation (ST), which translates source language speech into\ntarget language text, has attracted intensive attention in recent years.\nCompared to the traditional pipeline system, the end-to-end ST model has\npotential benefits of lower latency, smaller model size, and less error\npropagation. However, it is notoriously difficult to implement such a model\nwithout transcriptions as intermediate. Existing works generally apply\nmulti-task learning to improve translation quality by jointly training\nend-to-end ST along with automatic speech recognition (ASR). However, different\ntasks in this method cannot utilize information from each other, which limits\nthe improvement. Other works propose a two-stage model where the second model\ncan use the hidden state from the first one, but its cascade manner greatly\naffects the efficiency of training and inference process. In this paper, we\npropose a novel interactive attention mechanism which enables ASR and ST to\nperform synchronously and interactively in a single model. Specifically, the\ngeneration of transcriptions and translations not only relies on its previous\noutputs but also the outputs predicted in the other task. Experiments on TED\nspeech translation corpora have shown that our proposed model can outperform\nstrong baselines on the quality of speech translation and achieve better speech\nrecognition performances as well.</p>\n", "tags": ["AAAI","Model Architecture","Training Techniques"] },
{"key": "liu2019text", "citations": "1471", "year": "2019", "title":"Text Summarization With Pretrained Encoders", "abstract": "<p>Bidirectional Encoder Representations from Transformers (BERT) represents the\nlatest incarnation of pretrained language models which have recently advanced a\nwide range of natural language processing tasks. In this paper, we showcase how\nBERT can be usefully applied in text summarization and propose a general\nframework for both extractive and abstractive models. We introduce a novel\ndocument-level encoder based on BERT which is able to express the semantics of\na document and obtain representations for its sentences. Our extractive model\nis built on top of this encoder by stacking several inter-sentence Transformer\nlayers. For abstractive summarization, we propose a new fine-tuning schedule\nwhich adopts different optimizers for the encoder and the decoder as a means of\nalleviating the mismatch between the two (the former is pretrained while the\nlatter is not). We also demonstrate that a two-staged fine-tuning approach can\nfurther boost the quality of the generated summaries. Experiments on three\ndatasets show that our model achieves state-of-the-art results across the board\nin both extractive and abstractive settings. Our code is available at\nhttps://github.com/nlpyang/PreSumm</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "liu2019zero", "citations": "78", "year": "2019", "title":"Zero-shot Cross-lingual Dialogue Systems With Transferable Latent Variables", "abstract": "<p>Despite the surging demands for multilingual task-oriented dialog systems\n(e.g., Alexa, Google Home), there has been less research done in multilingual\nor cross-lingual scenarios. Hence, we propose a zero-shot adaptation of\ntask-oriented dialogue system to low-resource languages. To tackle this\nchallenge, we first use a set of very few parallel word pairs to refine the\naligned cross-lingual word-level representations. We then employ a latent\nvariable model to cope with the variance of similar sentences across different\nlanguages, which is induced by imperfect cross-lingual alignments and inherent\ndifferences in languages. Finally, the experimental results show that even\nthough we utilize much less external resources, our model achieves better\nadaptation performance for natural language understanding task (i.e., the\nintent detection and slot filling) compared to the current state-of-the-art\nmodel in the zero-shot scenario.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP"] },
{"key": "liu2020adversarial", "citations": "90", "year": "2020", "title":"Adversarial Training For Large Neural Language Models", "abstract": "<p>Generalization and robustness are both key desiderata for designing machine\nlearning methods. Adversarial training can enhance robustness, but past work\noften finds it hurts generalization. In natural language processing (NLP),\npre-training large neural language models such as BERT have demonstrated\nimpressive gain in generalization for a variety of tasks, with further\nimprovement from adversarial fine-tuning. However, these models are still\nvulnerable to adversarial attacks. In this paper, we show that adversarial\npre-training can improve both generalization and robustness. We propose a\ngeneral algorithm ALUM (Adversarial training for large neural LangUage Models),\nwhich regularizes the training objective by applying perturbations in the\nembedding space that maximizes the adversarial loss. We present the first\ncomprehensive study of adversarial training in all stages, including\npre-training from scratch, continual pre-training on a well-trained model, and\ntask-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide\nrange of NLP tasks, in both regular and adversarial scenarios. Even for models\nthat have been well trained on extremely large text corpora, such as RoBERTa,\nALUM can still produce significant gains from continual pre-training, whereas\nconventional non-adversarial methods can not. ALUM can be further combined with\ntask-specific fine-tuning to attain additional gains. The ALUM code is publicly\navailable at https://github.com/namisan/mt-dnn.</p>\n", "tags": ["Fine-Tuning","Has Code","Model Architecture","Security","Training Techniques"] },
{"key": "liu2020asking", "citations": "83", "year": "2020", "title":"Asking Questions The Human Way: Scalable Question-answer Generation From Text Corpus", "abstract": "<p>The ability to ask questions is important in both human and machine\nintelligence. Learning to ask questions helps knowledge acquisition, improves\nquestion-answering and machine reading comprehension tasks, and helps a chatbot\nto keep the conversation flowing with a human. Existing question generation\nmodels are ineffective at generating a large amount of high-quality\nquestion-answer pairs from unstructured text, since given an answer and an\ninput passage, question generation is inherently a one-to-many mapping. In this\npaper, we propose Answer-Clue-Style-aware Question Generation (ACS-QG), which\naims at automatically generating high-quality and diverse question-answer pairs\nfrom unlabeled text corpus at scale by imitating the way a human asks\nquestions. Our system consists of: i) an information extractor, which samples\nfrom the text multiple types of assistive information to guide question\ngeneration; ii) neural question generators, which generate diverse and\ncontrollable questions, leveraging the extracted assistive information; and\niii) a neural quality controller, which removes low-quality generated data\nbased on text entailment. We compare our question generation models with\nexisting approaches and resort to voluntary human evaluation to assess the\nquality of the generated question-answer pairs. The evaluation results suggest\nthat our system dramatically outperforms state-of-the-art neural question\ngeneration models in terms of the generation quality, while being scalable in\nthe meantime. With models trained on a relatively smaller amount of data, we\ncan generate 2.8 million quality-assured question-answer pairs from a million\nsentences found in Wikipedia.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "liu2020coach", "citations": "98", "year": "2020", "title":"Coach: A Coarse-to-fine Approach For Cross-domain Slot Filling", "abstract": "<p>As an essential task in task-oriented dialog systems, slot filling requires\nextensive training data in a certain domain. However, such data are not always\navailable. Hence, cross-domain slot filling has naturally arisen to cope with\nthis data scarcity problem. In this paper, we propose a Coarse-to-fine approach\n(Coach) for cross-domain slot filling. Our model first learns the general\npattern of slot entities by detecting whether the tokens are slot entities or\nnot. It then predicts the specific types for the slot entities. In addition, we\npropose a template regularization approach to improve the adaptation robustness\nby regularizing the representation of utterances based on utterance templates.\nExperimental results show that our model significantly outperforms\nstate-of-the-art approaches in slot filling. Furthermore, our model can also be\napplied to the cross-domain named entity recognition task, and it achieves\nbetter adaptation performance than other existing baselines. The code is\navailable at https://github.com/zliucr/coach.</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "liu2020fastbert", "citations": "237", "year": "2020", "title":"Fastbert: A Self-distilling BERT With Adaptive Inference Time", "abstract": "<p>Pre-trained language models like BERT have proven to be highly performant.\nHowever, they are often computationally expensive in many practical scenarios,\nfor such heavy models can hardly be readily implemented with limited resources.\nTo improve their efficiency with an assured model performance, we propose a\nnovel speed-tunable FastBERT with adaptive inference time. The speed at\ninference can be flexibly adjusted under varying demands, while redundant\ncalculation of samples is avoided. Moreover, this model adopts a unique\nself-distillation mechanism at fine-tuning, further enabling a greater\ncomputational efficacy with minimal loss in performance. Our model achieves\npromising results in twelve English and Chinese datasets. It is able to speed\nup by a wide range from 1 to 12 times than BERT if given different speedup\nthresholds to make a speed-performance tradeoff.</p>\n", "tags": ["Datasets","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "liu2020kg", "citations": "149", "year": "2021", "title":"KG-BART: Knowledge Graph-augmented BART For Generative Commonsense Reasoning", "abstract": "<p>Generative commonsense reasoning which aims to empower machines to generate\nsentences with the capacity of reasoning over a set of concepts is a critical\nbottleneck for text generation. Even the state-of-the-art pre-trained language\ngeneration models struggle at this task and often produce implausible and\nanomalous sentences. One reason is that they rarely consider incorporating the\nknowledge graph which can provide rich relational information among the\ncommonsense concepts. To promote the ability of commonsense reasoning for text\ngeneration, we propose a novel knowledge graph augmented pre-trained language\ngeneration model KG-BART, which encompasses the complex relations of concepts\nthrough the knowledge graph and produces more logical and natural sentences as\noutput. Moreover, KG-BART can leverage the graph attention to aggregate the\nrich concept semantics that enhances the model generalization on unseen concept\nsets. Experiments on benchmark CommonGen dataset verify the effectiveness of\nour proposed approach by comparing with several strong pre-trained language\ngeneration models, particularly KG-BART outperforms BART by 5.80, 4.60, in\nterms of BLEU-3, 4. Moreover, we also show that the generated context by our\nmodel can work as background scenarios to benefit downstream commonsense QA\ntasks.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture"] },
{"key": "liu2020logiqa", "citations": "91", "year": "2020", "title":"Logiqa: A Challenge Dataset For Machine Reading Comprehension With Logical Reasoning", "abstract": "<p>Machine reading is a fundamental task for testing the capability of natural\nlanguage understanding, which is closely related to human cognition in many\naspects. With the rising of deep learning techniques, algorithmic models rival\nhuman performances on simple QA, and thus increasingly challenging machine\nreading datasets have been proposed. Though various challenges such as evidence\nintegration and commonsense knowledge have been integrated, one of the\nfundamental capabilities in human reading, namely logical reasoning, is not\nfully investigated. We build a comprehensive dataset, named LogiQA, which is\nsourced from expert-written questions for testing human Logical reasoning. It\nconsists of 8,678 QA instances, covering multiple types of deductive reasoning.\nResults show that state-of-the-art neural models perform by far worse than\nhuman ceiling. Our dataset can also serve as a benchmark for reinvestigating\nlogical AI under the deep learning NLP setting. The dataset is freely available\nat https://github.com/lgw863/LogiQA-dataset</p>\n", "tags": ["Datasets","Evaluation","Has Code","IJCAI"] },
{"key": "liu2020multilingual", "citations": "759", "year": "2020", "title":"Multilingual Denoising Pre-training For Neural Machine Translation", "abstract": "<p>This paper demonstrates that multilingual denoising pre-training produces\nsignificant performance gains across a wide variety of machine translation (MT)\ntasks. We present mBART – a sequence-to-sequence denoising auto-encoder\npre-trained on large-scale monolingual corpora in many languages using the BART\nobjective. mBART is one of the first methods for pre-training a complete\nsequence-to-sequence model by denoising full texts in multiple languages, while\nprevious approaches have focused only on the encoder, decoder, or\nreconstructing parts of the text. Pre-training a complete model allows it to be\ndirectly fine tuned for supervised (both sentence-level and document-level) and\nunsupervised machine translation, with no task-specific modifications. We\ndemonstrate that adding mBART initialization produces performance gains in all\nbut the highest-resource settings, including up to 12 BLEU points for low\nresource MT and over 5 BLEU points for many document-level and unsupervised\nmodels. We also show it also enables new types of transfer to language pairs\nwith no bi-text or that were not in the pre-training corpus, and present\nextensive analysis of which factors contribute the most to effective\npre-training.</p>\n", "tags": ["Datasets","TACL","Training Techniques"] },
{"key": "liu2020norm", "citations": "99", "year": "2020", "title":"Norm-based Curriculum Learning For Neural Machine Translation", "abstract": "<p>A neural machine translation (NMT) system is expensive to train, especially\nwith high-resource settings. As the NMT architectures become deeper and wider,\nthis issue gets worse and worse. In this paper, we aim to improve the\nefficiency of training an NMT by introducing a novel norm-based curriculum\nlearning method. We use the norm (aka length or module) of a word embedding as\na measure of 1) the difficulty of the sentence, 2) the competence of the model,\nand 3) the weight of the sentence. The norm-based sentence difficulty takes the\nadvantages of both linguistically motivated and model-based sentence\ndifficulties. It is easy to determine and contains learning-dependent features.\nThe norm-based model competence makes NMT learn the curriculum in a fully\nautomated way, while the norm-based sentence weight further enhances the\nlearning of the vector representation of the NMT. Experimental results for the\nWMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate\nthat the proposed method outperforms strong baselines in terms of BLEU score\n(+1.17/+1.56) and training speedup (2.22x/3.33x).</p>\n", "tags": ["Training Techniques"] },
{"key": "liu2020pre", "citations": "62", "year": "2021", "title":"Pre-training Graph Transformer With Multimodal Side Information For Recommendation", "abstract": "<p>Side information of items, e.g., images and text description, has shown to be\neffective in contributing to accurate recommendations. Inspired by the recent\nsuccess of pre-training models on natural language and images, we propose a\npre-training strategy to learn item representations by considering both item\nside information and their relationships. We relate items by common user\nactivities, e.g., co-purchase, and construct a homogeneous item graph. This\ngraph provides a unified view of item relations and their associated side\ninformation in multimodality. We develop a novel sampling algorithm named\nMCNSampling to select contextual neighbors for each item. The proposed\nPre-trained Multimodal Graph Transformer (PMGT) learns item representations\nwith two objectives: 1) graph structure reconstruction, and 2) masked node\nfeature reconstruction. Experimental results on real datasets demonstrate that\nthe proposed PMGT model effectively exploits the multimodality side information\nto achieve better accuracies in downstream tasks including item recommendation,\nitem classification, and click-through ratio prediction. We also report a case\nstudy of testing the proposed PMGT model in an online setting with 600 thousand\nusers.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "liu2020survey", "citations": "112", "year": "2020", "title":"A Survey On Contextual Embeddings", "abstract": "<p>Contextual embeddings, such as ELMo and BERT, move beyond global word\nrepresentations like Word2Vec and achieve ground-breaking performance on a wide\nrange of natural language processing tasks. Contextual embeddings assign each\nword a representation based on its context, thereby capturing uses of words\nacross varied contexts and encoding knowledge that transfers across languages.\nIn this survey, we review existing contextual embedding models, cross-lingual\npolyglot pre-training, the application of contextual embeddings in downstream\ntasks, model compression, and model analyses.</p>\n", "tags": ["Model Architecture","Survey Paper","Training Techniques"] },
{"key": "liu2020towards", "citations": "143", "year": "2020", "title":"Towards Conversational Recommendation Over Multi-type Dialogs", "abstract": "<p>We propose a new task of conversational recommendation over multi-type\ndialogs, where the bots can proactively and naturally lead a conversation from\na non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into\naccount user’s interests and feedback. To facilitate the study of this task, we\ncreate a human-to-human Chinese dialog dataset <em>DuRecDial</em> (about 10k\ndialogs, 156k utterances), which contains multiple sequential dialogs for every\npair of a recommendation seeker (user) and a recommender (bot). In each dialog,\nthe recommender proactively leads a multi-type dialog to approach\nrecommendation targets and then makes multiple recommendations with rich\ninteraction behavior. This dataset allows us to systematically investigate\ndifferent parts of the overall problem, e.g., how to naturally lead a dialog,\nhow to interact with users for recommendation. Finally we establish baseline\nresults on DuRecDial for future studies. Dataset and codes are publicly\navailable at\nhttps://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2020-DuRecDial.</p>\n", "tags": ["Dialogue & Multi Turn","Has Code"] },
{"key": "liu2020understanding", "citations": "140", "year": "2020", "title":"Understanding And Improving Encoder Layer Fusion In Sequence-to-sequence Learning", "abstract": "<p>Encoder layer fusion (EncoderFusion) is a technique to fuse all the encoder\nlayers (instead of the uppermost layer) for sequence-to-sequence (Seq2Seq)\nmodels, which has proven effective on various NLP tasks. However, it is still\nnot entirely clear why and when EncoderFusion should work. In this paper, our\nmain contribution is to take a step further in understanding EncoderFusion.\nMany of previous studies believe that the success of EncoderFusion comes from\nexploiting surface and syntactic information embedded in lower encoder layers.\nUnlike them, we find that the encoder embedding layer is more important than\nother intermediate encoder layers. In addition, the uppermost decoder layer\nconsistently pays more attention to the encoder embedding layer across NLP\ntasks. Based on this observation, we propose a simple fusion method,\nSurfaceFusion, by fusing only the encoder embedding layer for the softmax\nlayer. Experimental results show that SurfaceFusion outperforms EncoderFusion\non several NLP benchmarks, including machine translation, text summarization,\nand grammatical error correction. It obtains the state-of-the-art performance\non WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive\nanalyses reveal that SurfaceFusion learns more expressive bilingual word\nembeddings by building a closer relationship between relevant source and target\nembedding. Source code is freely available at\nhttps://github.com/SunbowLiu/SurfaceFusion.</p>\n", "tags": ["EMNLP","Has Code","Model Architecture"] },
{"key": "liu2020very", "citations": "73", "year": "2020", "title":"Very Deep Transformers For Neural Machine Translation", "abstract": "<p>We explore the application of very deep Transformer models for Neural Machine\nTranslation (NMT). Using a simple yet effective initialization technique that\nstabilizes training, we show that it is feasible to build standard\nTransformer-based models with up to 60 encoder layers and 12 decoder layers.\nThese deep models outperform their baseline 6-layer counterparts by as much as\n2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14\nEnglish-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14\nEnglish-German (30.1 BLEU).The code and trained models will be publicly\navailable at: https://github.com/namisan/exdeep-nmt.</p>\n", "tags": ["Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "liu2020you", "citations": "129", "year": "2020", "title":"You Impress Me: Dialogue Generation Via Mutual Persona Perception", "abstract": "<p>Despite the continuing efforts to improve the engagingness and consistency of\nchit-chat dialogue systems, the majority of current work simply focus on\nmimicking human-like responses, leaving understudied the aspects of modeling\nunderstanding between interlocutors. The research in cognitive science,\ninstead, suggests that understanding is an essential signal for a high-quality\nchit-chat conversation. Motivated by this, we propose P^2 Bot, a\ntransmitter-receiver based framework with the aim of explicitly modeling\nunderstanding. Specifically, P^2 Bot incorporates mutual persona perception to\nenhance the quality of personalized dialogue generation. Experiments on a large\npublic dataset, Persona-Chat, demonstrate the effectiveness of our approach,\nwith a considerable boost over the state-of-the-art baselines across both\nautomatic metrics and human evaluations.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Tools"] },
{"key": "liu2021contrastive", "citations": "74", "year": "2021", "title":"Contrastive Self-supervised Sequential Recommendation With Robust Augmentation", "abstract": "<p>Sequential Recommendationdescribes a set of techniques to model dynamic user\nbehavior in order to predict future interactions in sequential user data. At\ntheir core, such approaches model transition probabilities between items in a\nsequence, whether through Markov chains, recurrent networks, or more recently,\nTransformers. However both old and new issues remain, including data-sparsity\nand noisy data; such issues can impair the performance, especially in complex,\nparameter-hungry models. In this paper, we investigate the application of\ncontrastive Self-Supervised Learning (SSL) to the sequential recommendation, as\na way to alleviate some of these issues. Contrastive SSL constructs\naugmentations from unlabelled instances, where agreements among positive pairs\nare maximized. It is challenging to devise a contrastive SSL framework for a\nsequential recommendation, due to its discrete nature, correlations among\nitems, and skewness of length distributions. To this end, we propose a novel\nframework, Contrastive Self-supervised Learning for sequential Recommendation\n(CoSeRec). We introduce two informative augmentation operators leveraging item\ncorrelations to create high-quality views for contrastive learning.\nExperimental results on three real-world datasets demonstrate the effectiveness\nof the proposed method on improving model performance and the robustness\nagainst sparse and noisy data. Our implementation is available online at\nhttps://github.com/YChen1993/CoSeRec</p>\n", "tags": ["Datasets","Has Code","Tools","Training Techniques"] },
{"key": "liu2021cptr", "citations": "105", "year": "2021", "title":"CPTR: Full Transformer Network For Image Captioning", "abstract": "<p>In this paper, we consider the image captioning task from a new\nsequence-to-sequence prediction perspective and propose CaPtion TransformeR\n(CPTR) which takes the sequentialized raw images as the input to Transformer.\nCompared to the “CNN+Transformer” design paradigm, our model can model global\ncontext at every encoder layer from the beginning and is totally\nconvolution-free. Extensive experiments demonstrate the effectiveness of the\nproposed model and we surpass the conventional “CNN+Transformer” methods on the\nMSCOCO dataset. Besides, we provide detailed visualizations of the\nself-attention between patches in the encoder and the “words-to-patches”\nattention in the decoder thanks to the full Transformer architecture.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "liu2021design", "citations": "309", "year": "2022", "title":"Design Guidelines For Prompt Engineering Text-to-image Generative Models", "abstract": "<p>Text-to-image generative models are a new and powerful way to generate visual\nartwork. However, the open-ended nature of text as interaction is double-edged;\nwhile users can input anything and have access to an infinite range of\ngenerations, they also must engage in brute-force trial and error with the text\nprompt when the result quality is poor. We conduct a study exploring what\nprompt keywords and model hyperparameters can help produce coherent outputs. In\nparticular, we study prompts structured to include subject and style keywords\nand investigate success and failure modes of these prompts. Our evaluation of\n5493 generations over the course of five experiments spans 51 abstract and\nconcrete subjects as well as 51 abstract and figurative styles. From this\nevaluation, we present design guidelines that can help people produce better\noutcomes from text-to-image generative models.</p>\n", "tags": ["Evaluation","Prompting"] },
{"key": "liu2021dexperts", "citations": "118", "year": "2021", "title":"Dexperts: Decoding-time Controlled Text Generation With Experts And Anti-experts", "abstract": "<p>Despite recent advances in natural language generation, it remains\nchallenging to control attributes of generated text. We propose DExperts:\nDecoding-time Experts, a decoding-time method for controlled text generation\nthat combines a pretrained language model with “expert” LMs and/or\n“anti-expert” LMs in a product of experts. Intuitively, under the ensemble,\ntokens only get high probability if they are considered likely by the experts,\nand unlikely by the anti-experts. We apply DExperts to language detoxification\nand sentiment-controlled generation, where we outperform existing controllable\ngeneration methods on both automatic and human evaluations. Moreover, because\nDExperts operates only on the output of the pretrained LM, it is effective with\n(anti-)experts of smaller size, including when operating on GPT-3. Our work\nhighlights the promise of tuning small LMs on text with (un)desirable\nattributes for efficient decoding-time steering.</p>\n", "tags": ["Model Architecture"] },
{"key": "liu2021generated", "citations": "114", "year": "2022", "title":"Generated Knowledge Prompting For Commonsense Reasoning", "abstract": "<p>It remains an open question whether incorporating external knowledge benefits\ncommonsense reasoning while maintaining the flexibility of pretrained sequence\nmodels. To investigate this question, we develop generated knowledge prompting,\nwhich consists of generating knowledge from a language model, then providing\nthe knowledge as additional input when answering a question. Our method does\nnot require task-specific supervision for knowledge integration, or access to a\nstructured knowledge base, yet it improves performance of large-scale,\nstate-of-the-art models on four commonsense reasoning tasks, achieving\nstate-of-the-art results on numerical commonsense (NumerSense), general\ncommonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks.\nGenerated knowledge prompting highlights large-scale language models as\nflexible sources of external knowledge for improving commonsense reasoning. Our\ncode is available at https://github.com/liujch1998/GKP</p>\n", "tags": ["Has Code","Prompting"] },
{"key": "liu2021image", "citations": "108", "year": "2021", "title":"Image Retrieval On Real-life Images With Pre-trained Vision-and-language Models", "abstract": "<p>We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&amp;L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.</p>\n", "tags": ["Datasets","ICCV","Model Architecture"] },
{"key": "liu2021more", "citations": "115", "year": "2023", "title":"More Control For Free! Image Synthesis With Semantic Diffusion Guidance", "abstract": "<p>Controllable image synthesis models allow creation of diverse images based on\ntext instructions or guidance from a reference image. Recently, denoising\ndiffusion probabilistic models have been shown to generate more realistic\nimagery than prior methods, and have been successfully demonstrated in\nunconditional and class-conditional settings. We investigate fine-grained,\ncontinuous control of this model class, and introduce a novel unified framework\nfor semantic diffusion guidance, which allows either language or image\nguidance, or both. Guidance is injected into a pretrained unconditional\ndiffusion model using the gradient of image-text or image matching scores,\nwithout re-training the diffusion model. We explore CLIP-based language\nguidance as well as both content and style-based image guidance in a unified\nframework. Our text-guided synthesis approach can be applied to datasets\nwithout associated text annotations. We conduct experiments on FFHQ and LSUN\ndatasets, and show results on fine-grained text-guided image synthesis,\nsynthesis of images related to a style or content reference image, and examples\nwith both textual and image guidance.</p>\n", "tags": ["Applications","Datasets","Tools","Training Techniques"] },
{"key": "liu2021p", "citations": "251", "year": "2021", "title":"P-tuning V2: Prompt Tuning Can Be Comparable To Fine-tuning Universally Across Scales And Tasks", "abstract": "<p>Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work reveals that prompt tuning does not\nperform well for normal-sized pretrained models. We also find that existing\nmethods of prompt tuning cannot handle hard sequence labeling tasks, indicating\na lack of universality. We present a novel empirical finding that properly\noptimized prompt tuning can be universally effective across a wide range of\nmodel scales and NLU tasks. It matches the performance of finetuning while\nhaving only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an\nimplementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning}\noptimized and adapted for NLU. Given the universality and simplicity of\nP-Tuning v2, we believe it can serve as an alternative to finetuning and a\nstrong baseline for future research.Our code and data are released at\nhttps://github.com/THUDM/P-tuning-v2.</p>\n", "tags": ["Fine-Tuning","Has Code","Prompting","Training Techniques"] },
{"key": "liu2021simcls", "citations": "189", "year": "2021", "title":"Simcls: A Simple Framework For Contrastive Learning Of Abstractive Summarization", "abstract": "<p>In this paper, we present a conceptually simple while empirically powerful\nframework for abstractive summarization, SimCLS, which can bridge the gap\nbetween the learning objective and evaluation metrics resulting from the\ncurrently dominated sequence-to-sequence learning framework by formulating text\ngeneration as a reference-free evaluation problem (i.e., quality estimation)\nassisted by contrastive learning. Experimental results show that, with minor\nmodification over existing top-scoring systems, SimCLS can improve the\nperformance of existing top-performing models by a large margin. Particularly,\n2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on\nthe CNN/DailyMail dataset, driving the state-of-the-art performance to a new\nlevel. We have open-sourced our codes and results:\nhttps://github.com/yixinL7/SimCLS. Results of our proposed models have been\ndeployed into ExplainaBoard platform, which allows researchers to understand\nour systems in a more fine-grained way.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Tools"] },
{"key": "liu2021survey", "citations": "273", "year": "2023", "title":"A Survey Of Visual Transformers", "abstract": "<p>Transformer, an attention-based encoder-decoder model, has already\nrevolutionized the field of natural language processing (NLP). Inspired by such\nsignificant achievements, some pioneering works have recently been done on\nemploying Transformer-liked architectures in the computer vision (CV) field,\nwhich have demonstrated their effectiveness on three fundamental CV tasks\n(classification, detection, and segmentation) as well as multiple sensory data\nstream (images, point clouds, and vision-language data). Because of their\ncompetitive modeling capabilities, the visual Transformers have achieved\nimpressive performance improvements over multiple benchmarks as compared with\nmodern Convolution Neural Networks (CNNs). In this survey, we have reviewed\nover one hundred of different visual Transformers comprehensively according to\nthree fundamental CV tasks and different data stream types, where a taxonomy is\nproposed to organize the representative methods according to their motivations,\nstructures, and application scenarios. Because of their differences on training\nsettings and dedicated vision tasks, we have also evaluated and compared all\nthese existing visual Transformers under different configurations. Furthermore,\nwe have revealed a series of essential but unexploited aspects that may empower\nsuch visual Transformers to stand out from numerous architectures, e.g., slack\nhigh-level semantic embeddings to bridge the gap between the visual\nTransformers and the sequential ones. Finally, three promising research\ndirections are suggested for future investment. We will continue to update the\nlatest articles and their released source codes at\nhttps://github.com/liuyang-ict/awesome-visual-transformers.</p>\n", "tags": ["Model Architecture","Survey Paper","Training Techniques"] },
{"key": "liu2021tapex", "citations": "82", "year": "2021", "title":"TAPEX: Table Pre-training Via Learning A Neural SQL Executor", "abstract": "<p>Recent progress in language model pre-training has achieved a great success\nvia leveraging large-scale unstructured textual data. However, it is still a\nchallenge to apply pre-training on structured tabular data due to the absence\nof large-scale high-quality tabular data. In this paper, we propose TAPEX to\nshow that table pre-training can be achieved by learning a neural SQL executor\nover a synthetic corpus, which is obtained by automatically synthesizing\nexecutable SQL queries and their execution outputs. TAPEX addresses the data\nscarcity challenge via guiding the language model to mimic a SQL executor on\nthe diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX\non four benchmark datasets. Experimental results demonstrate that TAPEX\noutperforms previous table pre-training approaches by a large margin and\nachieves new state-of-the-art results on all of them. This includes the\nimprovements on the weakly-supervised WikiSQL denotation accuracy to 89.5%\n(+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA\ndenotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2%\n(+3.2%). To our knowledge, this is the first work to exploit table pre-training\nvia synthetic executable programs and to achieve new state-of-the-art results\non various downstream tasks. Our code can be found at\nhttps://github.com/microsoft/Table-Pretraining.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Training Techniques"] },
{"key": "liu2021visually", "citations": "71", "year": "2021", "title":"Visually Grounded Reasoning Across Languages And Cultures", "abstract": "<p>The design of widespread vision-and-language datasets and pre-trained\nencoders directly adopts, or draws inspiration from, the concepts and images of\nImageNet. While one can hardly overestimate how much this benchmark contributed\nto progress in computer vision, it is mostly derived from lexical databases and\nimage queries in English, resulting in source material with a North American or\nWestern European bias. Therefore, we devise a new protocol to construct an\nImageNet-style hierarchy representative of more languages and cultures. In\nparticular, we let the selection of both concepts and images be entirely driven\nby native speakers, rather than scraping them automatically. Specifically, we\nfocus on a typologically diverse set of languages, namely, Indonesian, Mandarin\nChinese, Swahili, Tamil, and Turkish. On top of the concepts and images\nobtained through this new protocol, we create a multilingual dataset for\n{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting\nstatements from native speaker annotators about pairs of images. The task\nconsists of discriminating whether each grounded statement is true or false. We\nestablish a series of baselines using state-of-the-art models and find that\ntheir cross-lingual transfer performance lags dramatically behind supervised\nperformance in English. These results invite us to reassess the robustness and\naccuracy of current state-of-the-art models beyond a narrow domain, but also\nopen up new exciting challenges for the development of truly multilingual and\nmulticultural systems.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "liu2021what", "citations": "331", "year": "2022", "title":"What Makes Good In-context Examples For GPT-\\(3\\)?", "abstract": "<p>GPT-\\(3\\) has attracted lots of attention due to its superior performance\nacross a wide range of NLP tasks, especially with its powerful and versatile\nin-context few-shot learning ability. Despite its success, we found that the\nempirical results of GPT-\\(3\\) depend heavily on the choice of in-context\nexamples. In this work, we investigate whether there are more effective\nstrategies for judiciously selecting in-context examples (relative to random\nsampling) that better leverage GPT-\\(3\\)’s few-shot capabilities. Inspired by the\nrecent success of leveraging a retrieval module to augment large-scale neural\nnetwork models, we propose to retrieve examples that are semantically-similar\nto a test sample to formulate its corresponding prompt. Intuitively, the\nin-context examples selected with such a strategy may serve as more informative\ninputs to unleash GPT-\\(3\\)’s extensive knowledge. We evaluate the proposed\napproach on several natural language understanding and generation benchmarks,\nwhere the retrieval-based prompt selection approach consistently outperforms\nthe random baseline. Moreover, it is observed that the sentence encoders\nfine-tuned on task-related datasets yield even more helpful retrieval results.\nNotably, significant gains are observed on tasks such as table-to-text\ngeneration (41.9% on the ToTTo dataset) and open-domain question answering\n(45.5% on the NQ dataset). We hope our investigation could help understand the\nbehaviors of GPT-\\(3\\) and large-scale pre-trained LMs in general and enhance\ntheir few-shot capabilities.</p>\n", "tags": ["Datasets","Few-Shot","In Context Learning","Model Architecture","Prompting"] },
{"key": "liu20223dall", "citations": "89", "year": "2023", "title":"3DALL-E: Integrating Text-to-image AI In 3D Design Workflows", "abstract": "<p>Text-to-image AI are capable of generating novel images for inspiration, but\ntheir applications for 3D design workflows and how designers can build 3D\nmodels using AI-provided inspiration have not yet been explored. To investigate\nthis, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a\nplugin that generates 2D image inspiration for 3D design. 3DALL-E allows users\nto construct text and image prompts based on what they are modeling. In a study\nwith 13 designers, we found that designers saw great potential in 3DALL-E\nwithin their workflows and could use text-to-image AI to produce reference\nimages, prevent design fixation, and inspire design considerations. We\nelaborate on prompting patterns observed across 3D modeling tasks and provide\nmeasures of prompt complexity observed across participants. From our findings,\nwe discuss how 3DALL-E can merge with existing generative design workflows and\npropose prompt bibliographies as a form of human-AI design history.</p>\n", "tags": ["Applications","Model Architecture","Prompting","Tools"] },
{"key": "liu2022competence", "citations": "83", "year": "2021", "title":"Competence-based Multimodal Curriculum Learning For Medical Report Generation", "abstract": "<p>Medical report generation task, which targets to produce long and coherent\ndescriptions of medical images, has attracted growing research interests\nrecently. Different from the general image captioning tasks, medical report\ngeneration is more challenging for data-driven neural models. This is mainly\ndue to 1) the serious data bias and 2) the limited medical data. To alleviate\nthe data bias and make best use of available data, we propose a\nCompetence-based Multimodal Curriculum Learning framework (CMCL). Specifically,\nCMCL simulates the learning process of radiologists and optimizes the model in\na step by step manner. Firstly, CMCL estimates the difficulty of each training\ninstance and evaluates the competence of current model; Secondly, CMCL selects\nthe most suitable batch of training instances considering current model\ncompetence. By iterating above two steps, CMCL can gradually improve the\nmodel’s performance. The experiments on the public IU-Xray and MIMIC-CXR\ndatasets show that CMCL can be incorporated into existing models to improve\ntheir performance.</p>\n", "tags": ["Datasets","Ethics & Fairness","Tools","Training Techniques"] },
{"key": "liu2022compositional", "citations": "169", "year": "2022", "title":"Compositional Visual Generation With Composable Diffusion Models", "abstract": "<p>Large text-guided diffusion models, such as DALLE-2, are able to generate\nstunning photorealistic images given natural language descriptions. While such\nmodels are highly flexible, they struggle to understand the composition of\ncertain concepts, such as confusing the attributes of different objects or\nrelations between objects. In this paper, we propose an alternative structured\napproach for compositional generation using diffusion models. An image is\ngenerated by composing a set of diffusion models, with each of them modeling a\ncertain component of the image. To do this, we interpret diffusion models as\nenergy-based models in which the data distributions defined by the energy\nfunctions may be explicitly combined. The proposed method can generate scenes\nat test time that are substantially more complex than those seen in training,\ncomposing sentence descriptions, object relations, human facial attributes, and\neven generalizing to new combinations that are rarely seen in the real world.\nWe further illustrate how our approach may be used to compose pre-trained\ntext-guided diffusion models and generate photorealistic images containing all\nthe details described in the input descriptions, including the binding of\ncertain object attributes that have been shown difficult for DALLE-2. These\nresults point to the effectiveness of the proposed method in promoting\nstructured generalization for visual generation. Project page:\nhttps://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "liu2022cross", "citations": "272", "year": "2023", "title":"Cross-modal Causal Relational Reasoning For Event-level Visual Question Answering", "abstract": "<p>Existing visual question answering methods often suffer from cross-modal\nspurious correlations and oversimplified event-level reasoning processes that\nfail to capture event temporality, causality, and dynamics spanning over the\nvideo. In this work, to address the task of event-level visual question\nanswering, we propose a framework for cross-modal causal relational reasoning.\nIn particular, a set of causal intervention operations is introduced to\ndiscover the underlying causal structures across visual and linguistic\nmodalities. Our framework, named Cross-Modal Causal RelatIonal Reasoning\n(CMCIR), involves three modules: i) Causality-aware Visual-Linguistic Reasoning\n(CVLR) module for collaboratively disentangling the visual and linguistic\nspurious correlations via front-door and back-door causal interventions; ii)\nSpatial-Temporal Transformer (STT) module for capturing the fine-grained\ninteractions between visual and linguistic semantics; iii) Visual-Linguistic\nFeature Fusion (VLFF) module for learning the global semantic-aware\nvisual-linguistic representations adaptively. Extensive experiments on four\nevent-level datasets demonstrate the superiority of our CMCIR in discovering\nvisual-linguistic causal structures and achieving robust event-level visual\nquestion answering. The datasets, code, and models are available at\nhttps://github.com/HCPLab-SYSU/CMCIR.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "liu2022few", "citations": "216", "year": "2022", "title":"Few-shot Parameter-efficient Fine-tuning Is Better And Cheaper Than In-context Learning", "abstract": "<p>Few-shot in-context learning (ICL) enables pre-trained language models to\nperform a previously-unseen task without any gradient-based training by feeding\na small number of training examples as part of the input. ICL incurs\nsubstantial computational, memory, and storage costs because it involves\nprocessing all of the training examples every time a prediction is made.\nParameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning,\nsparse update methods, etc.) offers an alternative paradigm where a small set\nof parameters are trained to enable a model to perform the new task. In this\npaper, we rigorously compare few-shot ICL and PEFT and demonstrate that the\nlatter offers better accuracy as well as dramatically lower computational\ncosts. Along the way, we introduce a new PEFT method called (IA)\\(^3\\) that\nscales activations by learned vectors, attaining stronger performance while\nonly introducing a relatively tiny amount of new parameters. We also propose a\nsimple recipe based on the T0 model called T-Few that can be applied to new\ntasks without task-specific tuning or modifications. We validate the\neffectiveness of T-Few on completely unseen tasks by applying it to the RAFT\nbenchmark, attaining super-human performance for the first time and\noutperforming the state-of-the-art by 6% absolute. All of the code used in our\nexperiments is publicly available.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Prompting","Training Techniques"] },
{"key": "liu2022fill", "citations": "62", "year": "2023", "title":"Fill In The Blank: Context-aware Automated Text Input Generation For Mobile GUI Testing", "abstract": "<p>Automated GUI testing is widely used to help ensure the quality of mobile\napps. However, many GUIs require appropriate text inputs to proceed to the next\npage which remains a prominent obstacle for testing coverage. Considering the\ndiversity and semantic requirement of valid inputs (e.g., flight departure,\nmovie name), it is challenging to automate the text input generation. Inspired\nby the fact that the pre-trained Large Language Model (LLM) has made\noutstanding progress in text generation, we propose an approach named QTypist\nbased on LLM for intelligently generating semantic input text according to the\nGUI context. To boost the performance of LLM in the mobile testing scenario, we\ndevelop a prompt-based data construction and tuning method which automatically\nextracts the prompts and answers for model tuning. We evaluate QTypist on 106\napps from Google Play and the result shows that the passing rate of QTypist is\n87%, which is 93% higher than the best baseline. We also integrate QTypist with\nthe automated GUI testing tools and it can cover 42% more app activities, 52%\nmore pages, and subsequently help reveal 122% more bugs compared with the raw\ntool.</p>\n", "tags": ["Llm For Code","Prompting","Tools"] },
{"key": "liu2022opal", "citations": "71", "year": "2022", "title":"Opal: Multimodal Image Generation For News Illustration", "abstract": "<p>Advances in multimodal AI have presented people with powerful ways to create\nimages from text. Recent work has shown that text-to-image generations are able\nto represent a broad range of subjects and artistic styles. However, finding\nthe right visual language for text prompts is difficult. In this paper, we\naddress this challenge with Opal, a system that produces text-to-image\ngenerations for news illustration. Given an article, Opal guides users through\na structured search for visual concepts and provides a pipeline allowing users\nto generate illustrations based on an article’s tone, keywords, and related\nartistic styles. Our evaluation shows that Opal efficiently generates diverse\nsets of news illustrations, visual assets, and concept ideas. Users with Opal\ngenerated two times more usable results than users without. We discuss how\nstructured exploration can help users better understand the capabilities of\nhuman AI co-creative systems.</p>\n", "tags": ["Evaluation"] },
{"key": "liu2022wanli", "citations": "99", "year": "2022", "title":"WANLI: Worker And AI Collaboration For Natural Language Inference Dataset Creation", "abstract": "<p>A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel approach for dataset\ncreation based on worker and AI collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI for natural language inference\n(NLI), our approach uses dataset cartography to automatically identify examples\nthat demonstrate challenging reasoning patterns, and instructs GPT-3 to compose\nnew examples with similar patterns. Machine generated examples are then\nautomatically filtered, and finally revised and labeled by human crowdworkers.\nThe resulting dataset, WANLI, consists of 107,885 NLI examples and presents\nunique empirical strengths over existing NLI datasets. Remarkably, training a\nmodel on WANLI improves performance on eight out-of-domain test sets we\nconsider, including by 11% on HANS and 9% on Adversarial NLI, compared to\ntraining on the 4x larger MultiNLI. Moreover, it continues to be more effective\nthan MultiNLI augmented with other NLI datasets. Our results demonstrate the\npromise of leveraging natural language generation techniques and re-imagining\nthe role of humans in the dataset creation process.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "liu2023evaluating", "citations": "82", "year": "2023", "title":"Evaluating Large Language Models On Graphs: Performance Insights And Comparative Analysis", "abstract": "<p>Large Language Models (LLMs) have garnered considerable interest within both\nacademic and industrial. Yet, the application of LLMs to graph data remains\nunder-explored. In this study, we evaluate the capabilities of four LLMs in\naddressing several analytical problems with graph data. We employ four distinct\nevaluation metrics: Comprehension, Correctness, Fidelity, and Rectification.\nOur results show that: 1) LLMs effectively comprehend graph data in natural\nlanguage and reason with graph topology. 2) GPT models can generate logical and\ncoherent results, outperforming alternatives in correctness. 3) All examined\nLLMs face challenges in structural reasoning, with techniques like zero-shot\nchain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT\nmodels often produce erroneous answers in multi-answer tasks, raising concerns\nin fidelity. 5) GPT models exhibit elevated confidence in their outputs,\npotentially hindering their rectification capacities. Notably, GPT-4 has\ndemonstrated the capacity to rectify responses from GPT-3.5-turbo and its own\nprevious iterations. The code is available at:\nhttps://github.com/Ayame1006/LLMtoGraph.</p>\n", "tags": ["Evaluation","Few-Shot","In Context Learning","Model Architecture","Prompting"] },
{"key": "liu2023g", "citations": "240", "year": "2023", "title":"G-eval: NLG Evaluation Using GPT-4 With Better Human Alignment", "abstract": "<p>The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval</p>\n", "tags": ["EMNLP","Ethics & Fairness","Evaluation","Has Code","Model Architecture","Prompting","Tools"] },
{"key": "liu2023graphprompt", "citations": "83", "year": "2023", "title":"Graphprompt: Unifying Pre-training And Downstream Tasks For Graph Neural Networks", "abstract": "<p>Graphs can model complex relationships between objects, enabling a myriad of\nWeb applications such as online page/article classification and social\nrecommendation. While graph neural networks(GNNs) have emerged as a powerful\ntool for graph representation learning, in an end-to-end supervised setting,\ntheir performance heavily rely on a large amount of task-specific supervision.\nTo reduce labeling requirement, the “pre-train, fine-tune” and “pre-train,\nprompt” paradigms have become increasingly common. In particular, prompting is\na popular alternative to fine-tuning in natural language processing, which is\ndesigned to narrow the gap between pre-training and downstream objectives in a\ntask-specific manner. However, existing study of prompting on graphs is still\nlimited, lacking a universal treatment to appeal to different downstream tasks.\nIn this paper, we propose GraphPrompt, a novel pre-training and prompting\nframework on graphs. GraphPrompt not only unifies pre-training and downstream\ntasks into a common task template, but also employs a learnable prompt to\nassist a downstream task in locating the most relevant knowledge from the\npre-train model in a task-specific manner. Finally, we conduct extensive\nexperiments on five public datasets to evaluate and analyze GraphPrompt.</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Prompting","Tools","Training Techniques"] },
{"key": "liu2023jailbreaking", "citations": "71", "year": "2023", "title":"Jailbreaking Chatgpt Via Prompt Engineering: An Empirical Study", "abstract": "<p>Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.</p>\n", "tags": ["Datasets","Prompting"] },
{"key": "liu2023llm", "citations": "70", "year": "2023", "title":"Llm-powered Hierarchical Language Agent For Real-time Human-ai Coordination", "abstract": "<p>AI agents powered by Large Language Models (LLMs) have made significant\nadvances, enabling them to assist humans in diverse complex tasks and leading\nto a revolution in human-AI coordination. LLM-powered agents typically require\ninvoking LLM APIs and employing artificially designed complex prompts, which\nresults in high inference latency. While this paradigm works well in scenarios\nwith minimal interactive demands, such as code generation, it is unsuitable for\nhighly interactive and real-time applications, such as gaming. Traditional\ngaming AI often employs small models or reactive policies, enabling fast\ninference but offering limited task completion and interaction abilities. In\nthis work, we consider Overcooked as our testbed where players could\ncommunicate with natural language and cooperate to serve orders. We propose a\nHierarchical Language Agent (HLA) for human-AI coordination that provides both\nstrong reasoning abilities while keeping real-time execution. In particular,\nHLA adopts a hierarchical framework and comprises three modules: a proficient\nLLM, referred to as Slow Mind, for intention reasoning and language\ninteraction, a lightweight LLM, referred to as Fast Mind, for generating macro\nactions, and a reactive policy, referred to as Executor, for transforming macro\nactions into atomic actions. Human studies show that HLA outperforms other\nbaseline agents, including slow-mind-only agents and fast-mind-only agents,\nwith stronger cooperation abilities, faster responses, and more consistent\nlanguage communications.</p>\n", "tags": ["Agentic","Applications","Llm For Code","Tools"] },
{"key": "liu2023lost", "citations": "279", "year": "2024", "title":"Lost In The Middle: How Language Models Use Long Contexts", "abstract": "<p>While recent language models have the ability to take long contexts as input,\nrelatively little is known about how well they use longer context. We analyze\nthe performance of language models on two tasks that require identifying\nrelevant information in their input contexts: multi-document question answering\nand key-value retrieval. We find that performance can degrade significantly\nwhen changing the position of relevant information, indicating that current\nlanguage models do not robustly make use of information in long input contexts.\nIn particular, we observe that performance is often highest when relevant\ninformation occurs at the beginning or end of the input context, and\nsignificantly degrades when models must access relevant information in the\nmiddle of long contexts, even for explicitly long-context models. Our analysis\nprovides a better understanding of how language models use their input context\nand provides new evaluation protocols for future long-context language models.</p>\n", "tags": ["Evaluation","Memory & Context","TACL"] },
{"key": "liu2023mmbench", "citations": "76", "year": "2024", "title":"Mmbench: Is Your Multi-modal Model An All-around Player?", "abstract": "<p>Large vision-language models (VLMs) have recently achieved remarkable\nprogress, exhibiting impressive multimodal perception and reasoning abilities.\nHowever, effectively evaluating these large VLMs remains a major challenge,\nhindering future development in this domain. Traditional benchmarks like VQAv2\nor COCO Caption provide quantitative performance measurements but lack\nfine-grained ability assessment and robust evaluation metrics. Meanwhile,\nsubjective benchmarks, such as OwlEval, offer comprehensive evaluations of a\nmodel’s abilities by incorporating human labor, which is not scalable and may\ndisplay significant bias. In response to these challenges, we propose MMBench,\na bilingual benchmark for assessing the multi-modal capabilities of VLMs.\nMMBench methodically develops a comprehensive evaluation pipeline, primarily\ncomprised of the following key features: 1. MMBench is meticulously curated\nwith well-designed quality control schemes, surpassing existing similar\nbenchmarks in terms of the number and variety of evaluation questions and\nabilities; 2. MMBench introduces a rigorous CircularEval strategy and\nincorporates large language models to convert free-form predictions into\npre-defined choices, which helps to yield accurate evaluation results for\nmodels with limited instruction-following capabilities. 3. MMBench incorporates\nmultiple-choice questions in both English and Chinese versions, enabling an\napples-to-apples comparison of VLMs’ performance under a bilingual context. To\nsummarize, MMBench is a systematically designed objective benchmark for a\nrobust and holistic evaluation of vision-language models. We hope MMBench will\nassist the research community in better evaluating their models and facilitate\nfuture progress in this area. The evalutation code of MMBench has been\nintegrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.</p>\n", "tags": ["Datasets","Evaluation","Has Code"] },
{"key": "liu2023summary", "citations": "468", "year": "2023", "title":"Summary Of Chatgpt-related Research And Perspective Towards The Future Of Large Language Models", "abstract": "<p>This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and\nGPT-4) research, state-of-the-art large language models (LLM) from the GPT\nseries, and their prospective applications across diverse domains. Indeed, key\ninnovations such as large-scale pre-training that captures knowledge across the\nentire world wide web, instruction fine-tuning and Reinforcement Learning from\nHuman Feedback (RLHF) have played significant roles in enhancing LLMs’\nadaptability and performance. We performed an in-depth analysis of 194 relevant\npapers on arXiv, encompassing trend analysis, word cloud representation, and\ndistribution analysis across various application domains. The findings reveal a\nsignificant and increasing interest in ChatGPT-related research, predominantly\ncentered on direct natural language processing applications, while also\ndemonstrating considerable potential in areas ranging from education and\nhistory to mathematics, medicine, and physics. This study endeavors to furnish\ninsights into ChatGPT’s capabilities, potential implications, ethical concerns,\nand offer direction for future advancements in this field.</p>\n", "tags": ["Agentic","Applications","Fine-Tuning","Model Architecture","Reinforcement Learning","Survey Paper","Training Techniques"] },
{"key": "liu2023visual", "citations": "537", "year": "2023", "title":"Visual Storytelling With Question-answer Plans", "abstract": "<p>Visual storytelling aims to generate compelling narratives from image\nsequences. Existing models often focus on enhancing the representation of the\nimage sequence, e.g., with external knowledge sources or advanced graph\nstructures. Despite recent progress, the stories are often repetitive,\nillogical, and lacking in detail. To mitigate these issues, we present a novel\nframework which integrates visual representations with pretrained language\nmodels and planning. Our model translates the image sequence into a visual\nprefix, a sequence of continuous embeddings which language models can\ninterpret. It also leverages a sequence of question-answer pairs as a blueprint\nplan for selecting salient visual concepts and determining how they should be\nassembled into a narrative. Automatic and human evaluation on the VIST\nbenchmark (Huang et al., 2016) demonstrates that blueprint-based models\ngenerate stories that are more coherent, interesting, and natural compared to\ncompetitive baselines and state-of-the-art systems.</p>\n", "tags": ["Datasets","Evaluation","Tools"] },
{"key": "liu2023what", "citations": "71", "year": "2023", "title":"What Makes Good Data For Alignment? A Comprehensive Study Of Automatic Data Selection In Instruction Tuning", "abstract": "<p>Instruction tuning is a standard technique employed to align large language\nmodels to end tasks and user preferences after the initial pretraining phase.\nRecent research indicates the critical role of data engineering in instruction\ntuning – when appropriately selected, only limited data is necessary to\nachieve superior performance. However, we still lack a principled understanding\nof what makes good instruction tuning data for alignment, and how we should\nselect data automatically and effectively. In this work, we delve deeply into\nautomatic data selection strategies for alignment. We start with controlled\nstudies to measure data across three dimensions: complexity, quality, and\ndiversity, along which we examine existing methods and introduce novel\ntechniques for enhanced data measurement. Subsequently, we propose a simple\nstrategy to select data samples based on the measurement. We present deita\n(short for Data-Efficient Instruction Tuning for Alignment), a series of models\nfine-tuned from LLaMA and Mistral models using data samples automatically\nselected with our proposed approach. Empirically, deita performs better or on\npar with the state-of-the-art open-source alignment models with only 6K SFT\ntraining data samples – over 10x less than the data used in the baselines.\nWhen further trained with direct preference optimization (DPO),\ndeita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\nMT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\non automatic data selection, facilitating data-efficient alignment. We release\nour models as well as the selected datasets for future researches to\neffectively align models more efficiently.</p>\n", "tags": ["Datasets","Efficiency","Fine-Tuning","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "liévin2022can", "citations": "135", "year": "2024", "title":"Can Large Language Models Reason About Medical Questions?", "abstract": "<p>Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nclose- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer\nand reason about difficult real-world-based questions. We focus on three\npopular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple\nprompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and\nretrieval augmentation. Based on an expert annotation of the generated CoTs, we\nfound that InstructGPT can often read, reason and recall expert knowledge.\nLast, by leveraging advances in prompt engineering (few-shot and ensemble\nmethods), we demonstrated that GPT-3.5 not only yields calibrated predictive\ndistributions, but also reaches the passing score on three datasets:\nMedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are\nclosing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.</p>\n", "tags": ["Datasets","Few-Shot","Model Architecture","Prompting"] },
{"key": "locascio2016neural", "citations": "71", "year": "2016", "title":"Neural Generation Of Regular Expressions From Natural Language With Minimal Domain Knowledge", "abstract": "<p>This paper explores the task of translating natural language queries into\nregular expressions which embody their meaning. In contrast to prior work, the\nproposed neural model does not utilize domain-specific crafting, learning to\ntranslate directly from a parallel corpus. To fully explore the potential of\nneural models, we propose a methodology for collecting a large corpus of\nregular expression, natural language pairs. Our resulting model achieves a\nperformance gain of 19.6% over previous state-of-the-art models.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "logan2021cutting", "citations": "110", "year": "2022", "title":"Cutting Down On Prompts And Parameters: Simple Few-shot Learning With Language Models", "abstract": "<p>Prompting language models (LMs) with training examples and task descriptions\nhas been seen as critical to recent successes in few-shot learning. In this\nwork, we show that finetuning LMs in the few-shot setting can considerably\nreduce the need for prompt engineering. In fact, one can use null prompts,\nprompts that contain neither task-specific templates nor training examples, and\nachieve competitive accuracy to manually-tuned prompts across a wide range of\ntasks. While finetuning LMs does introduce new parameters for each downstream\ntask, we show that this memory overhead can be substantially reduced:\nfinetuning only the bias terms can achieve comparable or better accuracy than\nstandard finetuning while only updating 0.1% of the parameters. All in all, we\nrecommend finetuning LMs for few-shot learning as it is more accurate, robust\nto different prompts, and can be made nearly as efficient as using frozen LMs.</p>\n", "tags": ["Few-Shot","Prompting","Training Techniques"] },
{"key": "logeswaran2016sentence", "citations": "87", "year": "2018", "title":"Sentence Ordering And Coherence Modeling Using Recurrent Neural Networks", "abstract": "<p>Modeling the structure of coherent texts is a key NLP problem. The task of\ncoherently organizing a given set of sentences has been commonly used to build\nand evaluate models that understand such structure. We propose an end-to-end\nunsupervised deep learning approach based on the set-to-sequence framework to\naddress this problem. Our model strongly outperforms prior methods in the order\ndiscrimination task and a novel task of ordering abstracts from scientific\narticles. Furthermore, our work shows that useful text representations can be\nobtained by learning to order sentences. Visualizing the learned sentence\nrepresentations shows that the model captures high-level logical structure in\nparagraphs. Our representations perform comparably to state-of-the-art\npre-training methods on sentence similarity and paraphrase detection tasks.</p>\n", "tags": ["AAAI","Tools","Training Techniques"] },
{"key": "logeswaran2018content", "citations": "84", "year": "2018", "title":"Content Preserving Text Generation With Attribute Controls", "abstract": "<p>In this work, we address the problem of modifying textual attributes of\nsentences. Given an input sentence and a set of attribute labels, we attempt to\ngenerate sentences that are compatible with the conditioning information. To\nensure that the model generates content compatible sentences, we introduce a\nreconstruction loss which interpolates between auto-encoding and\nback-translation loss components. We propose an adversarial loss to enforce\ngenerated samples to be attribute compatible and realistic. Through\nquantitative, qualitative and human evaluations we demonstrate that our model\nis capable of generating fluent sentences that better reflect the conditioning\ninformation compared to prior methods. We further demonstrate that the model is\ncapable of simultaneously controlling multiple attributes.</p>\n", "tags": ["Security"] },
{"key": "long2016video", "citations": "89", "year": "2018", "title":"Video Captioning With Multi-faceted Attention", "abstract": "<p>Recently, video captioning has been attracting an increasing amount of\ninterest, due to its potential for improving accessibility and information\nretrieval. While existing methods rely on different kinds of visual features\nand model structures, they do not fully exploit relevant semantic information.\nWe present an extensible approach to jointly leverage several sorts of visual\nfeatures and semantic attributes. Our novel architecture builds on LSTMs for\nsentence generation, with several attention layers and two multimodal layers.\nThe attention mechanism learns to automatically select the most salient visual\nfeatures or semantic attributes, and the multimodal layer yields overall\nrepresentations for the input and outputs of the sentence generation component.\nExperimental results on the challenging MSVD and MSR-VTT datasets show that our\nframework outperforms the state-of-the-art approaches, while ground truth based\nsemantic attributes are able to further elevate the output quality to a\nnear-human level.</p>\n", "tags": ["Datasets","Model Architecture","TACL","Tools"] },
{"key": "longpre2020mkqa", "citations": "94", "year": "2021", "title":"MKQA: A Linguistically Diverse Benchmark For Multilingual Open Domain Question Answering", "abstract": "<p>Progress in cross-lingual modeling depends on challenging, realistic, and\ndiverse evaluation sets. We introduce Multilingual Knowledge Questions and\nAnswers (MKQA), an open-domain question answering evaluation set comprising 10k\nquestion-answer pairs aligned across 26 typologically diverse languages (260k\nquestion-answer pairs in total). Answers are based on a heavily curated,\nlanguage-independent data representation, making results comparable across\nlanguages and independent of language-specific passages. With 26 languages,\nthis dataset supplies the widest range of languages to-date for evaluating\nquestion answering. We benchmark a variety of state-of-the-art methods and\nbaselines for generative and extractive question answering, trained on Natural\nQuestions, in zero shot and translation settings. Results indicate this dataset\nis challenging even in English, but especially in low-resource languages</p>\n", "tags": ["Datasets","Evaluation","TACL"] },
{"key": "longpre2023flan", "citations": "75", "year": "2023", "title":"The Flan Collection: Designing Data And Methods For Effective Instruction Tuning", "abstract": "<p>We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Has Code","Prompting","Training Techniques"] },
{"key": "lopezlira2023can", "citations": "258", "year": "2023", "title":"Can Chatgpt Forecast Stock Price Movements? Return Predictability And Large Language Models", "abstract": "<p>We document the capability of large language models (LLMs) like ChatGPT to\npredict stock price movements using news headlines, even without direct\nfinancial training. ChatGPT scores significantly predict out-of-sample daily\nstock returns, subsuming traditional methods, and predictability is stronger\namong smaller stocks and following negative news. To explain these findings, we\ndevelop a theoretical model incorporating information capacity constraints,\nunderreaction, limits-to-arbitrage, and LLMs. The model generates several key\npredictions, which we empirically test: (i) it establishes a critical threshold\nin AI capabilities necessary for profitable predictions, (ii) it demonstrates\nthat only advanced LLMs can effectively interpret complex information, and\n(iii) it predicts that widespread LLM adoption can enhance market efficiency.\nOur results suggest that sophisticated return forecasting is an emerging\ncapability of AI systems and that these technologies can alter information\ndiffusion and decision-making processes in financial markets. Finally, we\nintroduce an interpretability framework to evaluate LLMs’ reasoning,\ncontributing to AI transparency and economic decision-making.</p>\n", "tags": ["Time Series","Tools","Training Techniques"] },
{"key": "lopezpaz2017gradient", "citations": "872", "year": "2017", "title":"Gradient Episodic Memory For Continual Learning", "abstract": "<p>One major obstacle towards AI is the poor ability of models to solve new\nproblems quicker, and without forgetting previously acquired knowledge. To\nbetter understand this issue, we study the problem of continual learning, where\nthe model observes, once and one by one, examples concerning a sequence of\ntasks. First, we propose a set of metrics to evaluate models learning over a\ncontinuum of data. These metrics characterize models not only by their test\naccuracy, but also in terms of their ability to transfer knowledge across\ntasks. Second, we propose a model for continual learning, called Gradient\nEpisodic Memory (GEM) that alleviates forgetting, while allowing beneficial\ntransfer of knowledge to previous tasks. Our experiments on variants of the\nMNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when\ncompared to the state-of-the-art.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "loula2018rearranging", "citations": "106", "year": "2018", "title":"Rearranging The Familiar: Testing Compositional Generalization In Recurrent Networks", "abstract": "<p>Systematic compositionality is the ability to recombine meaningful units with\nregular and predictable outcomes, and it’s seen as key to humans’ capacity for\ngeneralization in language. Recent work has studied systematic compositionality\nin modern seq2seq models using generalization to novel navigation instructions\nin a grounded environment as a probing tool, requiring models to quickly\nbootstrap the meaning of new words. We extend this framework here to settings\nwhere the model needs only to recombine well-trained functional words (such as\n“around” and “right”) in novel contexts. Our findings confirm and strengthen\nthe earlier ones: seq2seq models can be impressively good at generalizing to\nnovel combinations of previously-seen input, but only when they receive\nextensive training on the specific pattern to be generalized (e.g.,\ngeneralizing from many examples of “X around right” to “jump around right”),\nwhile failing when generalization requires novel application of compositional\nrules (e.g., inferring the meaning of “around right” from those of “right” and\n“around”).</p>\n", "tags": ["EMNLP","Tools","Training Techniques"] },
{"key": "lourie2021unicorn", "citations": "77", "year": "2021", "title":"UNICORN On RAINBOW: A Universal Commonsense Reasoning Model On A New Multitask Benchmark", "abstract": "<p>Commonsense AI has long been seen as a near impossible goal – until\nrecently. Now, research interest has sharply increased with an influx of new\nbenchmarks and models.\n  We propose two new ways to evaluate commonsense models, emphasizing their\ngenerality on new tasks and building on diverse, recently introduced\nbenchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote\nresearch on commonsense models that generalize well over multiple tasks and\ndatasets. Second, we propose a novel evaluation, the cost equivalent curve,\nthat sheds new insight on how the choice of source datasets, pretrained\nlanguage models, and transfer learning methods impacts performance and data\nefficiency.\n  We perform extensive experiments – over 200 experiments encompassing 4800\nmodels – and report multiple valuable and sometimes surprising findings, e.g.,\nthat transfer almost always leads to better or equivalent performance if\nfollowing a particular recipe, that QA-based commonsense datasets transfer well\nwith each other, while commonsense knowledge graphs do not, and that perhaps\ncounter-intuitively, larger models benefit more from transfer than smaller\nones.\n  Last but not least, we introduce a new universal commonsense reasoning model,\nUNICORN, that establishes new state-of-the-art performance across 8 popular\ncommonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA\n(90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA\n(79.3%).</p>\n", "tags": ["AAAI","Datasets","Evaluation","Fine-Tuning"] },
{"key": "louvan2020recent", "citations": "75", "year": "2020", "title":"Recent Neural Methods On Slot Filling And Intent Classification For Task-oriented Dialogue Systems: A Survey", "abstract": "<p>In recent years, fostered by deep learning technologies and by the high\ndemand for conversational AI, various approaches have been proposed that\naddress the capacity to elicit and understand user’s needs in task-oriented\ndialogue systems. We focus on two core tasks, slot filling (SF) and intent\nclassification (IC), and survey how neural-based models have rapidly evolved to\naddress natural language understanding in dialogue systems. We introduce three\nneural architectures: independent model, which model SF and IC separately,\njoint models, which exploit the mutual benefit of the two tasks simultaneously,\nand transfer learning models, that scale the model to new domains. We discuss\nthe current state of the research in SF and IC and highlight challenges that\nstill require attention.</p>\n", "tags": ["COLING","Dialogue & Multi Turn","Fine-Tuning","Model Architecture","Survey Paper"] },
{"key": "lowe2017towards", "citations": "362", "year": "2017", "title":"Towards An Automatic Turing Test: Learning To Evaluate Dialogue Responses", "abstract": "<p>Automatically evaluating the quality of dialogue responses for unstructured\ndomains is a challenging problem. Unfortunately, existing automatic evaluation\nmetrics are biased and correlate very poorly with human judgements of response\nquality. Yet having an accurate automatic evaluation procedure is crucial for\ndialogue research, as it allows rapid prototyping and testing of new models\nwith fewer expensive human evaluations. In response to this challenge, we\nformulate automatic dialogue evaluation as a learning problem. We present an\nevaluation model (ADEM) that learns to predict human-like scores to input\nresponses, using a new dataset of human response scores. We show that the ADEM\nmodel’s predictions correlate significantly, and at a level much higher than\nword-overlap metrics such as BLEU, with human judgements at both the utterance\nand system-level. We also show that ADEM can generalize to evaluating dialogue\nmodels unseen during training, an important step for automatic dialogue\nevaluation.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Training Techniques"] },
{"key": "lowphansirikul2021wangchanberta", "citations": "63", "year": "2021", "title":"Wangchanberta: Pretraining Transformer-based Thai Language Models", "abstract": "<p>Transformer-based language models, more specifically BERT-based architectures\nhave achieved state-of-the-art performance in many downstream tasks. However,\nfor a relatively low-resource language such as Thai, the choices of models are\nlimited to training a BERT-based model based on a much smaller dataset or\nfinetuning multi-lingual models, both of which yield suboptimal downstream\nperformance. Moreover, large-scale multi-lingual pretraining does not take into\naccount language-specific features for Thai. To overcome these limitations, we\npretrain a language model based on RoBERTa-base architecture on a large,\ndeduplicated, cleaned training set (78GB in total size), curated from diverse\ndomains of social media posts, news articles and other publicly available\ndatasets. We apply text processing rules that are specific to Thai most\nimportantly preserving spaces, which are important chunk and sentence\nboundaries in Thai before subword tokenization. We also experiment with\nword-level, syllable-level and SentencePiece tokenization with a smaller\ndataset to explore the effects on tokenization on downstream performance. Our\nmodel wangchanberta-base-att-spm-uncased trained on the 78.5GB dataset\noutperforms strong baselines (NBSVM, CRF and ULMFit) and multi-lingual models\n(XLMR and mBERT) on both sequence classification and token classification tasks\nin human-annotated, mono-lingual contexts.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "loyola2017neural", "citations": "112", "year": "2017", "title":"A Neural Architecture For Generating Natural Language Descriptions From Source Code Changes", "abstract": "<p>We propose a model to automatically describe changes introduced in the source\ncode of a program using natural language. Our method receives as input a set of\ncode commits, which contains both the modifications and message introduced by\nan user. These two modalities are used to train an encoder-decoder\narchitecture. We evaluated our approach on twelve real world open source\nprojects from four different programming languages. Quantitative and\nqualitative results showed that the proposed approach can generate feasible and\nsemantically sound descriptions not only in standard in-project settings, but\nalso in a cross-project setting.</p>\n", "tags": ["Model Architecture"] },
{"key": "lu2016hierarchical", "citations": "1233", "year": "2016", "title":"Hierarchical Question-image Co-attention For Visual Question Answering", "abstract": "<p>A number of recent works have proposed attention models for Visual Question\nAnswering (VQA) that generate spatial maps highlighting image regions relevant\nto answering the question. In this paper, we argue that in addition to modeling\n“where to look” or visual attention, it is equally important to model “what\nwords to listen to” or question attention. We present a novel co-attention\nmodel for VQA that jointly reasons about image and question attention. In\naddition, our model reasons about the question (and consequently the image via\nthe co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional\nconvolution neural networks (CNN). Our model improves the state-of-the-art on\nthe VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA\ndataset. By using ResNet, the performance is further improved to 62.1% for VQA\nand 65.4% for COCO-QA.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "lu2016knowing", "citations": "1619", "year": "2017", "title":"Knowing When To Look: Adaptive Attention Via A Visual Sentinel For Image Captioning", "abstract": "<p>Attention-based neural encoder-decoder frameworks have been widely adopted\nfor image captioning. Most methods force visual attention to be active for\nevery generated word. However, the decoder likely requires little to no visual\ninformation from the image to predict non-visual words such as “the” and “of”.\nOther words that may seem visual can often be predicted reliably just from the\nlanguage model e.g., “sign” after “behind a red stop” or “phone” following\n“talking on a cell”. In this paper, we propose a novel adaptive attention model\nwith a visual sentinel. At each time step, our model decides whether to attend\nto the image (and if so, to which regions) or to the visual sentinel. The model\ndecides whether to attend to the image and where, in order to extract\nmeaningful information for sequential word generation. We test our method on\nthe COCO image captioning 2015 challenge dataset and Flickr30K. Our approach\nsets the new state-of-the-art by a significant margin.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "lu2017best", "citations": "93", "year": "2017", "title":"Best Of Both Worlds: Transferring Knowledge From Discriminative Learning To A Generative Visual Dialog Model", "abstract": "<p>We present a novel training framework for neural sequence models,\nparticularly for grounded dialog generation. The standard training paradigm for\nthese models is maximum likelihood estimation (MLE), or minimizing the\ncross-entropy of the human responses. Across a variety of domains, a recurring\nproblem with MLE trained generative neural dialog models (G) is that they tend\nto produce ‘safe’ and generic responses (“I don’t know”, “I can’t tell”). In\ncontrast, discriminative dialog models (D) that are trained to rank a list of\ncandidate human responses outperform their generative counterparts; in terms of\nautomatic metrics, diversity, and informativeness of the responses. However, D\nis not useful in practice since it cannot be deployed to have real\nconversations with users.\n  Our work aims to achieve the best of both worlds – the practical usefulness\nof G and the strong performance of D – via knowledge transfer from D to G. Our\nprimary contribution is an end-to-end trainable generative visual dialog model,\nwhere G receives gradients from D as a perceptual (not adversarial) loss of the\nsequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS)\napproximation to the discrete distribution – specifically, an RNN augmented\nwith a sequence of GS samplers, coupled with the straight-through gradient\nestimator to enable end-to-end differentiability. We also introduce a stronger\nencoder for visual dialog, and employ a self-attention mechanism for answer\nencoding along with a metric learning loss to aid D in better capturing\nsemantic similarities in answer responses. Overall, our proposed model\noutperforms state-of-the-art on the VisDial dataset by a significant margin\n(2.67% on recall@10). The source code can be downloaded from\nhttps://github.com/jiasenlu/visDial.pytorch.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "lu2017co", "citations": "82", "year": "2018", "title":"Co-attending Free-form Regions And Detections With Multi-modal Multiplicative Feature Embedding For Visual Question Answering", "abstract": "<p>Recently, the Visual Question Answering (VQA) task has gained increasing\nattention in artificial intelligence. Existing VQA methods mainly adopt the\nvisual attention mechanism to associate the input question with corresponding\nimage regions for effective question answering. The free-form region based and\nthe detection-based visual attention mechanisms are mostly investigated, with\nthe former ones attending free-form image regions and the latter ones attending\npre-specified detection-box regions. We argue that the two attention mechanisms\nare able to provide complementary information and should be effectively\nintegrated to better solve the VQA problem. In this paper, we propose a novel\ndeep neural network for VQA that integrates both attention mechanisms. Our\nproposed framework effectively fuses features from free-form image regions,\ndetection boxes, and question representations via a multi-modal multiplicative\nfeature embedding scheme to jointly attend question-related free-form image\nregions and detection boxes for more accurate question answering. The proposed\nmethod is extensively evaluated on two publicly available datasets, COCO-QA and\nVQA, and outperforms state-of-the-art approaches. Source code is available at\nhttps://github.com/lupantech/dual-mfa-vqa.</p>\n", "tags": ["AAAI","Datasets","Has Code","Model Architecture","Tools"] },
{"key": "lu201912", "citations": "411", "year": "2020", "title":"12-in-1: Multi-task Vision And Language Representation Learning", "abstract": "<p>Much of vision-and-language research focuses on a small but diverse set of\nindependent tasks and supporting datasets often studied in isolation; however,\nthe visually-grounded language understanding skills required for success at\nthese tasks overlap significantly. In this work, we investigate these\nrelationships between vision-and-language tasks by developing a large-scale,\nmulti-task training regime. Our approach culminates in a single model on 12\ndatasets from four broad categories of task including visual question\nanswering, caption-based image retrieval, grounding referring expressions, and\nmulti-modal verification. Compared to independently trained single-task models,\nthis represents a reduction from approximately 3 billion parameters to 270\nmillion while simultaneously improving performance by 2.05 points on average\nacross tasks. We use our multi-task framework to perform in-depth analysis of\nthe effect of joint training diverse tasks. Further, we show that finetuning\ntask-specific models from our single multi-task model can lead to further\nimprovements, achieving performance at or above the state-of-the-art.</p>\n", "tags": ["CVPR","Datasets","Tools","Training Techniques"] },
{"key": "lu2019vilbert", "citations": "1522", "year": "2019", "title":"Vilbert: Pretraining Task-agnostic Visiolinguistic Representations For Vision-and-language Tasks", "abstract": "<p>We present ViLBERT (short for Vision-and-Language BERT), a model for learning\ntask-agnostic joint representations of image content and natural language. We\nextend the popular BERT architecture to a multi-modal two-stream model,\npro-cessing both visual and textual inputs in separate streams that interact\nthrough co-attentional transformer layers. We pretrain our model through two\nproxy tasks on the large, automatically collected Conceptual Captions dataset\nand then transfer it to multiple established vision-and-language tasks –\nvisual question answering, visual commonsense reasoning, referring expressions,\nand caption-based image retrieval – by making only minor additions to the base\narchitecture. We observe significant improvements across tasks compared to\nexisting task-specific models – achieving state-of-the-art on all four tasks.\nOur work represents a shift away from learning groundings between vision and\nlanguage only as part of task training and towards treating visual grounding as\na pretrainable and transferable capability.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "lu2021codexglue", "citations": "383", "year": "2021", "title":"Codexglue: A Machine Learning Benchmark Dataset For Code Understanding And Generation", "abstract": "<p>Benchmark datasets have a significant impact on accelerating research in\nprogramming language tasks. In this paper, we introduce CodeXGLUE, a benchmark\ndataset to foster machine learning research for program understanding and\ngeneration. CodeXGLUE includes a collection of 10 tasks across 14 datasets and\na platform for model evaluation and comparison. CodeXGLUE also features three\nbaseline systems, including the BERT-style, GPT-style, and Encoder-Decoder\nmodels, to make it easy for researchers to use the platform. The availability\nof such data and baselines can help the development and validation of new\nmethods that can be applied to various program understanding and generation\nproblems.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "lu2021fantastically", "citations": "279", "year": "2022", "title":"Fantastically Ordered Prompts And Where To Find Them: Overcoming Few-shot Prompt Order Sensitivity", "abstract": "<p>When primed with only a handful of training samples, very large, pretrained\nlanguage models such as GPT-3 have shown competitive results when compared to\nfully-supervised, fine-tuned, large, pretrained language models. We demonstrate\nthat the order in which the samples are provided can make the difference\nbetween near state-of-the-art and random guess performance: essentially some\npermutations are “fantastic” and some not. We analyse this phenomenon in\ndetail, establishing that: it is present across model sizes (even for the\nlargest current models), it is not related to a specific subset of samples, and\nthat a given good permutation for one model is not transferable to another.\nWhile one could use a development set to determine which permutations are\nperformant, this would deviate from the true few-shot setting as it requires\nadditional annotated data. Instead, we use the generative nature of language\nmodels to construct an artificial development set and based on entropy\nstatistics of the candidate permutations on this set, we identify performant\nprompts. Our method yields a 13% relative improvement for GPT-family models\nacross eleven different established text classification tasks.</p>\n", "tags": ["Few-Shot","Model Architecture","Prompting","Training Techniques"] },
{"key": "lu2021pretrained", "citations": "93", "year": "2021", "title":"Pretrained Transformers As Universal Computation Engines", "abstract": "<p>We investigate the capability of a transformer pretrained on natural language\nto generalize to other modalities with minimal finetuning – in particular,\nwithout finetuning of the self-attention and feedforward layers of the residual\nblocks. We consider such a model, which we call a Frozen Pretrained Transformer\n(FPT), and study finetuning it on a variety of sequence classification tasks\nspanning numerical computation, vision, and protein fold prediction. In\ncontrast to prior works which investigate finetuning on the same modality as\nthe pretraining dataset, we show that pretraining on natural language can\nimprove performance and compute efficiency on non-language downstream tasks.\nAdditionally, we perform an analysis of the architecture, comparing the\nperformance of a random initialized transformer to a random LSTM. Combining the\ntwo insights, we find language-pretrained transformers can obtain strong\nperformance on a variety of non-language tasks.</p>\n", "tags": ["Datasets","Efficiency","Model Architecture"] },
{"key": "lu2022learn", "citations": "113", "year": "2022", "title":"Learn To Explain: Multimodal Reasoning Via Thought Chains For Science Question Answering", "abstract": "<p>When answering a question, humans utilize the information available across\ndifferent modalities to synthesize a consistent and complete chain of thought\n(CoT). This process is normally a black box in the case of deep learning models\nlike large-scale language models. Recently, science question benchmarks have\nbeen used to diagnose the multi-hop reasoning ability and interpretability of\nan AI system. However, existing datasets fail to provide annotations for the\nanswers, or are restricted to the textual-only modality, small scales, and\nlimited domain diversity. To this end, we present Science Question Answering\n(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice\nquestions with a diverse set of science topics and annotations of their answers\nwith corresponding lectures and explanations. We further design language models\nto learn to generate lectures and explanations as the chain of thought (CoT) to\nmimic the multi-hop reasoning process when answering ScienceQA questions.\nScienceQA demonstrates the utility of CoT in language models, as CoT improves\nthe question answering performance by 1.20% in few-shot GPT-3 and 3.99% in\nfine-tuned UnifiedQA. We also explore the upper bound for models to leverage\nexplanations by feeding those in the input; we observe that it improves the\nfew-shot performance of GPT-3 by 18.96%. Our analysis further shows that\nlanguage models, similar to humans, benefit from explanations to learn from\nfewer data and achieve the same performance with just 40% of the data. The data\nand code are available at https://scienceqa.github.io.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Has Code","Model Architecture","Prompting"] },
{"key": "lu2022prompt", "citations": "135", "year": "2022", "title":"Prompt Distribution Learning", "abstract": "<p>We present prompt distribution learning for effectively adapting a\npre-trained vision-language model to address downstream recognition tasks. Our\nmethod not only learns low-bias prompts from a few samples but also captures\nthe distribution of diverse prompts to handle the varying visual\nrepresentations. In this way, we provide high-quality task-related content for\nfacilitating recognition. This prompt distribution learning is realized by an\nefficient approach that learns the output embeddings of prompts instead of the\ninput embeddings. Thus, we can employ a Gaussian distribution to model them\neffectively and derive a surrogate loss for efficient training. Extensive\nexperiments on 12 datasets demonstrate that our method consistently and\nsignificantly outperforms existing methods. For example, with 1 sample per\ncategory, it relatively improves the average result by 9.1% compared to\nhuman-crafted prompts.</p>\n", "tags": ["CVPR","Datasets","Prompting","Training Techniques"] },
{"key": "lu2022unified", "citations": "270", "year": "2022", "title":"Unified BERT For Few-shot Natural Language Understanding", "abstract": "<p>Even as pre-trained language models share a semantic encoder, natural\nlanguage understanding suffers from a diversity of output schemas. In this\npaper, we propose UBERT, a unified bidirectional language understanding model\nbased on BERT framework, which can universally model the training objects of\ndifferent NLU tasks through a biaffine network. Specifically, UBERT encodes\nprior knowledge from various aspects, uniformly constructing learning\nrepresentations across multiple NLU tasks, which is conducive to enhancing the\nability to capture common semantic understanding. By using the biaffine to\nmodel scores pair of the start and end position of the original text, various\nclassification and extraction structures can be converted into a universal,\nspan-decoding approach. Experiments show that UBERT wins the first price in the\n2022 AIWIN - World Artificial Intelligence Innovation Competition, Chinese\ninsurance few-shot multi-task track, and realizes the unification of extensive\ninformation extraction and linguistic reasoning tasks.</p>\n", "tags": ["Few-Shot","Model Architecture","Tools","Training Techniques"] },
{"key": "luan2016lstm", "citations": "60", "year": "2016", "title":"LSTM Based Conversation Models", "abstract": "<p>In this paper, we present a conversational model that incorporates both\ncontext and participant role for two-party conversations. Different\narchitectures are explored for integrating participant role and context\ninformation into a Long Short-term Memory (LSTM) language model. The\nconversational model can function as a language model or a language generation\nmodel. Experiments on the Ubuntu Dialog Corpus show that our model can capture\nmultiple turn interaction between participants. The proposed method outperforms\na traditional LSTM model as measured by language model perplexity and response\nranking. Generated responses show characteristic differences between the two\nparticipant roles.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "luan2017multi", "citations": "66", "year": "2017", "title":"Multi-task Learning For Speaker-role Adaptation In Neural Conversation Models", "abstract": "<p>Building a persona-based conversation agent is challenging owing to the lack\nof large amounts of speaker-specific conversation data for model training. This\npaper addresses the problem by proposing a multi-task learning approach to\ntraining neural conversation models that leverages both conversation data\nacross speakers and other types of data pertaining to the speaker and speaker\nroles to be modeled. Experiments show that our approach leads to significant\nimprovements over baseline model quality, generating responses that capture\nmore precisely speakers’ traits and speaking styles. The model offers the\nbenefits of being algorithmically simple and easy to implement, and not relying\non large quantities of data representing specific individual speakers.</p>\n", "tags": ["Agentic","Training Techniques"] },
{"key": "lugosch2019speech", "citations": "241", "year": "2019", "title":"Speech Model Pre-training For End-to-end Spoken Language Understanding", "abstract": "<p>Whereas conventional spoken language understanding (SLU) systems map speech\nto text, and then text to intent, end-to-end SLU systems map speech directly to\nintent through a single trainable model. Achieving high accuracy with these\nend-to-end models without a large amount of training data is difficult. We\npropose a method to reduce the data requirements of end-to-end SLU in which the\nmodel is first pre-trained to predict words and phonemes, thus learning good\nfeatures for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and\nshow that our method improves performance both when the full dataset is used\nfor training and when only a small subset is used. We also describe preliminary\nexperiments to gauge the model’s ability to generalize to new phrases not heard\nduring training.</p>\n", "tags": ["Datasets","INTERSPEECH","Training Techniques"] },
{"key": "lund2023chatgpt", "citations": "514", "year": "2023", "title":"Chatgpt And A New Academic Reality: Artificial Intelligence-written Research Papers And The Ethics Of The Large Language Models In Scholarly Publishing", "abstract": "<p>This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.</p>\n", "tags": ["Ethics & Fairness","Model Architecture"] },
{"key": "luo2017comprehension", "citations": "148", "year": "2017", "title":"Comprehension-guided Referring Expressions", "abstract": "<p>We consider generation and comprehension of natural language referring\nexpression for objects in an image. Unlike generic “image captioning” which\nlacks natural standard evaluation criteria, quality of a referring expression\nmay be measured by the receiver’s ability to correctly infer which object is\nbeing described. Following this intuition, we propose two approaches to utilize\nmodels trained for comprehension task to generate better expressions. First, we\nuse a comprehension module trained on human-generated expressions, as a\n“critic” of referring expression generator. The comprehension module serves as\na differentiable proxy of human evaluation, providing training signal to the\ngeneration module. Second, we use the comprehension module in a\ngenerate-and-rerank pipeline, which chooses from candidate expressions\ngenerated by a model according to their performance on the comprehension task.\nWe show that both approaches lead to improved referring expression generation\non multiple benchmark datasets.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Training Techniques"] },
{"key": "luo2018learning", "citations": "69", "year": "2019", "title":"Learning Personalized End-to-end Goal-oriented Dialog", "abstract": "<p>Most existing works on dialog systems only consider conversation content\nwhile neglecting the personality of the user the bot is interacting with, which\nbegets several unsolved issues. In this paper, we present a personalized\nend-to-end model in an attempt to leverage personalization in goal-oriented\ndialogs. We first introduce a Profile Model which encodes user profiles into\ndistributed embeddings and refers to conversation history from other similar\nusers. Then a Preference Model captures user preferences over knowledge base\nentities to handle the ambiguity in user requests. The two models are combined\ninto the Personalized MemN2N. Experiments show that the proposed model achieves\nqualitative performance improvements over state-of-the-art methods. As for\nhuman evaluation, it also outperforms other approaches in terms of task\ncompletion rate and user satisfaction.</p>\n", "tags": ["AAAI","Evaluation"] },
{"key": "luo2020univl", "citations": "164", "year": "2020", "title":"Univl: A Unified Video And Language Pre-training Model For Multimodal Understanding And Generation", "abstract": "<p>With the recent success of the pre-training technique for NLP and\nimage-linguistic tasks, some video-linguistic pre-training works are gradually\ndeveloped to improve video-text related downstream tasks. However, most of the\nexisting multimodal models are pre-trained for understanding tasks, leading to\na pretrain-finetune discrepancy for generation tasks. This paper proposes\nUniVL: a Unified Video and Language pre-training model for both multimodal\nunderstanding and generation. It comprises four components, including two\nsingle-modal encoders, a cross encoder, and a decoder with the Transformer\nbackbone. Five objectives, including video-text joint, conditioned masked\nlanguage model (CMLM), conditioned masked frame model (CMFM), video-text\nalignment, and language reconstruction, are designed to train each of the\ncomponents. We further develop two pre-training strategies, stage by stage\npre-training (StagedP) and enhanced video representation (EnhancedV), to make\nthe training process of the UniVL more effective. The pre-train is carried out\non a sizeable instructional video dataset HowTo100M. Experimental results\ndemonstrate that the UniVL can learn strong video-text representation and\nachieves state-of-the-art results on five downstream tasks.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "luo2021clip4clip", "citations": "429", "year": "2022", "title":"Clip4clip: An Empirical Study Of CLIP For End To End Video Clip Retrieval", "abstract": "<p>Video-text retrieval plays an essential role in multi-modal research and has\nbeen widely used in many real-world web applications. The CLIP (Contrastive\nLanguage-Image Pre-training), an image-language pre-training model, has\ndemonstrated the power of visual concepts learning from web collected\nimage-text datasets. In this paper, we propose a CLIP4Clip model to transfer\nthe knowledge of the CLIP model to video-language retrieval in an end-to-end\nmanner. Several questions are investigated via empirical studies: 1) Whether\nimage feature is enough for video-text retrieval? 2) How a post-pretraining on\na large-scale video-text dataset based on the CLIP affect the performance? 3)\nWhat is the practical mechanism to model temporal dependency between video\nframes? And 4) The Hyper-parameters sensitivity of the model on video-text\nretrieval task. Extensive experimental results present that the CLIP4Clip model\ntransferred from the CLIP can achieve SOTA results on various video-text\nretrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We\nrelease our code at https://github.com/ArrowLuo/CLIP4Clip.</p>\n", "tags": ["Applications","Datasets","Has Code","Training Techniques"] },
{"key": "luo2022biogpt", "citations": "578", "year": "2022", "title":"Biogpt: Generative Pre-trained Transformer For Biomedical Text Generation And Mining", "abstract": "<p>Pre-trained language models have attracted increasing attention in the\nbiomedical domain, inspired by their great success in the general natural\nlanguage domain. Among the two main branches of pre-trained language models in\nthe general language domain, i.e., BERT (and its variants) and GPT (and its\nvariants), the first one has been extensively studied in the biomedical domain,\nsuch as BioBERT and PubMedBERT. While they have achieved great success on a\nvariety of discriminative downstream biomedical tasks, the lack of generation\nability constrains their application scope. In this paper, we propose BioGPT, a\ndomain-specific generative Transformer language model pre-trained on large\nscale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and\ndemonstrate that our model outperforms previous models on most tasks.\nEspecially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI\nend-to-end relation extraction tasks respectively, and 78.2% accuracy on\nPubMedQA, creating a new record. Our case study on text generation further\ndemonstrates the advantage of BioGPT on biomedical literature to generate\nfluent descriptions for biomedical terms. Code is available at\nhttps://github.com/microsoft/BioGPT.</p>\n", "tags": ["Has Code","Model Architecture"] },
{"key": "luo2023wizardcoder", "citations": "60", "year": "2023", "title":"Wizardcoder: Empowering Code Large Language Models With Evol-instruct", "abstract": "<p>Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic’s Claude and Google’s Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM</p>\n", "tags": ["Fine-Tuning","Has Code","ICLR","Llm For Code","Training Techniques"] },
{"key": "lv2019adapting", "citations": "76", "year": "2019", "title":"Adapting Meta Knowledge Graph Information For Multi-hop Reasoning Over Few-shot Relations", "abstract": "<p>Multi-hop knowledge graph (KG) reasoning is an effective and explainable\nmethod for predicting the target entity via reasoning paths in query answering\n(QA) task. Most previous methods assume that every relation in KGs has enough\ntraining triples, regardless of those few-shot relations which cannot provide\nsufficient triples for training robust reasoning models. In fact, the\nperformance of existing multi-hop reasoning methods drops significantly on\nfew-shot relations. In this paper, we propose a meta-based multi-hop reasoning\nmethod (Meta-KGR), which adopts meta-learning to learn effective meta\nparameters from high-frequency relations that could quickly adapt to few-shot\nrelations. We evaluate Meta-KGR on two public datasets sampled from Freebase\nand NELL, and the experimental results show that Meta-KGR outperforms the\ncurrent state-of-the-art methods in few-shot scenarios. Our code and datasets\ncan be obtained from https://github.com/ THU-KEG/MetaKGR.</p>\n", "tags": ["Datasets","EMNLP","Few-Shot","Has Code","Training Techniques"] },
{"key": "lynch2020language", "citations": "69", "year": "2021", "title":"Language Conditioned Imitation Learning Over Unstructured Data", "abstract": "<p>Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image – something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. “open the drawer…now pick up the block…now\npress the green button…”). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io</p>\n", "tags": ["Agentic","Has Code","Instruction Following"] },
{"key": "läubli2018has", "citations": "278", "year": "2018", "title":"Has Machine Translation Achieved Human Parity? A Case For Document-level Evaluation", "abstract": "<p>Recent research suggests that neural machine translation achieves parity with\nprofessional human translation on the WMT Chinese–English news translation\ntask. We empirically test this claim with alternative evaluation protocols,\ncontrasting the evaluation of single sentences and entire documents. In a\npairwise ranking experiment, human raters assessing adequacy and fluency show a\nstronger preference for human over machine translation when evaluating\ndocuments as compared to isolated sentences. Our findings emphasise the need to\nshift towards document-level evaluation as machine translation improves to the\ndegree that errors which are hard or impossible to spot at the sentence-level\nbecome decisive in discriminating quality of different translation outputs.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "ma2017improving", "citations": "62", "year": "2017", "title":"Improving Semantic Relevance For Sequence-to-sequence Learning Of Chinese Social Media Text Summarization", "abstract": "<p>Current Chinese social media text summarization models are based on an\nencoder-decoder framework. Although its generated summaries are similar to\nsource texts literally, they have low semantic relevance. In this work, our\ngoal is to improve semantic relevance between source texts and summaries for\nChinese social media summarization. We introduce a Semantic Relevance Based\nneural model to encourage high semantic similarity between texts and summaries.\nIn our model, the source text is represented by a gated attention encoder,\nwhile the summary representation is produced by a decoder. Besides, the\nsimilarity score between the representations is maximized during training. Our\nexperiments show that the proposed model outperforms baseline systems on a\nsocial media corpus.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "ma2017visual", "citations": "106", "year": "2018", "title":"Visual Question Answering With Memory-augmented Networks", "abstract": "<p>In this paper, we exploit a memory-augmented neural network to predict\naccurate answers to visual questions, even when those answers occur rarely in\nthe training set. The memory network incorporates both internal and external\nmemory blocks and selectively pays attention to each training exemplar. We show\nthat memory-augmented neural networks are able to maintain a relatively\nlong-term memory of scarce training exemplars, which is important for visual\nquestion answering due to the heavy-tailed distribution of answers in a general\nVQA setting. Experimental results on two large-scale benchmark datasets show\nthe favorable performance of the proposed algorithm with a comparison to state\nof the art.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "ma2018bag", "citations": "81", "year": "2018", "title":"Bag-of-words As Target For Neural Machine Translation", "abstract": "<p>A sentence can be translated into more than one correct sentences. However,\nmost of the existing neural machine translation models only use one of the\ncorrect translations as the targets, and the other correct sentences are\npunished as the incorrect sentences in the training stage. Since most of the\ncorrect translations for one sentence share the similar bag-of-words, it is\npossible to distinguish the correct translations from the incorrect ones by the\nbag-of-words. In this paper, we propose an approach that uses both the\nsentences and the bag-of-words as targets in the training stage, in order to\nencourage the model to generate the potentially correct sentences that are not\nappeared in the training set. We evaluate our model on a Chinese-English\ntranslation dataset, and experiments show our model outperforms the strong\nbaselines by the BLEU score of 4.55.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "ma2018query", "citations": "66", "year": "2018", "title":"Query And Output: Generating Words By Querying Distributed Word Representations For Paraphrase Generation", "abstract": "<p>Most recent approaches use the sequence-to-sequence model for paraphrase\ngeneration. The existing sequence-to-sequence model tends to memorize the words\nand the patterns in the training dataset instead of learning the meaning of the\nwords. Therefore, the generated sentences are often grammatically correct but\nsemantically improper. In this work, we introduce a novel model based on the\nencoder-decoder framework, called Word Embedding Attention Network (WEAN). Our\nproposed model generates the words by querying distributed word representations\n(i.e. neural word embeddings), hoping to capturing the meaning of the according\nwords. Following previous work, we evaluate our model on two\nparaphrase-oriented tasks, namely text simplification and short text\nabstractive summarization. Experimental results show that our model outperforms\nthe sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two\nEnglish text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a\nChinese summarization dataset. Moreover, our model achieves state-of-the-art\nperformances on these three benchmark datasets.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","NAACL","Tools","Training Techniques"] },
{"key": "ma2018stacl", "citations": "190", "year": "2019", "title":"STACL: Simultaneous Translation With Implicit Anticipation And Controllable Latency Using Prefix-to-prefix Framework", "abstract": "<p>Simultaneous translation, which translates sentences before they are\nfinished, is useful in many scenarios but is notoriously difficult due to\nword-order differences. While the conventional seq-to-seq framework is only\nsuitable for full-sentence translation, we propose a novel prefix-to-prefix\nframework for simultaneous translation that implicitly learns to anticipate in\na single translation model. Within this framework, we present a very simple yet\nsurprisingly effective wait-k policy trained to generate the target sentence\nconcurrently with the source sentence, but always k words behind. Experiments\nshow our strategy achieves low latency and reasonable quality (compared to\nfull-sentence translation) on 4 directions: zh&lt;-&gt;en and de&lt;-&gt;en.</p>\n", "tags": ["Tools"] },
{"key": "ma2019flowseq", "citations": "152", "year": "2019", "title":"Flowseq: Non-autoregressive Conditional Sequence Generation With Generative Flow", "abstract": "<p>Most sequence-to-sequence (seq2seq) models are autoregressive; they generate\neach token by conditioning on previously generated tokens. In contrast,\nnon-autoregressive seq2seq models generate all tokens in one pass, which leads\nto increased efficiency through parallel processing on hardware such as GPUs.\nHowever, directly modeling the joint distribution of all tokens simultaneously\nis challenging, and even with increasingly complex model structures accuracy\nlags significantly behind autoregressive models. In this paper, we propose a\nsimple, efficient, and effective model for non-autoregressive sequence\ngeneration using latent variable models. Specifically, we turn to generative\nflow, an elegant technique to model complex distributions using neural\nnetworks, and design several layers of flow tailored for modeling the\nconditional density of sequential latent variables. We evaluate this model on\nthree neural machine translation (NMT) benchmark datasets, achieving comparable\nperformance with state-of-the-art non-autoregressive NMT models and almost\nconstant decoding time w.r.t the sequence length.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "ma2019monotonic", "citations": "78", "year": "2019", "title":"Monotonic Multihead Attention", "abstract": "<p>Simultaneous machine translation models start generating a target sequence\nbefore they have encoded or read the source sequence. Recent approaches for\nthis task either apply a fixed policy on a state-of-the art Transformer model,\nor a learnable monotonic attention on a weaker recurrent neural network-based\nstructure. In this paper, we propose a new attention mechanism, Monotonic\nMultihead Attention (MMA), which extends the monotonic attention mechanism to\nmultihead attention. We also introduce two novel and interpretable approaches\nfor latency control that are specifically designed for multiple attentions\nheads. We apply MMA to the simultaneous machine translation task and\ndemonstrate better latency-quality tradeoffs compared to MILk, the previous\nstate-of-the-art approach. We also analyze how the latency controls affect the\nattention span and we motivate the introduction of our model by analyzing the\neffect of the number of decoder layers and heads on quality and latency.</p>\n", "tags": ["Model Architecture"] },
{"key": "ma2019tensorized", "citations": "63", "year": "2019", "title":"A Tensorized Transformer For Language Modeling", "abstract": "<p>Latest development of neural models has connected the encoder and decoder\nthrough a self-attention mechanism. In particular, Transformer, which is solely\nbased on self-attention, has led to breakthroughs in Natural Language\nProcessing (NLP) tasks. However, the multi-head attention mechanism, as a key\ncomponent of Transformer, limits the effective deployment of the model to a\nresource-limited setting. In this paper, based on the ideas of tensor\ndecomposition and parameters sharing, we propose a novel self-attention model\n(namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We\ntest and verify the proposed attention method on three language modeling tasks\n(i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task\n(i.e., WMT-2016 English-German). Multi-linear attention can not only largely\ncompress the model parameters but also obtain performance improvements,\ncompared with a number of language modeling approaches, such as Transformer,\nTransformer-XL, and Transformer with tensor train decomposition.</p>\n", "tags": ["Model Architecture"] },
{"key": "ma2019towards", "citations": "73", "year": "2019", "title":"Towards Generalizable Neuro-symbolic Systems For Commonsense Question Answering", "abstract": "<p>Non-extractive commonsense QA remains a challenging AI task, as it requires\nsystems to reason about, synthesize, and gather disparate pieces of\ninformation, in order to generate responses to queries. Recent approaches on\nsuch tasks show increased performance, only when models are either pre-trained\nwith additional information or when domain-specific heuristics are used,\nwithout any special consideration regarding the knowledge resource type. In\nthis paper, we perform a survey of recent commonsense QA methods and we provide\na systematic analysis of popular knowledge resources and knowledge-integration\nmethods, across benchmarks from multiple commonsense datasets. Our results and\nanalysis show that attention-based injection seems to be a preferable choice\nfor knowledge integration and that the degree of domain overlap, between\nknowledge bases and datasets, plays a crucial role in determining model\nsuccess.</p>\n", "tags": ["Datasets","Model Architecture","Survey Paper"] },
{"key": "ma2020charbert", "citations": "79", "year": "2020", "title":"Charbert: Character-aware Pre-trained Language Model", "abstract": "<p>Most pre-trained language models (PLMs) construct word representations at\nsubword level with Byte-Pair Encoding (BPE) or its variations, by which OOV\n(out-of-vocab) words are almost avoidable. However, those methods split a word\ninto subword units and make the representation incomplete and fragile. In this\npaper, we propose a character-aware pre-trained language model named CharBERT\nimproving on the previous methods (such as BERT, RoBERTa) to tackle these\nproblems. We first construct the contextual word embedding for each token from\nthe sequential character representations, then fuse the representations of\ncharacters and the subword representations by a novel heterogeneous interaction\nmodule. We also propose a new pre-training task named NLM (Noisy LM) for\nunsupervised character representation learning. We evaluate our method on\nquestion answering, sequence labeling, and text classification tasks, both on\nthe original datasets and adversarial misspelling test sets. The experimental\nresults show that our method can significantly improve the performance and\nrobustness of PLMs simultaneously. Pretrained models, evaluation sets, and code\nare available at https://github.com/wtma/CharBERT</p>\n", "tags": ["COLING","Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "ma2020knowledge", "citations": "68", "year": "2021", "title":"Knowledge-driven Data Construction For Zero-shot Evaluation In Commonsense Question Answering", "abstract": "<p>Recent developments in pre-trained neural language modeling have led to leaps\nin accuracy on commonsense question-answering benchmarks. However, there is\nincreasing concern that models overfit to specific tasks, without learning to\nutilize external knowledge or perform general semantic reasoning. In contrast,\nzero-shot evaluations have shown promise as a more robust measure of a model’s\ngeneral reasoning abilities. In this paper, we propose a novel neuro-symbolic\nframework for zero-shot question answering across commonsense tasks. Guided by\na set of hypotheses, the framework studies how to transform various\npre-existing knowledge resources into a form that is most effective for\npre-training models. We vary the set of language models, training regimes,\nknowledge sources, and data generation strategies, and measure their impact\nacross tasks. Extending on prior work, we devise and compare four constrained\ndistractor-sampling strategies. We provide empirical results across five\ncommonsense question-answering tasks with data generated from five external\nknowledge resources. We show that, while an individual knowledge graph is\nbetter suited for specific tasks, a global knowledge graph brings consistent\ngains across different tasks. In addition, both preserving the structure of the\ntask as well as generating fair and informative questions help language models\nlearn more effectively.</p>\n", "tags": ["AAAI","Evaluation","Tools","Training Techniques"] },
{"key": "ma2020prop", "citations": "70", "year": "2021", "title":"PROP: Pre-training With Representative Words Prediction For Ad-hoc Retrieval", "abstract": "<p>Recently pre-trained language representation models such as BERT have shown\ngreat success when fine-tuned on downstream tasks including information\nretrieval (IR). However, pre-training objectives tailored for ad-hoc retrieval\nhave not been well explored. In this paper, we propose Pre-training with\nRepresentative wOrds Prediction (PROP) for ad-hoc retrieval. PROP is inspired\nby the classical statistical language model for IR, specifically the query\nlikelihood model, which assumes that the query is generated as the piece of\ntext representative of the “ideal” document. Based on this idea, we construct\nthe representative words prediction (ROP) task for pre-training. Given an input\ndocument, we sample a pair of word sets according to the document language\nmodel, where the set with higher likelihood is deemed as more representative of\nthe document. We then pre-train the Transformer model to predict the pairwise\npreference between the two word sets, jointly with the Masked Language Model\n(MLM) objective. By further fine-tuning on a variety of representative\ndownstream ad-hoc retrieval tasks, PROP achieves significant improvements over\nbaselines without pre-training or with other pre-training methods. We also show\nthat PROP can achieve exciting performance under both the zero- and\nlow-resource IR settings. The code and pre-trained models are available at\nhttps://github.com/Albert-Ma/PROP.</p>\n", "tags": ["Fine-Tuning","Has Code","Model Architecture","Retrieval Systems","Training Techniques"] },
{"key": "ma2020zero", "citations": "69", "year": "2021", "title":"Zero-shot Neural Passage Retrieval Via Domain-targeted Synthetic Question Generation", "abstract": "<p>A major obstacle to the wide-spread adoption of neural retrieval models is\nthat they require large supervised training sets to surpass traditional\nterm-based techniques, which are constructed from raw corpora. In this paper,\nwe propose an approach to zero-shot learning for passage retrieval that uses\nsynthetic question generation to close this gap. The question generation system\nis trained on general domain data, but is applied to documents in the targeted\ndomain. This allows us to create arbitrarily large, yet noisy, question-passage\nrelevance pairs that are domain specific. Furthermore, when this is coupled\nwith a simple hybrid term-neural model, first-stage retrieval performance can\nbe improved further. Empirically, we show that this is an effective strategy\nfor building neural passage retrieval models in the absence of large training\ncorpora. Depending on the domain, this technique can even approach the accuracy\nof supervised models.</p>\n", "tags": ["EACL","NAACL","Training Techniques"] },
{"key": "ma2021end", "citations": "166", "year": "2021", "title":"End-to-end Audio-visual Speech Recognition With Conformers", "abstract": "<p>In this work, we present a hybrid CTC/Attention model based on a ResNet-18\nand Convolution-augmented transformer (Conformer), that can be trained in an\nend-to-end manner. In particular, the audio and visual encoders learn to\nextract features directly from raw pixels and audio waveforms, respectively,\nwhich are then fed to conformers and then fusion takes place via a Multi-Layer\nPerceptron (MLP). The model learns to recognise characters using a combination\nof CTC and an attention mechanism. We show that end-to-end training, instead of\nusing pre-computed visual features which is common in the literature, the use\nof a conformer, instead of a recurrent network, and the use of a\ntransformer-based language model, significantly improve the performance of our\nmodel. We present results on the largest publicly available datasets for\nsentence-level speech recognition, Lip Reading Sentences 2 (LRS2) and Lip\nReading Sentences 3 (LRS3), respectively. The results show that our proposed\nmodels raise the state-of-the-art performance by a large margin in audio-only,\nvisual-only, and audio-visual experiments.</p>\n", "tags": ["Datasets","ICASSP","Model Architecture","Training Techniques"] },
{"key": "ma2021template", "citations": "84", "year": "2022", "title":"Template-free Prompt Tuning For Few-shot NER", "abstract": "<p>Prompt-based methods have been successfully applied in sentence-level\nfew-shot learning tasks, mostly owing to the sophisticated design of templates\nand label words. However, when applied to token-level labeling tasks such as\nNER, it would be time-consuming to enumerate the template queries over all\npotential entity spans. In this work, we propose a more elegant method to\nreformulate NER tasks as LM problems without any templates. Specifically, we\ndiscard the template construction process while maintaining the word prediction\nparadigm of pre-training models to predict a class-related pivot word (or label\nword) at the entity position. Meanwhile, we also explore principled ways to\nautomatically search for appropriate label words that the pre-trained models\ncan easily adapt to. While avoiding complicated template-based process, the\nproposed LM objective also reduces the gap between different objectives used in\npre-training and fine-tuning, thus it can better benefit the few-shot\nperformance. Experimental results demonstrate the effectiveness of the proposed\nmethod over bert-tagger and template-based method under few-shot setting.\nMoreover, the decoding speed of the proposed method is up to 1930.12 times\nfaster than the template-based method.</p>\n", "tags": ["Few-Shot","Fine-Tuning","Model Architecture","NAACL","Prompting","Training Techniques"] },
{"key": "ma2022prompt", "citations": "84", "year": "2022", "title":"Prompt For Extraction? PAIE: Prompting Argument Interaction For Event Argument Extraction", "abstract": "<p>In this paper, we propose an effective yet efficient model PAIE for both\nsentence-level and document-level Event Argument Extraction (EAE), which also\ngeneralizes well when there is a lack of training data. On the one hand, PAIE\nutilizes prompt tuning for extractive objectives to take the best advantages of\nPre-trained Language Models (PLMs). It introduces two span selectors based on\nthe prompt to select start/end tokens among input texts for each role. On the\nother hand, it captures argument interactions via multi-role prompts and\nconducts joint optimization with optimal span assignments via a bipartite\nmatching loss. Also, with a flexible prompt design, PAIE can extract multiple\narguments with the same role instead of conventional heuristic threshold\ntuning. We have conducted extensive experiments on three benchmarks, including\nboth sentence- and document-level EAE. The results present promising\nimprovements from PAIE (3.5% and 2.3% F1 gains in average on three\nbenchmarks, for PAIE-base and PAIE-large respectively). Further analysis\ndemonstrates the efficiency, generalization to few-shot settings, and\neffectiveness of different extractive prompt tuning strategies. Our code is\navailable at https://github.com/mayubo2333/PAIE.</p>\n", "tags": ["Few-Shot","Has Code","Prompting","Training Techniques"] },
{"key": "ma2023towards", "citations": "73", "year": "2023", "title":"Towards A Holistic Landscape Of Situated Theory Of Mind In Large Language Models", "abstract": "<p>Large Language Models (LLMs) have generated considerable interest and debate\nregarding their potential emergence of Theory of Mind (ToM). Several recent\ninquiries reveal a lack of robust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current ones primarily focus on different\naspects of ToM and are prone to shortcuts and data leakage. In this position\npaper, we seek to answer two road-blocking questions: (1) How can we taxonomize\na holistic landscape of machine ToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psychological studies, we taxonomize\nmachine ToM into 7 mental state categories and delineate existing benchmarks to\nidentify under-explored aspects of ToM. We argue for a holistic and situated\nevaluation of ToM to break ToM into individual components and treat LLMs as an\nagent who is physically situated in environments and socially situated in\ninteractions with humans. Such situated evaluation provides a more\ncomprehensive assessment of mental states and potentially mitigates the risk of\nshortcuts and data leakage. We further present a pilot study in a grid world\nsetup as a proof of concept. We hope this position paper can facilitate future\nresearch to integrate ToM with LLMs and offer an intuitive means for\nresearchers to better position their work in the landscape of ToM. Project\npage: https://github.com/Mars-tin/awesome-theory-of-mind</p>\n", "tags": ["Agentic","Evaluation","Has Code"] },
{"key": "maaz2023video", "citations": "105", "year": "2024", "title":"Video-chatgpt: Towards Detailed Video Understanding Via Large Vision And Language Models", "abstract": "<p>Conversation agents fueled by Large Language Models (LLMs) are providing a\nnew way to interact with visual data. While there have been initial attempts\nfor image-based conversation models, this work addresses the under-explored\nfield of <em>video-based conversation</em> by introducing Video-ChatGPT. It is a\nmultimodal model that merges a video-adapted visual encoder with an LLM. The\nresulting model is capable of understanding and generating detailed\nconversations about videos. We introduce a new dataset of 100,000\nvideo-instruction pairs used to train Video-ChatGPT acquired via manual and\nsemi-automated pipeline that is easily scalable and robust to label noise. We\nalso develop a quantitative evaluation framework for video-based dialogue\nmodels to objectively analyze the strengths and weaknesses of video-based\ndialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Tools"] },
{"key": "machines2020task", "citations": "100", "year": "2020", "title":"Task-oriented Dialogue As Dataflow Synthesis", "abstract": "<p>We describe an approach to task-oriented dialogue in which dialogue state is\nrepresented as a dataflow graph. A dialogue agent maps each user utterance to a\nprogram that extends this graph. Programs include metacomputation operators for\nreference and revision that reuse dataflow fragments from previous turns. Our\ngraph-based state enables the expression and manipulation of complex user\nintents, and explicit metacomputation makes these intents easier for learned\nmodels to predict. We introduce a new dataset, SMCalFlow, featuring complex\ndialogues about events, weather, places, and people. Experiments show that\ndataflow graphs and metacomputation substantially improve representability and\npredictability in these natural dialogues. Additional experiments on the\nMultiWOZ dataset show that our dataflow representation enables an otherwise\noff-the-shelf sequence-to-sequence model to match the best existing\ntask-specific state tracking model. The SMCalFlow dataset and code for\nreplicating experiments are available at\nhttps://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.</p>\n", "tags": ["Agentic","Datasets","TACL"] },
{"key": "macneil2022experiences", "citations": "134", "year": "2023", "title":"Experiences From Using Code Explanations Generated By Large Language Models In A Web Software Development E-book", "abstract": "<p>Advances in natural language processing have resulted in large language\nmodels (LLMs) that are capable of generating understandable and sensible\nwritten text. Recent versions of these models, such as OpenAI Codex and GPT-3,\ncan generate code and code explanations. However, it is unclear whether and how\nstudents might engage with such explanations. In this paper, we report on our\nexperiences generating multiple code explanation types using LLMs and\nintegrating them into an interactive e-book on web software development. We\nmodified the e-book to make LLM-generated code explanations accessible through\nbuttons next to code snippets in the materials, which allowed us to track the\nuse of the explanations as well as to ask for feedback on their utility. Three\ndifferent types of explanations were available for students for each\nexplainable code snippet; a line-by-line explanation, a list of important\nconcepts, and a high-level summary of the code. Our preliminary results show\nthat all varieties of explanations were viewed by students and that the\nmajority of students perceived the code explanations as helpful to them.\nHowever, student engagement appeared to vary by code snippet complexity,\nexplanation type, and code snippet length. Drawing on our experiences, we\ndiscuss future directions for integrating explanations generated by LLMs into\nexisting computer science classrooms.</p>\n", "tags": ["Llm For Code"] },
{"key": "madaan2023self", "citations": "151", "year": "2023", "title":"Self-refine: Iterative Refinement With Self-feedback", "abstract": "<p>Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.</p>\n", "tags": ["Evaluation","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "maddela2020controllable", "citations": "67", "year": "2021", "title":"Controllable Text Simplification With Explicit Paraphrasing", "abstract": "<p>Text Simplification improves the readability of sentences through several\nrewriting transformations, such as lexical paraphrasing, deletion, and\nsplitting. Current simplification systems are predominantly\nsequence-to-sequence models that are trained end-to-end to perform all these\noperations simultaneously. However, such systems limit themselves to mostly\ndeleting words and cannot easily adapt to the requirements of different target\naudiences. In this paper, we propose a novel hybrid approach that leverages\nlinguistically-motivated rules for splitting and deletion, and couples them\nwith a neural paraphrasing model to produce varied rewriting styles. We\nintroduce a new data augmentation method to improve the paraphrasing capability\nof our model. Through automatic and manual evaluations, we show that our\nproposed model establishes a new state-of-the-art for the task, paraphrasing\nmore often than the existing systems, and can control the degree of each\nsimplification operation applied to the input texts.</p>\n", "tags": ["NAACL"] },
{"key": "madotto2018mem2seq", "citations": "303", "year": "2018", "title":"Mem2seq: Effectively Incorporating Knowledge Bases Into End-to-end Task-oriented Dialog Systems", "abstract": "<p>End-to-end task-oriented dialog systems usually suffer from the challenge of\nincorporating knowledge bases. In this paper, we propose a novel yet simple\nend-to-end differentiable model called memory-to-sequence (Mem2Seq) to address\nthis issue. Mem2Seq is the first neural generative model that combines the\nmulti-hop attention over memories with the idea of pointer network. We\nempirically show how Mem2Seq controls each generation step, and how its\nmulti-hop attention mechanism helps in learning correlations between memories.\nIn addition, our model is quite general without complicated task-specific\ndesigns. As a result, we show that Mem2Seq can be trained faster and attain the\nstate-of-the-art performance on three different task-oriented dialog datasets.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "madotto2020continual", "citations": "65", "year": "2021", "title":"Continual Learning In Task-oriented Dialogue Systems", "abstract": "<p>Continual learning in task-oriented dialogue systems can allow us to add new\ndomains and functionalities through time without incurring the high cost of a\nwhole system retraining. In this paper, we propose a continual learning\nbenchmark for task-oriented dialogue systems with 37 domains to be learned\ncontinuously in four settings, such as intent recognition, state tracking,\nnatural language generation, and end-to-end. Moreover, we implement and compare\nmultiple existing continual learning baselines, and we propose a simple yet\neffective architectural method based on residual adapters. Our experiments\ndemonstrate that the proposed architectural method and a simple replay-based\nstrategy perform comparably well but they both achieve inferior performance to\nthe multi-task learning baseline, in where all the data are shown at once,\nshowing that continual learning in task-oriented dialogue systems is a\nchallenging task. Furthermore, we reveal several trade-offs between different\ncontinual learning methods in term of parameter usage and memory size, which\nare important in the design of a task-oriented dialogue system. The proposed\nbenchmark is released together with several baselines to promote more research\nin this direction.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","EMNLP","Evaluation"] },
{"key": "madsen2021post", "citations": "131", "year": "2022", "title":"Post-hoc Interpretability For Neural NLP: A Survey", "abstract": "<p>Neural networks for NLP are becoming increasingly complex and widespread, and\nthere is a growing concern if these models are responsible to use. Explaining\nmodels helps to address the safety and ethical concerns and is essential for\naccountability. Interpretability serves to provide these explanations in terms\nthat are understandable to humans. Additionally, post-hoc methods provide\nexplanations after a model is learned and are generally model-agnostic. This\nsurvey provides a categorization of how recent post-hoc interpretability\nmethods communicate explanations to humans, it discusses each method in-depth,\nand how they are validated, as the latter is often a common concern.</p>\n", "tags": ["Survey Paper"] },
{"key": "mager2020gpt", "citations": "80", "year": "2020", "title":"Gpt-too: A Language-model-first Approach For Amr-to-text Generation", "abstract": "<p>Meaning Representations (AMRs) are broad-coverage sentence-level semantic\ngraphs. Existing approaches to generating text from AMR have focused on\ntraining sequence-to-sequence or graph-to-sequence models on AMR annotated data\nonly. In this paper, we propose an alternative approach that combines a strong\npre-trained language model with cycle consistency-based re-scoring. Despite the\nsimplicity of the approach, our experimental results show these models\noutperform all previous techniques on the English LDC2017T10dataset, including\nthe recent use of transformer architectures. In addition to the standard\nevaluation metrics, we provide human evaluation experiments that further\nsubstantiate the strength of our approach.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "magister2022teaching", "citations": "60", "year": "2023", "title":"Teaching Small Language Models To Reason", "abstract": "<p>Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.</p>\n", "tags": ["Datasets","Prompting"] },
{"key": "magueresse2020low", "citations": "87", "year": "2020", "title":"Low-resource Languages: A Review Of Past Work And Future Challenges", "abstract": "<p>A current problem in NLP is massaging and processing low-resource languages\nwhich lack useful training attributes such as supervised data, number of native\nspeakers or experts, etc. This review paper concisely summarizes previous\ngroundbreaking achievements made towards resolving this problem, and analyzes\npotential improvements in the context of the overall future research direction.</p>\n", "tags": ["Survey Paper","Training Techniques"] },
{"key": "mahabadi2019end", "citations": "140", "year": "2020", "title":"End-to-end Bias Mitigation By Modelling Biases In Corpora", "abstract": "<p>Several recent studies have shown that strong natural language understanding\n(NLU) models are prone to relying on unwanted dataset biases without learning\nthe underlying task, resulting in models that fail to generalize to\nout-of-domain datasets and are likely to perform poorly in real-world\nscenarios. We propose two learning strategies to train neural models, which are\nmore robust to such biases and transfer better to out-of-domain datasets. The\nbiases are specified in terms of one or more bias-only models, which learn to\nleverage the dataset biases. During training, the bias-only models’ predictions\nare used to adjust the loss of the base model to reduce its reliance on biases\nby down-weighting the biased examples and focusing the training on the hard\nexamples. We experiment on large-scale natural language inference and fact\nverification benchmarks, evaluating on out-of-domain datasets that are\nspecifically designed to assess the robustness of models against known biases\nin the training data. Results show that our debiasing methods greatly improve\nrobustness in all settings and better transfer to other textual entailment\ndatasets. Our code and data are publicly available in\nhttps://github.com/rabeehk/robust-nli.</p>\n", "tags": ["Datasets","Ethics & Fairness","Has Code","Training Techniques"] },
{"key": "mahabadi2021compacter", "citations": "157", "year": "2021", "title":"Compacter: Efficient Low-rank Hypercomplex Adapter Layers", "abstract": "<p>Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers. Specifically, Compacter inserts task-specific weight matrices into a\npretrained model’s weights, which are computed efficiently as a sum of\nKronecker products between shared “slow” weights and “fast” rank-one matrices\ndefined per Compacter layer. By only training 0.047% of a pretrained model’s\nparameters, Compacter performs on par with standard fine-tuning on GLUE and\noutperforms standard fine-tuning on SuperGLUE and low-resource settings. Our\ncode is publicly available at~https://github.com/rabeehk/compacter.</p>\n", "tags": ["Efficiency","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "mahabadi2021parameter", "citations": "115", "year": "2021", "title":"Parameter-efficient Multi-task Fine-tuning For Transformers Via Shared Hypernetworks", "abstract": "<p>State-of-the-art parameter-efficient fine-tuning methods rely on introducing\nadapter modules between the layers of a pretrained language model. However,\nsuch modules are trained separately for each task and thus do not enable\nsharing information across tasks. In this paper, we show that we can learn\nadapter parameters for all layers and tasks by generating them using shared\nhypernetworks, which condition on task, adapter position, and layer id in a\ntransformer model. This parameter-efficient multi-task learning framework\nallows us to achieve the best of both worlds by sharing knowledge across tasks\nvia hypernetworks while enabling the model to adapt to each individual task\nthrough task-specific adapters. Experiments on the well-known GLUE benchmark\nshow improved performance in multi-task learning while adding only 0.29%\nparameters per task. We additionally demonstrate substantial performance\nimprovements in few-shot domain generalization across a variety of tasks. Our\ncode is publicly available in https://github.com/rabeehk/hyperformer.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "mahowald2023dissociating", "citations": "140", "year": "2024", "title":"Dissociating Language And Thought In Large Language Models", "abstract": "<p>Large Language Models (LLMs) have come closest among all models to date to\nmastering human language, yet opinions about their linguistic and cognitive\ncapabilities remain split. Here, we evaluate LLMs using a distinction between\nformal linguistic competence – knowledge of linguistic rules and patterns –\nand functional linguistic competence – understanding and using language in the\nworld. We ground this distinction in human neuroscience, which has shown that\nformal and functional competence rely on different neural mechanisms. Although\nLLMs are surprisingly good at formal competence, their performance on\nfunctional competence tasks remains spotty and often requires specialized\nfine-tuning and/or coupling with external modules. We posit that models that\nuse language in human-like ways would need to master both of these competence\ntypes, which, in turn, could require the emergence of mechanisms specialized\nfor formal linguistic competence, distinct from functional competence.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "mai2020communicative", "citations": "72", "year": "2021", "title":"Communicative Message Passing For Inductive Relation Reasoning", "abstract": "<p>Relation prediction for knowledge graphs aims at predicting missing\nrelationships between entities. Despite the importance of inductive relation\nprediction, most previous works are limited to a transductive setting and\ncannot process previously unseen entities. The recent proposed subgraph-based\nrelation reasoning models provided alternatives to predict links from the\nsubgraph structure surrounding a candidate triplet inductively. However, we\nobserve that these methods often neglect the directed nature of the extracted\nsubgraph and weaken the role of relation information in the subgraph modeling.\nAs a result, they fail to effectively handle the asymmetric/anti-symmetric\ntriplets and produce insufficient embeddings for the target triplets. To this\nend, we introduce a \\textbf{C}\\textbf{o}mmunicative \\textbf{M}essage\n\\textbf{P}assing neural network for \\textbf{I}nductive re\\textbf{L}ation\nr\\textbf{E}asoning, \\textbf{CoMPILE}, that reasons over local directed subgraph\nstructures and has a vigorous inductive bias to process entity-independent\nsemantic relations. In contrast to existing models, CoMPILE strengthens the\nmessage interactions between edges and entitles through a communicative kernel\nand enables a sufficient flow of relation information. Moreover, we demonstrate\nthat CoMPILE can naturally handle asymmetric/anti-symmetric relations without\nthe need for explosively increasing the number of model parameters by\nextracting the directed enclosing subgraphs. Extensive experiments show\nsubstantial performance gains in comparison to state-of-the-art methods on\ncommonly used benchmark datasets with variant inductive settings.</p>\n", "tags": ["AAAI","Datasets","Ethics & Fairness","Evaluation"] },
{"key": "majumdar2020improving", "citations": "139", "year": "2020", "title":"Improving Vision-and-language Navigation With Image-text Pairs From The Web", "abstract": "<p>Following a navigation instruction such as ‘Walk down the stairs and stop at\nthe brown sofa’ requires embodied AI agents to ground scene elements referenced\nvia language (e.g. ‘stairs’) to visual content in the environment (pixels\ncorresponding to ‘stairs’).\n  We ask the following question – can we leverage abundant ‘disembodied’\nweb-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn\nvisual groundings (what do ‘stairs’ look like?) that improve performance on a\nrelatively data-starved embodied perception task (Vision-and-Language\nNavigation)? Specifically, we develop VLN-BERT, a visiolinguistic\ntransformer-based model for scoring the compatibility between an instruction\n(‘…stop at the brown sofa’) and a sequence of panoramic RGB images captured\nby the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from\nthe web before fine-tuning on embodied path-instruction data significantly\nimproves performance on VLN – outperforming the prior state-of-the-art in the\nfully-observed setting by 4 absolute percentage points on success rate.\nAblations of our pretraining curriculum show each stage to be impactful – with\ntheir combination resulting in further positive synergistic effects.</p>\n", "tags": ["Agentic","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "majumder2019generating", "citations": "64", "year": "2019", "title":"Generating Personalized Recipes From Historical User Preferences", "abstract": "<p>Existing approaches to recipe generation are unable to create recipes for\nusers with culinary preferences but incomplete knowledge of ingredients in\nspecific dishes. We propose a new task of personalized recipe generation to\nhelp these users: expanding a name and incomplete ingredient details into\ncomplete natural-text instructions aligned with the user’s historical\npreferences. We attend on technique- and recipe-level representations of a\nuser’s previously consumed recipes, fusing these ‘user-aware’ representations\nin an attention fusion layer to control recipe text generation. Experiments on\na new dataset of 180K recipes and 700K interactions show our model’s ability to\ngenerate plausible and personalized recipes compared to non-personalized\nbaselines.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "majumder2020mime", "citations": "154", "year": "2020", "title":"MIME: Mimicking Emotions For Empathetic Response Generation", "abstract": "<p>Current approaches to empathetic response generation view the set of emotions\nexpressed in the input text as a flat structure, where all the emotions are\ntreated uniformly. We argue that empathetic responses often mimic the emotion\nof the user to a varying degree, depending on its positivity or negativity and\ncontent. We show that the consideration of this polarity-based emotion clusters\nand emotional mimicry results in improved empathy and contextual relevance of\nthe response as compared to the state-of-the-art. Also, we introduce\nstochasticity into the emotion mixture that yields emotionally more varied\nempathetic responses than the previous work. We demonstrate the importance of\nthese factors to empathetic response generation using both automatic- and\nhuman-based evaluations. The implementation of MIME is publicly available at\nhttps://github.com/declare-lab/MIME.</p>\n", "tags": ["EMNLP","Has Code"] },
{"key": "malinowski2016ask", "citations": "97", "year": "2017", "title":"Ask Your Neurons: A Deep Learning Approach To Visual Question Answering", "abstract": "<p>We address a question answering task on real-world images that is set up as a\nVisual Turing Test. By combining latest advances in image representation and\nnatural language processing, we propose Ask Your Neurons, a scalable, jointly\ntrained, end-to-end formulation to this problem.\n  In contrast to previous efforts, we are facing a multi-modal problem where\nthe language output (answer) is conditioned on visual and natural language\ninputs (image and question). We provide additional insights into the problem by\nanalyzing how much information is contained only in the language part for which\nwe provide a new human baseline. To study human consensus, which is related to\nthe ambiguities inherent in this challenging task, we propose two novel metrics\nand collect additional answers which extend the original DAQUAR dataset to\nDAQUAR-Consensus.\n  Moreover, we also extend our analysis to VQA, a large-scale question\nanswering about images dataset, where we investigate some particular design\nchoices and show the importance of stronger visual models. At the same time, we\nachieve strong performance of our model that still uses a global image\nrepresentation. Finally, based on such analysis, we refine our Ask Your Neurons\non DAQUAR, which also leads to a better performance on this challenging task.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "malinowski2018learning", "citations": "102", "year": "2018", "title":"Learning Visual Question Answering By Bootstrapping Hard Attention", "abstract": "<p>Attention mechanisms in biological perception are thought to select subsets\nof perceptual information for more sophisticated processing which would be\nprohibitive to perform on all sensory inputs. In computer vision, however,\nthere has been relatively little exploration of hard attention, where some\ninformation is selectively ignored, in spite of the success of soft attention,\nwhere information is re-weighted and aggregated, but never filtered out. Here,\nwe introduce a new approach for hard attention and find it achieves very\ncompetitive performance on a recently-released visual question answering\ndatasets, equalling and in some cases surpassing similar soft attention\narchitectures while entirely ignoring some features. Even though the hard\nattention mechanism is thought to be non-differentiable, we found that the\nfeature magnitudes correlate with semantic relevance, and provide a useful\nsignal for our mechanism’s attentional selection criterion. Because hard\nattention selects important features of the input information, it can also be\nmore efficient than analogous soft attention mechanisms. This is especially\nimportant for recent approaches that use non-local pairwise operations, whereby\ncomputational and memory costs are quadratic in the size of the set of\nfeatures.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "mallen2022when", "citations": "109", "year": "2023", "title":"When Not To Trust Language Models: Investigating Effectiveness Of Parametric And Non-parametric Memories", "abstract": "<p>Despite their impressive performance on diverse tasks, large language models\n(LMs) still struggle with tasks requiring rich world knowledge, implying the\nlimitations of relying solely on their parameters to encode a wealth of world\nknowledge. This paper aims to understand LMs’ strengths and limitations in\nmemorizing factual knowledge, by conducting large-scale knowledge probing\nexperiments of 10 models and 4 augmentation methods on PopQA, our new\nopen-domain QA dataset with 14k questions. We find that LMs struggle with less\npopular factual knowledge, and that scaling fails to appreciably improve\nmemorization of factual knowledge in the long tail. We then show that\nretrieval-augmented LMs largely outperform orders of magnitude larger LMs,\nwhile unassisted LMs remain competitive in questions about high-popularity\nentities. Based on those findings, we devise a simple, yet effective, method\nfor powerful and efficient retrieval-augmented LMs, which retrieves\nnon-parametric memories only when necessary. Experimental results show that\nthis significantly improves models’ performance while reducing the inference\ncosts.</p>\n", "tags": ["Datasets","RAG"] },
{"key": "malmasi2022multiconer", "citations": "78", "year": "2022", "title":"Multiconer: A Large-scale Multilingual Dataset For Complex Named Entity Recognition", "abstract": "<p>We present MultiCoNER, a large multilingual dataset for Named Entity\nRecognition that covers 3 domains (Wiki sentences, questions, and search\nqueries) across 11 languages, as well as multilingual and code-mixing subsets.\nThis dataset is designed to represent contemporary challenges in NER, including\nlow-context scenarios (short and uncased text), syntactically complex entities\nlike movie titles, and long-tail entity distributions. The 26M token dataset is\ncompiled from public resources using techniques such as heuristic-based\nsentence sampling, template extraction and slotting, and machine translation.\nWe applied two NER models on our dataset: a baseline XLM-RoBERTa model, and a\nstate-of-the-art GEMNET model that leverages gazetteers. The baseline achieves\nmoderate performance (macro-F1=54%), highlighting the difficulty of our data.\nGEMNET, which uses gazetteers, improvement significantly (average improvement\nof macro-F1=+30%). MultiCoNER poses challenges even for large pre-trained\nlanguage models, and we believe that it can help further research in building\nrobust NER systems. MultiCoNER is publicly available at\nhttps://registry.opendata.aws/multiconer/ and we hope that this resource will\nhelp advance research in various aspects of NER.</p>\n", "tags": ["Datasets"] },
{"key": "manakul2023selfcheckgpt", "citations": "142", "year": "2023", "title":"Selfcheckgpt: Zero-resource Black-box Hallucination Detection For Generative Large Language Models", "abstract": "<p>Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n“SelfCheckGPT”, a simple sampling-based approach that can be used to fact-check\nthe responses of black-box models in a zero-resource fashion, i.e. without an\nexternal database. SelfCheckGPT leverages the simple idea that if an LLM has\nknowledge of a given concept, sampled responses are likely to be similar and\ncontain consistent facts. However, for hallucinated facts, stochastically\nsampled responses are likely to diverge and contradict one another. We\ninvestigate this approach by using GPT-3 to generate passages about individuals\nfrom the WikiBio dataset, and manually annotate the factuality of the generated\npassages. We demonstrate that SelfCheckGPT can: i) detect non-factual and\nfactual sentences; and ii) rank passages in terms of factuality. We compare our\napproach to several baselines and show that our approach has considerably\nhigher AUC-PR scores in sentence-level hallucination detection and higher\ncorrelation scores in passage-level factuality assessment compared to grey-box\nmethods.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "mao2020generation", "citations": "120", "year": "2021", "title":"Generation-augmented Retrieval For Open-domain Question Answering", "abstract": "<p>We propose Generation-Augmented Retrieval (GAR) for answering open-domain\nquestions, which augments a query through text generation of heuristically\ndiscovered relevant contexts without external resources as supervision. We\ndemonstrate that the generated contexts substantially enrich the semantics of\nthe queries and GAR with sparse representations (BM25) achieves comparable or\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\nWe show that generating diverse contexts for a query is beneficial as fusing\ntheir results consistently yields better retrieval accuracy. Moreover, as\nsparse and dense representations are often complementary, GAR can be easily\ncombined with DPR to achieve even better performance. GAR achieves\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\nthe extractive QA setup when equipped with an extractive reader, and\nconsistently outperforms other retrieval methods when the same generative\nreader is used.</p>\n", "tags": ["Datasets","Retrieval Systems"] },
{"key": "marcus2022very", "citations": "78", "year": "2022", "title":"A Very Preliminary Analysis Of DALL-E 2", "abstract": "<p>The DALL-E 2 system generates original synthetic images corresponding to an\ninput text as caption. We report here on the outcome of fourteen tests of this\nsystem designed to assess its common sense, reasoning and ability to understand\ncomplex texts. All of our prompts were intentionally much more challenging than\nthe typical ones that have been showcased in recent weeks. Nevertheless, for 5\nout of the 14 prompts, at least one of the ten images fully satisfied our\nrequests. On the other hand, on no prompt did all of the ten images satisfy our\nrequests.</p>\n", "tags": ["Prompting"] },
{"key": "marie2021scientific", "citations": "61", "year": "2021", "title":"Scientific Credibility Of Machine Translation Research: A Meta-evaluation Of 769 Papers", "abstract": "<p>This paper presents the first large-scale meta-evaluation of machine\ntranslation (MT). We annotated MT evaluations conducted in 769 research papers\npublished from 2010 to 2020. Our study shows that practices for automatic MT\nevaluation have dramatically changed during the past decade and follow\nconcerning trends. An increasing number of MT evaluations exclusively rely on\ndifferences between BLEU scores to draw conclusions, without performing any\nkind of statistical significance testing nor human evaluation, while at least\n108 metrics claiming to be better than BLEU have been proposed. MT evaluations\nin recent papers tend to copy and compare automatic metric scores from previous\nwork to claim the superiority of a method or an algorithm without confirming\nneither exactly the same training, validating, and testing data have been used\nnor the metric scores are comparable. Furthermore, tools for reporting\nstandardized metric scores are still far from being widely adopted by the MT\ncommunity. After showing how the accumulation of these pitfalls leads to\ndubious evaluation, we propose a guideline to encourage better automatic MT\nevaluation along with a simple meta-evaluation scoring method to assess its\ncredibility.</p>\n", "tags": ["Evaluation Frameworks","Evaluation","Tools","Training Techniques"] },
{"key": "marino2019ok", "citations": "447", "year": "2019", "title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge", "abstract": "<p>Visual Question Answering (VQA) in its ideal form lets us study reasoning in\nthe joint space of vision and language and serves as a proxy for the AI task of\nscene understanding. However, most VQA benchmarks to date are focused on\nquestions such as simple counting, visual attributes, and object detection that\ndo not require reasoning or knowledge beyond what is in the image. In this\npaper, we address the task of knowledge-based visual question answering and\nprovide a benchmark, called OK-VQA, where the image content is not sufficient\nto answer the questions, encouraging methods that rely on external knowledge\nresources. Our new dataset includes more than 14,000 questions that require\nexternal knowledge to answer. We show that the performance of the\nstate-of-the-art VQA models degrades drastically in this new setting. Our\nanalysis shows that our knowledge-based VQA task is diverse, difficult, and\nlarge compared to previous knowledge-based VQA datasets. We hope that this\ndataset enables researchers to open up new avenues for research in this domain.\nSee http://okvqa.allenai.org to download and browse the dataset.</p>\n", "tags": ["CVPR","Datasets","Evaluation"] },
{"key": "marino2020krisp", "citations": "126", "year": "2021", "title":"KRISP: Integrating Implicit And Symbolic Knowledge For Open-domain Knowledge-based VQA", "abstract": "<p>One of the most challenging question types in VQA is when answering the\nquestion requires outside knowledge not present in the image. In this work we\nstudy open-domain knowledge, the setting when the knowledge required to answer\na question is not given/annotated, neither at training nor test time. We tap\ninto two types of knowledge representations and reasoning. First, implicit\nknowledge which can be learned effectively from unsupervised language\npre-training and supervised training data with transformer-based models.\nSecond, explicit, symbolic knowledge encoded in knowledge bases. Our approach\ncombines both - exploiting the powerful implicit reasoning of transformer\nmodels for answer prediction, and integrating symbolic representations from a\nknowledge graph, while never losing their explicit semantics to an implicit\nembedding. We combine diverse sources of knowledge to cover the wide variety of\nknowledge needed to solve knowledge-based questions. We show our approach,\nKRISP (Knowledge Reasoning with Implicit and Symbolic rePresentations),\nsignificantly outperforms state-of-the-art on OK-VQA, the largest available\ndataset for open-domain knowledge-based VQA. We show with extensive ablations\nthat while our model successfully exploits implicit knowledge reasoning, the\nsymbolic answer module which explicitly connects the knowledge graph to the\nanswer vocabulary is critical to the performance of our method and generalizes\nto rare answers.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "martin2019camembert", "citations": "412", "year": "2020", "title":"Camembert: A Tasty French Language Model", "abstract": "<p>Pretrained language models are now ubiquitous in Natural Language Processing.\nDespite their success, most available models have either been trained on\nEnglish data or on the concatenation of data in multiple languages. This makes\npractical use of such models –in all languages except English– very limited.\nIn this paper, we investigate the feasibility of training monolingual\nTransformer-based language models for other languages, taking French as an\nexample and evaluating our language models on part-of-speech tagging,\ndependency parsing, named entity recognition and natural language inference\ntasks. We show that the use of web crawled data is preferable to the use of\nWikipedia data. More surprisingly, we show that a relatively small web crawled\ndataset (4GB) leads to results that are as good as those obtained using larger\ndatasets (130+GB). Our best performing model CamemBERT reaches or improves the\nstate of the art in all four downstream tasks.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "maruf2019selective", "citations": "168", "year": "2019", "title":"Selective Attention For Context-aware Neural Machine Translation", "abstract": "<p>Despite the progress made in sentence-level NMT, current systems still fall\nshort at achieving fluent, good quality translation for a full document. Recent\nworks in context-aware NMT consider only a few previous sentences as context\nand may not scale to entire documents. To this end, we propose a novel and\nscalable top-down approach to hierarchical attention for context-aware NMT\nwhich uses sparse attention to selectively focus on relevant sentences in the\ndocument context and then attends to key words in those sentences. We also\npropose single-level attention approaches based on sentence or word-level\ninformation in the context. The document-level context representation, produced\nfrom these attention modules, is integrated into the encoder or decoder of the\nTransformer model depending on whether we use monolingual or bilingual context.\nOur experiments and evaluation on English-German datasets in different document\nMT settings show that our selective attention approach not only significantly\noutperforms context-agnostic baselines but also surpasses context-aware\nbaselines in most cases.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "marvin2018targeted", "citations": "401", "year": "2018", "title":"Targeted Syntactic Evaluation Of Language Models", "abstract": "<p>We present a dataset for evaluating the grammaticality of the predictions of\na language model. We automatically construct a large number of minimally\ndifferent pairs of English sentences, each consisting of a grammatical and an\nungrammatical sentence. The sentence pairs represent different variations of\nstructure-sensitive phenomena: subject-verb agreement, reflexive anaphora and\nnegative polarity items. We expect a language model to assign a higher\nprobability to the grammatical sentence than the ungrammatical one. In an\nexperiment using this data set, an LSTM language model performed poorly on many\nof the constructions. Multi-task training with a syntactic objective (CCG\nsupertagging) improved the LSTM’s accuracy, but a large gap remained between\nits performance and the accuracy of human participants recruited online. This\nsuggests that there is considerable room for improvement over LSTMs in\ncapturing syntax in a language model.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture","Training Techniques"] },
{"key": "mascharka2018transparency", "citations": "150", "year": "2018", "title":"Transparency By Design: Closing The Gap Between Performance And Interpretability In Visual Reasoning", "abstract": "<p>Visual question answering requires high-order reasoning about an image, which\nis a fundamental capability needed by machine systems to follow complex\ndirectives. Recently, modular networks have been shown to be an effective\nframework for performing visual reasoning tasks. While modular networks were\ninitially designed with a degree of model transparency, their performance on\ncomplex visual reasoning benchmarks was lacking. Current state-of-the-art\napproaches do not provide an effective mechanism for understanding the\nreasoning process. In this paper, we close the performance gap between\ninterpretable models and state-of-the-art visual reasoning methods. We propose\na set of visual-reasoning primitives which, when composed, manifest as a model\ncapable of performing complex reasoning tasks in an explicitly-interpretable\nmanner. The fidelity and interpretability of the primitives’ outputs enable an\nunparalleled ability to diagnose the strengths and weaknesses of the resulting\nmodel. Critically, we show that these primitives are highly performant,\nachieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show\nthat our model is able to effectively learn generalized representations when\nprovided a small amount of data containing novel object attributes. Using the\nCoGenT generalization task, we show more than a 20 percentage point improvement\nover the current state of the art.</p>\n", "tags": ["CVPR","Tools"] },
{"key": "mastropaolo2021studying", "citations": "181", "year": "2021", "title":"Studying The Usage Of Text-to-text Transfer Transformer To Support Code-related Tasks", "abstract": "<p>Deep learning (DL) techniques are gaining more and more attention in the\nsoftware engineering community. They have been used to support several\ncode-related tasks, such as automatic bug fixing and code comments generation.\nRecent studies in the Natural Language Processing (NLP) field have shown that\nthe Text-To-Text Transfer Transformer (T5) architecture can achieve\nstate-of-the-art performance for a variety of NLP tasks. The basic idea behind\nT5 is to first pre-train a model on a large and generic dataset using a\nself-supervised task ( e.g: filling masked words in sentences). Once the model\nis pre-trained, it is fine-tuned on smaller and specialized datasets, each one\nrelated to a specific task ( e.g: language translation, sentence\nclassification). In this paper, we empirically investigate how the T5 model\nperforms when pre-trained and fine-tuned to support code-related tasks. We\npre-train a T5 model on a dataset composed of natural language English text and\nsource code. Then, we fine-tune such a model by reusing datasets used in four\nprevious works that used DL techniques to: (i) fix bugs, (ii) inject code\nmutants, (iii) generate assert statements, and (iv) generate code comments. We\ncompared the performance of this single model with the results reported in the\nfour original papers proposing DL-based solutions for those four tasks. We show\nthat our T5 model, exploiting additional data for the self-supervised\npre-training phase, can achieve performance improvements over the four\nbaselines.</p>\n", "tags": ["Datasets","Llm For Code","Model Architecture","Training Techniques"] },
{"key": "mastropaolo2023robustness", "citations": "62", "year": "2023", "title":"On The Robustness Of Code Generation Techniques: An Empirical Study On Github Copilot", "abstract": "<p>Software engineering research has always being concerned with the improvement\nof code completion approaches, which suggest the next tokens a developer will\nlikely type while coding. The release of GitHub Copilot constitutes a big step\nforward, also because of its unprecedented ability to automatically generate\neven entire functions from their natural language description. While the\nusefulness of Copilot is evident, it is still unclear to what extent it is\nrobust. Specifically, we do not know the extent to which semantic-preserving\nchanges in the natural language description provided to the model have an\neffect on the generated code function. In this paper we present an empirical\nstudy in which we aim at understanding whether different but semantically\nequivalent natural language descriptions result in the same recommended\nfunction. A negative answer would pose questions on the robustness of deep\nlearning (DL)-based code generators since it would imply that developers using\ndifferent wordings to describe the same code would obtain different\nrecommendations. We asked Copilot to automatically generate 892 Java methods\nstarting from their original Javadoc description. Then, we generated different\nsemantically equivalent descriptions for each method both manually and\nautomatically, and we analyzed the extent to which predictions generated by\nCopilot changed. Our results show that modifying the description results in\ndifferent code recommendations in ~46% of cases. Also, differences in the\nsemantically equivalent descriptions might impact the correctness of the\ngenerated code ~28%.</p>\n", "tags": ["Has Code","Llm For Code","Security"] },
{"key": "mathew2020docvqa", "citations": "212", "year": "2021", "title":"Docvqa: A Dataset For VQA On Document Images", "abstract": "<p>We present a new dataset for Visual Question Answering (VQA) on document\nimages called DocVQA. The dataset consists of 50,000 questions defined on\n12,000+ document images. Detailed analysis of the dataset in comparison with\nsimilar datasets for VQA and reading comprehension is presented. We report\nseveral baseline results by adopting existing VQA and reading comprehension\nmodels. Although the existing models perform reasonably well on certain types\nof questions, there is large performance gap compared to human performance\n(94.36% accuracy). The models need to improve specifically on questions where\nunderstanding structure of the document is crucial. The dataset, code and\nleaderboard are available at docvqa.org</p>\n", "tags": ["Applications","Datasets","Evaluation"] },
{"key": "mathews2018semstyle", "citations": "132", "year": "2018", "title":"Semstyle: Learning To Generate Stylised Image Captions Using Unaligned Text", "abstract": "<p>Linguistic style is an essential part of written communication, with the\npower to affect both clarity and attractiveness. With recent advances in vision\nand language, we can start to tackle the problem of generating image captions\nthat are both visually grounded and appropriately styled. Existing approaches\neither require styled training captions aligned to images or generate captions\nwith low relevance. We develop a model that learns to generate visually\nrelevant styled captions from a large corpus of styled text without aligned\nimages. The core idea of this model, called SemStyle, is to separate semantics\nand style. One key component is a novel and concise semantic term\nrepresentation generated using natural language processing techniques and frame\nsemantics. In addition, we develop a unified language model that decodes\nsentences with diverse word choices and syntax for different styles.\nEvaluations, both automatic and manual, show captions from SemStyle preserve\nimage semantics, are descriptive, and are style shifted. More broadly, this\nwork provides possibilities to learn richer image descriptions from the\nplethora of linguistic data available on the web.</p>\n", "tags": ["CVPR","Datasets","Training Techniques"] },
{"key": "matiisen2017teacher", "citations": "238", "year": "2019", "title":"Teacher-student Curriculum Learning", "abstract": "<p>We propose Teacher-Student Curriculum Learning (TSCL), a framework for\nautomatic curriculum learning, where the Student tries to learn a complex task\nand the Teacher automatically chooses subtasks from a given set for the Student\nto train on. We describe a family of Teacher algorithms that rely on the\nintuition that the Student should practice more those tasks on which it makes\nthe fastest progress, i.e. where the slope of the learning curve is highest. In\naddition, the Teacher algorithms address the problem of forgetting by also\nchoosing tasks where the Student’s performance is getting worse. We demonstrate\nthat TSCL matches or surpasses the results of carefully hand-crafted curricula\nin two tasks: addition of decimal numbers with LSTM and navigation in\nMinecraft. Using our automatically generated curriculum enabled to solve a\nMinecraft maze that could not be solved at all when training directly on\nsolving the maze, and the learning was an order of magnitude faster than\nuniform sampling of subtasks.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "maynez2020faithfulness", "citations": "704", "year": "2020", "title":"On Faithfulness And Factuality In Abstractive Summarization", "abstract": "<p>It is well known that the standard likelihood training and approximate\ndecoding objectives in neural text generation models lead to less human-like\nresponses for open-ended tasks such as language modeling and story generation.\nIn this paper we have analyzed limitations of these models for abstractive\ndocument summarization and found that these models are highly prone to\nhallucinate content that is unfaithful to the input document. We conducted a\nlarge scale human evaluation of several neural abstractive summarization\nsystems to better understand the types of hallucinations they produce. Our\nhuman annotators found substantial amounts of hallucinated content in all model\ngenerated summaries. However, our analysis does show that pretrained models are\nbetter summarizers not only in terms of raw metrics, i.e., ROUGE, but also in\ngenerating faithful and factual summaries as evaluated by humans. Furthermore,\nwe show that textual entailment measures better correlate with faithfulness\nthan standard metrics, potentially leading the way to automatic evaluation\nmetrics as well as training and decoding criteria.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "mazaré2018training", "citations": "217", "year": "2018", "title":"Training Millions Of Personalized Dialogue Agents", "abstract": "<p>Current dialogue systems are not very engaging for users, especially when\ntrained end-to-end without relying on proactive reengaging scripted strategies.\nZhang et al. (2018) showed that the engagement level of end-to-end dialogue\nmodels increases when conditioning them on text personas providing some\npersonalized back-story to the model. However, the dataset used in Zhang et al.\n(2018) is synthetic and of limited size as it contains around 1k different\npersonas. In this paper we introduce a new dataset providing 5 million personas\nand 700 million persona-based dialogues. Our experiments show that, at this\nscale, training using personas still improves the performance of end-to-end\nsystems. In addition, we show that other tasks benefit from the wide coverage\nof our dataset by fine-tuning our model on the data from Zhang et al. (2018)\nand achieving state-of-the-art results.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "mccann2017learned", "citations": "538", "year": "2017", "title":"Learned In Translation: Contextualized Word Vectors", "abstract": "<p>Computer vision has benefited from initializing multiple deep layers with\nweights pretrained on large supervised training sets like ImageNet. Natural\nlanguage processing (NLP) typically sees initialization of only the lowest\nlayer of deep models with pretrained word vectors. In this paper, we use a deep\nLSTM encoder from an attentional sequence-to-sequence model trained for machine\ntranslation (MT) to contextualize word vectors. We show that adding these\ncontext vectors (CoVe) improves performance over using only unsupervised word\nand character vectors on a wide variety of common NLP tasks: sentiment analysis\n(SST, IMDb), question classification (TREC), entailment (SNLI), and question\nanswering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe\nimproves performance of our baseline models to the state of the art.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "mccann2018natural", "citations": "349", "year": "2018", "title":"The Natural Language Decathlon: Multitask Learning As Question Answering", "abstract": "<p>Deep learning has improved performance on many natural language processing\n(NLP) tasks individually. However, general NLP models cannot emerge within a\nparadigm that focuses on the particularities of a single metric, dataset, and\ntask. We introduce the Natural Language Decathlon (decaNLP), a challenge that\nspans ten tasks: question answering, machine translation, summarization,\nnatural language inference, sentiment analysis, semantic role labeling,\nzero-shot relation extraction, goal-oriented dialogue, semantic parsing, and\ncommonsense pronoun resolution. We cast all tasks as question answering over a\ncontext. Furthermore, we present a new Multitask Question Answering Network\n(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or\nparameters in the multitask setting. MQAN shows improvements in transfer\nlearning for machine translation and named entity recognition, domain\nadaptation for sentiment analysis and natural language inference, and zero-shot\ncapabilities for text classification. We demonstrate that the MQAN’s\nmulti-pointer-generator decoder is key to this success and performance further\nimproves with an anti-curriculum training strategy. Though designed for\ndecaNLP, MQAN also achieves state of the art results on the WikiSQL semantic\nparsing task in the single-task setting. We also release code for procuring and\nprocessing data, training and evaluating models, and reproducing all\nexperiments for decaNLP.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "mccarley2019structured", "citations": "77", "year": "2019", "title":"Structured Pruning Of A Bert-based Question Answering Model", "abstract": "<p>The recent trend in industry-setting Natural Language Processing (NLP)\nresearch has been to operate large %scale pretrained language models like BERT\nunder strict computational limits. While most model compression work has\nfocused on “distilling” a general-purpose language representation using\nexpensive pretraining distillation, less attention has been paid to creating\nsmaller task-specific language representations which, arguably, are more useful\nin an industry setting. In this paper, we investigate compressing BERT- and\nRoBERTa-based question answering systems by structured pruning of parameters\nfrom the underlying transformer model. We find that an inexpensive combination\nof task-specific structured pruning and task-specific distillation, without the\nexpense of pretraining distillation, yields highly-performing models across a\nrange of speed/accuracy tradeoff operating points. We start from existing\nfull-size models trained for SQuAD 2.0 or Natural Questions and introduce gates\nthat allow selected parts of transformers to be individually eliminated.\nSpecifically, we investigate (1) structured pruning to reduce the number of\nparameters in each transformer layer, (2) applicability to both BERT- and\nRoBERTa-based models, (3) applicability to both SQuAD 2.0 and Natural\nQuestions, and (4) combining structured pruning with distillation. We achieve a\nnear-doubling of inference speed with less than a 0.5 F1-point loss in short\nanswer accuracy on Natural Questions.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "mcclelland2019extending", "citations": "68", "year": "2019", "title":"Extending Machine Language Models Toward Human-level Language Understanding", "abstract": "<p>Language is crucial for human intelligence, but what exactly is its role? We\ntake language to be a part of a system for understanding and communicating\nabout situations. The human ability to understand and communicate about\nsituations emerges gradually from experience and depends on domain-general\nprinciples of biological neural networks: connection-based learning,\ndistributed representation, and context-sensitive, mutual constraint\nsatisfaction-based processing. Current artificial language processing systems\nrely on the same domain general principles, embodied in artificial neural\nnetworks. Indeed, recent progress in this field depends on <em>query-based\nattention</em>, which extends the ability of these systems to exploit context and\nhas contributed to remarkable breakthroughs. Nevertheless, most current models\nfocus exclusively on language-internal tasks, limiting their ability to perform\ntasks that depend on understanding situations. These systems also lack memory\nfor the contents of prior situations outside of a fixed contextual span. We\ndescribe the organization of the brain’s distributed understanding system,\nwhich includes a fast learning system that addresses the memory problem. We\nsketch a framework for future models of understanding drawing equally on\ncognitive neuroscience and artificial intelligence and exploiting query-based\nattention. We highlight relevant current directions and consider further\ndevelopments needed to fully capture human-level language understanding in a\ncomputational system.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "mccoy2019berts", "citations": "108", "year": "2020", "title":"Berts Of A Feather Do Not Generalize Together: Large Variability In Generalization Across Models With Similar Test Set Performance", "abstract": "<p>If the same neural network architecture is trained multiple times on the same\ndataset, will it make similar linguistic generalizations across runs? To study\nthis question, we fine-tuned 100 instances of BERT on the Multi-genre Natural\nLanguage Inference (MNLI) dataset and evaluated them on the HANS dataset, which\nevaluates syntactic generalization in natural language inference. On the MNLI\ndevelopment set, the behavior of all instances was remarkably consistent, with\naccuracy ranging between 83.6% and 84.8%. In stark contrast, the same models\nvaried widely in their generalization performance. For example, on the simple\ncase of subject-object swap (e.g., determining that “the doctor visited the\nlawyer” does not entail “the lawyer visited the doctor”), accuracy ranged from\n0.00% to 66.2%. Such variation is likely due to the presence of many local\nminima that are equally attractive to a low-bias learner such as a neural\nnetwork; decreasing the variability may therefore require models with stronger\ninductive biases.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Model Architecture"] },
{"key": "mccoy2019right", "citations": "945", "year": "2019", "title":"Right For The Wrong Reasons: Diagnosing Syntactic Heuristics In Natural Language Inference", "abstract": "<p>A machine learning system can score well on a given test set by relying on\nheuristics that are effective for frequent example types but break down in more\nchallenging cases. We study this issue within natural language inference (NLI),\nthe task of determining whether one sentence entails another. We hypothesize\nthat statistical NLI models may adopt three fallible syntactic heuristics: the\nlexical overlap heuristic, the subsequence heuristic, and the constituent\nheuristic. To determine whether models have adopted these heuristics, we\nintroduce a controlled evaluation set called HANS (Heuristic Analysis for NLI\nSystems), which contains many examples where the heuristics fail. We find that\nmodels trained on MNLI, including BERT, a state-of-the-art model, perform very\npoorly on HANS, suggesting that they have indeed adopted these heuristics. We\nconclude that there is substantial room for improvement in NLI systems, and\nthat the HANS dataset can motivate and measure progress in this area</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "mccoy2020does", "citations": "68", "year": "2020", "title":"Does Syntax Need To Grow On Trees? Sources Of Hierarchical Inductive Bias In Sequence-to-sequence Networks", "abstract": "<p>Learners that are exposed to the same training data might generalize\ndifferently due to differing inductive biases. In neural network models,\ninductive biases could in theory arise from any aspect of the model\narchitecture. We investigate which architectural factors affect the\ngeneralization behavior of neural sequence-to-sequence models trained on two\nsyntactic tasks, English question formation and English tense reinflection. For\nboth tasks, the training set is consistent with a generalization based on\nhierarchical structure and a generalization based on linear order. All\narchitectural factors that we investigated qualitatively affected how models\ngeneralized, including factors with no clear connection to hierarchical\nstructure. For example, LSTMs and GRUs displayed qualitatively different\ninductive biases. However, the only factor that consistently contributed a\nhierarchical bias across tasks was the use of a tree-structured model rather\nthan a model with sequential recurrence, suggesting that human-like syntactic\ngeneralization requires architectural syntactic structure.</p>\n", "tags": ["Ethics & Fairness","Model Architecture","TACL","Training Techniques"] },
{"key": "mcdonald2018deep", "citations": "114", "year": "2018", "title":"Deep Relevance Ranking Using Enhanced Document-query Interactions", "abstract": "<p>We explore several new models for document relevance ranking, building upon\nthe Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM,\nwhich uses context-insensitive encodings of terms and query-document term\ninteractions, we inject rich context-sensitive encodings throughout our models,\ninspired by PACRR’s (Hui et al., 2017) convolutional n-gram matching features,\nbut extended in several ways including multiple views of query and document\ninputs. We test our models on datasets from the BIOASQ question answering\nchallenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005),\nshowing they outperform BM25-based baselines, DRMM, and PACRR.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "mcguffie2020radicalization", "citations": "81", "year": "2020", "title":"The Radicalization Risks Of GPT-3 And Advanced Neural Language Models", "abstract": "<p>In this paper, we expand on our previous research of the potential for abuse\nof generative language models by assessing GPT-3. Experimenting with prompts\nrepresentative of different types of extremist narrative, structures of social\ninteraction, and radical ideologies, we find that GPT-3 demonstrates\nsignificant improvement over its predecessor, GPT-2, in generating extremist\ntexts. We also show GPT-3’s strength in generating text that accurately\nemulates interactive, informational, and influential content that could be\nutilized for radicalizing individuals into violent far-right extremist\nideologies and behaviors. While OpenAI’s preventative measures are strong, the\npossibility of unregulated copycat technology represents significant risk for\nlarge-scale online radicalization and recruitment; thus, in the absence of\nsafeguards, successful and efficient weaponization that requires little\nexperimentation is likely. AI stakeholders, the policymaking community, and\ngovernments should begin investing as soon as possible in building social\nnorms, public policy, and educational initiatives to preempt an influx of\nmachine-generated disinformation and propaganda. Mitigation will require\neffective policy and partnerships across industry, government, and civil\nsociety.</p>\n", "tags": ["Model Architecture"] },
{"key": "mckenna2023sources", "citations": "76", "year": "2023", "title":"Sources Of Hallucination By Large Language Models On Inference Tasks", "abstract": "<p>Large Language Models (LLMs) are claimed to be capable of Natural Language\nInference (NLI), necessary for applied tasks like question answering and\nsummarization. We present a series of behavioral studies on several LLM\nfamilies (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled\nexperiments. We establish two biases originating from pretraining which predict\nmuch of their behavior, and show that these are major sources of hallucination\nin generative LLMs. First, memorization at the level of sentences: we show\nthat, regardless of the premise, models falsely label NLI test samples as\nentailing when the hypothesis is attested in training data, and that entities\nare used as ``indices’’ to access the memorized data. Second, statistical\npatterns of usage learned at the level of corpora: we further show a similar\neffect when the premise predicate is less frequent than that of the hypothesis\nin the training data, a bias following from previous studies. We demonstrate\nthat LLMs perform significantly worse on NLI test samples which do not conform\nto these biases than those which do, and we offer these as valuable controls\nfor future LLM evaluation.</p>\n", "tags": ["EMNLP","Evaluation","Model Architecture","Training Techniques"] },
{"key": "mehri2019pretraining", "citations": "76", "year": "2019", "title":"Pretraining Methods For Dialog Context Representation Learning", "abstract": "<p>This paper examines various unsupervised pretraining objectives for learning\ndialog context representations. Two novel methods of pretraining dialog context\nencoders are proposed, and a total of four methods are examined. Each\npretraining objective is fine-tuned and evaluated on a set of downstream dialog\ntasks using the MultiWoz dataset and strong performance improvement is\nobserved. Further evaluation shows that our pretraining objectives result in\nnot only better performance, but also better convergence, models that are less\ndata hungry and have better domain generalizability.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Memory & Context"] },
{"key": "mehri2019structured", "citations": "93", "year": "2019", "title":"Structured Fusion Networks For Dialog", "abstract": "<p>Neural dialog models have exhibited strong performance, however their\nend-to-end nature lacks a representation of the explicit structure of dialog.\nThis results in a loss of generalizability, controllability and a data-hungry\nnature. Conversely, more traditional dialog systems do have strong models of\nexplicit structure. This paper introduces several approaches for explicitly\nincorporating structure into neural models of dialog. Structured Fusion\nNetworks first learn neural dialog modules corresponding to the structured\ncomponents of traditional dialog systems and then incorporate these modules in\na higher-level generative model. Structured Fusion Networks obtain strong\nresults on the MultiWOZ dataset, both with and without reinforcement learning.\nStructured Fusion Networks are shown to have several valuable properties,\nincluding better domain generalizability, improved performance in reduced data\nscenarios and robustness to divergence during reinforcement learning.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Reinforcement Learning"] },
{"key": "mehri2020dialoglue", "citations": "104", "year": "2020", "title":"Dialoglue: A Natural Language Understanding Benchmark For Task-oriented Dialogue", "abstract": "<p>A long-standing goal of task-oriented dialogue research is the ability to\nflexibly adapt dialogue models to new domains. To progress research in this\ndirection, we introduce DialoGLUE (Dialogue Language Understanding Evaluation),\na public benchmark consisting of 7 task-oriented dialogue datasets covering 4\ndistinct natural language understanding tasks, designed to encourage dialogue\nresearch in representation-based transfer, domain adaptation, and\nsample-efficient task learning. We release several strong baseline models,\ndemonstrating performance improvements over a vanilla BERT architecture and\nstate-of-the-art results on 5 out of 7 tasks, by pre-training on a large\nopen-domain dialogue corpus and task-adaptive self-supervised training. Through\nthe DialoGLUE benchmark, the baseline methods, and our evaluation scripts, we\nhope to facilitate progress towards the goal of developing more general\ntask-oriented dialogue models.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "mehri2020unsupervised", "citations": "83", "year": "2020", "title":"Unsupervised Evaluation Of Interactive Dialog With Dialogpt", "abstract": "<p>It is important to define meaningful and interpretable automatic evaluation\nmetrics for open-domain dialog research. Standard language generation metrics\nhave been shown to be ineffective for dialog. This paper introduces the FED\nmetric (fine-grained evaluation of dialog), an automatic evaluation metric\nwhich uses DialoGPT, without any fine-tuning or supervision. It also introduces\nthe FED dataset which is constructed by annotating a set of human-system and\nhuman-human conversations with eighteen fine-grained dialog qualities. The FED\nmetric (1) does not rely on a ground-truth response, (2) does not require\ntraining data and (3) measures fine-grained dialog qualities at both the turn\nand whole dialog levels. FED attains moderate to strong correlation with human\njudgement at both levels.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "mehri2020usr", "citations": "134", "year": "2020", "title":"USR: An Unsupervised And Reference Free Evaluation Metric For Dialog Generation", "abstract": "<p>The lack of meaningful automatic evaluation metrics for dialog has impeded\nopen-domain dialog research. Standard language generation metrics have been\nshown to be ineffective for evaluating dialog models. To this end, this paper\npresents USR, an UnSupervised and Reference-free evaluation metric for dialog.\nUSR is a reference-free metric that trains unsupervised models to measure\nseveral desirable qualities of dialog. USR is shown to strongly correlate with\nhuman judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and\nPersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces\ninterpretable measures for several desirable properties of dialog.</p>\n", "tags": ["Evaluation"] },
{"key": "mei2016coherent", "citations": "67", "year": "2017", "title":"Coherent Dialogue With Attention-based Language Models", "abstract": "<p>We model coherent conversation continuation via RNN-based dialogue models\nequipped with a dynamic attention mechanism. Our attention-RNN language model\ndynamically increases the scope of attention on the history as the conversation\ncontinues, as opposed to standard attention (or alignment) models with a fixed\ninput scope in a sequence-to-sequence model. This allows each generated word to\nbe associated with the most relevant words in its corresponding conversation\nhistory. We evaluate the model on two popular dialogue datasets, the\nopen-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot\ndataset, and achieve significant improvements over the state-of-the-art and\nbaselines on several metrics, including complementary diversity-based metrics,\nhuman evaluation, and qualitative visualizations. We also show that a vanilla\nRNN with dynamic attention outperforms more complex memory models (e.g., LSTM\nand GRU) by allowing for flexible, long-distance memory. We promote further\ncoherence via topic modeling-based reranking.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture"] },
{"key": "melamud2016role", "citations": "105", "year": "2016", "title":"The Role Of Context Types And Dimensionality In Learning Word Embeddings", "abstract": "<p>We provide the first extensive evaluation of how using different types of\ncontext to learn skip-gram word embeddings affects performance on a wide range\nof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic\ntasks tend to exhibit a clear preference to particular types of contexts and\nhigher dimensionality, more careful tuning is required for finding the optimal\nsettings for most of the extrinsic tasks that we considered. Furthermore, for\nthese extrinsic tasks, we find that once the benefit from increasing the\nembedding dimensionality is mostly exhausted, simple concatenation of word\nembeddings, learned with different context types, can yield further performance\ngains. As an additional contribution, we propose a new variant of the skip-gram\nmodel that learns word embeddings from weighted contexts of substitute words.</p>\n", "tags": ["Evaluation","NAACL"] },
{"key": "melis2017state", "citations": "320", "year": "2018", "title":"On The State Of The Art Of Evaluation In Neural Language Models", "abstract": "<p>Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "meng2016interactive", "citations": "65", "year": "2016", "title":"Interactive Attention For Neural Machine Translation", "abstract": "<p>Conventional attention-based Neural Machine Translation (NMT) conducts\ndynamic alignment in generating the target sentence. By repeatedly reading the\nrepresentation of source sentence, which keeps fixed after generated by the\nencoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced\nstate-of-the-art NMT. In this paper, we propose a new attention mechanism,\ncalled INTERACTIVE ATTENTION, which models the interaction between the decoder\nand the representation of source sentence during translation by both reading\nand writing operations. INTERACTIVE ATTENTION can keep track of the interaction\nhistory and therefore improve the translation performance. Experiments on NIST\nChinese-English translation task show that INTERACTIVE ATTENTION can achieve\nsignificant improvements over both the previous attention-based NMT baseline\nand some state-of-the-art variants of attention-based NMT (i.e., coverage\nmodels (Tu et al., 2016)). And neural machine translator with our INTERACTIVE\nATTENTION can outperform the open source attention-based NMT system Groundhog\nby 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU\npoints averagely on multiple test sets.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "meng2019conditional", "citations": "89", "year": "2019", "title":"Conditional Teacher-student Learning", "abstract": "<p>The teacher-student (T/S) learning has been shown to be effective for a\nvariety of problems such as domain adaptation and model compression. One\nshortcoming of the T/S learning is that a teacher model, not always perfect,\nsporadically produces wrong guidance in form of posterior probabilities that\nmisleads the student model towards a suboptimal performance. To overcome this\nproblem, we propose a conditional T/S learning scheme, in which a “smart”\nstudent model selectively chooses to learn from either the teacher model or the\nground truth labels conditioned on whether the teacher can correctly predict\nthe ground truth. Unlike a naive linear combination of the two knowledge\nsources, the conditional learning is exclusively engaged with the teacher model\nwhen the teacher model’s prediction is correct, and otherwise backs off to the\nground truth. Thus, the student model is able to learn effectively from the\nteacher and even potentially surpass the teacher. We examine the proposed\nlearning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker\nadaptation on Microsoft short message dictation dataset. The proposed method\nachieves 9.8% and 12.8% relative word error rate reductions, respectively, over\nT/S learning for environment adaptation and speaker-independent model for\nspeaker adaptation.</p>\n", "tags": ["Datasets","Fine-Tuning","ICASSP"] },
{"key": "meng2020internal", "citations": "83", "year": "2021", "title":"Internal Language Model Estimation For Domain-adaptive End-to-end Speech Recognition", "abstract": "<p>The external language models (LM) integration remains a challenging task for\nend-to-end (E2E) automatic speech recognition (ASR) which has no clear division\nbetween acoustic and language models. In this work, we propose an internal LM\nestimation (ILME) method to facilitate a more effective integration of the\nexternal LM with all pre-existing E2E models with no additional model training,\nincluding the most popular recurrent neural network transducer (RNN-T) and\nattention-based encoder-decoder (AED) models. Trained with audio-transcript\npairs, an E2E model implicitly learns an internal LM that characterizes the\ntraining data in the source domain. With ILME, the internal LM scores of an E2E\nmodel are estimated and subtracted from the log-linear interpolation between\nthe scores of the E2E model and the external LM. The internal LM scores are\napproximated as the output of an E2E model when eliminating its acoustic\ncomponents. ILME can alleviate the domain mismatch between training and\ntesting, or improve the multi-domain E2E ASR. Experimented with 30K-hour\ntrained RNN-T and AED models, ILME achieves up to 15.5% and 6.8% relative word\nerror rate reductions from Shallow Fusion on out-of-domain LibriSpeech and\nin-domain Microsoft production test sets, respectively.</p>\n", "tags": ["Evaluation","Model Architecture","SLT","Training Techniques"] },
{"key": "meng2021coco", "citations": "121", "year": "2021", "title":"COCO-LM: Correcting And Contrasting Text Sequences For Language Model Pretraining", "abstract": "<p>We present a self-supervised learning framework, COCO-LM, that pretrains\nLanguage Models by COrrecting and COntrasting corrupted text sequences.\nFollowing ELECTRA-style pretraining, COCO-LM employs an auxiliary language\nmodel to corrupt text sequences, upon which it constructs two new tasks for\npretraining the main model. The first token-level task, Corrective Language\nModeling, is to detect and correct tokens replaced by the auxiliary model, in\norder to better capture token-level semantics. The second sequence-level task,\nSequence Contrastive Learning, is to align text sequences originated from the\nsame source input while ensuring uniformity in the representation space.\nExperiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms\nrecent state-of-the-art pretrained models in accuracy, but also improves\npretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of\nits pretraining GPU hours. With the same pretraining steps of standard\nbase/large-sized models, COCO-LM outperforms the previous best models by 1+\nGLUE average points.</p>\n", "tags": ["Efficiency","Tools","Training Techniques"] },
{"key": "meng2022generating", "citations": "72", "year": "2022", "title":"Generating Training Data With Language Models: Towards Zero-shot Language Understanding", "abstract": "<p>Pretrained language models (PLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks: Unidirectional PLMs (e.g., GPT) are\nwell known for their superior text generation capabilities; bidirectional PLMs\n(e.g., BERT) have been the prominent choice for natural language understanding\n(NLU) tasks. While both types of models have achieved promising few-shot\nlearning performance, their potential for zero-shot learning has been\nunderexplored. In this paper, we present a simple approach that uses both types\nof PLMs for fully zero-shot learning of NLU tasks without requiring any\ntask-specific data: A unidirectional PLM generates class-conditioned texts\nguided by prompts, which are used as the training data for fine-tuning a\nbidirectional PLM. With quality training data selected based on the generation\nprobability and regularization techniques (label smoothing and temporal\nensembling) applied to the fine-tuning stage for better generalization and\nstability, our approach demonstrates strong performance across seven\nclassification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and\n92.8 on SST-2), significantly outperforming zero-shot prompting methods and\nachieving even comparable results to strong few-shot approaches using 32\ntraining samples per class.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "meng2022locating", "citations": "154", "year": "2022", "title":"Locating And Editing Factual Associations In GPT", "abstract": "<p>We analyze the storage and recall of factual associations in autoregressive\ntransformer language models, finding evidence that these associations\ncorrespond to localized, directly-editable computations. We first develop a\ncausal intervention for identifying neuron activations that are decisive in a\nmodel’s factual predictions. This reveals a distinct set of steps in\nmiddle-layer feed-forward modules that mediate factual predictions while\nprocessing subject tokens. To test our hypothesis that these computations\ncorrespond to factual association recall, we modify feed-forward weights to\nupdate specific factual associations using Rank-One Model Editing (ROME). We\nfind that ROME is effective on a standard zero-shot relation extraction (zsRE)\nmodel-editing task, comparable to existing methods. To perform a more sensitive\nevaluation, we also evaluate ROME on a new dataset of counterfactual\nassertions, on which it simultaneously maintains both specificity and\ngeneralization, whereas other methods sacrifice one or another. Our results\nconfirm an important role for mid-layer feed-forward modules in storing factual\nassociations and suggest that direct manipulation of computational mechanisms\nmay be a feasible approach for model editing. The code, dataset,\nvisualizations, and an interactive demo notebook are available at\nhttps://rome.baulab.info/</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "merchant2020what", "citations": "126", "year": "2020", "title":"What Happens To BERT Embeddings During Fine-tuning?", "abstract": "<p>While there has been much recent work studying how linguistic information is\nencoded in pre-trained sentence representations, comparatively little is\nunderstood about how these models change when adapted to solve downstream\ntasks. Using a suite of analysis techniques (probing classifiers,\nRepresentational Similarity Analysis, and model ablations), we investigate how\nfine-tuning affects the representations of the BERT model. We find that while\nfine-tuning necessarily makes significant changes, it does not lead to\ncatastrophic forgetting of linguistic phenomena. We instead find that\nfine-tuning primarily affects the top layers of BERT, but with noteworthy\nvariation across tasks. In particular, dependency parsing reconfigures most of\nthe model, whereas SQuAD and MNLI appear to involve much shallower processing.\nFinally, we also find that fine-tuning has a weaker effect on representations\nof out-of-domain sentences, suggesting room for improvement in model\ngeneralization.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "merity2016pointer", "citations": "683", "year": "2016", "title":"Pointer Sentinel Mixture Models", "abstract": "<p>Recent neural network sequence models with softmax classifiers have achieved\ntheir best language modeling performance only with very large hidden states and\nlarge vocabularies. Even then they struggle to predict rare or unseen words\neven if the context makes the prediction unambiguous. We introduce the pointer\nsentinel mixture architecture for neural sequence models which has the ability\nto either reproduce a word from the recent context or produce a word from a\nstandard softmax classifier. Our pointer sentinel-LSTM model achieves state of\nthe art language modeling performance on the Penn Treebank (70.9 perplexity)\nwhile using far fewer parameters than a standard softmax LSTM. In order to\nevaluate how well language models can exploit longer contexts and deal with\nmore realistic vocabularies and larger corpora we also introduce the freely\navailable WikiText corpus.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "merity2017regularizing", "citations": "430", "year": "2017", "title":"Regularizing And Optimizing LSTM Language Models", "abstract": "<p>Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.</p>\n", "tags": ["Memory & Context","Model Architecture"] },
{"key": "merkx2020human", "citations": "77", "year": "2021", "title":"Human Sentence Processing: Recurrence Or Attention?", "abstract": "<p>Recurrent neural networks (RNNs) have long been an architecture of interest\nfor computational models of human sentence processing. The recently introduced\nTransformer architecture outperforms RNNs on many natural language processing\ntasks but little is known about its ability to model human language processing.\nWe compare Transformer- and RNN-based language models’ ability to account for\nmeasures of human reading effort. Our analysis shows Transformers to outperform\nRNNs in explaining self-paced reading times and neural activity during reading\nEnglish sentences, challenging the widely held idea that human sentence\nprocessing involves recurrent and immediate processing and provides evidence\nfor cue-based retrieval.</p>\n", "tags": ["Model Architecture"] },
{"key": "messina2020fine", "citations": "116", "year": "2021", "title":"Fine-grained Visual Textual Alignment For Cross-modal Retrieval Using Transformer Encoders", "abstract": "<p>Despite the evolution of deep-learning-based visual-textual processing\nsystems, precise multi-modal matching remains a challenging task. In this work,\nwe tackle the task of cross-modal retrieval through image-sentence matching\nbased on word-region alignments, using supervision only at the global\nimage-sentence level. Specifically, we present a novel approach called\nTransformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a\nfine-grained match between the underlying components of images and sentences,\ni.e., image regions and words, respectively, in order to preserve the\ninformative richness of both modalities. TERAN obtains state-of-the-art results\non the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover,\non MS-COCO, it also outperforms current approaches on the sentence retrieval\ntask.\n  Focusing on scalable cross-modal information retrieval, TERAN is designed to\nkeep the visual and textual data pipelines well separated. Cross-attention\nlinks invalidate any chance to separately extract visual and textual features\nneeded for the online search and the offline indexing steps in large-scale\nretrieval systems. In this respect, TERAN merges the information from the two\ndomains only during the final alignment phase, immediately before the loss\ncomputation. We argue that the fine-grained alignments produced by TERAN pave\nthe way towards the research for effective and efficient methods for\nlarge-scale cross-modal information retrieval. We compare the effectiveness of\nour approach against relevant state-of-the-art methods. On the MS-COCO 1K test\nset, we obtain an improvement of 5.7% and 3.5% respectively on the image and\nthe sentence retrieval tasks on the Recall@1 metric. The code used for the\nexperiments is publicly available on GitHub at\nhttps://github.com/mesnico/TERAN.</p>\n", "tags": ["Applications","Datasets","Has Code","Model Architecture","Retrieval Systems"] },
{"key": "mi2016coverage", "citations": "137", "year": "2016", "title":"Coverage Embedding Models For Neural Machine Translation", "abstract": "<p>In this paper, we enhance the attention-based neural machine translation\n(NMT) by adding explicit coverage embedding models to alleviate issues of\nrepeating and dropping translations in NMT. For each source word, our model\nstarts with a full coverage embedding vector to track the coverage status, and\nthen keeps updating it with neural networks as the translation goes.\nExperiments on the large-scale Chinese-to-English task show that our enhanced\nmodel improves the translation quality significantly on various test sets over\nthe strong large vocabulary NMT system.</p>\n", "tags": ["EMNLP","Evaluation","Model Architecture"] },
{"key": "mi2016supervised", "citations": "129", "year": "2016", "title":"Supervised Attentions For Neural Machine Translation", "abstract": "<p>In this paper, we improve the attention or alignment accuracy of neural\nmachine translation by utilizing the alignments of training sentence pairs. We\nsimply compute the distance between the machine attentions and the “true”\nalignments, and minimize this cost in the training procedure. Our experiments\non large-scale Chinese-to-English task show that our model improves both\ntranslation and alignment qualities significantly over the large-vocabulary\nneural machine translation system, and even beats a state-of-the-art\ntraditional syntax-based system.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "mi2016vocabulary", "citations": "68", "year": "2016", "title":"Vocabulary Manipulation For Neural Machine Translation", "abstract": "<p>In order to capture rich language phenomena, neural machine translation\nmodels have to use a large vocabulary size, which requires high computing time\nand large memory usage. In this paper, we alleviate this issue by introducing a\nsentence-level or batch-level vocabulary, which is only a very small sub-set of\nthe full output vocabulary. For each sentence or batch, we only predict the\ntarget words in its sentence-level or batch-level vocabulary. Thus, we reduce\nboth the computing time and the memory usage. Our method simply takes into\naccount the translation options of each word or phrase in the source sentence,\nand picks a very small target vocabulary for each sentence based on a\nword-to-word translation model or a bilingual phrase library learned from a\ntraditional machine translation model. Experimental results on the large-scale\nEnglish-to-French task show that our method achieves better translation\nperformance by 1 BLEU point over the large vocabulary neural machine\ntranslation system of Jean et al. (2015).</p>\n", "tags": ["Tools"] },
{"key": "mi2019meta", "citations": "85", "year": "2019", "title":"Meta-learning For Low-resource Natural Language Generation In Task-oriented Dialogue Systems", "abstract": "<p>Natural language generation (NLG) is an essential component of task-oriented\ndialogue systems. Despite the recent success of neural approaches for NLG, they\nare typically developed for particular domains with rich annotated training\nexamples. In this paper, we study NLG in a low-resource setting to generate\nsentences in new scenarios with handful training examples. We formulate the\nproblem from a meta-learning perspective, and propose a generalized\noptimization-based approach (Meta-NLG) based on the well-recognized\nmodel-agnostic meta-learning (MAML) algorithm. Meta-NLG defines a set of meta\ntasks, and directly incorporates the objective of adapting to new low-resource\nNLG tasks into the meta-learning optimization process. Extensive experiments\nare conducted on a large multi-domain dataset (MultiWoz) with diverse\nlinguistic variations. We show that Meta-NLG significantly outperforms other\ntraining procedures in various low-resource configurations. We analyze the\nresults, and demonstrate that Meta-NLG adapts extremely fast and well to\nlow-resource situations.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","IJCAI","Training Techniques"] },
{"key": "miao2016language", "citations": "195", "year": "2016", "title":"Language As A Latent Variable: Discrete Generative Models For Sentence Compression", "abstract": "<p>In this work we explore deep generative models of text in which the latent\nrepresentation of a document is itself drawn from a discrete language model\ndistribution. We formulate a variational auto-encoder for inference in this\nmodel and apply it to the task of compressing sentences. In this application\nthe generative model first draws a latent summary sentence from a background\nlanguage model, and then subsequently draws the observed sentence conditioned\non this latent summary. In our empirical evaluation we show that generative\nformulations of both abstractive and extractive compression yield\nstate-of-the-art results when trained on a large amount of supervised data.\nFurther, we explore semi-supervised compression scenarios where we show that it\nis possible to achieve performance competitive with previously proposed\nsupervised models while training on a fraction of the supervised data.</p>\n", "tags": ["EMNLP","Evaluation","Training Techniques"] },
{"key": "miao2020transformer", "citations": "98", "year": "2020", "title":"Transformer-based Online Ctc/attention End-to-end Speech Recognition Architecture", "abstract": "<p>Recently, Transformer has gained success in automatic speech recognition\n(ASR) field. However, it is challenging to deploy a Transformer-based\nend-to-end (E2E) model for online speech recognition. In this paper, we propose\nthe Transformer-based online CTC/attention E2E ASR architecture, which contains\nthe chunk self-attention encoder (chunk-SAE) and the monotonic truncated\nattention (MTA) based self-attention decoder (SAD). Firstly, the chunk-SAE\nsplits the speech into isolated chunks. To reduce the computational cost and\nimprove the performance, we propose the state reuse chunk-SAE. Sencondly, the\nMTA based SAD truncates the speech features monotonically and performs\nattention on the truncated features. To support the online recognition, we\nintegrate the state reuse chunk-SAE and the MTA based SAD into online\nCTC/attention architecture. We evaluate the proposed online models on the HKUST\nMandarin ASR benchmark and achieve a 23.66% character error rate (CER) with a\n320 ms latency. Our online model yields as little as 0.19% absolute CER\ndegradation compared with the offline baseline, and achieves significant\nimprovement over our prior work on Long Short-Term Memory (LSTM) based online\nE2E models.</p>\n", "tags": ["Evaluation","ICASSP","Model Architecture"] },
{"key": "michael2017crowdsourcing", "citations": "79", "year": "2018", "title":"Crowdsourcing Question-answer Meaning Representations", "abstract": "<p>We introduce Question-Answer Meaning Representations (QAMRs), which represent\nthe predicate-argument structure of a sentence as a set of question-answer\npairs. We also develop a crowdsourcing scheme to show that QAMRs can be labeled\nwith very little training, and gather a dataset with over 5,000 sentences and\n100,000 questions. A detailed qualitative analysis demonstrates that the\ncrowd-generated question-answer pairs cover the vast majority of\npredicate-argument relationships in existing datasets (including PropBank,\nNomBank, QA-SRL, and AMR) along with many previously under-resourced ones,\nincluding implicit arguments and relations. The QAMR data and annotation code\nis made publicly available to enable future work on how best to model these\ncomplex phenomena.</p>\n", "tags": ["Datasets","NAACL","Training Techniques"] },
{"key": "michael2019evaluating", "citations": "107", "year": "2019", "title":"Evaluating Sequence-to-sequence Models For Handwritten Text Recognition", "abstract": "<p>Encoder-decoder models have become an effective approach for sequence\nlearning tasks like machine translation, image captioning and speech\nrecognition, but have yet to show competitive results for handwritten text\nrecognition. To this end, we propose an attention-based sequence-to-sequence\nmodel. It combines a convolutional neural network as a generic feature\nextractor with a recurrent neural network to encode both the visual\ninformation, as well as the temporal context between characters in the input\nimage, and uses a separate recurrent neural network to decode the actual\ncharacter sequence. We make experimental comparisons between various attention\nmechanisms and positional encodings, in order to find an appropriate alignment\nbetween the input and output sequence. The model can be trained end-to-end and\nthe optional integration of a hybrid loss allows the encoder to retain an\ninterpretable and usable output, if desired. We achieve competitive results on\nthe IAM and ICFHR2016 READ data sets compared to the state-of-the-art without\nthe use of a language model, and we significantly improve over any recent\nsequence-to-sequence approaches.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "michel2018extreme", "citations": "98", "year": "2018", "title":"Extreme Adaptation For Personalized Neural Machine Translation", "abstract": "<p>Every person speaks or writes their own flavor of their native language,\ninfluenced by a number of factors: the content they tend to talk about, their\ngender, their social status, or their geographical origin.\n  When attempting to perform Machine Translation (MT), these variations have a\nsignificant effect on how the system should perform translation, but this is\nnot captured well by standard one-size-fits-all models.\n  In this paper, we propose a simple and parameter-efficient adaptation\ntechnique that only requires adapting the bias of the output softmax to each\nparticular user of the MT system, either directly or through a factored\napproximation.\n  Experiments on TED talks in three languages demonstrate improvements in\ntranslation accuracy, and better reflection of speaker traits in the target\ntext.</p>\n", "tags": ["Ethics & Fairness"] },
{"key": "michel2018mtnt", "citations": "137", "year": "2018", "title":"MTNT: A Testbed For Machine Translation Of Noisy Text", "abstract": "<p>Noisy or non-standard input text can cause disastrous mistranslations in most\nmodern Machine Translation (MT) systems, and there has been growing research\ninterest in creating noise-robust MT systems. However, as of yet there are no\npublicly available parallel corpora of with naturally occurring noisy inputs\nand translations, and thus previous work has resorted to evaluating on\nsynthetically created datasets. In this paper, we propose a benchmark dataset\nfor Machine Translation of Noisy Text (MTNT), consisting of noisy comments on\nReddit (www.reddit.com) and professionally sourced translations. We\ncommissioned translations of English comments into French and Japanese, as well\nas French and Japanese comments into English, on the order of 7k-37k sentences\nper language pair. We qualitatively and quantitatively examine the types of\nnoise included in this dataset, then demonstrate that existing MT models fail\nbadly on a number of noise-related phenomena, even after performing adaptation\non a small training set of in-domain data. This indicates that this dataset can\nprovide an attractive testbed for methods tailored to handling noisy text in\nMT. The data is publicly available at www.cs.cmu.edu/~pmichel1/mtnt/.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "michel2019are", "citations": "315", "year": "2019", "title":"Are Sixteen Heads Really Better Than One?", "abstract": "<p>Attention is a powerful and ubiquitous mechanism for allowing neural models\nto focus on particular salient pieces of information by taking their weighted\naverage when making predictions. In particular, multi-headed attention is a\ndriving force behind many recent state-of-the-art NLP models such as\nTransformer-based MT models and BERT. These models apply multiple attention\nmechanisms in parallel, with each attention “head” potentially focusing on\ndifferent parts of the input, which makes it possible to express sophisticated\nfunctions beyond the simple weighted average. In this paper we make the\nsurprising observation that even if models have been trained using multiple\nheads, in practice, a large percentage of attention heads can be removed at\ntest time without significantly impacting performance. In fact, some layers can\neven be reduced to a single head. We further examine greedy algorithms for\npruning down models, and the potential speed, memory efficiency, and accuracy\nimprovements obtainable therefrom. Finally, we analyze the results with respect\nto which parts of the model are more reliant on having multiple heads, and\nprovide precursory evidence that training dynamics play a role in the gains\nprovided by multi-head attention.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "michel2019evaluation", "citations": "118", "year": "2019", "title":"On Evaluation Of Adversarial Perturbations For Sequence-to-sequence Models", "abstract": "<p>Adversarial examples — perturbations to the input of a model that elicit\nlarge changes in the output — have been shown to be an effective way of\nassessing the robustness of sequence-to-sequence (seq2seq) models. However,\nthese perturbations only indicate weaknesses in the model if they do not change\nthe input so significantly that it legitimately results in changes in the\nexpected output. This fact has largely been ignored in the evaluations of the\ngrowing body of related literature. Using the example of untargeted attacks on\nmachine translation (MT), we propose a new evaluation framework for adversarial\nattacks on seq2seq models that takes the semantic equivalence of the pre- and\npost-perturbation input into account. Using this framework, we demonstrate that\nexisting methods may not preserve meaning in general, breaking the\naforementioned assumption that source side perturbations should not result in\nchanges in the expected output. We further use this framework to demonstrate\nthat adding additional constraints on attacks allows for adversarial\nperturbations that are more meaning-preserving, but nonetheless largely change\nthe output sequence. Finally, we show that performing untargeted adversarial\ntraining with meaning-preserving attacks is beneficial to the model in terms of\nadversarial robustness, without hurting test performance. A toolkit\nimplementing our evaluation framework is released at\nhttps://github.com/pmichel31415/teapot-nlp.</p>\n", "tags": ["Evaluation","Has Code","Security","Tools","Training Techniques"] },
{"key": "miculicich2018document", "citations": "281", "year": "2018", "title":"Document-level Neural Machine Translation With Hierarchical Attention Networks", "abstract": "<p>Neural Machine Translation (NMT) can be improved by including document-level\ncontextual information. For this purpose, we propose a hierarchical attention\nmodel to capture the context in a structured and dynamic manner. The model is\nintegrated in the original NMT architecture as another level of abstraction,\nconditioning on the NMT model’s own previous hidden states. Experiments show\nthat hierarchical attention significantly improves the BLEU score over a strong\nNMT baseline with the state-of-the-art in context-aware methods, and that both\nthe encoder and decoder benefit from context in complementary ways.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "miech2019howto100m", "citations": "782", "year": "2019", "title":"Howto100m: Learning A Text-video Embedding By Watching Hundred Million Narrated Video Clips", "abstract": "<p>Learning text-video embeddings usually requires a dataset of video clips with\nmanually provided captions. However, such datasets are expensive and time\nconsuming to create and therefore difficult to obtain on a large scale. In this\nwork, we propose instead to learn such embeddings from video data with readily\navailable natural language annotations in the form of automatically transcribed\nnarrations. The contributions of this work are three-fold. First, we introduce\nHowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M\nnarrated instructional web videos depicting humans performing and describing\nover 23k different visual tasks. Our data collection procedure is fast,\nscalable and does not require any additional manual annotation. Second, we\ndemonstrate that a text-video embedding trained on this data leads to\nstate-of-the-art results for text-to-video retrieval and action localization on\ninstructional video datasets such as YouCook2 or CrossTask. Finally, we show\nthat this embedding transfers well to other domains: fine-tuning on generic\nYoutube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models\ntrained on these datasets alone. Our dataset, code and models will be publicly\navailable at: www.di.ens.fr/willow/research/howto100m/.</p>\n", "tags": ["Datasets","Fine-Tuning","ICCV","Training Techniques"] },
{"key": "mihaylov2018can", "citations": "503", "year": "2018", "title":"Can A Suit Of Armor Conduct Electricity? A New Dataset For Open Book Question Answering", "abstract": "<p>We present a new kind of question answering dataset, OpenBookQA, modeled\nafter open book exams for assessing human understanding of a subject. The open\nbook that comes with our questions is a set of 1329 elementary level science\nfacts. Roughly 6000 questions probe an understanding of these facts and their\napplication to novel situations. This requires combining an open book fact\n(e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of\narmor is made of metal) obtained from other sources. While existing QA datasets\nover documents or knowledge bases, being generally self-contained, focus on\nlinguistic understanding, OpenBookQA probes a deeper understanding of both the\ntopic—in the context of common knowledge—and the language it is expressed\nin. Human performance on OpenBookQA is close to 92%, but many state-of-the-art\npre-trained QA methods perform surprisingly poorly, worse than several simple\nneural baselines we develop. Our oracle experiments designed to circumvent the\nknowledge retrieval bottleneck demonstrate the value of both the open book and\nadditional facts. We leave it as a challenge to solve the retrieval problem in\nthis multi-hop setting and to close the large gap to human performance.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "mihaylov2018knowledgeable", "citations": "190", "year": "2018", "title":"Knowledgeable Reader: Enhancing Cloze-style Reading Comprehension With External Commonsense Knowledge", "abstract": "<p>We introduce a neural reading comprehension model that integrates external\ncommonsense knowledge, encoded as a key-value memory, in a cloze-style setting.\nInstead of relying only on document-to-question interaction or discrete\nfeatures as in prior work, our model attends to relevant external knowledge and\ncombines this knowledge with the context representation before inferring the\nanswer. This allows the model to attract and imply knowledge from an external\nknowledge source that is not explicitly stated in the text, but that is\nrelevant for inferring the answer. Our model improves results over a very\nstrong baseline on a hard Common Nouns dataset, making it a strong competitor\nof much more complex models. By including knowledge explicitly, our model can\nalso provide evidence about the background knowledge used in the RC process.</p>\n", "tags": ["Datasets"] },
{"key": "miller2017explainable", "citations": "194", "year": "2017", "title":"Explainable AI: Beware Of Inmates Running The Asylum Or: How I Learnt To Stop Worrying And Love The Social And Behavioural Sciences", "abstract": "<p>In his seminal book <code class=\"language-plaintext highlighter-rouge\">The Inmates are Running the Asylum: Why High-Tech\nProducts Drive Us Crazy And How To Restore The Sanity' [2004, Sams\nIndianapolis, IN, USA], Alan Cooper argues that a major reason why software is\noften poorly designed (from a user perspective) is that programmers are in\ncharge of design decisions, rather than interaction designers. As a result,\nprogrammers design software for themselves, rather than for their target\naudience, a phenomenon he refers to as the </code>inmates running the asylum’. This\npaper argues that explainable AI risks a similar fate. While the re-emergence\nof explainable AI is positive, this paper argues most of us as AI researchers\nare building explanatory agents for ourselves, rather than for the intended\nusers. But explainable AI is more likely to succeed if researchers and\npractitioners understand, adopt, implement, and improve models from the vast\nand valuable bodies of research in philosophy, psychology, and cognitive\nscience, and if evaluation of these models is focused more on people than on\ntechnology. From a light scan of literature, we demonstrate that there is\nconsiderable scope to infuse more results from the social and behavioural\nsciences into explainable AI, and present some key results from these fields\nthat are relevant to explainable AI.</p>\n", "tags": ["Evaluation"] },
{"key": "miller2017parlai", "citations": "307", "year": "2017", "title":"Parlai: A Dialog Research Software Platform", "abstract": "<p>We introduce ParlAI (pronounced “par-lay”), an open-source software platform\nfor dialog research implemented in Python, available at http://parl.ai. Its\ngoal is to provide a unified framework for sharing, training and testing of\ndialog models, integration of Amazon Mechanical Turk for data collection, human\nevaluation, and online/reinforcement learning; and a repository of machine\nlearning models for comparing with others’ models, and improving upon existing\narchitectures. Over 20 tasks are supported in the first release, including\npopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,\nCBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,\nincluding neural models such as memory networks, seq2seq and attentive LSTMs.</p>\n", "tags": ["EMNLP","Evaluation","Has Code","Tools"] },
{"key": "miller2020effect", "citations": "62", "year": "2020", "title":"The Effect Of Natural Distribution Shift On Question Answering Models", "abstract": "<p>We build four new test sets for the Stanford Question Answering Dataset\n(SQuAD) and evaluate the ability of question-answering systems to generalize to\nnew data. Our first test set is from the original Wikipedia domain and measures\nthe extent to which existing systems overfit the original test set. Despite\nseveral years of heavy test set re-use, we find no evidence of adaptive\noverfitting. The remaining three test sets are constructed from New York Times\narticles, Reddit posts, and Amazon product reviews and measure robustness to\nnatural distribution shifts. Across a broad range of models, we observe average\nperformance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast,\na strong human baseline matches or exceeds the performance of SQuAD models on\nthe original domain and exhibits little to no drop in new domains. Taken\ntogether, our results confirm the surprising resilience of the holdout method\nand emphasize the need to move towards evaluation metrics that incorporate\nrobustness to natural distribution shifts.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "min2017question", "citations": "120", "year": "2017", "title":"Question Answering Through Transfer Learning From Large Fine-grained Supervision Data", "abstract": "<p>We show that the task of question answering (QA) can significantly benefit\nfrom the transfer learning of models trained on a different large, fine-grained\nQA dataset. We achieve the state of the art in two well-studied QA datasets,\nWikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique\nfrom SQuAD. For WikiQA, our model outperforms the previous best model by more\nthan 8%. We demonstrate that finer supervision provides better guidance for\nlearning lexical and syntactic information than coarser supervision, through\nquantitative results and visual analysis. We also show that a similar transfer\nlearning procedure achieves the state of the art on an entailment task.</p>\n", "tags": ["Datasets","Fine-Tuning"] },
{"key": "min2018efficient", "citations": "150", "year": "2018", "title":"Efficient And Robust Question Answering From Minimal Context Over Documents", "abstract": "<p>Neural models for question answering (QA) over documents have achieved\nsignificant performance improvements. Although effective, these models do not\nscale to large corpora due to their complex modeling of interactions between\nthe document and the question. Moreover, recent work has shown that such models\nare sensitive to adversarial inputs. In this paper, we study the minimal\ncontext required to answer the question, and find that most questions in\nexisting datasets can be answered with a small set of sentences. Inspired by\nthis observation, we propose a simple sentence selector to select the minimal\nset of sentences to feed into the QA model. Our overall system achieves\nsignificant reductions in training (up to 15 times) and inference times (up to\n13 times), with accuracy comparable to or better than the state-of-the-art on\nSQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results\nand analyses show that our approach is more robust to adversarial inputs.</p>\n", "tags": ["Datasets","Efficiency","Memory & Context","Training Techniques"] },
{"key": "min2019compositional", "citations": "148", "year": "2019", "title":"Compositional Questions Do Not Necessitate Multi-hop Reasoning", "abstract": "<p>Multi-hop reading comprehension (RC) questions are challenging because they\nrequire reading and reasoning over multiple paragraphs. We argue that it can be\ndifficult to construct large multi-hop RC datasets. For example, even highly\ncompositional questions can be answered with a single hop if they target\nspecific entity types, or the facts needed to answer them are redundant. Our\nanalysis is centered on HotpotQA, where we show that single-hop reasoning can\nsolve much more of the dataset than previously thought. We introduce a\nsingle-hop BERT-based RC model that achieves 67 F1—comparable to\nstate-of-the-art multi-hop models. We also design an evaluation setting where\nhumans are not shown all of the necessary paragraphs for the intended multi-hop\nreasoning but can still answer over 80% of questions. Together with detailed\nerror analysis, these results suggest there should be an increasing focus on\nthe role of evidence in multi-hop reasoning and possibly even a shift towards\ninformation retrieval style evaluations with large and diverse evidence\ncollections.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "min2019multi", "citations": "185", "year": "2019", "title":"Multi-hop Reading Comprehension Through Question Decomposition And Rescoring", "abstract": "<p>Multi-hop Reading Comprehension (RC) requires reasoning and aggregation\nacross several paragraphs. We propose a system for multi-hop RC that decomposes\na compositional question into simpler sub-questions that can be answered by\noff-the-shelf single-hop RC models. Since annotations for such decomposition\nare expensive, we recast sub-question generation as a span prediction problem\nand show that our method, trained using only 400 labeled examples, generates\nsub-questions that are as effective as human-authored sub-questions. We also\nintroduce a new global rescoring approach that considers each decomposition\n(i.e. the sub-questions and their answers) to select the best final answer,\ngreatly improving overall performance. Our experiments on HotpotQA show that\nthis approach achieves the state-of-the-art results, while providing\nexplainable evidence for its decision making in the form of sub-questions.</p>\n", "tags": [] },
{"key": "min2020ambigqa", "citations": "154", "year": "2020", "title":"Ambigqa: Answering Ambiguous Open-domain Questions", "abstract": "<p>Ambiguity is inherent to open-domain question answering; especially when\nexploring new topics, it can be difficult to ask questions that have a single,\nunambiguous answer. In this paper, we introduce AmbigQA, a new open-domain\nquestion answering task which involves finding every plausible answer, and then\nrewriting the question for each one to resolve the ambiguity. To study this\ntask, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open,\nan existing open-domain QA benchmark. We find that over half of the questions\nin NQ-open are ambiguous, with diverse sources of ambiguity such as event and\nentity references. We also present strong baseline models for AmbigQA which we\nshow benefit from weakly supervised learning that incorporates NQ-open,\nstrongly suggesting our new task and data will support significant future\nresearch effort. Our data and baselines are available at\nhttps://nlp.cs.washington.edu/ambigqa.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "min2020syntactic", "citations": "150", "year": "2020", "title":"Syntactic Data Augmentation Increases Robustness To Inference Heuristics", "abstract": "<p>Pretrained neural models such as BERT, when fine-tuned to perform natural\nlanguage inference (NLI), often show high accuracy on standard datasets, but\ndisplay a surprising lack of sensitivity to word order on controlled challenge\nsets. We hypothesize that this issue is not primarily caused by the pretrained\nmodel’s limitations, but rather by the paucity of crowdsourced NLI examples\nthat might convey the importance of syntactic structure at the fine-tuning\nstage. We explore several methods to augment standard training sets with\nsyntactically informative examples, generated by applying syntactic\ntransformations to sentences from the MNLI corpus. The best-performing\naugmentation method, subject/object inversion, improved BERT’s accuracy on\ncontrolled examples that diagnose sensitivity to word order from 0.28 to 0.73,\nwithout affecting performance on the MNLI test set. This improvement\ngeneralized beyond the particular construction used for data augmentation,\nsuggesting that augmentation causes BERT to recruit abstract syntactic\nrepresentations.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "min2021metaicl", "citations": "84", "year": "2022", "title":"Metaicl: Learning To Learn In Context", "abstract": "<p>We introduce MetaICL (Meta-training for In-Context Learning), a new\nmeta-training framework for few-shot learning where a pretrained language model\nis tuned to do in-context learning on a large set of training tasks. This\nmeta-training enables the model to more effectively learn a new task in context\nat test time, by simply conditioning on a few training examples with no\nparameter updates or task-specific templates. We experiment on a large, diverse\ncollection of tasks consisting of 142 NLP datasets including classification,\nquestion answering, natural language inference, paraphrase detection and more,\nacross seven different meta-training/target splits. MetaICL outperforms a range\nof baselines including in-context learning without meta-training and multi-task\nlearning followed by zero-shot transfer. We find that the gains are\nparticularly significant for target tasks that have domain shifts from the\nmeta-training tasks, and that using a diverse set of the meta-training tasks is\nkey to improvements. We also show that MetaICL approaches (and sometimes beats)\nthe performance of models fully finetuned on the target task, and outperforms\nmuch bigger models with nearly 8x parameters. Finally, we show that MetaICL is\ncomplementary to human-written instructions, and the best performance can be\nachieved by combining both approaches.</p>\n", "tags": ["Datasets","Few-Shot","In Context Learning","NAACL","Tools","Training Techniques"] },
{"key": "min2021noisy", "citations": "66", "year": "2022", "title":"Noisy Channel Language Model Prompting For Few-shot Text Classification", "abstract": "<p>We introduce a noisy channel approach for language model prompting in\nfew-shot text classification. Instead of computing the likelihood of the label\ngiven the input (referred as direct models), channel models compute the\nconditional probability of the input given the label, and are thereby required\nto explain every word in the input. We use channel models for recently proposed\nfew-shot learning methods with no or very limited updates to the language model\nparameters, via either in-context demonstration or prompt tuning. Our\nexperiments show that, for both methods, channel models significantly\noutperform their direct counterparts, which we attribute to their stability,\ni.e., lower variance and higher worst-case accuracy. We also present extensive\nablations that provide recommendations for when to use channel prompt tuning\ninstead of other competitive methods (e.g., direct head tuning): channel prompt\ntuning is preferred when the number of training examples is small, labels in\nthe training data are imbalanced, or generalization to unseen labels is\nrequired.</p>\n", "tags": ["Few-Shot","Prompting","Training Techniques"] },
{"key": "min2021recent", "citations": "667", "year": "2023", "title":"Recent Advances In Natural Language Processing Via Large Pre-trained Language Models: A Survey", "abstract": "<p>Large, pre-trained transformer-based language models such as BERT have\ndrastically changed the Natural Language Processing (NLP) field. We present a\nsurvey of recent work that uses these large language models to solve NLP tasks\nvia pre-training then fine-tuning, prompting, or text generation approaches. We\nalso present approaches that use pre-trained language models to generate data\nfor training augmentation or other purposes. We conclude with discussions on\nlimitations and suggested directions for future research.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Prompting","Survey Paper","Training Techniques"] },
{"key": "min2022rethinking", "citations": "421", "year": "2022", "title":"Rethinking The Role Of Demonstrations: What Makes In-context Learning Work?", "abstract": "<p>Large language models (LMs) are able to in-context learn – perform a new\ntask via inference alone by conditioning on a few input-label pairs\n(demonstrations) and making predictions for new inputs. However, there has been\nlittle understanding of how the model learns and which aspects of the\ndemonstrations contribute to end task performance. In this paper, we show that\nground truth demonstrations are in fact not required – randomly replacing\nlabels in the demonstrations barely hurts performance on a range of\nclassification and multi-choce tasks, consistently over 12 different models\nincluding GPT-3. Instead, we find that other aspects of the demonstrations are\nthe key drivers of end task performance, including the fact that they provide a\nfew examples of (1) the label space, (2) the distribution of the input text,\nand (3) the overall format of the sequence. Together, our analysis provides a\nnew way of understanding how and why in-context learning works, while opening\nup new questions about how much can be learned from large language models\nthrough inference alone.</p>\n", "tags": ["EMNLP","In Context Learning","Model Architecture"] },
{"key": "min2023factscore", "citations": "75", "year": "2023", "title":"Factscore: Fine-grained Atomic Evaluation Of Factual Precision In Long Form Text Generation", "abstract": "<p>Evaluating the factuality of long-form text generated by large language\nmodels (LMs) is non-trivial because (1) generations often contain a mixture of\nsupported and unsupported pieces of information, making binary judgments of\nquality inadequate, and (2) human evaluation is time-consuming and costly. In\nthis paper, we introduce FACTSCORE, a new evaluation that breaks a generation\ninto a series of atomic facts and computes the percentage of atomic facts\nsupported by a reliable knowledge source. We conduct an extensive human\nevaluation to obtain FACTSCOREs of people biographies generated by several\nstate-of-the-art commercial LMs – InstructGPT, ChatGPT, and the\nretrieval-augmented PerplexityAI – and report new analysis demonstrating the\nneed for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since\nhuman evaluation is costly, we also introduce an automated model that estimates\nFACTSCORE using retrieval and a strong language model, with less than a 2%\nerror rate. Finally, we use this automated metric to evaluate 6,500 generations\nfrom a new set of 13 recent LMs that would have cost $26K if evaluated by\nhumans, with various findings: GPT-4 and ChatGPT are more factual than public\nmodels, and Vicuna and Alpaca are some of the best public models. FACTSCORE is\navailable for public use via <code class=\"language-plaintext highlighter-rouge\">pip install factscore</code>.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "minaee2024large", "citations": "123", "year": "2024", "title":"Large Language Models: A Survey", "abstract": "<p>Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs’ ability of general-purpose language\nunderstanding and generation is acquired by training billions of model’s\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "mirowski2022co", "citations": "149", "year": "2023", "title":"Co-writing Screenplays And Theatre Scripts With Language Models: An Evaluation By Industry Professionals", "abstract": "<p>Language models are increasingly attracting interest from writers. However,\nsuch models lack long-range semantic coherence, limiting their usefulness for\nlongform creative writing. We address this limitation by applying language\nmodels hierarchically, in a system we call Dramatron. By building structural\ncontext via prompt chaining, Dramatron can generate coherent scripts and\nscreenplays complete with title, characters, story beats, location\ndescriptions, and dialogue. We illustrate Dramatron’s usefulness as an\ninteractive co-creative system with a user study of 15 theatre and film\nindustry professionals. Participants co-wrote theatre scripts and screenplays\nwith Dramatron and engaged in open-ended interviews. We report critical\nreflections both from our interviewees and from independent reviewers who\nwatched stagings of the works to illustrate how both Dramatron and hierarchical\ntext generation could be useful for human-machine co-creativity. Finally, we\ndiscuss the suitability of Dramatron for co-creativity, ethical considerations\n– including plagiarism and bias – and participatory models for the design and\ndeployment of such tools.</p>\n", "tags": ["Ethics & Fairness","Evaluation","Prompting","Tools"] },
{"key": "mishra2018tracking", "citations": "118", "year": "2018", "title":"Tracking State Changes In Procedural Text: A Challenge Dataset And Models For Process Paragraph Comprehension", "abstract": "<p>We present a new dataset and models for comprehending paragraphs about\nprocesses (e.g., photosynthesis), an important genre of text describing a\ndynamic world. The new dataset, ProPara, is the first to contain natural\n(rather than machine-generated) text about a changing world along with a full\nannotation of entity states (location and existence) during those changes (81k\ndatapoints). The end-task, tracking the location and existence of entities\nthrough the text, is challenging because the causal effects of actions are\noften implicit and need to be inferred. We find that previous models that have\nworked well on synthetic data achieve only mediocre performance on ProPara, and\nintroduce two new neural models that exploit alternative mechanisms for state\nprediction, in particular using LSTM input encoding and span prediction. The\nnew models improve accuracy by up to 19%. The dataset and models are available\nto the community at http://data.allenai.org/propara.</p>\n", "tags": ["Datasets","Model Architecture","NAACL"] },
{"key": "mishra2021cross", "citations": "200", "year": "2022", "title":"Cross-task Generalization Via Natural Language Crowdsourcing Instructions", "abstract": "<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. Despite the success of the conventional supervised learning on\nindividual datasets, such models often struggle with generalization across\ntasks (e.g., a question-answering system cannot solve classification tasks). A\nlong-standing challenge in AI is to build a model that learns a new task by\nunderstanding the human-readable instructions that define it. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions, and 193k task instances (input-output pairs). The\ninstructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. Using this meta-dataset,\nwe measure cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks (19%\nbetter for models utilizing instructions). These models, however, are far\nbehind an estimated performance upperbound indicating significant room for more\nprogress in this direction.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "mishra2022lila", "citations": "69", "year": "2022", "title":"Lila: A Unified Benchmark For Mathematical Reasoning", "abstract": "<p>Mathematical reasoning skills are essential for general-purpose intelligent\nsystems to perform tasks from grocery shopping to climate modeling. Towards\nevaluating and improving AI systems in this domain, we propose LILA, a unified\nmathematical reasoning benchmark consisting of 23 diverse tasks along four\ndimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language\nformat e.g., question-answering, fill-in-the-blanks (iii) language diversity\ne.g., no language, simple language (iv) external knowledge e.g., commonsense,\nphysics. We construct our benchmark by extending 20 datasets benchmark by\ncollecting task instructions and solutions in the form of Python programs,\nthereby obtaining explainable solutions in addition to the correct answer. We\nadditionally introduce two evaluation datasets to measure out-of-distribution\nperformance and robustness to language perturbation. Finally, we introduce\nBHASKARA, a general-purpose mathematical reasoning model trained on LILA.\nImportantly, we find that multi-tasking leads to significant improvements\n(average relative improvement of 21.83% F1 score vs. single-task models), while\nthe best performing model only obtains 60.40%, indicating the room for\nimprovement in general mathematical reasoning and understanding.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "mishra2022numglue", "citations": "112", "year": "2022", "title":"Numglue: A Suite Of Fundamental Yet Challenging Mathematical Reasoning Tasks", "abstract": "<p>Given the ubiquitous nature of numbers in text, reasoning with numbers to\nperform simple calculations is an important skill of AI systems. While many\ndatasets and models have been developed to this end, state-of-the-art AI\nsystems are brittle; failing to perform the underlying mathematical reasoning\nwhen they appear in a slightly different scenario. Drawing inspiration from\nGLUE that was proposed in the context of natural language understanding, we\npropose NumGLUE, a multi-task benchmark that evaluates the performance of AI\nsystems on eight different tasks, that at their core require simple arithmetic\nunderstanding. We show that this benchmark is far from being solved with neural\nmodels including state-of-the-art large-scale language models performing\nsignificantly worse than humans (lower by 46.4%). Further, NumGLUE promotes\nsharing knowledge across tasks, especially those with limited training data as\nevidenced by the superior performance (average gain of 3.4% on each task) when\na model is jointly trained on all the tasks as opposed to task-specific\nmodeling. Finally, we hope that NumGLUE will encourage systems that perform\nrobust and general arithmetic reasoning within language, a first step towards\nbeing able to perform more complex mathematical reasoning.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "misra2017learning", "citations": "63", "year": "2018", "title":"Learning By Asking Questions", "abstract": "<p>We introduce an interactive learning framework for the development and\ntesting of intelligent visual systems, called learning-by-asking (LBA). We\nexplore LBA in context of the Visual Question Answering (VQA) task. LBA differs\nfrom standard VQA training in that most questions are not observed during\ntraining time, and the learner must ask questions it wants answers to. Thus,\nLBA more closely mimics natural learning and has the potential to be more\ndata-efficient than the traditional VQA setting. We present a model that\nperforms LBA on the CLEVR dataset, and show that it automatically discovers an\neasy-to-hard curriculum when learning interactively from an oracle. Our LBA\ngenerated data consistently matches or outperforms the CLEVR train data and is\nmore sample efficient. We also show that our model asks questions that\ngeneralize to state-of-the-art VQA models and to novel test time distributions.</p>\n", "tags": ["CVPR","Datasets","Tools","Training Techniques"] },
{"key": "misra2018mapping", "citations": "142", "year": "2018", "title":"Mapping Instructions To Actions In 3D Environments With Visual Goal Prediction", "abstract": "<p>We propose to decompose instruction execution to goal prediction and action\ngeneration. We design a model that maps raw visual observations to goals using\nLINGUNET, a language-conditioned image generation network, and then generates\nthe actions required to complete them. Our model is trained from demonstration\nonly without external resources. To evaluate our approach, we introduce two\nbenchmarks for instruction following: LANI, a navigation task; and CHAI, where\nan agent executes household instructions. Our evaluation demonstrates the\nadvantages of our model decomposition, and illustrates the challenges posed by\nour new benchmarks.</p>\n", "tags": ["Agentic","EMNLP","Evaluation","Instruction Following"] },
{"key": "mitchell2021fast", "citations": "74", "year": "2021", "title":"Fast Model Editing At Scale", "abstract": "<p>While large pre-trained models have enabled impressive results on a variety\nof downstream tasks, the largest existing models still make errors, and even\naccurate predictions may become outdated over time. Because detecting all such\nfailures at training time is impossible, enabling both developers and end users\nof such models to correct inaccurate outputs while leaving the model otherwise\nintact is desirable. However, the distributed, black-box nature of the\nrepresentations learned by large neural networks makes producing such targeted\nedits difficult. If presented with only a single problematic input and new\ndesired output, fine-tuning approaches tend to overfit; other editing\nalgorithms are either computationally infeasible or simply ineffective when\napplied to very large models. To enable easy post-hoc editing at scale, we\npropose Model Editor Networks using Gradient Decomposition (MEND), a collection\nof small auxiliary editing networks that use a single desired input-output pair\nto make fast, local edits to a pre-trained model’s behavior. MEND learns to\ntransform the gradient obtained by standard fine-tuning, using a low-rank\ndecomposition of the gradient to make the parameterization of this\ntransformation tractable. MEND can be trained on a single GPU in less than a\nday even for 10 billion+ parameter models; once trained MEND enables rapid\napplication of new edits to the pre-trained model. Our experiments with T5,\nGPT, BERT, and BART models show that MEND is the only approach to model editing\nthat effectively edits the behavior of models with more than 10 billion\nparameters. Code and data available at\nhttps://sites.google.com/view/mend-editing.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "mitrović2023chatgpt", "citations": "66", "year": "2023", "title":"Chatgpt Or Human? Detect And Explain. Explaining Decisions Of Machine Learning Model For Detecting Short Chatgpt-generated Text", "abstract": "<p>ChatGPT has the ability to generate grammatically flawless and\nseemingly-human replies to different types of questions from various domains.\nThe number of its users and of its applications is growing at an unprecedented\nrate. Unfortunately, use and abuse come hand in hand. In this paper, we study\nwhether a machine learning model can be effectively trained to accurately\ndistinguish between original human and seemingly human (that is,\nChatGPT-generated) text, especially when this text is short. Furthermore, we\nemploy an explainable artificial intelligence framework to gain insight into\nthe reasoning behind the model trained to differentiate between\nChatGPT-generated and human-generated text. The goal is to analyze model’s\ndecisions and determine if any specific patterns or characteristics can be\nidentified. Our study focuses on short online reviews, conducting two\nexperiments comparing human-generated and ChatGPT-generated text. The first\nexperiment involves ChatGPT text generated from custom queries, while the\nsecond experiment involves text generated by rephrasing original\nhuman-generated reviews. We fine-tune a Transformer-based model and use it to\nmake predictions, which are then explained using SHAP. We compare our model\nwith a perplexity score-based approach and find that disambiguation between\nhuman and ChatGPT-generated reviews is more challenging for the ML model when\nusing rephrased text. However, our proposed approach still achieves an accuracy\nof 79%. Using explainability, we observe that ChatGPT’s writing is polite,\nwithout specific details, using fancy and atypical vocabulary, impersonal, and\ntypically it does not express feelings.</p>\n", "tags": ["Applications","Model Architecture","Tools"] },
{"key": "miura2020improving", "citations": "96", "year": "2021", "title":"Improving Factual Completeness And Consistency Of Image-to-text Radiology Report Generation", "abstract": "<p>Neural image-to-text radiology report generation systems offer the potential\nto improve radiology reporting by reducing the repetitive process of report\ndrafting and identifying possible medical errors. However, existing report\ngeneration systems, despite achieving high performances on natural language\ngeneration metrics such as CIDEr or BLEU, still suffer from incomplete and\ninconsistent generations. Here we introduce two new simple rewards to encourage\nthe generation of factually complete and consistent radiology reports: one that\nencourages the system to generate radiology domain entities consistent with the\nreference, and one that uses natural language inference to encourage these\nentities to be described in inferentially consistent ways. We combine these\nwith the novel use of an existing semantic equivalence metric (BERTScore). We\nfurther propose a report generation system that optimizes these rewards via\nreinforcement learning. On two open radiology report datasets, our system\nsubstantially improved the F1 score of a clinical information extraction\nperformance by +22.1 (Delta +63.9%). We further show via a human evaluation and\na qualitative analysis that our system leads to generations that are more\nfactually complete and consistent compared to the baselines.</p>\n", "tags": ["Datasets","Evaluation","NAACL"] },
{"key": "mo2016personalizing", "citations": "100", "year": "2018", "title":"Personalizing A Dialogue System With Transfer Reinforcement Learning", "abstract": "<p>It is difficult to train a personalized task-oriented dialogue system because\nthe data collected from each individual is often insufficient. Personalized\ndialogue systems trained on a small dataset can overfit and make it difficult\nto adapt to different user needs. One way to solve this problem is to consider\na collection of multiple users’ data as a source domain and an individual\nuser’s data as a target domain, and to perform a transfer learning from the\nsource to the target domain. By following this idea, we propose\n“PETAL”(PErsonalized Task-oriented diALogue), a transfer-learning framework\nbased on POMDP to learn a personalized dialogue system. The system first learns\ncommon dialogue knowledge from the source domain and then adapts this knowledge\nto the target user. This framework can avoid the negative transfer problem by\nconsidering differences between source and target users. The policy in the\npersonalized POMDP can learn to choose different actions appropriately for\ndifferent users. Experimental results on a real-world coffee-shopping data and\nsimulation data show that our personalized dialogue system can choose different\noptimal actions for different users, and thus effectively improve the dialogue\nquality under the personalized setting.</p>\n", "tags": ["AAAI","Dialogue & Multi Turn","Fine-Tuning","Reinforcement Learning"] },
{"key": "moghe2018towards", "citations": "168", "year": "2018", "title":"Towards Exploiting Background Knowledge For Building Conversation Systems", "abstract": "<p>Existing dialog datasets contain a sequence of utterances and responses\nwithout any explicit background knowledge associated with them. This has\nresulted in the development of models which treat conversation as a\nsequence-to-sequence generation task i.e, given a sequence of utterances\ngenerate the response sequence). This is not only an overly simplistic view of\nconversation but it is also emphatically different from the way humans converse\nby heavily relying on their background knowledge about the topic (as opposed to\nsimply relying on the previous sequence of utterances). For example, it is\ncommon for humans to (involuntarily) produce utterances which are copied or\nsuitably modified from background articles they have read about the topic. To\nfacilitate the development of such natural conversation models which mimic the\nhuman process of conversing, we create a new dataset containing movie chats\nwherein each response is explicitly generated by copying and/or modifying\nsentences from unstructured background knowledge such as plots, comments and\nreviews about the movie. We establish baseline results on this dataset (90K\nutterances from 9K conversations) using three different models: (i) pure\ngeneration based models which ignore the background knowledge (ii) generation\nbased models which learn to copy information from the background knowledge when\nrequired and (iii) span prediction based models which predict the appropriate\nresponse span in the background knowledge.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "mohammad2019megatron", "citations": "792", "year": "2019", "title":"Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism", "abstract": "<p>Recent work in language modeling demonstrates that training large transformer\nmodels advances the state of the art in Natural Language Processing\napplications. However, very large models can be quite difficult to train due to\nmemory constraints. In this work, we present our techniques for training very\nlarge transformer models and implement a simple, efficient intra-layer model\nparallel approach that enables training transformer models with billions of\nparameters. Our approach does not require a new compiler or library changes, is\northogonal and complimentary to pipeline model parallelism, and can be fully\nimplemented with the insertion of a few communication operations in native\nPyTorch. We illustrate this approach by converging transformer based models up\nto 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the\nentire application with 76% scaling efficiency when compared to a strong single\nGPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can further advance the state of the art\n(SOTA), we train an 8.3 billion parameter transformer language model similar to\nGPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful\nattention to the placement of layer normalization in BERT-like models is\ncritical to achieving increased performance as the model size grows. Using the\nGPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA\nperplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)\ndatasets. Our BERT model achieves SOTA results on the RACE dataset (90.9%\ncompared to SOTA accuracy of 89.4%).</p>\n", "tags": ["Applications","Datasets","Efficiency","Model Architecture","Tools","Training Techniques"] },
{"key": "mohankumar2020towards", "citations": "71", "year": "2020", "title":"Towards Transparent And Explainable Attention Models", "abstract": "<p>Recent studies on interpretability of attention distributions have led to\nnotions of faithful and plausible explanations for a model’s predictions.\nAttention distributions can be considered a faithful explanation if a higher\nattention weight implies a greater impact on the model’s prediction. They can\nbe considered a plausible explanation if they provide a human-understandable\njustification for the model’s predictions. In this work, we first explain why\ncurrent attention mechanisms in LSTM based encoders can neither provide a\nfaithful nor a plausible explanation of the model’s predictions. We observe\nthat in LSTM based encoders the hidden representations at different time-steps\nare very similar to each other (high conicity) and attention weights in these\nsituations do not carry much meaning because even a random permutation of the\nattention weights does not affect the model’s predictions. Based on experiments\non a wide variety of tasks and datasets, we observe attention distributions\noften attribute the model’s predictions to unimportant words such as\npunctuation and fail to offer a plausible explanation for the predictions. To\nmake attention mechanisms more faithful and plausible, we propose a modified\nLSTM cell with a diversity-driven training objective that ensures that the\nhidden representations learned at different time steps are diverse. We show\nthat the resulting attention distributions offer more transparency as they (i)\nprovide a more precise importance ranking of the hidden states (ii) are better\nindicative of words important for the model’s predictions (iii) correlate\nbetter with gradient-based attribution methods. Human evaluations indicate that\nthe attention distributions learned by our model offer a plausible explanation\nof the model’s predictions. Our code has been made publicly available at\nhttps://github.com/akashkm99/Interpretable-Attention</p>\n", "tags": ["Ethics & Fairness","Has Code","Model Architecture","Training Techniques"] },
{"key": "mokady2021clipcap", "citations": "275", "year": "2021", "title":"Clipcap: CLIP Prefix For Image Captioning", "abstract": "<p>Image captioning is a fundamental task in vision-language understanding,\nwhere the model predicts a textual informative caption to a given input image.\nIn this paper, we present a simple approach to address this task. We use CLIP\nencoding as a prefix to the caption, by employing a simple mapping network, and\nthen fine-tunes a language model to generate the image captions. The recently\nproposed CLIP model contains rich semantic features which were trained with\ntextual context, making it best for vision-language perception. Our key idea is\nthat together with a pre-trained language model (GPT2), we obtain a wide\nunderstanding of both visual and textual data. Hence, our approach only\nrequires rather quick training to produce a competent captioning model. Without\nadditional annotations or pre-training, it efficiently generates meaningful\ncaptions for large-scale and diverse datasets. Surprisingly, our method works\nwell even when only the mapping network is trained, while both CLIP and the\nlanguage model remain frozen, allowing a lighter architecture with less\ntrainable parameters. Through quantitative evaluation, we demonstrate our model\nachieves comparable results to state-of-the-art methods on the challenging\nConceptual Captions and nocaps datasets, while it is simpler, faster, and\nlighter. Our code is available in\nhttps://github.com/rmokady/CLIP_prefix_caption.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "moon2018multimodal", "citations": "146", "year": "2018", "title":"Multimodal Named Entity Recognition For Short Social Media Posts", "abstract": "<p>We introduce a new task called Multimodal Named Entity Recognition (MNER) for\nnoisy user-generated data such as tweets or Snapchat captions, which comprise\nshort text with accompanying images. These social media posts often come in\ninconsistent or incomplete syntax and lexical notations with very limited\nsurrounding textual contexts, bringing significant challenges for NER. To this\nend, we create a new dataset for MNER called SnapCaptions (Snapchat\nimage-caption pairs submitted to public and crowd-sourced stories with fully\nannotated named entities). We then build upon the state-of-the-art Bi-LSTM\nword/character based NER models with 1) a deep image network which incorporates\nrelevant visual context to augment textual information, and 2) a generic\nmodality-attention module which learns to attenuate irrelevant modalities while\namplifying the most informative ones to extract contexts from, adaptive to each\nsample and token. The proposed MNER model with modality attention significantly\noutperforms the state-of-the-art text-only NER models by successfully\nleveraging provided visual contexts, opening up potential applications of MNER\non myriads of social media platforms.</p>\n", "tags": ["Applications","Datasets","Model Architecture","NAACL"] },
{"key": "moon2021multi", "citations": "129", "year": "2022", "title":"Multi-modal Understanding And Generation For Medical Images And Text Via Vision-language Pre-training", "abstract": "<p>Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL),\nwhich adopts a BERT-based architecture combined with a novel multi-modal\nattention masking scheme to maximize generalization performance for both\nvision-language understanding tasks (diagnosis classification, medical\nimage-report retrieval, medical visual question answering) and vision-language\ngeneration task (radiology report generation). By statistically and rigorously\nevaluating the proposed model on four downstream tasks with three radiographic\nimage-report datasets (MIMIC-CXR, Open-I, and VQA-RAD), we empirically\ndemonstrate the superior downstream task performance of MedViLL against various\nbaselines, including task-specific architectures. The source code is publicly\navailable at: https://github.com/SuperSupermoon/MedViLL</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "moore2016is", "citations": "66", "year": "2016", "title":"Is Spoken Language All-or-nothing? Implications For Future Speech-based Human-machine Interaction", "abstract": "<p>Recent years have seen significant market penetration for voice-based\npersonal assistants such as Apple’s Siri. However, despite this success, user\ntake-up is frustratingly low. This position paper argues that there is a\nhabitability gap caused by the inevitable mismatch between the capabilities and\nexpectations of human users and the features and benefits provided by\ncontemporary technology. Suggestions are made as to how such problems might be\nmitigated, but a more worrisome question emerges: “is spoken language\nall-or-nothing”? The answer, based on contemporary views on the special nature\nof (spoken) language, is that there may indeed be a fundamental limit to the\ninteraction that can take place between mismatched interlocutors (such as\nhumans and machines). However, it is concluded that interactions between native\nand non-native speakers, or between adults and children, or even between humans\nand dogs, might provide critical inspiration for the design of future\nspeech-based human-machine interaction.</p>\n", "tags": [] },
{"key": "mostafazadeh2016generating", "citations": "279", "year": "2016", "title":"Generating Natural Questions About An Image", "abstract": "<p>There has been an explosion of work in the vision &amp; language community during\nthe past few years from image captioning to video transcription, and answering\nquestions about images. These tasks have focused on literal descriptions of the\nimage. To move beyond the literal, we choose to explore how questions about an\nimage are often directed at commonsense inference and the abstract events\nevoked by objects in the image. In this paper, we introduce the novel task of\nVisual Question Generation (VQG), where the system is tasked with asking a\nnatural and engaging question when shown an image. We provide three datasets\nwhich cover a variety of images from object-centric to event-centric, with\nconsiderably more abstract training data than provided to state-of-the-art\ncaptioning systems thus far. We train and test several generative and retrieval\nmodels to tackle the task of VQG. Evaluation results show that while such\nmodels ask reasonable questions for a variety of images, there is still a wide\ngap with human performance which motivates further work on connecting images\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\nchallenge to the community which we hope furthers interest in exploring deeper\nconnections between vision &amp; language.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "mostafazadeh2017image", "citations": "121", "year": "2017", "title":"Image-grounded Conversations: Multimodal Context For Natural Question And Response Generation", "abstract": "<p>The popularity of image sharing on social media and the engagement it creates\nbetween users reflects the important role that visual context plays in everyday\nconversations. We present a novel task, Image-Grounded Conversations (IGC), in\nwhich natural-sounding conversations are generated about a shared image. To\nbenchmark progress, we introduce a new multiple-reference dataset of\ncrowd-sourced, event-centric conversations on images. IGC falls on the\ncontinuum between chit-chat and goal-directed conversation models, where visual\ngrounding constrains the topic of conversation to event-driven utterances.\nExperiments with models trained on social media data show that the combination\nof visual and textual context enhances the quality of generated conversational\nturns. In human evaluation, the gap between human performance and that of both\nneural and retrieval architectures suggests that multi-modal IGC presents an\ninteresting challenge for dialogue research.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "mostafazadeh2020glucose", "citations": "85", "year": "2020", "title":"GLUCOSE: Generalized And Contextualized Story Explanations", "abstract": "<p>When humans read or listen, they make implicit commonsense inferences that\nframe their understanding of what happened and why. As a step toward AI systems\nthat can build similar mental models, we introduce GLUCOSE, a large-scale\ndataset of implicit commonsense causal knowledge, encoded as causal\nmini-theories about the world, each grounded in a narrative context. To\nconstruct GLUCOSE, we drew on cognitive psychology to identify ten dimensions\nof causal explanation, focusing on events, states, motivations, and emotions.\nEach GLUCOSE entry includes a story-specific causal statement paired with an\ninference rule generalized from the statement. This paper details two concrete\ncontributions. First, we present our platform for effectively crowdsourcing\nGLUCOSE data at scale, which uses semi-structured templates to elicit causal\nexplanations. Using this platform, we collected a total of ~670K specific\nstatements and general rules that capture implicit commonsense knowledge about\neveryday situations. Second, we show that existing knowledge resources and\npretrained language models do not include or readily predict GLUCOSE’s rich\ninferential content. However, when state-of-the-art neural models are trained\non this knowledge, they can start to make commonsense inferences on unseen\nstories that match humans’ mental models.</p>\n", "tags": ["Datasets","EMNLP","Tools"] },
{"key": "mou2016sequence", "citations": "200", "year": "2016", "title":"Sequence To Backward And Forward Sequences: A Content-introducing Approach To Generative Short-text Conversation", "abstract": "<p>Using neural networks to generate replies in human-computer dialogue systems\nis attracting increasing attention over the past few years. However, the\nperformance is not satisfactory: the neural network tends to generate safe,\nuniversally relevant replies which carry little meaning. In this paper, we\npropose a content-introducing approach to neural network-based generative\ndialogue systems. We first use pointwise mutual information (PMI) to predict a\nnoun as a keyword, reflecting the main gist of the reply. We then propose\nseq2BF, a “sequence to backward and forward sequences” model, which generates a\nreply containing the given keyword. Experimental results show that our approach\nsignificantly outperforms traditional sequence-to-sequence models in terms of\nhuman evaluation and the entropy measure, and that the predicted keyword can\nappear at an appropriate position in the reply.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","Model Architecture"] },
{"key": "mou2023t2i", "citations": "330", "year": "2024", "title":"T2i-adapter: Learning Adapters To Dig Out More Controllable Ability For Text-to-image Diffusion Models", "abstract": "<p>The incredible generative ability of large-scale text-to-image (T2I) models\nhas demonstrated strong power of learning complex structures and meaningful\nsemantics. However, relying solely on text prompts cannot fully take advantage\nof the knowledge learned by the model, especially when flexible and accurate\ncontrolling (e.g., color and structure) is needed. In this paper, we aim to\n``dig out” the capabilities that T2I models have implicitly learned, and then\nexplicitly use them to control the generation more granularly. Specifically, we\npropose to learn simple and lightweight T2I-Adapters to align internal\nknowledge in T2I models with external control signals, while freezing the\noriginal large T2I models. In this way, we can train various adapters according\nto different conditions, achieving rich control and editing effects in the\ncolor and structure of the generation results. Further, the proposed\nT2I-Adapters have attractive properties of practical value, such as\ncomposability and generalization ability. Extensive experiments demonstrate\nthat our T2I-Adapter has promising generation quality and a wide range of\napplications.</p>\n", "tags": ["AAAI","Applications"] },
{"key": "mozannar2019neural", "citations": "121", "year": "2019", "title":"Neural Arabic Question Answering", "abstract": "<p>This paper tackles the problem of open domain factual Arabic question\nanswering (QA) using Wikipedia as our knowledge source. This constrains the\nanswer of any question to be a span of text in Wikipedia. Open domain QA for\nArabic entails three challenges: annotated QA datasets in Arabic, large scale\nefficient information retrieval and machine reading comprehension. To deal with\nthe lack of Arabic QA datasets we present the Arabic Reading Comprehension\nDataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia\narticles, and a machine translation of the Stanford Question Answering Dataset\n(Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL)\nis based on two components: (1) a document retriever using a hierarchical\nTF-IDF approach and (2) a neural reading comprehension model using the\npre-trained bi-directional transformer BERT. Our experiments on ARCD indicate\nthe effectiveness of our approach with our BERT-based reader achieving a 61.3\nF1 score, and our open domain system SOQAL achieving a 27.6 F1 score.</p>\n", "tags": ["Datasets","Model Architecture","Retrieval Systems"] },
{"key": "mrini2019rethinking", "citations": "109", "year": "2020", "title":"Rethinking Self-attention: Towards Interpretability In Neural Parsing", "abstract": "<p>Attention mechanisms have improved the performance of NLP tasks while\nallowing models to remain explainable. Self-attention is currently widely used,\nhowever interpretability is difficult due to the numerous attention\ndistributions. Recent work has shown that model representations can benefit\nfrom label-specific information, while facilitating interpretation of\npredictions. We introduce the Label Attention Layer: a new form of\nself-attention where attention heads represent labels. We test our novel layer\nby running constituency and dependency parsing experiments and show our new\nmodel obtains new state-of-the-art results for both tasks on both the Penn\nTreebank (PTB) and Chinese Treebank. Additionally, our model requires fewer\nself-attention layers compared to existing work. Finally, we find that the\nLabel Attention heads learn relations between syntactic categories and show\npathways to analyze errors.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "mrkšić2016neural", "citations": "499", "year": "2017", "title":"Neural Belief Tracker: Data-driven Dialogue State Tracking", "abstract": "<p>One of the core components of modern spoken dialogue systems is the belief\ntracker, which estimates the user’s goal at every step of the dialogue.\nHowever, most current approaches have difficulty scaling to larger, more\ncomplex dialogue domains. This is due to their dependency on either: a) Spoken\nLanguage Understanding models that require large amounts of annotated training\ndata; or b) hand-crafted lexicons for capturing some of the linguistic\nvariation in users’ language. We propose a novel Neural Belief Tracking (NBT)\nframework which overcomes these problems by building on recent advances in\nrepresentation learning. NBT models reason over pre-trained word vectors,\nlearning to compose them into distributed representations of user utterances\nand dialogue context. Our evaluation on two datasets shows that this approach\nsurpasses past limitations, matching the performance of state-of-the-art models\nwhich rely on hand-crafted semantic lexicons and outperforming them when such\nlexicons are not provided.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Tools","Training Techniques"] },
{"key": "mu2021slip", "citations": "160", "year": "2022", "title":"SLIP: Self-supervision Meets Language-image Pre-training", "abstract": "<p>Recent work has shown that self-supervised pre-training leads to improvements\nover supervised learning on challenging visual recognition tasks. CLIP, an\nexciting new approach to learning with language supervision, demonstrates\npromising performance on a wide variety of benchmarks. In this work, we explore\nwhether self-supervised learning can aid in the use of language supervision for\nvisual representation learning. We introduce SLIP, a multi-task learning\nframework for combining self-supervised learning and CLIP pre-training. After\npre-training with Vision Transformers, we thoroughly evaluate representation\nquality and compare performance to both CLIP and self-supervised learning under\nthree distinct settings: zero-shot transfer, linear classification, and\nend-to-end finetuning. Across ImageNet and a battery of additional datasets, we\nfind that SLIP improves accuracy by a large margin. We validate our results\nfurther with experiments on different model sizes, training schedules, and\npre-training datasets. Our findings show that SLIP enjoys the best of both\nworlds: better performance than self-supervision (+8.1% linear accuracy) and\nlanguage supervision (+5.2% zero-shot accuracy).</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "mudigere2021software", "citations": "76", "year": "2022", "title":"Software-hardware Co-design For Fast And Scalable Training Of Deep Learning Recommendation Models", "abstract": "<p>Deep learning recommendation models (DLRMs) are used across many\nbusiness-critical services at Facebook and are the single largest AI\napplication in terms of infrastructure demand in its data-centers. In this\npaper we discuss the SW/HW co-designed solution for high-performance\ndistributed training of large-scale DLRMs. We introduce a high-performance\nscalable software stack based on PyTorch and pair it with the new evolution of\nZion platform, namely ZionEX. We demonstrate the capability to train very large\nDLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup\nin terms of time to solution over previous systems. We achieve this by (i)\ndesigning the ZionEX platform with dedicated scale-out network, provisioned\nwith high bandwidth, optimal topology and efficient transport (ii) implementing\nan optimized PyTorch-based training stack supporting both model and data\nparallelism (iii) developing sharding algorithms capable of hierarchical\npartitioning of the embedding tables along row, column dimensions and load\nbalancing them across multiple workers; (iv) adding high-performance core\noperators while retaining flexibility to support optimizers with fully\ndeterministic updates (v) leveraging reduced precision communications,\nmulti-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we\ndevelop and briefly comment on distributed data ingestion and other supporting\nservices that are required for the robust and efficient end-to-end training in\nproduction environments.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "mudrakarta2018did", "citations": "159", "year": "2018", "title":"Did The Model Understand The Question?", "abstract": "<p>We analyze state-of-the-art deep learning models for three tasks: question\nanswering on (1) images, (2) tables, and (3) passages of text. Using the notion\nof <em>attribution</em> (word importance), we find that these deep networks often\nignore important question terms. Leveraging such behavior, we perturb questions\nto craft a variety of adversarial examples. Our strongest attacks drop the\naccuracy of a visual question answering model from \\(61.1%\\) to \\(19%\\), and that\nof a tabular question answering model from \\(33.5%\\) to \\(3.3%\\). Additionally,\nwe show how attributions can strengthen attacks proposed by Jia and Liang\n(2017) on paragraph comprehension models. Our results demonstrate that\nattributions can augment standard measures of accuracy and empower\ninvestigation of model performance. When a model is accurate but for the wrong\nreasons, attributions can surface erroneous logic in the model that indicates\ninadequacies in the test data.</p>\n", "tags": ["Security"] },
{"key": "muennighoff2022crosslingual", "citations": "244", "year": "2023", "title":"Crosslingual Generalization Through Multitask Finetuning", "abstract": "<p>Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are freely\navailable at https://github.com/bigscience-workshop/xmtf.</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "muennighoff2022mteb", "citations": "167", "year": "2023", "title":"MTEB: Massive Text Embedding Benchmark", "abstract": "<p>Text embeddings are commonly evaluated on a small set of datasets from a\nsingle task not covering their possible applications to other tasks. It is\nunclear whether state-of-the-art embeddings on semantic textual similarity\n(STS) can be equally well applied to other tasks like clustering or reranking.\nThis makes progress in the field difficult to track, as various models are\nconstantly being proposed without proper evaluation. To solve this problem, we\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\ntasks covering a total of 58 datasets and 112 languages. Through the\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\nbenchmark of text embeddings to date. We find that no particular text embedding\nmethod dominates across all tasks. This suggests that the field has yet to\nconverge on a universal text embedding method and scale it up sufficiently to\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\nopen-source code and a public leaderboard at\nhttps://github.com/embeddings-benchmark/mteb.</p>\n", "tags": ["Applications","Datasets","Evaluation","Has Code"] },
{"key": "mun2016text", "citations": "90", "year": "2017", "title":"Text-guided Attention Model For Image Captioning", "abstract": "<p>Visual attention plays an important role to understand images and\ndemonstrates its effectiveness in generating natural language descriptions of\nimages. On the other hand, recent studies show that language associated with an\nimage can steer visual attention in the scene during our cognitive process.\nInspired by this, we introduce a text-guided attention model for image\ncaptioning, which learns to drive visual attention using associated captions.\nFor this model, we propose an exemplar-based learning approach that retrieves\nfrom training data associated captions with each image, and use them to learn\nattention on visual features. Our attention model enables to describe a\ndetailed state of scenes by distinguishing small or confusable objects\neffectively. We validate our model on MS-COCO Captioning benchmark and achieve\nthe state-of-the-art performance in standard metrics.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "munkhdalai2016neural", "citations": "92", "year": "2017", "title":"Neural Semantic Encoders", "abstract": "<p>We present a memory augmented neural network for natural language\nunderstanding: Neural Semantic Encoders. NSE is equipped with a novel memory\nupdate rule and has a variable sized encoding memory that evolves over time and\nmaintains the understanding of input sequences through read}, compose and write\noperations. NSE can also access multiple and shared memories. In this paper, we\ndemonstrated the effectiveness and the flexibility of NSE on five different\nnatural language tasks: natural language inference, question answering,\nsentence classification, document sentiment analysis and machine translation\nwhere NSE achieved state-of-the-art performance when evaluated on publically\navailable benchmarks. For example, our shared-memory model showed an\nencouraging result on neural machine translation, improving an attention-based\nbaseline by approximately 1.0 BLEU.</p>\n", "tags": ["Model Architecture"] },
{"key": "murahari2019large", "citations": "103", "year": "2020", "title":"Large-scale Pretraining For Visual Dialog: A Simple State-of-the-art Baseline", "abstract": "<p>Prior work in visual dialog has focused on training deep neural models on\nVisDial in isolation. Instead, we present an approach to leverage pretraining\non related vision-language datasets before transferring to visual dialog. We\nadapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn\nvisually-grounded conversations. Our model is pretrained on the Conceptual\nCaptions and Visual Question Answering datasets, and finetuned on VisDial. Our\nbest single model outperforms prior published work (including model ensembles)\nby more than 1% absolute on NDCG and MRR. Next, we find that additional\nfinetuning using “dense” annotations in VisDial leads to even higher NDCG –\nmore than 10% over our base model – but hurts MRR – more than 17% below our\nbase model! This highlights a trade-off between the two primary metrics – NDCG\nand MRR – which we find is due to dense annotations not correlating well with\nthe original ground-truth answers to questions.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "murdoch2018beyond", "citations": "124", "year": "2018", "title":"Beyond Word Importance: Contextual Decomposition To Extract Interactions From Lstms", "abstract": "<p>The driving force behind the recent success of LSTMs has been their ability\nto learn complex and non-linear relationships. Consequently, our inability to\ndescribe these relationships has led to LSTMs being characterized as black\nboxes. To this end, we introduce contextual decomposition (CD), an\ninterpretation algorithm for analysing individual predictions made by standard\nLSTMs, without any changes to the underlying model. By decomposing the output\nof a LSTM, CD captures the contributions of combinations of words or variables\nto the final prediction of an LSTM. On the task of sentiment analysis with the\nYelp and SST data sets, we show that CD is able to reliably identify words and\nphrases of contrasting sentiment, and how they are combined to yield the LSTM’s\nfinal prediction. Using the phrase-level labels in SST, we also demonstrate\nthat CD is able to successfully extract positive and negative negations from an\nLSTM, something which has not previously been done.</p>\n", "tags": ["Model Architecture"] },
{"key": "murray2018correcting", "citations": "140", "year": "2018", "title":"Correcting Length Bias In Neural Machine Translation", "abstract": "<p>We study two problems in neural machine translation (NMT). First, in beam\nsearch, whereas a wider beam should in principle help translation, it often\nhurts NMT. Second, NMT has a tendency to produce translations that are too\nshort. Here, we argue that these problems are closely related and both rooted\nin label bias. We show that correcting the brevity problem almost eliminates\nthe beam problem; we compare some commonly-used methods for doing this, finding\nthat a simple per-word reward works well; and we introduce a simple and quick\nway to tune this reward using the perceptron algorithm.</p>\n", "tags": ["Ethics & Fairness","Reinforcement Learning"] },
{"key": "mökander2023auditing", "citations": "101", "year": "2023", "title":"Auditing Large Language Models: A Three-layered Approach", "abstract": "<p>Large language models (LLMs) represent a major advance in artificial\nintelligence (AI) research. However, the widespread use of LLMs is also coupled\nwith significant ethical and social challenges. Previous research has pointed\ntowards auditing as a promising governance mechanism to help ensure that AI\nsystems are designed and deployed in ways that are ethical, legal, and\ntechnically robust. However, existing auditing procedures fail to address the\ngovernance challenges posed by LLMs, which display emergent capabilities and\nare adaptable to a wide range of downstream tasks. In this article, we address\nthat gap by outlining a novel blueprint for how to audit LLMs. Specifically, we\npropose a three-layered approach, whereby governance audits (of technology\nproviders that design and disseminate LLMs), model audits (of LLMs after\npre-training but prior to their release), and application audits (of\napplications based on LLMs) complement and inform each other. We show how\naudits, when conducted in a structured and coordinated manner on all three\nlevels, can be a feasible and effective mechanism for identifying and managing\nsome of the ethical and social risks posed by LLMs. However, it is important to\nremain realistic about what auditing can reasonably be expected to achieve.\nTherefore, we discuss the limitations not only of our three-layered approach\nbut also of the prospect of auditing LLMs at all. Ultimately, this article\nseeks to expand the methodological toolkit available to technology providers\nand policymakers who wish to analyse and evaluate LLMs from technical, ethical,\nand legal perspectives.</p>\n", "tags": ["Applications","Ethics & Fairness","Training Techniques"] },
{"key": "müller2018large", "citations": "152", "year": "2018", "title":"A Large-scale Test Set For The Evaluation Of Context-aware Pronoun Translation In Neural Machine Translation", "abstract": "<p>The translation of pronouns presents a special challenge to machine\ntranslation to this day, since it often requires context outside the current\nsentence. Recent work on models that have access to information across sentence\nboundaries has seen only moderate improvements in terms of automatic evaluation\nmetrics such as BLEU. However, metrics that quantify the overall translation\nquality are ill-equipped to measure gains from additional context. We argue\nthat a different kind of evaluation is needed to assess how well models\ntranslate inter-sentential phenomena such as pronouns. This paper therefore\npresents a test suite of contrastive translations focused specifically on the\ntranslation of pronouns. Furthermore, we perform experiments with several\ncontext-aware models. We show that, while gains in BLEU are moderate for those\nsystems, they outperform baselines by a large margin in terms of accuracy on\nour contrastive test set. Our experiments also show the effectiveness of\nparameter tying for multi-encoder architectures.</p>\n", "tags": ["Evaluation"] },
{"key": "na2017read", "citations": "95", "year": "2017", "title":"A Read-write Memory Network For Movie Story Understanding", "abstract": "<p>We propose a novel memory network model named Read-Write Memory Network\n(RWMN) to perform question and answering tasks for large-scale, multimodal\nmovie story understanding. The key focus of our RWMN model is to design the\nread network and the write network that consist of multiple convolutional\nlayers, which enable memory read and write operations to have high capacity and\nflexibility. While existing memory-augmented network models treat each memory\nslot as an independent block, our use of multi-layered CNNs allows the model to\nread and write sequential memory cells as chunks, which is more reasonable to\nrepresent a sequential story because adjacent memory blocks often have strong\ncorrelations. For evaluation, we apply our model to all the six tasks of the\nMovieQA benchmark, and achieve the best accuracies on several tasks, especially\non the visual QA task. Our model shows a potential to better understand not\nonly the content in the story, but also more abstract information, such as\nrelationships between characters and the reasons for their actions.</p>\n", "tags": ["Datasets","Evaluation","ICCV"] },
{"key": "nadejde2017predicting", "citations": "79", "year": "2017", "title":"Predicting Target Language CCG Supertags Improves Neural Machine Translation", "abstract": "<p>Neural machine translation (NMT) models are able to partially learn syntactic\ninformation from sequential lexical information. Still, some complex syntactic\nphenomena such as prepositional phrase attachment are poorly modeled. This work\naims to answer two questions: 1) Does explicitly modeling target language\nsyntax help NMT? 2) Is tight integration of words and syntax better than\nmultitask training? We introduce syntactic information in the form of CCG\nsupertags in the decoder, by interleaving the target supertags with the word\nsequence. Our results on WMT data show that explicitly modeling target-syntax\nimproves machine translation quality for German-&gt;English, a high-resource pair,\nand for Romanian-&gt;English, a low-resource pair and also several syntactic\nphenomena including prepositional phrase attachment. Furthermore, a tight\ncoupling of words and syntax improves translation quality more than multitask\ntraining. By combining target-syntax with adding source-side dependency labels\nin the embedding layer, we obtain a total improvement of 0.9 BLEU for\nGerman-&gt;English and 1.2 BLEU for Romanian-&gt;English.</p>\n", "tags": ["Training Techniques"] },
{"key": "nagoudi2021arat5", "citations": "62", "year": "2022", "title":"Arat5: Text-to-text Transformers For Arabic Language Generation", "abstract": "<p>Transfer learning with a unified Transformer framework (T5) that converts all\nlanguage problems into a text-to-text format was recently proposed as a simple\nand effective transfer learning approach. Although a multilingual version of\nthe T5 model (mT5) was also introduced, it is not clear how well it can fare on\nnon-English tasks involving diverse data. To investigate this question, we\napply mT5 on a language with a wide variety of dialects–Arabic. For\nevaluation, we introduce a novel benchmark for ARabic language GENeration\n(ARGEN), covering seven important tasks. For model comparison, we pre-train\nthree powerful Arabic T5-style models and evaluate them on ARGEN. Although\npre-trained with ~49 less data, our new models perform significantly better\nthan mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new\nSOTAs. Our models also establish new SOTA on the recently-proposed, large\nArabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al.,\n2021). Our new models are publicly available. We also link to ARGEN datasets\nthrough our repository: https://github.com/UBC-NLP/araT5.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture","Tools"] },
{"key": "naik2018stress", "citations": "303", "year": "2018", "title":"Stress Test Evaluation For Natural Language Inference", "abstract": "<p>Natural language inference (NLI) is the task of determining if a natural\nlanguage hypothesis can be inferred from a given premise in a justifiable\nmanner. NLI was proposed as a benchmark task for natural language\nunderstanding. Existing models perform well at standard datasets for NLI,\nachieving impressive results across different genres of text. However, the\nextent to which these models understand the semantic content of sentences is\nunclear. In this work, we propose an evaluation methodology consisting of\nautomatically constructed “stress tests” that allow us to examine whether\nsystems have the ability to make real inferential decisions. Our evaluation of\nsix sentence-encoder models on these stress tests reveals strengths and\nweaknesses of these models with respect to challenging linguistic phenomena,\nand suggests important directions for future work in this area.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "nakano2021webgpt", "citations": "187", "year": "2021", "title":"Webgpt: Browser-assisted Question-answering With Human Feedback", "abstract": "<p>We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. By setting up the task so that it can be performed by humans, we are able\nto train models on the task using imitation learning, and then optimize answer\nquality with human feedback. To make human evaluation of factual accuracy\neasier, models must collect references while browsing in support of their\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\ncloning, and then performing rejection sampling against a reward model trained\nto predict human preferences. This model’s answers are preferred by humans 56%\nof the time to those of our human demonstrators, and 69% of the time to the\nhighest-voted answer from Reddit.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "nam2016dual", "citations": "732", "year": "2017", "title":"Dual Attention Networks For Multimodal Reasoning And Matching", "abstract": "<p>We propose Dual Attention Networks (DANs) which jointly leverage visual and\ntextual attention mechanisms to capture fine-grained interplay between vision\nand language. DANs attend to specific regions in images and words in text\nthrough multiple steps and gather essential information from both modalities.\nBased on this framework, we introduce two types of DANs for multimodal\nreasoning and matching, respectively. The reasoning model allows visual and\ntextual attentions to steer each other during collaborative inference, which is\nuseful for tasks such as Visual Question Answering (VQA). In addition, the\nmatching model exploits the two attention mechanisms to estimate the similarity\nbetween images and sentences by focusing on their shared semantics. Our\nextensive experiments validate the effectiveness of DANs in combining vision\nand language, achieving the state-of-the-art performance on public benchmarks\nfor VQA and image-text matching.</p>\n", "tags": ["CVPR","Model Architecture","Tools"] },
{"key": "nam2023using", "citations": "84", "year": "2024", "title":"Using An LLM To Help With Code Understanding", "abstract": "<p>Understanding code is challenging, especially when working in new and complex\ndevelopment environments. Code comments and documentation can help, but are\ntypically scarce or hard to navigate. Large language models (LLMs) are\nrevolutionizing the process of writing code. Can they do the same for helping\nunderstand it? In this study, we provide a first investigation of an LLM-based\nconversational UI built directly in the IDE that is geared towards code\nunderstanding. Our IDE plugin queries OpenAI’s GPT-3.5-turbo model with four\nhigh-level requests without the user having to write explicit prompts: to\nexplain a highlighted section of code, provide details of API calls used in the\ncode, explain key domain-specific terms, and provide usage examples for an API.\nThe plugin also allows for open-ended prompts, which are automatically\ncontextualized to the LLM with the program being edited. We evaluate this\nsystem in a user study with 32 participants, which confirms that using our\nplugin can aid task completion more than web search. We additionally provide a\nthorough analysis of the ways developers use, and perceive the usefulness of,\nour system, among others finding that the usage and benefits differ between\nstudents and professionals. We conclude that in-IDE prompt-less interaction\nwith LLMs is a promising future direction for tool builders.</p>\n", "tags": ["Llm For Code","Model Architecture","Prompting","Tools"] },
{"key": "nan2020dart", "citations": "71", "year": "2021", "title":"DART: Open-domain Structured Data Record To Text Generation", "abstract": "<p>We present DART, an open domain structured DAta Record to Text generation\ndataset with over 82k instances (DARTs). Data-to-Text annotations can be a\ncostly process, especially when dealing with tables which are the major source\nof structured data and contain nontrivial structures. To this end, we propose a\nprocedure of extracting semantic triples from tables that encodes their\nstructures by exploiting the semantic dependencies among table headers and the\ntable title. Our dataset construction framework effectively merged\nheterogeneous sources from open domain semantic parsing and dialogue-act-based\nmeaning representation tasks by utilizing techniques such as: tree ontology\nannotation, question-answer pair to declarative sentence conversion, and\npredicate unification, all with minimum post-editing. We present systematic\nevaluation on DART as well as new state-of-the-art results on WebNLG 2017 to\nshow that DART (1) poses new challenges to existing data-to-text datasets and\n(2) facilitates out-of-domain generalization. Our data and code can be found at\nhttps://github.com/Yale-LILY/dart.</p>\n", "tags": ["Datasets","Evaluation","Has Code","NAACL","Tools"] },
{"key": "nan2021fetaqa", "citations": "60", "year": "2022", "title":"Fetaqa: Free-form Table Question Answering", "abstract": "<p>Existing table question answering datasets contain abundant factual questions\nthat primarily evaluate the query and schema comprehension capability of a\nsystem, but they fail to include questions that require complex reasoning and\nintegration of information due to the constraint of the associated short-form\nanswers. To address these issues and to demonstrate the full challenge of table\nquestion answering, we introduce FeTaQA, a new dataset with 10K Wikipedia-based\n{table, question, free-form answer, supporting table cells} pairs. FeTaQA\nyields a more challenging table question answering setting because it requires\ngenerating free-form text answers after retrieval, inference, and integration\nof multiple discontinuous facts from a structured knowledge source. Unlike\ndatasets of generative QA over text in which answers are prevalent with copies\nof short text spans from the source, answers in our dataset are human-generated\nexplanations involving entities and their high-level relations. We provide two\nbenchmark methods for the proposed task: a pipeline method based on\nsemantic-parsing-based QA systems and an end-to-end method based on large\npretrained text generation models, and show that FeTaQA poses a challenge for\nboth methods.</p>\n", "tags": ["Datasets","Evaluation","TACL"] },
{"key": "nangia2017repeval", "citations": "87", "year": "2017", "title":"The Repeval 2017 Shared Task: Multi-genre Natural Language Inference With Sentence Representations", "abstract": "<p>This paper presents the results of the RepEval 2017 Shared Task, which\nevaluated neural network sentence representation learning models on the\nMulti-Genre Natural Language Inference corpus (MultiNLI) recently introduced by\nWilliams et al. (2017). All of the five participating teams beat the\nbidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in\nWilliams et al.. The best single model used stacked BiLSTMs with residual\nconnections to extract sentence features and reached 74.5% accuracy on the\ngenre-matched test set. Surprisingly, the results of the competition were\nfairly consistent across the genre-matched and genre-mismatched test sets, and\nacross subsets of the test data representing a variety of linguistic phenomena,\nsuggesting that all of the submitted systems learned reasonably\ndomain-independent representations for sentence meaning.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "narang2020wt5", "citations": "110", "year": "2020", "title":"WT5?! Training Text-to-text Models To Explain Their Predictions", "abstract": "<p>Neural networks have recently achieved human-level performance on various\nchallenging natural language processing (NLP) tasks, but it is notoriously\ndifficult to understand why a neural network produced a particular prediction.\nIn this paper, we leverage the text-to-text framework proposed by Raffel et\nal.(2019) to train language models to output a natural text explanation\nalongside their prediction. Crucially, this requires no modifications to the\nloss function or training and decoding procedures – we simply train the model\nto output the explanation after generating the (natural text) prediction. We\nshow that this approach not only obtains state-of-the-art results on\nexplainability benchmarks, but also permits learning from a limited set of\nlabeled explanations and transferring rationalization abilities across\ndatasets. To facilitate reproducibility and future work, we release our code\nuse to train the models.</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "narang2021do", "citations": "69", "year": "2021", "title":"Do Transformer Modifications Transfer Across Implementations And Applications?", "abstract": "<p>The research community has proposed copious modifications to the Transformer\narchitecture since it was introduced over three years ago, relatively few of\nwhich have seen widespread adoption. In this paper, we comprehensively evaluate\nmany of these modifications in a shared experimental setting that covers most\nof the common uses of the Transformer in natural language processing.\nSurprisingly, we find that most modifications do not meaningfully improve\nperformance. Furthermore, most of the Transformer variants we found beneficial\nwere either developed in the same codebase that we used or are relatively minor\nchanges. We conjecture that performance improvements may strongly depend on\nimplementation details and correspondingly make some recommendations for\nimproving the generality of experimental results.</p>\n", "tags": ["Applications","EMNLP","Model Architecture"] },
{"key": "narasimhan2021clip", "citations": "66", "year": "2021", "title":"Clip-it! Language-guided Video Summarization", "abstract": "<p>A generic video summary is an abridged version of a video that conveys the\nwhole story and features the most important scenes. Yet the importance of\nscenes in a video is often subjective, and users should have the option of\ncustomizing the summary by using natural language to specify what is important\nto them. Further, existing models for fully automatic generic summarization\nhave not exploited available language models, which can serve as an effective\nprior for saliency. This work introduces CLIP-It, a single framework for\naddressing both generic and query-focused video summarization, typically\napproached separately in the literature. We propose a language-guided\nmultimodal transformer that learns to score frames in a video based on their\nimportance relative to one another and their correlation with a user-defined\nquery (for query-focused summarization) or an automatically generated dense\nvideo caption (for generic video summarization). Our model can be extended to\nthe unsupervised setting by training without ground-truth supervision. We\noutperform baselines and prior work by a significant margin on both standard\nvideo summarization datasets (TVSum and SumMe) and a query-focused video\nsummarization dataset (QFVS). Particularly, we achieve large improvements in\nthe transfer setting, attesting to our method’s strong generalization\ncapabilities.</p>\n", "tags": ["Datasets","Model Architecture","NEURIPS","Tools","Training Techniques"] },
{"key": "narayan2021planning", "citations": "80", "year": "2021", "title":"Planning With Learned Entity Prompts For Abstractive Summarization", "abstract": "<p>We introduce a simple but flexible mechanism to learn an intermediate plan to\nground the generation of abstractive summaries. Specifically, we prepend (or\nprompt) target summaries with entity chains – ordered sequences of entities\nmentioned in the summary. Transformer-based sequence-to-sequence models are\nthen trained to generate the entity chain and then continue generating the\nsummary conditioned on the entity chain and the input. We experimented with\nboth pretraining and finetuning with this content planning objective. When\nevaluated on CNN/DailyMail, XSum, SAMSum and BillSum, we demonstrate\nempirically that the grounded generation with the planning objective improves\nentity specificity and planning in summaries for all datasets, and achieves\nstate-of-the-art performance on XSum and SAMSum in terms of Rouge. Moreover, we\ndemonstrate empirically that planning with entity chains provides a mechanism\nto control hallucinations in abstractive summaries. By prompting the decoder\nwith a modified content plan that drops hallucinated entities, we outperform\nstate-of-the-art approaches for faithfulness when evaluated automatically and\nby humans.</p>\n", "tags": ["Datasets","Model Architecture","Prompting","TACL"] },
{"key": "narayanan2018how", "citations": "81", "year": "2018", "title":"How Do Humans Understand Explanations From Machine Learning Systems? An Evaluation Of The Human-interpretability Of Explanation", "abstract": "<p>Recent years have seen a boom in interest in machine learning systems that\ncan provide a human-understandable rationale for their predictions or\ndecisions. However, exactly what kinds of explanation are truly\nhuman-interpretable remains poorly understood. This work advances our\nunderstanding of what makes explanations interpretable in the specific context\nof verification. Suppose we have a machine learning system that predicts X, and\nwe provide rationale for this prediction X. Given an input, an explanation, and\nan output, is the output consistent with the input and the supposed rationale?\nVia a series of user-studies, we identify what kinds of increases in complexity\nhave the greatest effect on the time it takes for humans to verify the\nrationale, and which seem relatively insensitive.</p>\n", "tags": ["Evaluation"] },
{"key": "narayanan2021efficient", "citations": "260", "year": "2021", "title":"Efficient Large-scale Language Model Training On GPU Clusters Using Megatron-lm", "abstract": "<p>Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these models efficiently is challenging for two\nreasons: a) GPU memory capacity is limited, making it impossible to fit large\nmodels on even a multi-GPU server, and b) the number of compute operations\nrequired to train these models can result in unrealistically long training\ntimes. Consequently, new methods of model parallelism such as tensor and\npipeline parallelism have been proposed. Unfortunately, naive usage of these\nmethods leads to fundamental scaling issues at thousands of GPUs, e.g., due to\nexpensive cross-node communication or devices spending significant time waiting\non other devices to make progress.\n  In this paper, we show how different types of parallelism methods (tensor,\npipeline, and data parallelism) can be composed to scale to thousands of GPUs\nand models with trillions of parameters. We survey techniques for pipeline\nparallelism and propose a novel interleaved pipeline parallelism schedule that\ncan improve throughput by 10+% with memory footprint comparable to existing\napproaches. We quantitatively study the trade-offs between tensor, pipeline,\nand data parallelism, and provide intuition as to how to configure distributed\ntraining of a large model. Our approach allows us to perform training\niterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs\nwith achieved per-GPU throughput of 52% of theoretical peak. Our code is open\nsourced at https://github.com/nvidia/megatron-lm.</p>\n", "tags": ["Efficiency","Has Code","Training Techniques"] },
{"key": "narechania2020nl4dv", "citations": "135", "year": "2020", "title":"NL4DV: A Toolkit For Generating Analytic Specifications For Data Visualization From Natural Language Queries", "abstract": "<p>Natural language interfaces (NLIs) have shown great promise for visual data\nanalysis, allowing people to flexibly specify and interact with visualizations.\nHowever, developing visualization NLIs remains a challenging task, requiring\nlow-level implementation of natural language processing (NLP) techniques as\nwell as knowledge of visual analytic tasks and visualization design. We present\nNL4DV, a toolkit for natural language-driven data visualization. NL4DV is a\nPython package that takes as input a tabular dataset and a natural language\nquery about that dataset. In response, the toolkit returns an analytic\nspecification modeled as a JSON object containing data attributes, analytic\ntasks, and a list of Vega-Lite specifications relevant to the input query. In\ndoing so, NL4DV aids visualization developers who may not have a background in\nNLP, enabling them to create new visualization NLIs or incorporate natural\nlanguage input within their existing systems. We demonstrate NL4DV’s usage and\ncapabilities through four examples: 1) rendering visualizations using natural\nlanguage in a Jupyter notebook, 2) developing a NLI to specify and edit\nVega-Lite charts, 3) recreating data ambiguity widgets from the DataTone\nsystem, and 4) incorporating speech input to create a multimodal visualization\nsystem.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "naseem2019rewarding", "citations": "65", "year": "2019", "title":"Rewarding Smatch: Transition-based AMR Parsing With Reinforcement Learning", "abstract": "<p>Our work involves enriching the Stack-LSTM transition-based AMR parser\n(Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning\nand rewarding the Smatch score of sampled graphs. In addition, we also combined\nseveral AMR-to-text alignments with an attention mechanism and we supplemented\nthe parser with pre-processed concept identification, named entities and\ncontextualized embeddings. We achieve a highly competitive performance that is\ncomparable to the best published results. We show an in-depth study ablating\neach of the new components of the parser</p>\n", "tags": ["Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "naveed2023comprehensive", "citations": "233", "year": "2023", "title":"A Comprehensive Overview Of Large Language Models", "abstract": "<p>Large Language Models (LLMs) have recently demonstrated remarkable\ncapabilities in natural language processing tasks and beyond. This success of\nLLMs has led to a large influx of research contributions in this direction.\nThese works encompass diverse topics such as architectural innovations, better\ntraining strategies, context length improvements, fine-tuning, multi-modal\nLLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid\ndevelopment of techniques and regular breakthroughs in LLM research, it has\nbecome considerably challenging to perceive the bigger picture of the advances\nin this direction. Considering the rapidly emerging plethora of literature on\nLLMs, it is imperative that the research community is able to benefit from a\nconcise yet comprehensive overview of the recent developments in this field.\nThis article provides an overview of the existing literature on a broad range\nof LLM-related concepts. Our self-contained comprehensive overview of LLMs\ndiscusses relevant background concepts along with covering the advanced topics\nat the frontier of research in LLMs. This review article is intended to not\nonly provide a systematic survey but also a quick comprehensive reference for\nthe researchers and practitioners to draw insights from extensive informative\nsummaries of the existing works to advance the LLM research.</p>\n", "tags": ["Datasets","Fine-Tuning","Memory & Context","Survey Paper","Training Techniques"] },
{"key": "nazi2023large", "citations": "91", "year": "2024", "title":"Large Language Models In Healthcare And Medical Domain: A Review", "abstract": "<p>The deployment of large language models (LLMs) within the healthcare sector\nhas sparked both enthusiasm and apprehension. These models exhibit the\nremarkable capability to provide proficient responses to free-text queries,\ndemonstrating a nuanced understanding of professional medical knowledge. This\ncomprehensive survey delves into the functionalities of existing LLMs designed\nfor healthcare applications, elucidating the trajectory of their development,\nstarting from traditional Pretrained Language Models (PLMs) to the present\nstate of LLMs in healthcare sector. First, we explore the potential of LLMs to\namplify the efficiency and effectiveness of diverse healthcare applications,\nparticularly focusing on clinical language understanding tasks. These tasks\nencompass a wide spectrum, ranging from named entity recognition and relation\nextraction to natural language inference, multi-modal medical applications,\ndocument classification, and question-answering. Additionally, we conduct an\nextensive comparison of the most recent state-of-the-art LLMs in the healthcare\ndomain, while also assessing the utilization of various open-source LLMs and\nhighlighting their significance in healthcare applications. Furthermore, we\npresent the essential performance metrics employed to evaluate LLMs in the\nbiomedical domain, shedding light on their effectiveness and limitations.\nFinally, we summarize the prominent challenges and constraints faced by large\nlanguage models in the healthcare sector, offering a holistic perspective on\ntheir potential benefits and shortcomings. This review provides a comprehensive\nexploration of the current landscape of LLMs in healthcare, addressing their\nrole in transforming medical applications and the areas that warrant further\nresearch and development.</p>\n", "tags": ["Applications","Evaluation","Survey Paper"] },
{"key": "neelakantan2016learning", "citations": "78", "year": "2016", "title":"Learning A Natural Language Interface With Neural Programmer", "abstract": "<p>Learning a natural language interface for database tables is a challenging\ntask that involves deep language understanding and multi-step reasoning. The\ntask is often approached by mapping natural language queries to logical forms\nor programs that provide the desired response when executed on the database. To\nour knowledge, this paper presents the first weakly supervised, end-to-end\nneural network model to induce such programs on a real-world dataset. We\nenhance the objective function of Neural Programmer, a neural network with\nbuilt-in discrete operations, and apply it on WikiTableQuestions, a natural\nlanguage question-answering dataset. The model is trained end-to-end with weak\nsupervision of question-answer pairs, and does not require domain-specific\ngrammars, rules, or annotations that are key elements in previous approaches to\nprogram induction. The main experimental result in this paper is that a single\nNeural Programmer model achieves 34.2% accuracy using only 10,000 examples with\nweak supervision. An ensemble of 15 models, with a trivial combination\ntechnique, achieves 37.7% accuracy, which is competitive to the current\nstate-of-the-art accuracy of 37.1% obtained by a traditional natural language\nsemantic parser.</p>\n", "tags": ["Datasets"] },
{"key": "neelakantan2022text", "citations": "110", "year": "2022", "title":"Text And Code Embeddings By Contrastive Pre-training", "abstract": "<p>Text embeddings are useful features in many applications such as semantic\nsearch and computing text similarity. Previous work typically trains models\ncustomized for different use cases, varying in dataset choice, training\nobjective and model architecture. In this work, we show that contrastive\npre-training on unsupervised data at scale leads to high quality vector\nrepresentations of text and code. The same unsupervised text embeddings that\nachieve new state-of-the-art results in linear-probe classification also\ndisplay impressive semantic search capabilities and sometimes even perform\ncompetitively with fine-tuned models. On linear-probe classification accuracy\naveraging over 7 tasks, our best unsupervised model achieves a relative\nimprovement of 4% and 1.8% over previous best unsupervised and supervised text\nembedding models respectively. The same text embeddings when evaluated on\nlarge-scale semantic search attains a relative improvement of 23.4%, 14.7%, and\n10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and\nTriviaQA benchmarks, respectively. Similarly to text embeddings, we train code\nembedding models on (text, code) pairs, obtaining a 20.8% relative improvement\nover prior best work on code search.</p>\n", "tags": ["Applications","Datasets","Model Architecture","Training Techniques"] },
{"key": "nema2017diversity", "citations": "171", "year": "2017", "title":"Diversity Driven Attention Model For Query-based Abstractive Summarization", "abstract": "<p>Abstractive summarization aims to generate a shorter version of the document\ncovering all the salient points in a compact and coherent fashion. On the other\nhand, query-based summarization highlights those points that are relevant in\nthe context of a given query. The encode-attend-decode paradigm has achieved\nnotable success in machine translation, extractive summarization, dialog\nsystems, etc. But it suffers from the drawback of generation of repeated\nphrases. In this work we propose a model for the query-based summarization task\nbased on the encode-attend-decode paradigm with two key additions (i) a query\nattention model (in addition to document attention model) which learns to focus\non different portions of the query at different time steps (instead of using a\nstatic representation for the query) and (ii) a new diversity based attention\nmodel which aims to alleviate the problem of repeating phrases in the summary.\nIn order to enable the testing of this model we introduce a new query-based\nsummarization dataset building on debatepedia. Our experiments show that with\nthese two additions the proposed model clearly outperforms vanilla\nencode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "neubig2018rapid", "citations": "209", "year": "2018", "title":"Rapid Adaptation Of Neural Machine Translation To New Languages", "abstract": "<p>This paper examines the problem of adapting neural machine translation\nsystems to new, low-resourced languages (LRLs) as effectively and rapidly as\npossible. We propose methods based on starting with massively multilingual\n“seed models”, which can be trained ahead-of-time, and then continuing training\non data related to the LRL. We contrast a number of strategies, leading to a\nnovel, simple, yet effective method of “similar-language regularization”, where\nwe jointly train on both a LRL of interest and a similar high-resourced\nlanguage to prevent over-fitting to small LRL data. Experiments demonstrate\nthat massively multilingual models, even without any explicit adaptation, are\nsurprisingly effective, achieving BLEU scores of up to 15.5 with no data from\nthe LRL, and that the proposed similar-language regularization method improves\nover other adaptation methods by 1.7 BLEU points average over 4 LRL settings.\nCode to reproduce experiments at https://github.com/neubig/rapid-adaptation</p>\n", "tags": ["EMNLP","Has Code","Training Techniques"] },
{"key": "neubig2019compare", "citations": "117", "year": "2019", "title":"Compare-mt: A Tool For Holistic Comparison Of Language Generation Systems", "abstract": "<p>In this paper, we describe compare-mt, a tool for holistic analysis and\ncomparison of the results of systems for language generation tasks such as\nmachine translation. The main goal of the tool is to give the user a high-level\nand coherent view of the salient differences between systems that can then be\nused to guide further analysis or system improvement. It implements a number of\ntools to do so, such as analysis of accuracy of generation of particular types\nof words, bucketed histograms of sentence accuracies or counts based on salient\ncharacteristics, and extraction of characteristic \\(n\\)-grams for each system. It\nalso has a number of advanced features such as use of linguistic labels, source\nside data, or comparison of log likelihoods for probabilistic models, and also\naims to be easily extensible by users to new types of analysis. The code is\navailable at https://github.com/neulab/compare-mt</p>\n", "tags": ["Has Code","Tools"] },
{"key": "nguyen2017improving", "citations": "96", "year": "2018", "title":"Improving Lexical Choice In Neural Machine Translation", "abstract": "<p>We explore two solutions to the problem of mistranslating rare words in\nneural machine translation. First, we argue that the standard output layer,\nwhich computes the inner product of a vector representing the context with all\npossible output word embeddings, rewards frequent words disproportionately, and\nwe propose to fix the norms of both vectors to a constant value. Second, we\nintegrate a simple lexical module which is jointly trained with the rest of the\nmodel. We evaluate our approaches on eight language pairs with data sizes\nranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU,\nsurpassing phrase-based translation in nearly all settings.</p>\n", "tags": ["NAACL"] },
{"key": "nguyen2017reinforcement", "citations": "105", "year": "2017", "title":"Reinforcement Learning For Bandit Neural Machine Translation With Simulated Human Feedback", "abstract": "<p>Machine translation is a natural candidate problem for reinforcement learning\nfrom human feedback: users provide quick, dirty ratings on candidate\ntranslations to guide a system to improve. Yet, current neural machine\ntranslation training focuses on expensive human-generated reference\ntranslations. We describe a reinforcement learning algorithm that improves\nneural machine translation systems from simulated human feedback. Our algorithm\ncombines the advantage actor-critic algorithm (Mnih et al., 2016) with the\nattention-based neural encoder-decoder architecture (Luong et al., 2015). This\nalgorithm (a) is well-designed for problems with a large action space and\ndelayed rewards, (b) effectively optimizes traditional corpus-level machine\ntranslation metrics, and (c) is robust to skewed, high-variance, granular\nfeedback modeled after actual human behaviors.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "nguyen2018improved", "citations": "335", "year": "2018", "title":"Improved Fusion Of Visual And Language Representations By Dense Symmetric Co-attention For Visual Question Answering", "abstract": "<p>A key solution to visual question answering (VQA) exists in how to fuse\nvisual and language features extracted from an input image and question. We\nshow that an attention mechanism that enables dense, bi-directional\ninteractions between the two modalities contributes to boost accuracy of\nprediction of answers. Specifically, we present a simple architecture that is\nfully symmetric between visual and language representations, in which each\nquestion word attends on image regions and each image region attends on\nquestion words. It can be stacked to form a hierarchy for multi-step\ninteractions between an image-question pair. We show through experiments that\nthe proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0\ndespite its small size. We also present qualitative evaluation, demonstrating\nhow the proposed attention mechanism can generate reasonable attention maps on\nimages and questions, which leads to the correct answer prediction.</p>\n", "tags": ["CVPR","Evaluation","Model Architecture"] },
{"key": "nguyen2018vision", "citations": "93", "year": "2019", "title":"Vision-based Navigation With Language-based Assistance Via Imitation Learning With Indirect Intervention", "abstract": "<p>We present Vision-based Navigation with Language-based Assistance (VNLA), a\ngrounded vision-language task where an agent with visual perception is guided\nvia language to find objects in photorealistic indoor environments. The task\nemulates a real-world scenario in that (a) the requester may not know how to\nnavigate to the target objects and thus makes requests by only specifying\nhigh-level end-goals, and (b) the agent is capable of sensing when it is lost\nand querying an advisor, who is more qualified at the task, to obtain language\nsubgoals to make progress. To model language-based assistance, we develop a\ngeneral framework termed Imitation Learning with Indirect Intervention (I3L),\nand propose a solution that is effective on the VNLA task. Empirical results\nshow that this approach significantly improves the success rate of the learning\nagent over other baselines in both seen and unseen environments. Our code and\ndata are publicly available at https://github.com/debadeepta/vnla .</p>\n", "tags": ["Agentic","CVPR","Has Code","Tools"] },
{"key": "nguyen2019transformers", "citations": "133", "year": "2019", "title":"Transformers Without Tears: Improving The Normalization Of Self-attention", "abstract": "<p>We evaluate three simple, normalization-centric changes to improve\nTransformer training. First, we show that pre-norm residual connections\n(PreNorm) and smaller initializations enable warmup-free, validation-based\ntraining with large learning rates. Second, we propose \\(ℓ₂\\) normalization\nwith a single scale parameter (ScaleNorm) for faster training and better\nperformance. Finally, we reaffirm the effectiveness of normalizing word\nembeddings to a fixed length (FixNorm). On five low-resource translation pairs\nfrom TED Talks-based corpora, these changes always converge, giving an average\n+1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on\nIWSLT’15 English-Vietnamese. We observe sharper performance curves, more\nconsistent gradient norms, and a linear relationship between activation scaling\nand decoder depth. Surprisingly, in the high-resource setting (WMT’14\nEnglish-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades\nperformance.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "nguyen2020bertweet", "citations": "688", "year": "2020", "title":"Bertweet: A Pre-trained Language Model For English Tweets", "abstract": "<p>We present BERTweet, the first public large-scale pre-trained language model\nfor English Tweets. Our BERTweet, having the same architecture as BERT-base\n(Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu\net al., 2019). Experiments show that BERTweet outperforms strong baselines\nRoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better\nperformance results than the previous state-of-the-art models on three Tweet\nNLP tasks: Part-of-speech tagging, Named-entity recognition and text\nclassification. We release BERTweet under the MIT License to facilitate future\nresearch and applications on Tweet data. Our BERTweet is available at\nhttps://github.com/VinAIResearch/BERTweet</p>\n", "tags": ["Applications","EMNLP","Has Code","Model Architecture","Training Techniques"] },
{"key": "nguyen2020phobert", "citations": "310", "year": "2020", "title":"Phobert: Pre-trained Language Models For Vietnamese", "abstract": "<p>We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the\nfirst public large-scale monolingual language models pre-trained for\nVietnamese. Experimental results show that PhoBERT consistently outperforms the\nrecent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and\nimproves the state-of-the-art in multiple Vietnamese-specific NLP tasks\nincluding Part-of-speech tagging, Dependency parsing, Named-entity recognition\nand Natural language inference. We release PhoBERT to facilitate future\nresearch and downstream applications for Vietnamese NLP. Our PhoBERT models are\navailable at https://github.com/VinAIResearch/PhoBERT</p>\n", "tags": ["Applications","EMNLP","Has Code"] },
{"key": "ni2021recent", "citations": "159", "year": "2022", "title":"Recent Advances In Deep Learning Based Dialogue Systems: A Systematic Survey", "abstract": "<p>Dialogue systems are a popular natural language processing (NLP) task as it\nis promising in real-life applications. It is also a complicated task since\nmany NLP tasks deserving study are involved. As a result, a multitude of novel\nworks on this task are carried out, and most of them are deep learning based\ndue to the outstanding performance. In this survey, we mainly focus on the deep\nlearning based dialogue systems. We comprehensively review state-of-the-art\nresearch outcomes in dialogue systems and analyze them from two angles: model\ntype and system type. Specifically, from the angle of model type, we discuss\nthe principles, characteristics, and applications of different models that are\nwidely used in dialogue systems. This will help researchers acquaint these\nmodels and see how they are applied in state-of-the-art frameworks, which is\nrather helpful when designing a new dialogue system. From the angle of system\ntype, we discuss task-oriented and open-domain dialogue systems as two streams\nof research, providing insight into the hot topics related. Furthermore, we\ncomprehensively review the evaluation methods and datasets for dialogue systems\nto pave the way for future research. Finally, some possible research trends are\nidentified based on the recent research outcomes. To the best of our knowledge,\nthis survey is the most comprehensive and up-to-date one at present for deep\nlearning based dialogue systems, extensively covering the popular techniques.\nWe speculate that this work is a good starting point for academics who are new\nto the dialogue systems or those who want to quickly grasp up-to-date\ntechniques in this area.</p>\n", "tags": ["Applications","Datasets","Dialogue & Multi Turn","Evaluation","Survey Paper"] },
{"key": "ni2021sentence", "citations": "152", "year": "2022", "title":"Sentence-t5: Scalable Sentence Encoders From Pre-trained Text-to-text Models", "abstract": "<p>We provide the first exploration of sentence embeddings from text-to-text\ntransformers (T5). Sentence embeddings are broadly useful for language\nprocessing tasks. While T5 achieves impressive performance on language tasks\ncast as sequence-to-sequence mapping problems, it is unclear how to produce\nsentence embeddings from encoder-decoder models. We investigate three methods\nfor extracting T5 sentence embeddings: two utilize only the T5 encoder and one\nuses the full T5 encoder-decoder model. To support our investigation, we\nestablish a new sentence representation transfer benchmark, SentGLUE, which\nextends the SentEval toolkit to nine tasks from the GLUE benchmark. Our\nencoder-only models outperforms Sentence-BERT and SimCSE sentence embeddings on\nboth SentEval and SentGLUE transfer tasks, including semantic textual\nsimilarity (STS). Scaling up T5 from millions to billions of parameters is\nfound to produce consistent further improvements. Finally, our encoder-decoder\nmethod achieves a new state-of-the-art on STS when using sentence embeddings.\nOur models are released at https://tfhub.dev/google/collections/sentence-t5/1.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "ni2022expanding", "citations": "155", "year": "2022", "title":"Expanding Language-image Pretrained Models For General Video Recognition", "abstract": "<p>Contrastive language-image pretraining has shown great success in learning\nvisual-textual joint representation from web-scale data, demonstrating\nremarkable “zero-shot” generalization ability for various image tasks. However,\nhow to effectively expand such new language-image pretraining methods to video\ndomains is still an open problem. In this work, we present a simple yet\neffective approach that adapts the pretrained language-image models to video\nrecognition directly, instead of pretraining a new model from scratch. More\nconcretely, to capture the long-range dependencies of frames along the temporal\ndimension, we propose a cross-frame attention mechanism that explicitly\nexchanges information across frames. Such module is lightweight and can be\nplugged into pretrained language-image models seamlessly. Moreover, we propose\na video-specific prompting scheme, which leverages video content information\nfor generating discriminative textual prompts. Extensive experiments\ndemonstrate that our approach is effective and can be generalized to different\nvideo recognition scenarios. In particular, under fully-supervised settings,\nour approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using\n12 times fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot\nexperiments, our approach surpasses the current state-of-the-art methods by\n+7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In\nfew-shot scenarios, our approach outperforms previous best methods by +32.1%\nand +23.1% when the labeled data is extremely limited. Code and models are\navailable at https://aka.ms/X-CLIP</p>\n", "tags": ["Few-Shot","Has Code","Model Architecture","Prompting"] },
{"key": "nie2018analyzing", "citations": "94", "year": "2019", "title":"Analyzing Compositionality-sensitivity Of NLI Models", "abstract": "<p>Success in natural language inference (NLI) should require a model to\nunderstand both lexical and compositional semantics. However, through\nadversarial evaluation, we find that several state-of-the-art models with\ndiverse architectures are over-relying on the former and fail to use the\nlatter. Further, this compositionality unawareness is not reflected via\nstandard evaluation on current datasets. We show that removing RNNs in existing\nmodels or shuffling input words during training does not induce large\nperformance loss despite the explicit removal of compositional information.\nTherefore, we propose a compositionality-sensitivity testing setup that\nanalyzes models on natural examples from existing datasets that cannot be\nsolved via lexical features alone (i.e., on which a bag-of-words model gives a\nhigh probability to one wrong label), hence revealing the models’ actual\ncompositionality awareness. We show that this setup not only highlights the\nlimited compositional ability of current NLI models, but also differentiates\nmodel performance based on design, e.g., separating shallow bag-of-words models\nfrom deeper, linguistically-grounded tree-based models. Our evaluation setup is\nan important analysis tool: complementing currently existing adversarial and\nlinguistically driven diagnostic evaluations, and exposing opportunities for\nfuture work on evaluating models’ compositional understanding.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Training Techniques"] },
{"key": "nie2019adversarial", "citations": "599", "year": "2020", "title":"Adversarial NLI: A New Benchmark For Natural Language Understanding", "abstract": "<p>We introduce a new large-scale NLI benchmark dataset, collected via an\niterative, adversarial human-and-model-in-the-loop procedure. We show that\ntraining models on this new dataset leads to state-of-the-art performance on a\nvariety of popular NLI benchmarks, while posing a more difficult challenge with\nits new test set. Our analysis sheds light on the shortcomings of current\nstate-of-the-art models, and shows that non-expert annotators are successful at\nfinding their weaknesses. The data collection method can be applied in a\nnever-ending learning scenario, becoming a moving target for NLU, rather than a\nstatic benchmark that will quickly saturate.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "nie2019revealing", "citations": "152", "year": "2019", "title":"Revealing The Importance Of Semantic Retrieval For Machine Reading At Scale", "abstract": "<p>Machine Reading at Scale (MRS) is a challenging task in which a system is\ngiven an input query and is asked to produce a precise output by “reading”\ninformation from a large knowledge base. The task has gained popularity with\nits natural combination of information retrieval (IR) and machine comprehension\n(MC). Advancements in representation learning have led to separated progress in\nboth IR and MC; however, very few studies have examined the relationship and\ncombined design of retrieval and comprehension at different levels of\ngranularity, for development of MRS systems. In this work, we give general\nguidelines on system design for MRS by proposing a simple yet effective\npipeline system with special consideration on hierarchical semantic retrieval\nat both paragraph and sentence level, and their potential effects on the\ndownstream task. The system is evaluated on both fact verification and\nopen-domain multihop QA, achieving state-of-the-art results on the leaderboard\ntest sets of both FEVER and HOTPOTQA. To further demonstrate the importance of\nsemantic retrieval, we present ablation and analysis studies to quantify the\ncontribution of neural retrieval modules at both paragraph-level and\nsentence-level, and illustrate that intermediate semantic retrieval modules are\nvital for not only effectively filtering upstream information and thus saving\ndownstream computation, but also for shaping upstream data distribution and\nproviding better data for downstream modeling. Code/data made publicly\navailable at: https://github.com/easonnie/semanticRetrievalMRS</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code"] },
{"key": "niehues2016pre", "citations": "74", "year": "2016", "title":"Pre-translation For Neural Machine Translation", "abstract": "<p>Recently, the development of neural machine translation (NMT) has\nsignificantly improved the translation quality of automatic machine\ntranslation. While most sentences are more accurate and fluent than\ntranslations by statistical machine translation (SMT)-based systems, in some\ncases, the NMT system produces translations that have a completely different\nmeaning. This is especially the case when rare words occur.\n  When using statistical machine translation, it has already been shown that\nsignificant gains can be achieved by simplifying the input in a preprocessing\nstep. A commonly used example is the pre-reordering approach.\n  In this work, we used phrase-based machine translation to pre-translate the\ninput into the target language. Then a neural machine translation system\ngenerates the final hypothesis using the pre-translation. Thereby, we use\neither only the output of the phrase-based machine translation (PBMT) system or\na combination of the PBMT output and the source sentence.\n  We evaluate the technique on the English to German translation task. Using\nthis approach we are able to outperform the PBMT system as well as the baseline\nneural MT system by up to 2 BLEU points. We analyzed the influence of the\nquality of the initial system on the final result.</p>\n", "tags": [] },
{"key": "ning2020torque", "citations": "81", "year": "2020", "title":"TORQUE: A Reading Comprehension Dataset Of Temporal Ordering Questions", "abstract": "<p>A critical part of reading is being able to understand the temporal\nrelationships between events described in a passage of text, even when those\nrelationships are not explicitly stated. However, current machine reading\ncomprehension benchmarks have practically no questions that test temporal\nphenomena, so systems trained on these benchmarks have no capacity to answer\nquestions such as “what happened before/after [some event]?” We introduce\nTORQUE, a new English reading comprehension benchmark built on 3.2k news\nsnippets with 21k human-generated questions querying temporal relationships.\nResults show that RoBERTa-large achieves an exact-match score of 51% on the\ntest set of TORQUE, about 30% behind human performance.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "nishida2019answering", "citations": "100", "year": "2019", "title":"Answering While Summarizing: Multi-task Learning For Multi-hop QA With Evidence Extraction", "abstract": "<p>Question answering (QA) using textual sources for purposes such as reading\ncomprehension (RC) has attracted much attention. This study focuses on the task\nof explainable multi-hop QA, which requires the system to return the answer\nwith evidence sentences by reasoning and gathering disjoint pieces of the\nreference texts. It proposes the Query Focused Extractor (QFE) model for\nevidence extraction and uses multi-task learning with the QA model. QFE is\ninspired by extractive summarization models; compared with the existing method,\nwhich extracts each evidence sentence independently, it sequentially extracts\nevidence sentences by using an RNN with an attention mechanism on the question\nsentence. It enables QFE to consider the dependency among the evidence\nsentences and cover important information in the question sentence.\nExperimental results show that QFE with a simple RC baseline model achieves a\nstate-of-the-art evidence extraction score on HotpotQA. Although designed for\nRC, it also achieves a state-of-the-art evidence extraction score on FEVER,\nwhich is a recognizing textual entailment task on a large textual database.</p>\n", "tags": ["Model Architecture"] },
{"key": "nishida2019multi", "citations": "82", "year": "2019", "title":"Multi-style Generative Reading Comprehension", "abstract": "<p>This study tackles generative reading comprehension (RC), which consists of\nanswering questions based on textual evidence and natural language generation\n(NLG). We propose a multi-style abstractive summarization model for question\nanswering, called Masque. The proposed model has two key characteristics.\nFirst, unlike most studies on RC that have focused on extracting an answer span\nfrom the provided passages, our model instead focuses on generating a summary\nfrom the question and multiple passages. This serves to cover various answer\nstyles required for real-world applications. Second, whereas previous studies\nbuilt a specific model for each answer style because of the difficulty of\nacquiring one general model, our approach learns multi-style answers within a\nmodel to improve the NLG capability for all styles involved. This also enables\nour model to give an answer in the target style. Experiments show that our\nmodel achieves state-of-the-art performance on the Q&amp;A task and the Q&amp;A + NLG\ntask of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the\ntransfer of the style-independent NLG capability to the target style is the key\nto its success.</p>\n", "tags": ["Applications"] },
{"key": "niu2018adversarial", "citations": "61", "year": "2018", "title":"Adversarial Over-sensitivity And Over-stability Strategies For Dialogue Models", "abstract": "<p>We present two categories of model-agnostic adversarial strategies that\nreveal the weaknesses of several generative, task-oriented dialogue models:\nShould-Not-Change strategies that evaluate over-sensitivity to small and\nsemantics-preserving edits, as well as Should-Change strategies that test if a\nmodel is over-stable against subtle yet semantics-changing modifications. We\nnext perform adversarial training with each strategy, employing a max-margin\napproach for negative generative examples. This not only makes the target\ndialogue model more robust to the adversarial inputs, but also helps it perform\nsignificantly better on the original inputs. Moreover, training on all\nstrategies combined achieves further improvements, achieving a new\nstate-of-the-art performance on the original task (also verified via human\nevaluation). In addition to adversarial training, we also address the\nrobustness task at the model-level, by feeding it subword units as both inputs\nand outputs, and show that the resulting model is equally competitive, requires\nonly 1/4 of the original vocabulary size, and is robust to one of the\nadversarial strategies (to which the original model is vulnerable) even without\nadversarial training.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","Security","Training Techniques"] },
{"key": "niu2018bi", "citations": "80", "year": "2018", "title":"Bi-directional Differentiable Input Reconstruction For Low-resource Neural Machine Translation", "abstract": "<p>We aim to better exploit the limited amounts of parallel text available in\nlow-resource settings by introducing a differentiable reconstruction loss for\nneural machine translation (NMT). This loss compares original inputs to\nreconstructed inputs, obtained by back-translating translation hypotheses into\nthe input language. We leverage differentiable sampling and bi-directional NMT\nto train models end-to-end, without introducing additional parameters. This\napproach achieves small but consistent BLEU improvements on four language pairs\nin both translation directions, and outperforms an alternative differentiable\nreconstruction strategy based on hidden states.</p>\n", "tags": [] },
{"key": "niu2018polite", "citations": "173", "year": "2018", "title":"Polite Dialogue Generation Without Parallel Data", "abstract": "<p>Stylistic dialogue response generation, with valuable applications in\npersonality-based conversational agents, is a challenging task because the\nresponse needs to be fluent, contextually-relevant, as well as\nparalinguistically accurate. Moreover, parallel datasets for\nregular-to-stylistic pairs are usually unavailable. We present three\nweakly-supervised models that can generate diverse polite (or rude) dialogue\nresponses without parallel data. Our late fusion model (Fusion) merges the\ndecoder of an encoder-attention-decoder dialogue model with a language model\ntrained on stand-alone polite utterances. Our label-fine-tuning (LFT) model\nprepends to each source sequence a politeness-score scaled label (predicted by\nour state-of-the-art politeness classifier) during training, and at test time\nis able to generate polite, neutral, and rude responses by simply scaling the\nlabel embedding by the corresponding score. Our reinforcement learning model\n(Polite-RL) encourages politeness generation by assigning rewards proportional\nto the politeness classifier score of the sampled response. We also present two\nretrieval-based polite dialogue model baselines. Human evaluation validates\nthat while the Fusion and the retrieval-based models achieve politeness with\npoorer context-relevance, the LFT and Polite-RL models can produce\nsignificantly more polite responses without sacrificing dialogue quality.</p>\n", "tags": ["Applications","Datasets","Evaluation","Fine-Tuning","Model Architecture","Reinforcement Learning","TACL","Training Techniques"] },
{"key": "niu2018recursive", "citations": "139", "year": "2019", "title":"Recursive Visual Attention In Visual Dialog", "abstract": "<p>Visual dialog is a challenging vision-language task, which requires the agent\nto answer multi-round questions about an image. It typically needs to address\ntwo major problems: (1) How to answer visually-grounded questions, which is the\ncore challenge in visual question answering (VQA); (2) How to infer the\nco-reference between questions and the dialog history. An example of visual\nco-reference is: pronouns (\\eg, <code class=\"language-plaintext highlighter-rouge\">they'') in the question (\\eg,</code>Are they on\nor off?’’) are linked with nouns (\\eg, <code class=\"language-plaintext highlighter-rouge\">lamps'') appearing in the dialog\nhistory (\\eg,</code>How many lamps are there?’’) and the object grounded in the\nimage. In this work, to resolve the visual co-reference for visual dialog, we\npropose a novel attention mechanism called Recursive Visual Attention (RvA).\nSpecifically, our dialog agent browses the dialog history until the agent has\nsufficient confidence in the visual co-reference resolution, and refines the\nvisual attention recursively. The quantitative and qualitative experimental\nresults on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the\nproposed RvA not only outperforms the state-of-the-art methods, but also\nachieves reasonable recursion and interpretable attention maps without\nadditional annotations. The code is available at\nhttps://github.com/yuleiniu/rva.</p>\n", "tags": ["CVPR","Dialogue & Multi Turn","Has Code","Model Architecture"] },
{"key": "niu2020counterfactual", "citations": "327", "year": "2021", "title":"Counterfactual VQA: A Cause-effect Look At Language Bias", "abstract": "<p>VQA models may tend to rely on language bias as a shortcut and thus fail to\nsufficiently learn the multi-modal knowledge from both vision and language.\nRecent debiasing methods proposed to exclude the language prior during\ninference. However, they fail to disentangle the “good” language context and\n“bad” language bias from the whole. In this paper, we investigate how to\nmitigate language bias in VQA. Motivated by causal effects, we proposed a novel\ncounterfactual inference framework, which enables us to capture the language\nbias as the direct causal effect of questions on answers and reduce the\nlanguage bias by subtracting the direct language effect from the total causal\neffect. Experiments demonstrate that our proposed counterfactual inference\nframework 1) is general to various VQA backbones and fusion strategies, 2)\nachieves competitive performance on the language-bias sensitive VQA-CP dataset\nwhile performs robustly on the balanced VQA v2 dataset without any augmented\ndata. The code is available at https://github.com/yuleiniu/cfvqa.</p>\n", "tags": ["CVPR","Datasets","Ethics & Fairness","Has Code","Tools"] },
{"key": "niven2019probing", "citations": "399", "year": "2019", "title":"Probing Neural Network Comprehension Of Natural Language Arguments", "abstract": "<p>We are surprised to find that BERT’s peak performance of 77% on the Argument\nReasoning Comprehension Task reaches just three points below the average\nuntrained human baseline. However, we show that this result is entirely\naccounted for by exploitation of spurious statistical cues in the dataset. We\nanalyze the nature of these cues and demonstrate that a range of models all\nexploit them. This analysis informs the construction of an adversarial dataset\non which all models achieve random accuracy. Our adversarial dataset provides a\nmore robust assessment of argument comprehension and should be adopted as the\nstandard in future work.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "noah2023reflexion", "citations": "157", "year": "2023", "title":"Reflexion: Language Agents With Verbal Reinforcement Learning", "abstract": "<p>Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Fine-Tuning","Model Architecture","Prompting","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "nogueira2019passage", "citations": "392", "year": "2019", "title":"Passage Re-ranking With BERT", "abstract": "<p>Recently, neural models pretrained on a language modeling task, such as ELMo\n(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et\nal., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based\npassage re-ranking. Our system is the state of the art on the TREC-CAR dataset\nand the top entry in the leaderboard of the MS MARCO passage retrieval task,\noutperforming the previous state of the art by 27% (relative) in MRR@10. The\ncode to reproduce our results is available at\nhttps://github.com/nyu-dl/dl4marco-bert</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "nogueira2020document", "citations": "326", "year": "2020", "title":"Document Ranking With A Pretrained Sequence-to-sequence Model", "abstract": "<p>This work proposes a novel adaptation of a pretrained sequence-to-sequence\nmodel to the task of document ranking. Our approach is fundamentally different\nfrom a commonly-adopted classification-based formulation of ranking, based on\nencoder-only pretrained transformer architectures such as BERT. We show how a\nsequence-to-sequence model can be trained to generate relevance labels as\n“target words”, and how the underlying logits of these target words can be\ninterpreted as relevance probabilities for ranking. On the popular MS MARCO\npassage ranking task, experimental results show that our approach is at least\non par with previous classification-based models and can surpass them with\nlarger, more-recent models. On the test collection from the TREC 2004 Robust\nTrack, we demonstrate a zero-shot transfer-based approach that outperforms\nprevious state-of-the-art models requiring in-dataset cross-validation.\nFurthermore, we find that our approach significantly outperforms an\nencoder-only model in a data-poor regime (i.e., with few training examples). We\ninvestigate this observation further by varying target words to probe the\nmodel’s use of latent knowledge.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "noh2016training", "citations": "77", "year": "2016", "title":"Training Recurrent Answering Units With Joint Loss Minimization For VQA", "abstract": "<p>We propose a novel algorithm for visual question answering based on a\nrecurrent deep neural network, where every module in the network corresponds to\na complete answering unit with attention mechanism by itself. The network is\noptimized by minimizing loss aggregated from all the units, which share model\nparameters while receiving different information to compute attention\nprobability. For training, our model attends to a region within image feature\nmap, updates its memory based on the question and attended image feature, and\nanswers the question based on its memory state. This procedure is performed to\ncompute loss in each step. The motivation of this approach is our observation\nthat multi-step inferences are often required to answer questions while each\nproblem may have a unique desirable number of steps, which is difficult to\nidentify in practice. Hence, we always make the first unit in the network solve\nproblems, but allow it to learn the knowledge from the rest of units by\nbackpropagation unless it degrades the model. To implement this idea, we\nearly-stop training each unit as soon as it starts to overfit. Note that, since\nmore complex models tend to overfit on easier questions quickly, the last\nanswering unit in the unfolded recurrent neural network is typically killed\nfirst while the first one remains last. We make a single-step prediction for a\nnew question using the shared model. This strategy works better than the other\noptions within our framework since the selected model is trained effectively\nfrom all units without overfitting. The proposed algorithm outperforms other\nmulti-step attention based approaches using a single step prediction in VQA\ndataset.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "nooralahzadeh2020zero", "citations": "102", "year": "2020", "title":"Zero-shot Cross-lingual Transfer With Meta Learning", "abstract": "<p>Learning what to share between tasks has been a topic of great importance\nrecently, as strategic sharing of knowledge has been shown to improve\ndownstream task performance. This is particularly important for multilingual\napplications, as most languages in the world are under-resourced. Here, we\nconsider the setting of training models on multiple different languages at the\nsame time, when little or no data is available for languages other than\nEnglish. We show that this challenging setup can be approached using\nmeta-learning, where, in addition to training a source language model, another\nmodel learns to select which training instances are the most beneficial to the\nfirst. We experiment using standard supervised, zero-shot cross-lingual, as\nwell as few-shot cross-lingual settings for different natural language\nunderstanding tasks (natural language inference, question answering). Our\nextensive experimental setup demonstrates the consistent effectiveness of\nmeta-learning for a total of 15 languages. We improve upon the state-of-the-art\nfor zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA\ndataset). A comprehensive error analysis indicates that the correlation of\ntypological features between languages can partly explain when parameter\nsharing learned via meta-learning is beneficial.</p>\n", "tags": ["Applications","Datasets","EMNLP","Few-Shot","Training Techniques"] },
{"key": "nooralahzadeh2021progressive", "citations": "71", "year": "2021", "title":"Progressive Transformer-based Generation Of Radiology Reports", "abstract": "<p>Inspired by Curriculum Learning, we propose a consecutive (i.e.,\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using a transformer architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture","Tools"] },
{"key": "norcliffebrown2018learning", "citations": "114", "year": "2018", "title":"Learning Conditioned Graph Structures For Interpretable Visual Question Answering", "abstract": "<p>Visual Question answering is a challenging problem requiring a combination of\nconcepts from Computer Vision and Natural Language Processing. Most existing\napproaches use a two streams strategy, computing image and question features\nthat are consequently merged using a variety of techniques. Nonetheless, very\nfew rely on higher level image representations, which can capture semantic and\nspatial relationships. In this paper, we propose a novel graph-based approach\nfor Visual Question Answering. Our method combines a graph learner module,\nwhich learns a question specific graph representation of the input image, with\nthe recent concept of graph convolutions, aiming to learn image representations\nthat capture question specific interactions. We test our approach on the VQA v2\ndataset using a simple baseline architecture enhanced by the proposed graph\nlearner module. We obtain promising results with 66.18% accuracy and\ndemonstrate the interpretability of the proposed method. Code can be found at\ngithub.com/aimbrain/vqa-project.</p>\n", "tags": ["Has Code","Model Architecture"] },
{"key": "nori2023can", "citations": "86", "year": "2023", "title":"Can Generalist Foundation Models Outcompete Special-purpose Tuning? Case Study In Medicine", "abstract": "<p>Generalist foundation models such as GPT-4 have displayed surprising\ncapabilities in a wide variety of domains and tasks. Yet, there is a prevalent\nassumption that they cannot match specialist capabilities of fine-tuned models.\nFor example, most explorations to date on medical competency benchmarks have\nleveraged domain-specific training, as exemplified by efforts on BioGPT and\nMed-PaLM. We build on a prior study of GPT-4’s capabilities on medical\nchallenge benchmarks in the absence of special training. Rather than using\nsimple prompting to highlight the model’s out-of-the-box capabilities, we\nperform a systematic exploration of prompt engineering. We find that prompting\ninnovation can unlock deeper specialist capabilities and show that GPT-4 easily\ntops prior leading results for medical benchmarks. The prompting methods we\nexplore are general purpose, and make no specific use of domain expertise,\nremoving the need for expert-curated content. Our experimental design carefully\ncontrols for overfitting during the prompt engineering process. We introduce\nMedprompt, based on a composition of several prompting strategies. With\nMedprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark\ndatasets in the MultiMedQA suite. The method outperforms leading specialist\nmodels such as Med-PaLM 2 by a significant margin with an order of magnitude\nfewer calls to the model. Steering GPT-4 with Medprompt achieves a 27%\nreduction in error rate on the MedQA dataset over the best methods to date\nachieved with specialist models and surpasses a score of 90% for the first\ntime. Beyond medical problems, we show the power of Medprompt to generalize to\nother domains and provide evidence for the broad applicability of the approach\nvia studies of the strategy on exams in electrical engineering, machine\nlearning, philosophy, accounting, law, nursing, and clinical psychology.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Prompting","Training Techniques"] },
{"key": "nori2023capabilities", "citations": "392", "year": "2023", "title":"Capabilities Of GPT-4 On Medical Challenge Problems", "abstract": "<p>Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.</p>\n", "tags": ["Applications","Datasets","Evaluation","Model Architecture","Prompting","Training Techniques"] },
{"key": "norouzi2016reward", "citations": "93", "year": "2016", "title":"Reward Augmented Maximum Likelihood For Neural Structured Prediction", "abstract": "<p>A key problem in structured output prediction is direct optimization of the\ntask reward function that matters for test evaluation. This paper presents a\nsimple and computationally efficient approach to incorporate task reward into a\nmaximum likelihood framework. By establishing a link between the log-likelihood\nand expected reward objectives, we show that an optimal regularized expected\nreward is achieved when the conditional distribution of the outputs given the\ninputs is proportional to their exponentiated scaled rewards. Accordingly, we\npresent a framework to smooth the predictive probability of the outputs using\ntheir corresponding rewards. We optimize the conditional log-probability of\naugmented outputs that are sampled proportionally to their exponentiated scaled\nrewards. Experiments on neural sequence to sequence models for speech\nrecognition and machine translation show notable improvements over a maximum\nlikelihood baseline by using reward augmented maximum likelihood (RAML), where\nthe rewards are defined as the negative edit distance between the outputs and\nthe ground truth labels.</p>\n", "tags": ["Efficiency","Evaluation","Reinforcement Learning","Tools"] },
{"key": "novikova2017e2e", "citations": "186", "year": "2017", "title":"The E2E Dataset: New Challenges For End-to-end Generation", "abstract": "<p>This paper describes the E2E data, a new dataset for training end-to-end,\ndata-driven natural language generation systems in the restaurant domain, which\nis ten times bigger than existing, frequently used datasets in this area. The\nE2E dataset poses new challenges: (1) its human reference texts show more\nlexical richness and syntactic variation, including discourse phenomena; (2)\ngenerating from this set requires content selection. As such, learning from\nthis dataset promises more natural, varied and less template-like system\nutterances. We also establish a baseline on this dataset, which illustrates\nsome of the difficulties associated with this data.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "novikova2018rankme", "citations": "80", "year": "2018", "title":"Rankme: Reliable Human Ratings For Natural Language Generation", "abstract": "<p>Human evaluation for natural language generation (NLG) often suffers from\ninconsistent user ratings. While previous research tends to attribute this\nproblem to individual user preferences, we show that the quality of human\njudgements can also be improved by experimental design. We present a novel\nrank-based magnitude estimation method (RankME), which combines the use of\ncontinuous scales and relative assessments. We show that RankME significantly\nimproves the reliability and consistency of human ratings compared to\ntraditional evaluation methods. In addition, we show that it is possible to\nevaluate NLG systems according to multiple, distinct criteria, which is\nimportant for error analysis. Finally, we demonstrate that RankME, in\ncombination with Bayesian estimation of system quality, is a cost-effective\nalternative for ranking multiple NLG systems.</p>\n", "tags": ["Evaluation","NAACL"] },
{"key": "nozza2020what", "citations": "84", "year": "2020", "title":"What The [MASK]? Making Sense Of Language-specific BERT Models", "abstract": "<p>Recently, Natural Language Processing (NLP) has witnessed an impressive\nprogress in many areas, due to the advent of novel, pretrained contextual\nrepresentation models. In particular, Devlin et al. (2019) proposed a model,\ncalled BERT (Bidirectional Encoder Representations from Transformers), which\nenables researchers to obtain state-of-the art performance on numerous NLP\ntasks by fine-tuning the representations on their data set and task, without\nthe need for developing and training highly-specific architectures. The authors\nalso released multilingual BERT (mBERT), a model trained on a corpus of 104\nlanguages, which can serve as a universal language model. This model obtained\nimpressive results on a zero-shot cross-lingual natural inference task. Driven\nby the potential of BERT models, the NLP community has started to investigate\nand generate an abundant number of BERT models that are trained on a particular\nlanguage, and tested on a specific data domain and task. This allows us to\nevaluate the true potential of mBERT as a universal language model, by\ncomparing it to the performance of these more specific models. This paper\npresents the current state of the art in language-specific BERT models,\nproviding an overall picture with respect to different dimensions (i.e.\narchitectures, data domains, and tasks). Our aim is to provide an immediate and\nstraightforward overview of the commonalities and differences between\nLanguage-Specific (language-specific) BERT models and mBERT. We also provide an\ninteractive and constantly updated website that can be used to explore the\ninformation we have collected, at https://bertlang.unibocconi.it.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "nye2021show", "citations": "120", "year": "2021", "title":"Show Your Work: Scratchpads For Intermediate Computation With Language Models", "abstract": "<p>Large pre-trained language models perform remarkably well on tasks that can\nbe done “in one pass”, such as generating realistic text or synthesizing\ncomputer programs. However, they struggle with tasks that require unbounded\nmulti-step computation, such as adding integers or executing programs.\nSurprisingly, we find that these same models are able to perform complex\nmulti-step computations – even in the few-shot regime – when asked to perform\nthe operation “step by step”, showing the results of intermediate computations.\nIn particular, we train transformers to perform multi-step computations by\nasking them to emit intermediate computation steps into a “scratchpad”. On a\nseries of increasingly complex tasks ranging from long addition to the\nexecution of arbitrary programs, we show that scratchpads dramatically improve\nthe ability of language models to perform multi-step computations.</p>\n", "tags": ["Few-Shot","Prompting"] },
{"key": "obeid2020chart", "citations": "67", "year": "2020", "title":"Chart-to-text: Generating Natural Language Descriptions For Charts By Adapting The Transformer Model", "abstract": "<p>Information visualizations such as bar charts and line charts are very\npopular for exploring data and communicating insights. Interpreting and making\nsense of such visualizations can be challenging for some people, such as those\nwho are visually impaired or have low visualization literacy. In this work, we\nintroduce a new dataset and present a neural model for automatically generating\nnatural language summaries for charts. The generated summaries provide an\ninterpretation of the chart and convey the key insights found within that\nchart. Our neural model is developed by extending the state-of-the-art model\nfor the data-to-text generation task, which utilizes a transformer-based\nencoder-decoder architecture. We found that our approach outperforms the base\nmodel on a content selection metric by a wide margin (55.42% vs. 8.49%) and\ngenerates more informative, concise, and coherent summaries.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "ofir2022measuring", "citations": "119", "year": "2023", "title":"Measuring And Narrowing The Compositionality Gap In Language Models", "abstract": "<p>We investigate the ability of language models to perform compositional\nreasoning tasks where the overall solution depends on correctly composing the\nanswers to sub-problems. We measure how often models can correctly answer all\nsub-problems but not generate the overall solution, a ratio we call the\ncompositionality gap. We evaluate this ratio by asking multi-hop questions with\nanswers that require composing multiple facts unlikely to have been observed\ntogether during pretraining. In the GPT-3 family of models, as model size\nincreases we show that the single-hop question answering performance improves\nfaster than the multi-hop performance does, therefore the compositionality gap\ndoes not decrease. This surprising result suggests that while more powerful\nmodels memorize and recall more factual knowledge, they show no corresponding\nimprovement in their ability to perform this kind of compositional reasoning.\n  We then demonstrate how elicitive prompting (such as chain of thought)\nnarrows the compositionality gap by reasoning explicitly. We present a new\nmethod, self-ask, that further improves on chain of thought. In our method, the\nmodel explicitly asks itself (and answers) follow-up questions before answering\nthe initial question. We finally show that self-ask’s structured prompting lets\nus easily plug in a search engine to answer the follow-up questions, which\nadditionally improves accuracy.</p>\n", "tags": ["EMNLP","Model Architecture","Prompting"] },
{"key": "ohad2021learning", "citations": "225", "year": "2022", "title":"Learning To Retrieve Prompts For In-context Learning", "abstract": "<p>In-context learning is a recent paradigm in natural language understanding,\nwhere a large pre-trained language model (LM) observes a test instance and a\nfew training examples as its input, and directly decodes the output without any\nupdate to its parameters. However, performance has been shown to strongly\ndepend on the selected training examples (termed prompt). In this work, we\npropose an efficient method for retrieving prompts for in-context learning\nusing annotated data and a LM. Given an input-output pair, we estimate the\nprobability of the output given the input and a candidate training example as\nthe prompt, and label training examples as positive or negative based on this\nprobability. We then train an efficient dense retriever from this data, which\nis used to retrieve training examples as prompts at test time. We evaluate our\napproach on three sequence-to-sequence tasks where language utterances are\nmapped to meaning representations, and find that it substantially outperforms\nprior work and multiple baselines across the board.</p>\n", "tags": ["In Context Learning","NAACL","Prompting","Retrieval Systems","Training Techniques"] },
{"key": "oltramari2022generalizable", "citations": "73", "year": "2019", "title":"Generalizable Neuro-symbolic Systems For Commonsense Question Answering", "abstract": "<p>This chapter illustrates how suitable neuro-symbolic models for language\nunderstanding can enable domain generalizability and robustness in downstream\ntasks. Different methods for integrating neural language models and knowledge\ngraphs are discussed. The situations in which this combination is most\nappropriate are characterized, including quantitative evaluation and\nqualitative error analysis on a variety of commonsense question answering\nbenchmark datasets.</p>\n", "tags": ["Datasets","Evaluation","Nesy"] },
{"key": "omiye2023large", "citations": "138", "year": "2024", "title":"Large Language Models In Medicine: The Potentials And Pitfalls", "abstract": "<p>Large language models (LLMs) have been applied to tasks in healthcare,\nranging from medical exam questions to responding to patient questions. With\nincreasing institutional partnerships between companies producing LLMs and\nhealthcare systems, real world clinical application is coming closer to\nreality. As these models gain traction, it is essential for healthcare\npractitioners to understand what LLMs are, their development, their current and\npotential applications, and the associated pitfalls when utilized in medicine.\nThis review and accompanying tutorial aim to give an overview of these topics\nto aid healthcare practitioners in understanding the rapidly changing landscape\nof LLMs as applied to medicine.</p>\n", "tags": ["Applications"] },
{"key": "onishi2016who", "citations": "121", "year": "2016", "title":"Who Did What: A Large-scale Person-centered Cloze Dataset", "abstract": "<p>We have constructed a new “Who-did-What” dataset of over 200,000\nfill-in-the-gap (cloze) multiple choice reading comprehension problems\nconstructed from the LDC English Gigaword newswire corpus. The WDW dataset has\na variety of novel features. First, in contrast with the CNN and Daily Mail\ndatasets (Hermann et al., 2015) we avoid using article summaries for question\nformation. Instead, each problem is formed from two independent articles — an\narticle given as the passage to be read and a separate article on the same\nevents used to form the question. Second, we avoid anonymization — each\nchoice is a person named entity. Third, the problems have been filtered to\nremove a fraction that are easily solved by simple baselines, while remaining\n84% solvable by humans. We report performance benchmarks of standard systems\nand propose the WDW dataset as a challenge task for the community.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "oppenlaender2022creativity", "citations": "174", "year": "2022", "title":"The Creativity Of Text-to-image Generation", "abstract": "<p>Text-guided synthesis of images has made a giant leap towards becoming a\nmainstream phenomenon. With text-to-image generation systems, anybody can\ncreate digital images and artworks. This provokes the question of whether\ntext-to-image generation is creative. This paper expounds on the nature of\nhuman creativity involved in text-to-image art (so-called “AI art”) with a\nspecific focus on the practice of prompt engineering. The paper argues that the\ncurrent product-centered view of creativity falls short in the context of\ntext-to-image generation. A case exemplifying this shortcoming is provided and\nthe importance of online communities for the creative ecosystem of\ntext-to-image art is highlighted. The paper provides a high-level summary of\nthis online ecosystem drawing on Rhodes’ conceptual four P model of creativity.\nChallenges for evaluating the creativity of text-to-image generation and\nopportunities for research on text-to-image generation in the field of\nHuman-Computer Interaction (HCI) are discussed.</p>\n", "tags": ["Prompting"] },
{"key": "oppenlaender2022taxonomy", "citations": "92", "year": "2023", "title":"A Taxonomy Of Prompt Modifiers For Text-to-image Generation", "abstract": "<p>Text-to-image generation has seen an explosion of interest since 2021. Today,\nbeautiful and intriguing digital images and artworks can be synthesized from\ntextual inputs (“prompts”) with deep generative models. Online communities\naround text-to-image generation and AI generated art have quickly emerged. This\npaper identifies six types of prompt modifiers used by practitioners in the\nonline community based on a 3-month ethnographic study. The novel taxonomy of\nprompt modifiers provides researchers a conceptual starting point for\ninvestigating the practice of text-to-image generation, but may also help\npractitioners of AI generated art improve their images. We further outline how\nprompt modifiers are applied in the practice of “prompt engineering.” We\ndiscuss research opportunities of this novel creative practice in the field of\nHuman-Computer Interaction (HCI). The paper concludes with a discussion of\nbroader implications of prompt engineering from the perspective of Human-AI\nInteraction (HAI) in future applications beyond the use case of text-to-image\ngeneration and AI generated art.</p>\n", "tags": ["Applications","Prompting"] },
{"key": "oraby2018controlling", "citations": "65", "year": "2018", "title":"Controlling Personality-based Stylistic Variation With Neural Natural Language Generators", "abstract": "<p>Natural language generators for task-oriented dialogue must effectively\nrealize system dialogue actions and their associated semantics. In many\napplications, it is also desirable for generators to control the style of an\nutterance. To date, work on task-oriented neural generation has primarily\nfocused on semantic fidelity rather than achieving stylistic goals, while work\non style has been done in contexts where it is difficult to measure content\npreservation. Here we present three different sequence-to-sequence models and\ncarefully test how well they disentangle content and style. We use a\nstatistical generator, Personage, to synthesize a new corpus of over 88,000\nrestaurant domain utterances whose style varies according to models of\npersonality, giving us total control over both the semantic content and the\nstylistic variation in the training data. We then vary the amount of explicit\nstylistic supervision given to the three models. We show that our most explicit\nmodel can simultaneously achieve high fidelity to both semantic and stylistic\ngoals: this model adds a context vector of 36 stylistic parameters as input to\nthe hidden state of the encoder at each time step, showing the benefits of\nexplicit stylistic supervision, even when the amount of training data is large.</p>\n", "tags": ["Applications","Datasets","Training Techniques"] },
{"key": "ororbia2017learning", "citations": "67", "year": "2017", "title":"Learning Simpler Language Models With The Differential State Framework", "abstract": "<p>Learning useful information across long time lags is a critical and difficult\nproblem for temporal neural models in tasks such as language modeling. Existing\narchitectures that address the issue are often complex and costly to train. The\nDifferential State Framework (DSF) is a simple and high-performing design that\nunifies previously introduced gated neural models. DSF models maintain\nlonger-term memory by learning to interpolate between a fast-changing\ndata-driven representation and a slowly changing, implicitly stable state. This\nrequires hardly any more parameters than a classical, simple recurrent network.\nWithin the DSF framework, a new architecture is presented, the Delta-RNN. In\nlanguage modeling at the word and character levels, the Delta-RNN outperforms\npopular complex architectures, such as the Long Short Term Memory (LSTM) and\nthe Gated Recurrent Unit (GRU), and, when regularized, performs comparably to\nseveral state-of-the-art baselines. At the subword level, the Delta-RNN’s\nperformance is comparable to that of complex gated architectures.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "ostermann2018mcscript", "citations": "70", "year": "2018", "title":"Mcscript: A Novel Dataset For Assessing Machine Comprehension Using Script Knowledge", "abstract": "<p>We introduce a large dataset of narrative texts and questions about these\ntexts, intended to be used in a machine comprehension task that requires\nreasoning using commonsense knowledge. Our dataset complements similar datasets\nin that we focus on stories about everyday activities, such as going to the\nmovies or working in the garden, and that the questions require commonsense\nknowledge, or more specifically, script knowledge, to be answered. We show that\nour mode of data collection via crowdsourcing results in a substantial amount\nof such inference questions. The dataset forms the basis of a shared task on\ncommonsense and script knowledge organized at SemEval 2018 and provides\nchallenging test cases for the broader natural language understanding\ncommunity.</p>\n", "tags": ["Datasets"] },
{"key": "ott2018analyzing", "citations": "119", "year": "2018", "title":"Analyzing Uncertainty In Neural Machine Translation", "abstract": "<p>Machine translation is a popular test bed for research in neural\nsequence-to-sequence models but despite much recent research, there is still a\nlack of understanding of these models. Practitioners report performance\ndegradation with large beams, the under-estimation of rare words and a lack of\ndiversity in the final translations. Our study relates some of these issues to\nthe inherent uncertainty of the task, due to the existence of multiple valid\ntranslations for a single source sentence, and to the extrinsic uncertainty\ncaused by noisy training data. We propose tools and metrics to assess how\nuncertainty in the data is captured by the model distribution and how it\naffects search strategies that generate translations. Our results show that\nsearch works remarkably well but that models tend to spread too much\nprobability mass over the hypothesis space. Next, we propose tools to assess\nmodel calibration and show how to easily fix some shortcomings of current\nmodels. As part of this study, we release multiple human reference translations\nfor two popular benchmarks.</p>\n", "tags": ["Evaluation","Tools","Training Techniques"] },
{"key": "ott2018scaling", "citations": "565", "year": "2018", "title":"Scaling Neural Machine Translation", "abstract": "<p>Sequence to sequence learning models still require several days to reach\nstate of the art performance on large benchmark datasets using a single\nmachine. This paper shows that reduced precision and large batch training can\nspeedup training by nearly 5x on a single 8-GPU machine with careful tuning and\nimplementation. On WMT’14 English-German translation, we match the accuracy of\nVaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a\nnew state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We\nfurther improve these results to 29.8 BLEU by training on the much larger\nParacrawl dataset. On the WMT’14 English-French task, we obtain a\nstate-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "ouyang2020ernie", "citations": "64", "year": "2021", "title":"ERNIE-M: Enhanced Multilingual Representation By Aligning Cross-lingual Semantics With Monolingual Corpora", "abstract": "<p>Recent studies have demonstrated that pre-trained cross-lingual models\nachieve impressive performance in downstream cross-lingual tasks. This\nimprovement benefits from learning a large amount of monolingual and parallel\ncorpora. Although it is generally acknowledged that parallel corpora are\ncritical for improving the model performance, existing methods are often\nconstrained by the size of parallel corpora, especially for low-resource\nlanguages. In this paper, we propose ERNIE-M, a new training method that\nencourages the model to align the representation of multiple languages with\nmonolingual corpora, to overcome the constraint that the parallel corpus size\nplaces on the model performance. Our key insight is to integrate\nback-translation into the pre-training process. We generate pseudo-parallel\nsentence pairs on a monolingual corpus to enable the learning of semantic\nalignments between different languages, thereby enhancing the semantic modeling\nof cross-lingual models. Experimental results show that ERNIE-M outperforms\nexisting cross-lingual models and delivers new state-of-the-art results in\nvarious cross-lingual downstream tasks.</p>\n", "tags": ["Datasets","EMNLP","Training Techniques"] },
{"key": "ouyang2022training", "citations": "2938", "year": "2022", "title":"Training Language Models To Follow Instructions With Human Feedback", "abstract": "<p>Making language models bigger does not inherently make them better at\nfollowing a user’s intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.</p>\n", "tags": ["Agentic","Datasets","Fine-Tuning","Instruction Following","Model Architecture","Prompting","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "padmakumar2021teach", "citations": "67", "year": "2022", "title":"Teach: Task-driven Embodied Agents That Chat", "abstract": "<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human–human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from “Make Coffee” to “Prepare Breakfast”,\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models’ abilities in dialogue understanding, language\ngrounding, and task execution.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "palaskar2019multimodal", "citations": "94", "year": "2019", "title":"Multimodal Abstractive Summarization For How2 Videos", "abstract": "<p>In this paper, we study abstractive summarization for open-domain videos.\nUnlike the traditional text news summarization, the goal is less to “compress”\ntext information but rather to provide a fluent textual summary of information\nthat has been collected and fused from different source modalities, in our case\nvideo and audio transcripts (or text). We show how a multi-source\nsequence-to-sequence model with hierarchical attention can integrate\ninformation from different modalities into a coherent output, compare various\nmodels trained with different modalities and present pilot experiments on the\nHow2 corpus of instructional videos. We also propose a new evaluation metric\n(Content F1) for abstractive summarization task that measures semantic adequacy\nrather than fluency of the summaries, which is covered by metrics like ROUGE\nand BLEU.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "pampari2018emrqa", "citations": "159", "year": "2018", "title":"Emrqa: A Large Corpus For Question Answering On Electronic Medical Records", "abstract": "<p>We propose a novel methodology to generate domain-specific large-scale\nquestion answering (QA) datasets by re-purposing existing annotations for other\nNLP tasks. We demonstrate an instance of this methodology in generating a\nlarge-scale QA dataset for electronic medical records by leveraging existing\nexpert annotations on clinical notes for various NLP tasks from the community\nshared i2b2 datasets. The resulting corpus (emrQA) has 1 million\nquestion-logical form and 400,000+ question-answer evidence pairs. We\ncharacterize the dataset and explore its learning potential by training\nbaseline models for question to logical form and question to answer mapping.</p>\n", "tags": ["Datasets","EMNLP","Training Techniques"] },
{"key": "pan2017memen", "citations": "70", "year": "2017", "title":"MEMEN: Multi-layer Embedding With Memory Networks For Machine Comprehension", "abstract": "<p>Machine comprehension(MC) style question answering is a representative\nproblem in natural language processing. Previous methods rarely spend time on\nthe improvement of encoding layer, especially the embedding of syntactic\ninformation and name entity of the words, which are very crucial to the quality\nof encoding. Moreover, existing attention methods represent each query word as\na vector or use a single vector to represent the whole query sentence, neither\nof them can handle the proper weight of the key words in query sentence. In\nthis paper, we introduce a novel neural network architecture called Multi-layer\nEmbedding with Memory Network(MEMEN) for machine reading task. In the encoding\nlayer, we employ classic skip-gram model to the syntactic and semantic\ninformation of the words to train a new kind of embedding layer. We also\npropose a memory network of full-orientation matching of the query and passage\nto catch more pivotal information. Experiments show that our model has\ncompetitive results both from the perspectives of precision and efficiency in\nStanford Question Answering Dataset(SQuAD) among all published results and\nachieves the state-of-the-art results on TriviaQA dataset.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "pan2019recent", "citations": "84", "year": "2019", "title":"Recent Advances In Neural Question Generation", "abstract": "<p>Emerging research in Neural Question Generation (NQG) has started to\nintegrate a larger variety of inputs, and generating questions requiring higher\nlevels of cognition. These trends point to NQG as a bellwether for NLP, about\nhow human intelligence embodies the skills of curiosity and integration.\n  We present a comprehensive survey of neural question generation, examining\nthe corpora, methodologies, and evaluation methods. From this, we elaborate on\nwhat we see as emerging on NQG’s trend: in terms of the learning paradigms,\ninput modalities, and cognitive levels considered by NQG. We end by pointing\nout the potential directions ahead.</p>\n", "tags": ["Evaluation","Survey Paper"] },
{"key": "pan2020semantic", "citations": "73", "year": "2020", "title":"Semantic Graphs For Generating Deep Questions", "abstract": "<p>This paper proposes the problem of Deep Question Generation (DQG), which aims\nto generate complex questions that require reasoning over multiple pieces of\ninformation of the input passage. In order to capture the global structure of\nthe document and facilitate reasoning, we propose a novel framework which first\nconstructs a semantic-level graph for the input document and then encodes the\nsemantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards,\nwe fuse the document-level and graph-level representations to perform joint\ntraining of content selection and question decoding. On the HotpotQA\ndeep-question centric dataset, our model greatly improves performance over\nquestions requiring reasoning over multiple facts, leading to state-of-the-art\nperformance. The code is publicly available at\nhttps://github.com/WING-NUS/SG-Deep-Question-Generation.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "pan2020x", "citations": "580", "year": "2020", "title":"X-linear Attention Networks For Image Captioning", "abstract": "<p>Recent progress on fine-grained visual recognition and visual question\nanswering has featured Bilinear Pooling, which effectively models the 2\\(^{nd}\\)\norder interactions across multi-modal inputs. Nevertheless, there has not been\nevidence in support of building such interactions concurrently with attention\nmechanism for image captioning. In this paper, we introduce a unified attention\nblock – X-Linear attention block, that fully employs bilinear pooling to\nselectively capitalize on visual information or perform multi-modal reasoning.\nTechnically, X-Linear attention block simultaneously exploits both the spatial\nand channel-wise bilinear attention distributions to capture the 2\\(^{nd}\\) order\ninteractions between the input single-modal or multi-modal features. Higher and\neven infinity order feature interactions are readily modeled through stacking\nmultiple X-Linear attention blocks and equipping the block with Exponential\nLinear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we\npresent X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates\nX-Linear attention block(s) into image encoder and sentence decoder of image\ncaptioning model to leverage higher order intra- and inter-modal interactions.\nThe experiments on COCO benchmark demonstrate that our X-LAN obtains to-date\nthe best published CIDEr performance of 132.0% on COCO Karpathy test split.\nWhen further endowing Transformer with X-Linear attention blocks, CIDEr is\nboosted up to 132.8%. Source code is available at\nhttps://github.com/Panda-Peter/image-captioning.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "pan2021contrastive", "citations": "120", "year": "2021", "title":"Contrastive Learning For Many-to-many Multilingual Neural Machine Translation", "abstract": "<p>Existing multilingual machine translation approaches mainly focus on\nEnglish-centric directions, while the non-English directions still lag behind.\nIn this work, we aim to build a many-to-many translation system with an\nemphasis on the quality of non-English language directions. Our intuition is\nbased on the hypothesis that a universal cross-language representation leads to\nbetter multilingual translation performance. To this end, we propose mRASP2, a\ntraining method to obtain a single unified multilingual translation model.\nmRASP2 is empowered by two techniques: a) a contrastive learning scheme to\nclose the gap among representations of different languages, and b) data\naugmentation on both multiple parallel and monolingual data to further align\ntoken representations. For English-centric directions, mRASP2 outperforms\nexisting best unified model and achieves competitive or even better performance\nthan the pre-trained and fine-tuned model mBART on tens of WMT’s translation\ndirections. For non-English directions, mRASP2 achieves an improvement of\naverage 10+ BLEU compared with the multilingual Transformer baseline. Code,\ndata and trained models are available at https://github.com/PANXiao1994/mRASP2.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "pan2021improved", "citations": "68", "year": "2022", "title":"Improved Text Classification Via Contrastive Adversarial Training", "abstract": "<p>We propose a simple and general method to regularize the fine-tuning of\nTransformer-based encoders for text classification tasks. Specifically, during\nfine-tuning we generate adversarial examples by perturbing the word embeddings\nof the model and perform contrastive learning on clean and adversarial examples\nin order to teach the model to learn noise-invariant representations. By\ntraining on both clean and adversarial examples along with the additional\ncontrastive objective, we observe consistent improvement over standard\nfine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned\nBERT Large model outperforms BERT Large baseline by 1.7% on average, and our\nfine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We\nadditionally validate our method in different domains using three intent\nclassification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa\nLarge baseline by 1-2% on average.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "pan2023unifying", "citations": "375", "year": "2024", "title":"Unifying Large Language Models And Knowledge Graphs: A Roadmap", "abstract": "<p>Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.</p>\n", "tags": ["Emergent Abilities","Training Techniques"] },
{"key": "paperno2016lambada", "citations": "187", "year": "2016", "title":"The LAMBADA Dataset: Word Prediction Requiring A Broad Discourse Context", "abstract": "<p>We introduce LAMBADA, a dataset to evaluate the capabilities of computational\nmodels for text understanding by means of a word prediction task. LAMBADA is a\ncollection of narrative passages sharing the characteristic that human subjects\nare able to guess their last word if they are exposed to the whole passage, but\nnot if they only see the last sentence preceding the target word. To succeed on\nLAMBADA, computational models cannot simply rely on local context, but must be\nable to keep track of information in the broader discourse. We show that\nLAMBADA exemplifies a wide range of linguistic phenomena, and that none of\nseveral state-of-the-art language models reaches accuracy above 1% on this\nnovel benchmark. We thus propose LAMBADA as a challenging test set, meant to\nencourage the development of new models capable of genuine understanding of\nbroad context in natural language text.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "pappagari2019hierarchical", "citations": "192", "year": "2019", "title":"Hierarchical Transformers For Long Document Classification", "abstract": "<p>BERT, which stands for Bidirectional Encoder Representations from\nTransformers, is a recently introduced language representation model based upon\nthe transfer learning paradigm. We extend its fine-tuning procedure to address\none of its major limitations - applicability to inputs longer than a few\nhundred words, such as transcripts of human call conversations. Our method is\nconceptually simple. We segment the input into smaller chunks and feed each of\nthem into the base model. Then, we propagate each output through a single\nrecurrent layer, or another transformer, followed by a softmax activation. We\nobtain the final classification decision after the last segment has been\nconsumed. We show that both BERT extensions are quick to fine-tune and converge\nafter as little as 1 epoch of training on a small, domain-specific data set. We\nsuccessfully apply them in three different tasks involving customer call\nsatisfaction prediction and topic classification, and obtain a significant\nimprovement over the baseline models in two of them.</p>\n", "tags": ["ASRU","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "paranjape2020information", "citations": "80", "year": "2020", "title":"An Information Bottleneck Approach For Controlling Conciseness In Rationale Extraction", "abstract": "<p>Decisions of complex language understanding models can be rationalized by\nlimiting their inputs to a relevant subsequence of the original text. A\nrationale should be as concise as possible without significantly degrading task\nperformance, but this balance can be difficult to achieve in practice. In this\npaper, we show that it is possible to better manage this trade-off by\noptimizing a bound on the Information Bottleneck (IB) objective. Our fully\nunsupervised approach jointly learns an explainer that predicts sparse binary\nmasks over sentences, and an end-task predictor that considers only the\nextracted rationale. Using IB, we derive a learning objective that allows\ndirect control of mask sparsity levels through a tunable sparse prior.\nExperiments on ERASER benchmark tasks demonstrate significant gains over\nnorm-minimization techniques for both task performance and agreement with human\nrationales. Furthermore, we find that in the semi-supervised setting, a modest\namount of gold rationales (25% of training examples) closes the gap with a\nmodel that uses the full input.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "pardos2023learning", "citations": "63", "year": "2023", "title":"Learning Gain Differences Between Chatgpt And Human Tutor Generated Algebra Hints", "abstract": "<p>Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to\nthe frontiers of practical consumer use and leading industries to re-evaluate\nhow they allocate resources for content production. Authoring of open\neducational resources and hint content within adaptive tutoring systems is\nlabor intensive. Should LLMs like ChatGPT produce educational content on par\nwith human-authored content, the implications would be significant for further\nscaling of computer tutoring system approaches. In this paper, we conduct the\nfirst learning gain evaluation of ChatGPT by comparing the efficacy of its\nhints with hints authored by human tutors with 77 participants across two\nalgebra topic areas, Elementary Algebra and Intermediate Algebra. We find that\n70% of hints produced by ChatGPT passed our manual quality checks and that both\nhuman and ChatGPT conditions produced positive learning gains. However, gains\nwere only statistically significant for human tutor created hints. Learning\ngains from human-created hints were substantially and statistically\nsignificantly higher than ChatGPT hints in both topic areas, though ChatGPT\nparticipants in the Intermediate Algebra experiment were near ceiling and not\neven with the control at pre-test. We discuss the limitations of our study and\nsuggest several future directions for the field. Problem and hint content used\nin the experiment is provided for replicability.</p>\n", "tags": ["Evaluation"] },
{"key": "parikh2016decomposable", "citations": "1412", "year": "2016", "title":"A Decomposable Attention Model For Natural Language Inference", "abstract": "<p>We propose a simple neural architecture for natural language inference. Our\napproach uses attention to decompose the problem into subproblems that can be\nsolved separately, thus making it trivially parallelizable. On the Stanford\nNatural Language Inference (SNLI) dataset, we obtain state-of-the-art results\nwith almost an order of magnitude fewer parameters than previous work and\nwithout relying on any word-order information. Adding intra-sentence attention\nthat takes a minimum amount of order into account yields further improvements.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "parikh2020totto", "citations": "176", "year": "2020", "title":"Totto: A Controlled Table-to-text Generation Dataset", "abstract": "<p>We present ToTTo, an open-domain English table-to-text dataset with over\n120,000 training examples that proposes a controlled generation task: given a\nWikipedia table and a set of highlighted table cells, produce a one-sentence\ndescription. To obtain generated targets that are natural but also faithful to\nthe source table, we introduce a dataset construction process where annotators\ndirectly revise existing candidate sentences from Wikipedia. We present\nsystematic analyses of our dataset and annotation process as well as results\nachieved by several state-of-the-art baselines. While usually fluent, existing\nmethods often hallucinate phrases that are not supported by the table,\nsuggesting that this dataset can serve as a useful research benchmark for\nhigh-precision conditional text generation.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Training Techniques"] },
{"key": "parisotto2019stabilizing", "citations": "90", "year": "2019", "title":"Stabilizing Transformers For Reinforcement Learning", "abstract": "<p>Owing to their ability to both effectively integrate information over long\ntime horizons and scale to massive amounts of data, self-attention\narchitectures have recently shown breakthrough success in natural language\nprocessing (NLP), achieving state-of-the-art results in domains such as\nlanguage modeling and machine translation. Harnessing the transformer’s ability\nto process long time horizons of information could provide a similar\nperformance boost in partially observable reinforcement learning (RL) domains,\nbut the large-scale transformers used in NLP have yet to be successfully\napplied to the RL setting. In this work we demonstrate that the standard\ntransformer architecture is difficult to optimize, which was previously\nobserved in the supervised learning setting but becomes especially pronounced\nwith RL objectives. We propose architectural modifications that substantially\nimprove the stability and learning speed of the original Transformer and XL\nvariant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses\nLSTMs on challenging memory environments and achieves state-of-the-art results\non the multi-task DMLab-30 benchmark suite, exceeding the performance of an\nexternal memory architecture. We show that the GTrXL, trained using the same\nlosses, has stability and performance that consistently matches or exceeds a\ncompetitive LSTM baseline, including on more reactive tasks where memory is\nless critical. GTrXL offers an easy-to-train, simple-to-implement but\nsubstantially more expressive architectural alternative to the standard\nmulti-layer LSTM ubiquitously used for RL agents in partially observable\nenvironments.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Model Architecture","Reinforcement Learning"] },
{"key": "park2017attend", "citations": "156", "year": "2017", "title":"Attend To You: Personalized Image Captioning With Context Sequence Memory Networks", "abstract": "<p>We address personalization issues of image captioning, which have not been\ndiscussed yet in previous research. For a query image, we aim to generate a\ndescriptive sentence, accounting for prior knowledge such as the user’s active\nvocabularies in previous documents. As applications of personalized image\ncaptioning, we tackle two post automation tasks: hashtag prediction and post\ngeneration, on our newly collected Instagram dataset, consisting of 1.1M posts\nfrom 6.3K users. We propose a novel captioning model named Context Sequence\nMemory Network (CSMN). Its unique updates over previous memory network models\ninclude (i) exploiting memory as a repository for multiple types of context\ninformation, (ii) appending previously generated words into memory to capture\nlong-term information without suffering from the vanishing gradient problem,\nand (iii) adopting CNN memory structure to jointly represent nearby ordered\nmemory slots for better context understanding. With quantitative evaluation and\nuser studies via Amazon Mechanical Turk, we show the effectiveness of the three\nnovel features of CSMN and its performance enhancement for personalized image\ncaptioning over state-of-the-art captioning models.</p>\n", "tags": ["Applications","CVPR","Datasets","Evaluation","Has Code"] },
{"key": "park2018hierarchical", "citations": "109", "year": "2018", "title":"A Hierarchical Latent Structure For Variational Conversation Modeling", "abstract": "<p>Variational autoencoders (VAE) combined with hierarchical RNNs have emerged\nas a powerful framework for conversation modeling. However, they suffer from\nthe notorious degeneration problem, where the decoders learn to ignore latent\nvariables and reduce to vanilla RNNs. We empirically show that this degeneracy\noccurs mostly due to two reasons. First, the expressive power of hierarchical\nRNN decoders is often high enough to model the data using only its decoding\ndistributions without relying on the latent variables. Second, the conditional\nVAE structure whose generation process is conditioned on a context, makes the\nrange of training targets very sparse; that is, the RNN decoders can easily\noverfit to the training data ignoring the latent variables. To solve the\ndegeneration problem, we propose a novel model named Variational Hierarchical\nConversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical\nstructure of latent variables, and (2) exploiting an utterance drop\nregularization. With evaluations on two datasets of Cornell Movie Dialog and\nUbuntu Dialog Corpus, we show that our VHCR successfully utilizes latent\nvariables and outperforms state-of-the-art models for conversation generation.\nMoreover, it can perform several new utterance control tasks, thanks to its\nhierarchical latent structure.</p>\n", "tags": ["Dialogue & Multi Turn","NAACL","Tools","Training Techniques"] },
{"key": "park2018multimodal", "citations": "336", "year": "2018", "title":"Multimodal Explanations: Justifying Decisions And Pointing To The Evidence", "abstract": "<p>Deep models that are both effective and explainable are desirable in many\nsettings; prior explainable models have been unimodal, offering either\nimage-based visualization of attention weights or text-based generation of\npost-hoc justifications. We propose a multimodal approach to explanation, and\nargue that the two modalities provide complementary explanatory strengths. We\ncollect two new datasets to define and evaluate this task, and propose a novel\nmodel which can provide joint textual rationale generation and attention\nvisualization. Our datasets define visual and textual justifications of a\nclassification decision for activity recognition tasks (ACT-X) and for visual\nquestion answering tasks (VQA-X). We quantitatively show that training with the\ntextual explanations not only yields better textual justification models, but\nalso better localizes the evidence that supports the decision. We also\nqualitatively show cases where visual explanation is more insightful than\ntextual explanation, and vice versa, supporting our thesis that multimodal\nexplanation models offer significant benefits over unimodal approaches.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "park2019css10", "citations": "71", "year": "2019", "title":"CSS10: A Collection Of Single Speaker Speech Datasets For 10 Languages", "abstract": "<p>We describe our development of CSS10, a collection of single speaker speech\ndatasets for ten languages. It is composed of short audio clips from LibriVox\naudiobooks and their aligned texts. To validate its quality we train two neural\ntext-to-speech models on each dataset. Subsequently, we conduct Mean Opinion\nScore tests on the synthesized speech samples. We make our datasets,\npre-trained models, and test resources publicly available. We hope they will be\nused for future speech tasks.</p>\n", "tags": ["Datasets","INTERSPEECH"] },
{"key": "park2023generative", "citations": "616", "year": "2023", "title":"Generative Agents: Interactive Simulacra Of Human Behavior", "abstract": "<p>Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents–computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent’s experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine’s\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture–observation, planning, and reflection–each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.</p>\n", "tags": ["Agentic","Applications","Evaluation","Model Architecture","Tools"] },
{"key": "parmar2018image", "citations": "208", "year": "2018", "title":"Image Transformer", "abstract": "<p>Image generation has been successfully cast as an autoregressive sequence\ngeneration or transformation problem. Recent work has shown that self-attention\nis an effective way of modeling textual sequences. In this work, we generalize\na recently proposed model architecture based on self-attention, the\nTransformer, to a sequence modeling formulation of image generation with a\ntractable likelihood. By restricting the self-attention mechanism to attend to\nlocal neighborhoods we significantly increase the size of images the model can\nprocess in practice, despite maintaining significantly larger receptive fields\nper layer than typical convolutional neural networks. While conceptually\nsimple, our generative models significantly outperform the current state of the\nart in image generation on ImageNet, improving the best published negative\nlog-likelihood on ImageNet from 3.83 to 3.77. We also present results on image\nsuper-resolution with a large magnification ratio, applying an encoder-decoder\nconfiguration of our architecture. In a human evaluation study, we find that\nimages generated by our super-resolution model fool human observers three times\nmore often than the previous state of the art.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "parmar2023zero", "citations": "209", "year": "2023", "title":"Zero-shot Image-to-image Translation", "abstract": "<p>Large-scale text-to-image generative models have shown their remarkable\nability to synthesize diverse and high-quality images. However, it is still\nchallenging to directly apply these models for editing real images for two\nreasons. First, it is hard for users to come up with a perfect text prompt that\naccurately describes every visual detail in the input image. Second, while\nexisting models can introduce desirable changes in certain regions, they often\ndramatically alter the input content and introduce unexpected changes in\nunwanted regions. In this work, we propose pix2pix-zero, an image-to-image\ntranslation method that can preserve the content of the original image without\nmanual prompting. We first automatically discover editing directions that\nreflect desired edits in the text embedding space. To preserve the general\ncontent structure after editing, we further propose cross-attention guidance,\nwhich aims to retain the cross-attention maps of the input image throughout the\ndiffusion process. In addition, our method does not need additional training\nfor these edits and can directly use the existing pre-trained text-to-image\ndiffusion model. We conduct extensive experiments and show that our method\noutperforms existing and concurrent works for both real and synthetic image\nediting.</p>\n", "tags": ["Model Architecture","Prompting","Training Techniques"] },
{"key": "parthasarathi2018extending", "citations": "72", "year": "2018", "title":"Extending Neural Generative Conversational Model Using External Knowledge Sources", "abstract": "<p>The use of connectionist approaches in conversational agents has been\nprogressing rapidly due to the availability of large corpora. However current\ngenerative dialogue models often lack coherence and are content poor. This work\nproposes an architecture to incorporate unstructured knowledge sources to\nenhance the next utterance prediction in chit-chat type of generative dialogue\nmodels. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents\ntrained with the Reddit News dataset, and consider incorporating external\nknowledge from Wikipedia summaries as well as from the NELL knowledge base. Our\nexperiments show faster training time and improved perplexity when leveraging\nexternal knowledge.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "parthasarathi2019lessons", "citations": "72", "year": "2019", "title":"Lessons From Building Acoustic Models With A Million Hours Of Speech", "abstract": "<p>This is a report of our lessons learned building acoustic models from 1\nMillion hours of unlabeled speech, while labeled speech is restricted to 7,000\nhours. We employ student/teacher training on unlabeled data, helping scale out\ntarget generation in comparison to confidence model based methods, which\nrequire a decoder and a confidence model. To optimize storage and to\nparallelize target generation, we store high valued logits from the teacher\nmodel. Introducing the notion of scheduled learning, we interleave learning on\nunlabeled and labeled data. To scale distributed training across a large number\nof GPUs, we use BMUF with 64 GPUs, while performing sequence training only on\nlabeled data with gradient threshold compression SGD using 16 GPUs. Our\nexperiments show that extremely large amounts of data are indeed useful; with\nlittle hyper-parameter tuning, we obtain relative WER improvements in the 10 to\n20% range, with higher gains in noisier conditions.</p>\n", "tags": ["ICASSP","Training Techniques"] },
{"key": "pashevich2021episodic", "citations": "97", "year": "2021", "title":"Episodic Transformer For Vision-and-language Navigation", "abstract": "<p>Interaction and navigation defined by natural language instructions in\ndynamic environments pose significant challenges for neural agents. This paper\nfocuses on addressing two challenges: handling long sequence of subtasks, and\nunderstanding complex human instructions. We propose Episodic Transformer\n(E.T.), a multimodal transformer that encodes language inputs and the full\nepisode history of visual observations and actions. To improve training, we\nleverage synthetic instructions as an intermediate representation that\ndecouples understanding the visual appearance of an environment from the\nvariations of natural language instructions. We demonstrate that encoding the\nhistory with a transformer is critical to solve compositional tasks, and that\npretraining and joint training with synthetic instructions further improve the\nperformance. Our approach sets a new state of the art on the challenging ALFRED\nbenchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test\nsplits.</p>\n", "tags": ["Datasets","Evaluation","ICCV","Model Architecture","Training Techniques"] },
{"key": "pasunuru2017multi", "citations": "99", "year": "2017", "title":"Multi-task Video Captioning With Video And Entailment Generation", "abstract": "<p>Video captioning, the task of describing the content of a video, has seen\nsome promising improvements in recent years with sequence-to-sequence models,\nbut accurately learning the temporal and logical dynamics involved in the task\nstill remains a challenge, especially given the lack of sufficient annotated\ndata. We improve video captioning by sharing knowledge with two related\ndirected-generation tasks: a temporally-directed unsupervised video prediction\ntask to learn richer context-aware video encoder representations, and a\nlogically-directed language entailment generation task to learn better\nvideo-entailed caption decoder representations. For this, we present a\nmany-to-many multi-task learning model that shares parameters across the\nencoders and decoders of the three tasks. We achieve significant improvements\nand the new state-of-the-art on several standard video captioning datasets\nusing diverse automatic and human evaluations. We also show mutual multi-task\nimprovements on the entailment generation task.</p>\n", "tags": ["Datasets"] },
{"key": "pasunuru2017reinforced", "citations": "120", "year": "2017", "title":"Reinforced Video Captioning With Entailment Rewards", "abstract": "<p>Sequence-to-sequence models have shown promising improvements on the temporal\ntask of video captioning, but they optimize word-level cross-entropy loss\nduring training. First, using policy gradient and mixed-loss methods for\nreinforcement learning, we directly optimize sentence-level task-based metrics\n(as rewards), achieving significant improvements over the baseline, based on\nboth automatic metrics and human evaluation on multiple datasets. Next, we\npropose a novel entailment-enhanced reward (CIDEnt) that corrects\nphrase-matching based metrics (such as CIDEr) to only allow for\nlogically-implied partial matches and avoid contradictions, achieving further\nsignificant improvements over the CIDEr-reward model. Overall, our\nCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "patashnik2021styleclip", "citations": "744", "year": "2021", "title":"Styleclip: Text-driven Manipulation Of Stylegan Imagery", "abstract": "<p>Inspired by the ability of StyleGAN to generate highly realistic images in a\nvariety of domains, much recent work has focused on understanding how to use\nthe latent spaces of StyleGAN to manipulate generated and real images. However,\ndiscovering semantically meaningful latent manipulations typically involves\npainstaking human examination of the many degrees of freedom, or an annotated\ncollection of images for each desired manipulation. In this work, we explore\nleveraging the power of recently introduced Contrastive Language-Image\nPre-training (CLIP) models in order to develop a text-based interface for\nStyleGAN image manipulation that does not require such manual effort. We first\nintroduce an optimization scheme that utilizes a CLIP-based loss to modify an\ninput latent vector in response to a user-provided text prompt. Next, we\ndescribe a latent mapper that infers a text-guided latent manipulation step for\na given input image, allowing faster and more stable text-based manipulation.\nFinally, we present a method for mapping a text prompts to input-agnostic\ndirections in StyleGAN’s style space, enabling interactive text-driven image\nmanipulation. Extensive results and comparisons demonstrate the effectiveness\nof our approaches.</p>\n", "tags": ["ICCV","Prompting","Training Techniques"] },
{"key": "paulus2017deep", "citations": "1294", "year": "2017", "title":"A Deep Reinforced Model For Abstractive Summarization", "abstract": "<p>Attentional, RNN-based encoder-decoder models for abstractive summarization\nhave achieved good performance on short input and output sequences. For longer\ndocuments and summaries however these models often include repetitive and\nincoherent phrases. We introduce a neural network model with a novel\nintra-attention that attends over the input and continuously generated output\nseparately, and a new training method that combines standard supervised word\nprediction and reinforcement learning (RL). Models trained only with supervised\nlearning often exhibit “exposure bias” - they assume ground truth is provided\nat each step during training. However, when standard word prediction is\ncombined with the global sequence prediction training of RL the resulting\nsummaries become more readable. We evaluate this model on the CNN/Daily Mail\nand New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the\nCNN/Daily Mail dataset, an improvement over previous state-of-the-art models.\nHuman evaluation also shows that our model produces higher quality summaries.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "pavlopoulos2017deep", "citations": "103", "year": "2017", "title":"Deep Learning For User Comment Moderation", "abstract": "<p>Experimenting with a new dataset of 1.6M user comments from a Greek news\nportal and existing datasets of English Wikipedia comments, we show that an RNN\noutperforms the previous state of the art in moderation. A deep,\nclassification-specific attention mechanism improves further the overall\nperformance of the RNN. We also compare against a CNN and a word-list baseline,\nconsidering both fully automatic and semi-automatic moderation.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "pavlopoulos2020toxicity", "citations": "91", "year": "2020", "title":"Toxicity Detection: Does Context Really Matter?", "abstract": "<p>Moderation is crucial to promoting healthy on-line discussions. Although\nseveral `toxicity’ detection datasets and models have been published, most of\nthem ignore the context of the posts, implicitly assuming that comments maybe\njudged independently. We investigate this assumption by focusing on two\nquestions: (a) does context affect the human judgement, and (b) does\nconditioning on context improve performance of toxicity detection systems? We\nexperiment with Wikipedia conversations, limiting the notion of context to the\nprevious post in the thread and the discussion title. We find that context can\nboth amplify or mitigate the perceived toxicity of posts. Moreover, a small but\nsignificant subset of manually labeled posts (5% in one of our experiments) end\nup having the opposite toxicity labels if the annotators are not provided with\ncontext. Surprisingly, we also find no evidence that context actually improves\nthe performance of toxicity classifiers, having tried a range of classifiers\nand mechanisms to make them context aware. This points to the need for larger\ndatasets of comments annotated in context. We make our code and data publicly\navailable.</p>\n", "tags": ["Datasets"] },
{"key": "pearce2021examining", "citations": "89", "year": "2023", "title":"Examining Zero-shot Vulnerability Repair With Large Language Models", "abstract": "<p>Human developers can produce code with cybersecurity bugs. Can emerging\n‘smart’ code completion tools help repair those bugs? In this work, we examine\nthe use of large language models (LLMs) for code (such as OpenAI’s Codex and\nAI21’s Jurassic J-1) for zero-shot vulnerability repair. We investigate\nchallenges in the design of prompts that coax LLMs into generating repaired\nversions of insecure code. This is difficult due to the numerous ways to phrase\nkey information - both semantically and syntactically - with natural languages.\nWe perform a large scale study of five commercially available, black-box,\n“off-the-shelf” LLMs, as well as an open-source model and our own\nlocally-trained model, on a mix of synthetic, hand-crafted, and real-world\nsecurity bug scenarios. Our experiments demonstrate that while the approach has\npromise (the LLMs could collectively repair 100% of our synthetically generated\nand hand-crafted scenarios), a qualitative evaluation of the model’s\nperformance over a corpus of historical real-world examples highlights\nchallenges in generating functionally correct code.</p>\n", "tags": ["Evaluation","Llm For Code","Security"] },
{"key": "peng2017addressing", "citations": "77", "year": "2017", "title":"Addressing The Data Sparsity Issue In Neural AMR Parsing", "abstract": "<p>Neural attention models have achieved great success in different NLP tasks.\nHow- ever, they have not fulfilled their promise on the AMR parsing task due to\nthe data sparsity issue. In this paper, we de- scribe a sequence-to-sequence\nmodel for AMR parsing and present different ways to tackle the data sparsity\nproblem. We show that our methods achieve significant improvement over a\nbaseline neural atten- tion model and our results are also compet- itive\nagainst state-of-the-art systems that do not use extra linguistic resources.</p>\n", "tags": ["Model Architecture","NAACL"] },
{"key": "peng2017adversarial", "citations": "63", "year": "2018", "title":"Adversarial Advantage Actor-critic Model For Task-completion Dialogue Policy Learning", "abstract": "<p>This paper presents a new method — adversarial advantage actor-critic\n(Adversarial A2C), which significantly improves the efficiency of dialogue\npolicy learning in task-completion dialogue systems. Inspired by generative\nadversarial networks (GAN), we train a discriminator to differentiate\nresponses/actions generated by dialogue agents from responses/actions by\nexperts. Then, we incorporate the discriminator as another critic into the\nadvantage actor-critic (A2C) framework, to encourage the dialogue agent to\nexplore state-action within the regions where the agent takes actions similar\nto those of the experts. Experimental results in a movie-ticket booking domain\nshow that the proposed Adversarial A2C can accelerate policy exploration\nefficiently.</p>\n", "tags": ["Agentic","Dialogue & Multi Turn","ICASSP","Reinforcement Learning"] },
{"key": "peng2018deep", "citations": "169", "year": "2018", "title":"Deep Dyna-q: Integrating Planning For Task-completion Dialogue Policy Learning", "abstract": "<p>Training a task-completion dialogue agent via reinforcement learning (RL) is\ncostly because it requires many interactions with real users. One common\nalternative is to use a user simulator. However, a user simulator usually lacks\nthe language complexity of human interlocutors and the biases in its design may\ntend to degrade the agent. To address these issues, we present Deep Dyna-Q,\nwhich to our knowledge is the first deep RL framework that integrates planning\nfor task-completion dialogue policy learning. We incorporate into the dialogue\nagent a model of the environment, referred to as the world model, to mimic real\nuser response and generate simulated experience. During dialogue policy\nlearning, the world model is constantly updated with real user experience to\napproach real user behavior, and in turn, the dialogue agent is optimized using\nboth real experience and simulated experience. The effectiveness of our\napproach is demonstrated on a movie-ticket booking task in both simulated and\nhuman-in-the-loop settings.</p>\n", "tags": ["Agentic","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "peng2018dynamic", "citations": "359", "year": "2019", "title":"Dynamic Fusion With Intra- And Inter- Modality Attention Flow For Visual Question Answering", "abstract": "<p>Learning effective fusion of multi-modality features is at the heart of\nvisual question answering. We propose a novel method of dynamically fusing\nmulti-modal features with intra- and inter-modality information flow, which\nalternatively pass dynamic information between and across the visual and\nlanguage modalities. It can robustly capture the high-level interactions\nbetween language and vision domains, thus significantly improves the\nperformance of visual question answering. We also show that the proposed\ndynamic intra-modality attention flow conditioned on the other modality can\ndynamically modulate the intra-modality attention of the target modality, which\nis vital for multimodality feature fusion. Experimental evaluations on the VQA\n2.0 dataset show that the proposed method achieves state-of-the-art VQA\nperformance. Extensive ablation studies are carried out for the comprehensive\nanalysis of the proposed method.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "peng2019transfer", "citations": "750", "year": "2019", "title":"Transfer Learning In Biomedical Natural Language Processing: An Evaluation Of BERT And Elmo On Ten Benchmarking Datasets", "abstract": "<p>Inspired by the success of the General Language Understanding Evaluation\nbenchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE)\nbenchmark to facilitate research in the development of pre-training language\nrepresentations in the biomedicine domain. The benchmark consists of five tasks\nwith ten datasets that cover both biomedical and clinical texts with different\ndataset sizes and difficulties. We also evaluate several baselines based on\nBERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and\nMIMIC-III clinical notes achieves the best results. We make the datasets,\npre-trained models, and codes publicly available at\nhttps://github.com/ncbi-nlp/BLUE_Benchmark.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "peng2020few", "citations": "164", "year": "2020", "title":"Few-shot Natural Language Generation For Task-oriented Dialog", "abstract": "<p>As a crucial component in task-oriented dialog systems, the Natural Language\nGeneration (NLG) module converts a dialog act represented in a semantic form\ninto a response in natural language. The success of traditional template-based\nor statistical models typically relies on heavily annotated data, which is\ninfeasible for new domains. Therefore, it is pivotal for an NLG system to\ngeneralize well with limited labelled data in real applications. To this end,\nwe present FewShotWoz, the first NLG benchmark to simulate the few-shot\nlearning setting in task-oriented dialog systems. Further, we develop the\nSC-GPT model. It is pre-trained on a large set of annotated NLG corpus to\nacquire the controllable generation ability, and fine-tuned with only a few\ndomain-specific labels to adapt to new domains. Experiments on FewShotWoz and\nthe large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly\noutperforms existing methods, measured by various automatic metrics and human\nevaluations.</p>\n", "tags": ["Applications","Datasets","EMNLP","Evaluation","Few-Shot","Model Architecture"] },
{"key": "peng2020learning", "citations": "169", "year": "2020", "title":"Learning From Context Or Names? An Empirical Study On Neural Relation Extraction", "abstract": "<p>Neural models have achieved remarkable success on relation extraction (RE)\nbenchmarks. However, there is no clear understanding which type of information\naffects existing RE models to make decisions and how to further improve the\nperformance of these models. To this end, we empirically study the effect of\ntwo main information sources in text: textual context and entity mentions\n(names). We find that (i) while context is the main source to support the\npredictions, RE models also heavily rely on the information from entity\nmentions, most of which is type information, and (ii) existing datasets may\nleak shallow heuristics via entity mentions and thus contribute to the high\nperformance on RE benchmarks. Based on the analyses, we propose an\nentity-masked contrastive pre-training framework for RE to gain a deeper\nunderstanding on both textual context and type information while avoiding rote\nmemorization of entities or use of superficial cues in mentions. We carry out\nextensive experiments to support our views, and show that our framework can\nimprove the effectiveness and robustness of neural models in different RE\nscenarios. All the code and datasets are released at\nhttps://github.com/thunlp/RE-Context-or-Names.</p>\n", "tags": ["Datasets","EMNLP","Has Code","Tools","Training Techniques"] },
{"key": "peng2020soloist", "citations": "100", "year": "2020", "title":"SOLOIST: Building Task Bots At Scale With Transfer Learning And Machine Teaching", "abstract": "<p>We present a new method SOLOIST that uses transfer learning and machine\nteaching to build task bots at scale. We parameterize classical modular\ntask-oriented dialog systems using a Transformer-based auto-regressive language\nmodel, which subsumes different dialog modules into a single neural model. We\npre-train, on heterogeneous dialog corpora, a task-grounded response generation\nmodel, which can generate dialog responses grounded in user goals and\nreal-world knowledge for task completion. The pre-trained model can be\nefficiently adapted to accomplish new tasks with a handful of task-specific\ndialogs via machine teaching, where training samples are generated by human\nteachers interacting with the system. Experiments show that (i) SOLOIST creates\nnew state-of-the-art on well-studied task-oriented dialog benchmarks, including\nCamRest676 and MultiWOZ; (ii) in the few-shot fine-tuning settings, SOLOIST\nsignificantly outperforms existing methods, and (iii) the use of machine\nteaching substantially reduces the labeling cost of fine-tuning. The\npre-trained models and codes are available at https://aka.ms/soloist.</p>\n", "tags": ["Few-Shot","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "peng2021random", "citations": "132", "year": "2021", "title":"Random Feature Attention", "abstract": "<p>Transformers are state-of-the-art models for a variety of sequence modeling\ntasks. At their core is an attention function which models pairwise\ninteractions between the inputs at every timestep. While attention is powerful,\nit does not scale efficiently to long sequences due to its quadratic time and\nspace complexity in the sequence length. We propose RFA, a linear time and\nspace attention that uses random feature methods to approximate the softmax\nfunction, and explore its application in transformers. RFA can be used as a\ndrop-in replacement for conventional softmax attention and offers a\nstraightforward way of learning with recency bias through an optional gating\nmechanism. Experiments on language modeling and machine translation demonstrate\nthat RFA achieves similar or better performance compared to strong transformer\nbaselines. In the machine translation experiment, RFA decodes twice as fast as\na vanilla transformer. Compared to existing efficient transformer variants, RFA\nis competitive in terms of both accuracy and efficiency on three long text\nclassification datasets. Our analysis shows that RFA’s efficiency gains are\nespecially notable on long sequences, suggesting that RFA will be particularly\nuseful in tasks that require working with large inputs, fast decoding speed, or\nlow memory footprints.</p>\n", "tags": ["Datasets","Efficiency","Model Architecture"] },
{"key": "peng2023impact", "citations": "170", "year": "2023", "title":"The Impact Of AI On Developer Productivity: Evidence From Github Copilot", "abstract": "<p>Generative AI tools hold promise to increase human productivity. This paper\npresents results from a controlled experiment with GitHub Copilot, an AI pair\nprogrammer. Recruited software developers were asked to implement an HTTP\nserver in JavaScript as quickly as possible. The treatment group, with access\nto the AI pair programmer, completed the task 55.8% faster than the control\ngroup. Observed heterogenous effects show promise for AI pair programmers to\nhelp people transition into software development careers.</p>\n", "tags": ["Has Code","Tools"] },
{"key": "peng2023instruction", "citations": "166", "year": "2023", "title":"Instruction Tuning With GPT-4", "abstract": "<p>Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "peng2023kosmos", "citations": "98", "year": "2023", "title":"Kosmos-2: Grounding Multimodal Large Language Models To The World", "abstract": "<p>We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``<a href=\"bounding boxes\">text span</a>’’, where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Code and pretrained models are available at\nhttps://aka.ms/kosmos-2.</p>\n", "tags": ["Applications","In Context Learning"] },
{"key": "peng2023study", "citations": "184", "year": "2023", "title":"A Study Of Generative Large Language Model For Medical Research And Healthcare", "abstract": "<p>There is enormous enthusiasm and concerns in using large language models\n(LLMs) in healthcare, yet current assumptions are all based on general-purpose\nLLMs such as ChatGPT. This study develops a clinical generative LLM,\nGatorTronGPT, using 277 billion words of mixed clinical and English text with a\nGPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical\nnatural language processing for medical research. Synthetic NLP models trained\nusing GatorTronGPT generated text outperform NLP models trained using\nreal-world clinical text. Physicians Turing test using 1 (worst) to 9 (best)\nscale shows that there is no significant difference in linguistic readability\n(p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical\nrelevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that\nphysicians cannot differentiate them (p &lt; 0.001). This study provides insights\non the opportunities and challenges of LLMs for medical research and\nhealthcare.</p>\n", "tags": ["Model Architecture"] },
{"key": "peng2023towards", "citations": "76", "year": "2023", "title":"Towards Efficient And Effective Adaptation Of Large Language Models For Sequential Recommendation", "abstract": "<p>In recent years, with large language models (LLMs) achieving state-of-the-art\nperformance in context understanding, increasing efforts have been dedicated to\ndeveloping LLM-enhanced sequential recommendation (SR) methods. Considering\nthat most existing LLMs are not specifically optimized for recommendation\ntasks, adapting them for SR becomes a critical step in LLM-enhanced SR methods.\nThough numerous adaptation methods have been developed, it still remains a\nsignificant challenge to adapt LLMs for SR both efficiently and effectively. To\naddress this challenge, in this paper, we introduce a novel side sequential\nnetwork adaptation method, denoted as SSNA, for LLM enhanced SR. SSNA features\nthree key designs to allow both efficient and effective LLM adaptation. First,\nSSNA learns adapters separate from LLMs, while fixing all the pre-trained\nparameters within LLMs to allow efficient adaptation. In addition, SSNA adapts\nthe top-a layers of LLMs jointly, and integrates adapters sequentially for\nenhanced effectiveness (i.e., recommendation performance). We compare SSNA\nagainst five state-of-the-art baseline methods on five benchmark datasets using\nthree LLMs. The experimental results demonstrate that SSNA significantly\noutperforms all the baseline methods in terms of recommendation performance,\nand achieves substantial improvement over the best-performing baseline methods\nat both run-time and memory efficiency during training. Our analysis shows the\neffectiveness of integrating adapters in a sequential manner. Our parameter\nstudy demonstrates the effectiveness of jointly adapting the top-a layers of\nLLMs.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Training Techniques"] },
{"key": "perez2016gated", "citations": "107", "year": "2017", "title":"Gated End-to-end Memory Networks", "abstract": "<p>Machine reading using differentiable reasoning models has recently shown\nremarkable progress. In this context, End-to-End trainable Memory Networks,\nMemN2N, have demonstrated promising performance on simple natural language\nbased reasoning tasks such as factual reasoning and basic deduction. However,\nother tasks, namely multi-fact question-answering, positional reasoning or\ndialog related tasks, remain challenging particularly due to the necessity of\nmore complex interactions between the memory and controller modules composing\nthis family of models. In this paper, we introduce a novel end-to-end memory\naccess regulation mechanism inspired by the current progress on the connection\nshort-cutting principle in the field of computer vision. Concretely, we develop\na Gated End-to-End trainable Memory Network architecture, GMemN2N. From the\nmachine learning perspective, this new capability is learned in an end-to-end\nfashion without the use of any additional supervision signal which is, as far\nas our knowledge goes, the first of its kind. Our experiments show significant\nimprovements on the most challenging tasks in the 20 bAbI dataset, without the\nuse of any domain knowledge. Then, we show improvements on the dialog bAbI\ntasks including the real human-bot conversion-based Dialog State Tracking\nChallenge (DSTC-2) dataset. On these two datasets, our model sets the new state\nof the art.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "perez2021true", "citations": "183", "year": "2021", "title":"True Few-shot Learning With Language Models", "abstract": "<p>Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (“prompts”). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model’s true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.</p>\n", "tags": ["Few-Shot","Training Techniques"] },
{"key": "perez2022discovering", "citations": "85", "year": "2023", "title":"Discovering Language Model Behaviors With Model-written Evaluations", "abstract": "<p>As language models (LMs) scale, they develop many novel behaviors, good and\nbad, exacerbating the need to evaluate how they behave. Prior work creates\nevaluations with crowdwork (which is time-consuming and expensive) or existing\ndata sources (which are not always available). Here, we automatically generate\nevaluations with LMs. We explore approaches with varying amounts of human\neffort, from instructing LMs to write yes/no questions to making complex\nWinogender schemas with multiple stages of LM-based generation and filtering.\nCrowdworkers rate the examples as highly relevant and agree with 90-100% of\nlabels, sometimes more so than corresponding human-written datasets. We\ngenerate 154 datasets and discover new cases of inverse scaling where LMs get\nworse with size. Larger LMs repeat back a dialog user’s preferred answer\n(“sycophancy”) and express greater desire to pursue concerning goals like\nresource acquisition and goal preservation. We also find some of the first\nexamples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF\nmakes LMs worse. For example, RLHF makes LMs express stronger political views\n(on gun rights and immigration) and a greater desire to avoid shut down.\nOverall, LM-written evaluations are high-quality and let us quickly discover\nmany novel LM behaviors.</p>\n", "tags": ["Datasets","Evaluation","Reinforcement Learning"] },
{"key": "perez2022red", "citations": "146", "year": "2022", "title":"Red Teaming Language Models With Language Models", "abstract": "<p>Language Models (LMs) often cannot be deployed because of their potential to\nharm users in hard-to-predict ways. Prior work identifies harmful behaviors\nbefore deployment by using human annotators to hand-write test cases. However,\nhuman annotation is expensive, limiting the number and diversity of test cases.\nIn this work, we automatically find cases where a target LM behaves in a\nharmful way, by generating test cases (“red teaming”) using another LM. We\nevaluate the target LM’s replies to generated test questions using a classifier\ntrained to detect offensive content, uncovering tens of thousands of offensive\nreplies in a 280B parameter LM chatbot. We explore several methods, from\nzero-shot generation to reinforcement learning, for generating test cases with\nvarying levels of diversity and difficulty. Furthermore, we use prompt\nengineering to control LM-generated test cases to uncover a variety of other\nharms, automatically finding groups of people that the chatbot discusses in\noffensive ways, personal and hospital phone numbers generated as the chatbot’s\nown contact info, leakage of private training data in generated text, and harms\nthat occur over the course of a conversation. Overall, LM-based red teaming is\none promising tool (among many needed) for finding and fixing diverse,\nundesirable LM behaviors before impacting users.</p>\n", "tags": ["EMNLP","Prompting","Reinforcement Learning","Security","Training Techniques"] },
{"key": "perone2018evaluation", "citations": "105", "year": "2018", "title":"Evaluation Of Sentence Embeddings In Downstream And Linguistic Probing Tasks", "abstract": "<p>Despite the fast developmental pace of new sentence embedding methods, it is\nstill challenging to find comprehensive evaluations of these different\ntechniques. In the past years, we saw significant improvements in the field of\nsentence embeddings and especially towards the development of universal\nsentence encoders that could provide inductive transfer to a wide variety of\ndownstream tasks. In this work, we perform a comprehensive evaluation of recent\nmethods using a wide variety of downstream and linguistic feature probing\ntasks. We show that a simple approach using bag-of-words with a recently\nintroduced language model for deep context-dependent word embeddings proved to\nyield better results in many tasks when compared to sentence encoders trained\non entailment datasets. We also show, however, that we are still far away from\na universal encoder that can perform consistently across several downstream\ntasks.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "perry2022do", "citations": "77", "year": "2023", "title":"Do Users Write More Insecure Code With AI Assistants?", "abstract": "<p>We conduct the first large-scale user study examining how users interact with\nan AI Code assistant to solve a variety of security related tasks across\ndifferent programming languages. Overall, we find that participants who had\naccess to an AI assistant based on OpenAI’s codex-davinci-002 model wrote\nsignificantly less secure code than those without access. Additionally,\nparticipants with access to an AI assistant were more likely to believe they\nwrote secure code than those without access to the AI assistant. Furthermore,\nwe find that participants who trusted the AI less and engaged more with the\nlanguage and format of their prompts (e.g. re-phrasing, adjusting temperature)\nprovided code with fewer security vulnerabilities. Finally, in order to better\ninform the design of future AI-based Code assistants, we provide an in-depth\nanalysis of participants’ language and interaction behavior, as well as release\nour user interface as an instrument to conduct similar studies in the future.</p>\n", "tags": ["Security"] },
{"key": "peters2018dissecting", "citations": "446", "year": "2018", "title":"Dissecting Contextual Word Embeddings: Architecture And Representation", "abstract": "<p>Contextual word representations derived from pre-trained bidirectional\nlanguage models (biLMs) have recently been shown to provide significant\nimprovements to the state of the art for a wide range of NLP tasks. However,\nmany questions remain as to how and why these models are so effective. In this\npaper, we present a detailed empirical study of how the choice of neural\narchitecture (e.g. LSTM, CNN, or self attention) influences both end task\naccuracy and qualitative properties of the representations that are learned. We\nshow there is a tradeoff between speed and accuracy, but all architectures\nlearn high quality contextual representations that outperform word embeddings\nfor four challenging NLP tasks. Additionally, all architectures learn\nrepresentations that vary with network depth, from exclusively morphological\nbased at the word embedding layer through local syntax based in the lower\ncontextual layers to longer range semantics such coreference at the upper\nlayers. Together, these results suggest that unsupervised biLMs, independent of\narchitecture, are learning much more about the structure of language than\npreviously appreciated.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "peters2019knowledge", "citations": "650", "year": "2019", "title":"Knowledge Enhanced Contextual Word Representations", "abstract": "<p>Contextual word representations, typically trained on unstructured, unlabeled\ntext, do not contain any explicit grounding to real world entities and are\noften unable to remember facts about those entities. We propose a general\nmethod to embed multiple knowledge bases (KBs) into large scale models, and\nthereby enhance their representations with structured, human-curated knowledge.\nFor each KB, we first use an integrated entity linker to retrieve relevant\nentity embeddings, then update contextual word representations via a form of\nword-to-entity attention. In contrast to previous approaches, the entity\nlinkers and self-supervised language modeling objective are jointly trained\nend-to-end in a multitask setting that combines a small amount of entity\nlinking supervision with a large amount of raw text. After integrating WordNet\nand a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert)\ndemonstrates improved perplexity, ability to recall facts as measured in a\nprobing task and downstream performance on relationship extraction, entity\ntyping, and word sense disambiguation. KnowBert’s runtime is comparable to\nBERT’s and it scales to large KBs.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "peters2019tune", "citations": "397", "year": "2019", "title":"To Tune Or Not To Tune? Adapting Pretrained Representations To Diverse Tasks", "abstract": "<p>While most previous work has focused on different pretraining objectives and\narchitectures for transfer learning, we ask how to best adapt the pretrained\nmodel to a given target task. We focus on the two most common forms of\nadaptation, feature extraction (where the pretrained weights are frozen), and\ndirectly fine-tuning the pretrained model. Our empirical results across diverse\nNLP tasks with two state-of-the-art models show that the relative performance\nof fine-tuning vs. feature extraction depends on the similarity of the\npretraining and target tasks. We explore possible explanations for this finding\nand provide a set of adaptation guidelines for the NLP practitioner.</p>\n", "tags": ["Fine-Tuning","Training Techniques"] },
{"key": "petroni2019language", "citations": "1631", "year": "2019", "title":"Language Models As Knowledge Bases?", "abstract": "<p>Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n“fill-in-the-blank” cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.</p>\n", "tags": ["EMNLP","Fine-Tuning","Model Architecture"] },
{"key": "petroni2020how", "citations": "85", "year": "2020", "title":"How Context Affects Language Models' Factual Predictions", "abstract": "<p>When pre-trained on large unsupervised textual corpora, language models are\nable to store and retrieve factual knowledge to some extent, making it possible\nto use them directly for zero-shot cloze-style question answering. However,\nstoring factual knowledge in a fixed number of weights of a language model\nclearly has limitations. Previous approaches have successfully provided access\nto information outside the model weights using supervised architectures that\ncombine an information retrieval system with a machine reading component. In\nthis paper, we go a step further and integrate information from a retrieval\nsystem with a pre-trained language model in a purely unsupervised way. We\nreport that augmenting pre-trained language models in this way dramatically\nimproves performance and that the resulting system, despite being unsupervised,\nis competitive with a supervised machine reading baseline. Furthermore,\nprocessing query and context with different segment tokens allows BERT to\nutilize its Next Sentence Prediction pre-trained classifier to determine\nwhether the context is relevant or not, substantially improving BERT’s\nzero-shot cloze-style question-answering performance and making its predictions\nrobust to noisy contexts.</p>\n", "tags": ["Memory & Context","Model Architecture"] },
{"key": "petrovich2022temos", "citations": "187", "year": "2022", "title":"TEMOS: Generating Diverse Human Motions From Textual Descriptions", "abstract": "<p>We address the problem of generating diverse 3D human motions from textual\ndescriptions. This challenging task requires joint modeling of both modalities:\nunderstanding and extracting useful human-centric information from the text,\nand then generating plausible and realistic sequences of human poses. In\ncontrast to most previous work which focuses on generating a single,\ndeterministic, motion from a textual description, we design a variational\napproach that can produce multiple diverse human motions. We propose TEMOS, a\ntext-conditioned generative model leveraging variational autoencoder (VAE)\ntraining with human motion data, in combination with a text encoder that\nproduces distribution parameters compatible with the VAE latent space. We show\nthe TEMOS framework can produce both skeleton-based animations as in prior\nwork, as well more expressive SMPL body motions. We evaluate our approach on\nthe KIT Motion-Language benchmark and, despite being relatively\nstraightforward, demonstrate significant improvements over the state of the\nart. Code and models are available on our webpage.</p>\n", "tags": ["Datasets","Evaluation","Tools","Training Techniques"] },
{"key": "pfeiffer2020adapterhub", "citations": "399", "year": "2020", "title":"Adapterhub: A Framework For Adapting Transformers", "abstract": "<p>The current modus operandi in NLP involves downloading and fine-tuning\npre-trained models consisting of millions or billions of parameters. Storing\nand sharing such large trained models is expensive, slow, and time-consuming,\nwhich impedes progress towards more general and versatile NLP methods that\nlearn from and for many tasks. Adapters – small learnt bottleneck layers\ninserted within each layer of a pre-trained model – ameliorate this issue by\navoiding full fine-tuning of the entire model. However, sharing and integrating\nadapter layers is not straightforward. We propose AdapterHub, a framework that\nallows dynamic “stitching-in” of pre-trained adapters for different tasks and\nlanguages. The framework, built on top of the popular HuggingFace Transformers\nlibrary, enables extremely easy and quick adaptations of state-of-the-art\npre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\nDownloading, sharing, and training adapters is as seamless as possible using\nminimal changes to the training scripts and a specialized infrastructure. Our\nframework enables scalable and easy access to sharing of task-specific models,\nparticularly in low-resource scenarios. AdapterHub includes all recent adapter\narchitectures and can be found at https://AdapterHub.ml.</p>\n", "tags": ["EMNLP","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "pfeiffer2020mad", "citations": "405", "year": "2020", "title":"MAD-X: An Adapter-based Framework For Multi-task Cross-lingual Transfer", "abstract": "<p>The main goal behind state-of-the-art pre-trained multilingual models such as\nmultilingual BERT and XLM-R is enabling and bootstrapping NLP applications in\nlow-resource languages through zero-shot or few-shot cross-lingual transfer.\nHowever, due to limited model capacity, their transfer performance is the\nweakest exactly on such low-resource languages and languages unseen during\npre-training. We propose MAD-X, an adapter-based framework that enables high\nportability and parameter-efficient transfer to arbitrary tasks and languages\nby learning modular language and task representations. In addition, we\nintroduce a novel invertible adapter architecture and a strong baseline method\nfor adapting a pre-trained multilingual model to a new language. MAD-X\noutperforms the state of the art in cross-lingual transfer across a\nrepresentative set of typologically diverse languages on named entity\nrecognition and causal commonsense reasoning, and achieves competitive results\non question answering. Our code and adapters are available at AdapterHub.ml</p>\n", "tags": ["Applications","EMNLP","Few-Shot","Model Architecture","Tools","Training Techniques"] },
{"key": "pfeiffer2020unks", "citations": "70", "year": "2021", "title":"Unks Everywhere: Adapting Multilingual Language Models To New Scripts", "abstract": "<p>Massively multilingual language models such as multilingual BERT offer\nstate-of-the-art cross-lingual transfer performance on a range of NLP tasks.\nHowever, due to limited capacity and large differences in pretraining data\nsizes, there is a profound performance gap between resource-rich and\nresource-poor target languages. The ultimate challenge is dealing with\nunder-resourced languages not covered at all by the models and written in\nscripts unseen during pretraining. In this work, we propose a series of novel\ndata-efficient methods that enable quick and effective adaptation of pretrained\nmultilingual models to such low-resource languages and unseen scripts. Relying\non matrix factorization, our methods capitalize on the existing latent\nknowledge about multiple languages already available in the pretrained model’s\nembedding matrix. Furthermore, we show that learning of the new dedicated\nembedding matrix in the target language can be improved by leveraging a small\nnumber of vocabulary items (i.e., the so-called lexically overlapping tokens)\nshared between mBERT’s and target language vocabulary. Our adaptation\ntechniques offer substantial performance gains for languages with unseen\nscripts. We also demonstrate that they can yield improvements for low-resource\nlanguages written in scripts covered by the pretrained model.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "pham2019very", "citations": "182", "year": "2019", "title":"Very Deep Self-attention Networks For End-to-end Speech Recognition", "abstract": "<p>Recently, end-to-end sequence-to-sequence models for speech recognition have\ngained significant interest in the research community. While previous\narchitecture choices revolve around time-delay neural networks (TDNN) and long\nshort-term memory (LSTM) recurrent neural networks, we propose to use\nself-attention via the Transformer architecture as an alternative. Our analysis\nshows that deep Transformer networks with high learning capacity are able to\nexceed performance from previous end-to-end approaches and even match the\nconventional hybrid systems. Moreover, we trained very deep models with up to\n48 Transformer layers for both encoder and decoders combined with stochastic\nresidual connections, which greatly improve generalizability and training\nefficiency. The resulting models outperform all previous end-to-end ASR\napproaches on the Switchboard benchmark. An ensemble of these models achieve\n9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This\nfinding brings our end-to-end models to competitive levels with previous hybrid\nsystems. Further, with model ensembling the Transformers can outperform certain\nhybrid systems, which are more complicated in terms of both structure and\ntraining procedure.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "pham2020out", "citations": "68", "year": "2021", "title":"Out Of Order: How Important Is The Sequential Order Of Words In A Sentence In Natural Language Understanding Tasks?", "abstract": "<p>Do state-of-the-art natural language understanding models care about word\norder - one of the most important characteristics of a sequence? Not always! We\nfound 75% to 90% of the correct predictions of BERT-based classifiers, trained\non many GLUE tasks, remain constant after input words are randomly shuffled.\nDespite BERT embeddings are famously contextual, the contribution of each\nindividual word to downstream tasks is almost unchanged even after the word’s\ncontext is shuffled. BERT-based models are able to exploit superficial cues\n(e.g. the sentiment of keywords in sentiment analysis; or the word-wise\nsimilarity between sequence-pair inputs in natural language inference) to make\ncorrect decisions when tokens are arranged in random orders. Encouraging\nclassifiers to capture word order information improves the performance on most\nGLUE tasks, SQuAD 2.0 and out-of-samples. Our work suggests that many GLUE\ntasks are not challenging machines to understand the meaning of a sentence.</p>\n", "tags": ["Model Architecture"] },
{"key": "phan2021cotext", "citations": "75", "year": "2021", "title":"Cotext: Multi-task Learning With Code-text Transformer", "abstract": "<p>We present CoTexT, a pre-trained, transformer-based encoder-decoder model\nthat learns the representative context between natural language (NL) and\nprogramming language (PL). Using self-supervision, CoTexT is pre-trained on\nlarge programming language corpora to learn a general understanding of language\nand code. CoTexT supports downstream NL-PL tasks such as code\nsummarizing/documentation, code generation, defect detection, and code\ndebugging. We train CoTexT on different combinations of available PL corpus\nincluding both “bimodal” and “unimodal” data. Here, bimodal data is the\ncombination of text and corresponding code snippets, whereas unimodal data is\nmerely code snippets. We first evaluate CoTexT with multi-task learning: we\nperform Code Summarization on 6 different programming languages and Code\nRefinement on both small and medium size featured in the CodeXGLUE dataset. We\nfurther conduct extensive experiments to investigate CoTexT on other tasks\nwithin the CodeXGlue dataset, including Code Generation and Defect Detection.\nWe consistently achieve SOTA results in these tasks, demonstrating the\nversatility of our models.</p>\n", "tags": ["Datasets","Llm For Code","Model Architecture"] },
{"key": "phan2021scifive", "citations": "80", "year": "2021", "title":"Scifive: A Text-to-text Transformer Model For Biomedical Literature", "abstract": "<p>In this report, we introduce SciFive, a domain-specific T5 model that has\nbeen pre-trained on large biomedical corpora. Our model outperforms the current\nSOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation,\nrelation extraction, natural language inference, and question-answering. We\nshow that text-generation methods have significant potential in a broad array\nof biomedical NLP tasks, particularly those requiring longer, more complex\noutputs. Our results support the exploration of more difficult text generation\ntasks and the development of new methods in this area</p>\n", "tags": ["Model Architecture"] },
{"key": "phang2018sentence", "citations": "291", "year": "2018", "title":"Sentence Encoders On Stilts: Supplementary Training On Intermediate Labeled-data Tasks", "abstract": "<p>Pretraining sentence encoders with language modeling and related unsupervised\ntasks has recently been shown to be very effective for language understanding\ntasks. By supplementing language model-style pretraining with further training\non data-rich supervised tasks, such as natural language inference, we obtain\nadditional performance improvements on the GLUE benchmark. Applying\nsupplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of\n81.8—the state of the art (as of 02/24/2019) and a 1.4 point improvement over\nBERT. We also observe reduced variance across random restarts in this setting.\nOur approach yields similar improvements when applied to ELMo (Peters et al.,\n2018a) and Radford et al. (2018)’s model. In addition, the benefits of\nsupplementary training are particularly pronounced in data-constrained regimes,\nas we show in experiments with artificially limited training data.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "pichotta2016using", "citations": "65", "year": "2016", "title":"Using Sentence-level LSTM Language Models For Script Inference", "abstract": "<p>There is a small but growing body of research on statistical scripts, models\nof event sequences that allow probabilistic inference of implicit events from\ndocuments. These systems operate on structured verb-argument events produced by\nan NLP pipeline. We compare these systems with recent Recurrent Neural Net\nmodels that directly operate on raw tokens to predict sentences, finding the\nlatter to be roughly comparable to the former in terms of predicting missing\nevents in documents.</p>\n", "tags": ["Model Architecture"] },
{"key": "pillutla2021mauve", "citations": "89", "year": "2021", "title":"MAUVE: Measuring The Gap Between Neural Text And Human Text Using Divergence Frontiers", "abstract": "<p>As major progress is made in open-ended text generation, measuring how close\nmachine-generated text is to human language remains a critical open problem. We\nintroduce MAUVE, a comparison measure for open-ended text generation, which\ndirectly compares the learnt distribution from a text generation model to the\ndistribution of human-written text using divergence frontiers. MAUVE scales up\nto modern text generation models by computing information divergences in a\nquantized embedding space. Through an extensive empirical study on three\nopen-ended generation tasks, we find that MAUVE identifies known properties of\ngenerated text, scales naturally with model size, and correlates with human\njudgments, with fewer restrictions than existing distributional evaluation\nmetrics.</p>\n", "tags": ["Evaluation"] },
{"key": "ping2017deep", "citations": "281", "year": "2017", "title":"Deep Voice 3: Scaling Text-to-speech With Convolutional Sequence Learning", "abstract": "<p>We present Deep Voice 3, a fully-convolutional attention-based neural\ntext-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural\nspeech synthesis systems in naturalness while training ten times faster. We\nscale Deep Voice 3 to data set sizes unprecedented for TTS, training on more\nthan eight hundred hours of audio from over two thousand speakers. In addition,\nwe identify common error modes of attention-based speech synthesis networks,\ndemonstrate how to mitigate them, and compare several different waveform\nsynthesis methods. We also describe how to scale inference to ten million\nqueries per day on one single-GPU server.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "ping2018clarinet", "citations": "245", "year": "2018", "title":"Clarinet: Parallel Wave Generation In End-to-end Text-to-speech", "abstract": "<p>In this work, we propose a new solution for parallel wave generation by\nWaveNet. In contrast to parallel WaveNet (van den Oord et al., 2018), we\ndistill a Gaussian inverse autoregressive flow from the autoregressive WaveNet\nby minimizing a regularized KL divergence between their highly-peaked output\ndistributions. Our method computes the KL divergence in closed-form, which\nsimplifies the training algorithm and provides very efficient distillation. In\naddition, we introduce the first text-to-wave neural architecture for speech\nsynthesis, which is fully convolutional and enables fast end-to-end training\nfrom scratch. It significantly outperforms the previous pipeline that connects\na text-to-spectrogram model to a separately trained WaveNet (Ping et al.,\n2018). We also successfully distill a parallel waveform synthesizer conditioned\non the hidden representation in this end-to-end model.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "pires2019how", "citations": "1050", "year": "2019", "title":"How Multilingual Is Multilingual BERT?", "abstract": "<p>In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et\nal. (2018) as a single language model pre-trained from monolingual corpora in\n104 languages, is surprisingly good at zero-shot cross-lingual model transfer,\nin which task-specific annotations in one language are used to fine-tune the\nmodel for evaluation in another language. To understand why, we present a large\nnumber of probing experiments, showing that transfer is possible even to\nlanguages in different scripts, that transfer works best between typologically\nsimilar languages, that monolingual corpora can train models for\ncode-switching, and that the model can find translation pairs. From these\nresults, we can conclude that M-BERT does create multilingual representations,\nbut that these representations exhibit systematic deficiencies affecting\ncertain language pairs.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "plank2016multilingual", "citations": "473", "year": "2016", "title":"Multilingual Part-of-speech Tagging With Bidirectional Long Short-term Memory Models And Auxiliary Loss", "abstract": "<p>Bidirectional long short-term memory (bi-LSTM) networks have recently proven\nsuccessful for various NLP sequence modeling tasks, but little is known about\ntheir reliance to input representations, target languages, data set size, and\nlabel noise. We address these issues and evaluate bi-LSTMs with word,\ncharacter, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to\ntraditional POS taggers across languages and data sizes. We also present a\nnovel bi-LSTM model, which combines the POS tagging loss function with an\nauxiliary loss function that accounts for rare words. The model obtains\nstate-of-the-art performance across 22 languages, and works especially well for\nmorphologically complex languages. Our analysis suggests that bi-LSTMs are less\nsensitive to training data size and label corruptions (at small noise levels)\nthan previously assumed.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "platanios2018contextual", "citations": "170", "year": "2018", "title":"Contextual Parameter Generation For Universal Neural Machine Translation", "abstract": "<p>We propose a simple modification to existing neural machine translation (NMT)\nmodels that enables using a single universal model to translate between\nmultiple languages while allowing for language specific parameterization, and\nthat can also be used for domain adaptation. Our approach requires no changes\nto the model architecture of a standard NMT system, but instead introduces a\nnew component, the contextual parameter generator (CPG), that generates the\nparameters of the system (e.g., weights in a neural network). This parameter\ngenerator accepts source and target language embeddings as input, and generates\nthe parameters for the encoder and the decoder, respectively. The rest of the\nmodel remains unchanged and is shared across all languages. We show how this\nsimple modification enables the system to use monolingual data for training and\nalso perform zero-shot translation. We further show it is able to surpass\nstate-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and\nthat the learned language embeddings are able to uncover interesting\nrelationships between languages.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "platanios2019competence", "citations": "270", "year": "2019", "title":"Competence-based Curriculum Learning For Neural Machine Translation", "abstract": "<p>Current state-of-the-art NMT systems use large neural networks that are not\nonly slow to train, but also often require many heuristics and optimization\ntricks, such as specialized learning rate schedules and large batch sizes. This\nis undesirable as it requires extensive hyperparameter tuning. In this paper,\nwe propose a curriculum learning framework for NMT that reduces training time,\nreduces the need for specialized heuristics or large batch sizes, and results\nin overall better performance. Our framework consists of a principled way of\ndeciding which training samples are shown to the model at different times\nduring training, based on the estimated difficulty of a sample and the current\ncompetence of the model. Filtering training samples in this manner prevents the\nmodel from getting stuck in bad local optima, making it converge faster and\nreach a better solution than the common approach of uniformly sampling training\nexamples. Furthermore, the proposed method can be easily applied to existing\nNMT models by simply modifying their input data pipelines. We show that our\nframework can help improve the training time and the performance of both\nrecurrent neural network models and Transformers, achieving up to a 70%\ndecrease in training time, while at the same time obtaining accuracy\nimprovements of up to 2.2 BLEU.</p>\n", "tags": ["Efficiency","Tools","Training Techniques"] },
{"key": "podell2023sdxl", "citations": "166", "year": "2023", "title":"SDXL: Improving Latent Diffusion Models For High-resolution Image Synthesis", "abstract": "<p>We present SDXL, a latent diffusion model for text-to-image synthesis.\nCompared to previous versions of Stable Diffusion, SDXL leverages a three times\nlarger UNet backbone: The increase of model parameters is mainly due to more\nattention blocks and a larger cross-attention context as SDXL uses a second\ntext encoder. We design multiple novel conditioning schemes and train SDXL on\nmultiple aspect ratios. We also introduce a refinement model which is used to\nimprove the visual fidelity of samples generated by SDXL using a post-hoc\nimage-to-image technique. We demonstrate that SDXL shows drastically improved\nperformance compared the previous versions of Stable Diffusion and achieves\nresults competitive with those of black-box state-of-the-art image generators.\nIn the spirit of promoting open research and fostering transparency in large\nmodel training and evaluation, we provide access to code and model weights at\nhttps://github.com/Stability-AI/generative-models</p>\n", "tags": ["Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "poerner2019e", "citations": "126", "year": "2020", "title":"E-BERT: Efficient-yet-effective Entity Embeddings For BERT", "abstract": "<p>We present a novel way of injecting factual knowledge about entities into the\npretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity\nvectors (Yamada et al., 2016) with BERT’s native wordpiece vector space and use\nthe aligned entity vectors as if they were wordpiece vectors. The resulting\nentity-enhanced version of BERT (called E-BERT) is similar in spirit to ERNIE\n(Zhang et al., 2019) and KnowBert (Peters et al., 2019), but it requires no\nexpensive further pretraining of the BERT encoder. We evaluate E-BERT on\nunsupervised question answering (QA), supervised relation classification (RC)\nand entity linking (EL). On all three tasks, E-BERT outperforms BERT and other\nbaselines. We also show quantitatively that the original BERT model is overly\nreliant on the surface form of entity names (e.g., guessing that someone with\nan Italian-sounding name speaks Italian), and that E-BERT mitigates this\nproblem.</p>\n", "tags": ["EMNLP","Efficiency","Model Architecture"] },
{"key": "polak2023extracting", "citations": "138", "year": "2024", "title":"Extracting Accurate Materials Data From Research Papers With Conversational Language Models And Prompt Engineering", "abstract": "<p>There has been a growing effort to replace manual extraction of data from\nresearch papers with automated data extraction based on natural language\nprocessing, language models, and recently, large language models (LLMs).\nAlthough these methods enable efficient extraction of data from large sets of\nresearch papers, they require a significant amount of up-front effort,\nexpertise, and coding. In this work we propose the ChatExtract method that can\nfully automate very accurate data extraction with minimal initial effort and\nbackground, using an advanced conversational LLM. ChatExtract consists of a set\nof engineered prompts applied to a conversational LLM that both identify\nsentences with data, extract that data, and assure the data’s correctness\nthrough a series of follow-up questions. These follow-up questions largely\novercome known issues with LLMs providing factually inaccurate responses.\nChatExtract can be applied with any conversational LLMs and yields very high\nquality data extraction. In tests on materials data we find precision and\nrecall both close to 90% from the best conversational LLMs, like ChatGPT-4. We\ndemonstrate that the exceptional performance is enabled by the information\nretention in a conversational model combined with purposeful redundancy and\nintroducing uncertainty through follow-up prompts. These results suggest that\napproaches similar to ChatExtract, due to their simplicity, transferability,\nand accuracy are likely to become powerful tools for data extraction in the\nnear future. Finally, databases for critical cooling rates of metallic glasses\nand yield strengths of high entropy alloys are developed using ChatExtract.</p>\n", "tags": ["Prompting","Tools"] },
{"key": "poliak2018collecting", "citations": "125", "year": "2018", "title":"Collecting Diverse Natural Language Inference Problems For Sentence Representation Evaluation", "abstract": "<p>We present a large-scale collection of diverse natural language inference\n(NLI) datasets that help provide insight into how well a sentence\nrepresentation captures distinct types of reasoning. The collection results\nfrom recasting 13 existing datasets from 7 semantic phenomena into a common NLI\nstructure, resulting in over half a million labeled context-hypothesis pairs in\ntotal. We refer to our collection as the DNC: Diverse Natural Language\nInference Collection. The DNC is available online at https://www.decomp.net,\nand will grow over time as additional resources are recast and added from novel\nsources.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "poliak2018hypothesis", "citations": "577", "year": "2018", "title":"Hypothesis Only Baselines In Natural Language Inference", "abstract": "<p>We propose a hypothesis only baseline for diagnosing Natural Language\nInference (NLI). Especially when an NLI dataset assumes inference is occurring\nbased purely on the relationship between a context and a hypothesis, it follows\nthat assessing entailment relations while ignoring the provided context is a\ndegenerate solution. Yet, through experiments on ten distinct NLI datasets, we\nfind that this approach, which we refer to as a hypothesis-only model, is able\nto significantly outperform a majority class baseline across a number of NLI\ndatasets. Our analysis suggests that statistical irregularities may allow a\nmodel to perform NLI in some datasets beyond what should be achievable without\naccess to the context.</p>\n", "tags": ["Datasets"] },
{"key": "ponti2020xcopa", "citations": "127", "year": "2020", "title":"XCOPA: A Multilingual Dataset For Causal Commonsense Reasoning", "abstract": "<p>In order to simulate human language capacity, natural language processing\nsystems must be able to reason about the dynamics of everyday situations,\nincluding their possible causes and effects. Moreover, they should be able to\ngeneralise the acquired world knowledge to new languages, modulo cultural\ndifferences. Advances in machine reasoning and cross-lingual transfer depend on\nthe availability of challenging evaluation benchmarks. Motivated by both\ndemands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a\ntypologically diverse multilingual dataset for causal commonsense reasoning in\n11 languages, which includes resource-poor languages like Eastern Apur'imac\nQuechua and Haitian Creole. We evaluate a range of state-of-the-art models on\nthis novel dataset, revealing that the performance of current methods based on\nmultilingual pretraining and zero-shot fine-tuning falls short compared to\ntranslation-based transfer. Finally, we propose strategies to adapt\nmultilingual models to out-of-sample resource-lean languages where only a small\ncorpus or a bilingual dictionary is available, and report substantial\nimprovements over the random baseline. The XCOPA dataset is freely available at\ngithub.com/cambridgeltl/xcopa.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "popel2018training", "citations": "205", "year": "2018", "title":"Training Tips For The Transformer Model", "abstract": "<p>This article describes our experiments in neural machine translation using\nthe recent Tensor2Tensor framework and the Transformer sequence-to-sequence\nmodel (Vaswani et al., 2017). We examine some of the critical parameters that\naffect the final translation quality, memory usage, training stability and\ntraining time, concluding each experiment with a set of recommendations for\nfellow researchers. In addition to confirming the general mantra “more data and\nlarger models”, we address scaling to multiple GPUs and provide practical tips\nfor improved training regarding batch size, learning rate, warmup steps,\nmaximum sentence length and checkpoint averaging. We hope that our observations\nwill allow others to get better results given their particular hardware and\ndata constraints.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "poth2021what", "citations": "62", "year": "2021", "title":"What To Pre-train On? Efficient Intermediate Task Selection", "abstract": "<p>Intermediate task fine-tuning has been shown to culminate in large transfer\ngains across many NLP tasks. With an abundance of candidate datasets as well as\npre-trained language models, it has become infeasible to run the cross-product\nof all combinations to find the best transfer setting. In this work we first\nestablish that similar sequential fine-tuning gains can be achieved in adapter\nsettings, and subsequently consolidate previously proposed methods that\nefficiently identify beneficial tasks for intermediate transfer learning. We\nexperiment with a diverse set of 42 intermediate and 11 target English\nclassification, multiple choice, question answering, and sequence tagging\ntasks. Our results show that efficient embedding based methods that rely solely\non the respective datasets outperform computational expensive few-shot\nfine-tuning approaches. Our best methods achieve an average Regret@3 of less\nthan 1% across all target tasks, demonstrating that we are able to efficiently\nidentify the best datasets for intermediate training.</p>\n", "tags": ["Datasets","EMNLP","Few-Shot","Fine-Tuning","Training Techniques"] },
{"key": "powalski2021going", "citations": "116", "year": "2021", "title":"Going Full-tilt Boogie On Document Understanding With Text-image-layout Transformer", "abstract": "<p>We address the challenging problem of Natural Language Comprehension beyond\nplain-text documents by introducing the TILT neural network architecture which\nsimultaneously learns layout information, visual features, and textual\nsemantics. Contrary to previous approaches, we rely on a decoder capable of\nunifying a variety of problems involving natural language. The layout is\nrepresented as an attention bias and complemented with contextualized visual\ninformation, while the core of our model is a pretrained encoder-decoder\nTransformer. Our novel approach achieves state-of-the-art results in extracting\ninformation from documents and answering questions which demand layout\nunderstanding (DocVQA, CORD, SROIE). At the same time, we simplify the process\nby employing an end-to-end model.</p>\n", "tags": ["Ethics & Fairness","Model Architecture"] },
{"key": "pradel2019typewriter", "citations": "83", "year": "2020", "title":"Typewriter: Neural Type Prediction With Search-based Validation", "abstract": "<p>Maintaining large code bases written in dynamically typed languages, such as\nJavaScript or Python, can be challenging due to the absence of type\nannotations: simple data compatibility errors proliferate, IDE support is\nlimited, and APIs are hard to comprehend. Recent work attempts to address those\nissues through either static type inference or probabilistic type prediction.\nUnfortunately, static type inference for dynamic languages is inherently\nlimited, while probabilistic approaches suffer from imprecision. This paper\npresents TypeWriter, the first combination of probabilistic type prediction\nwith search-based refinement of predicted types. TypeWriter’s predictor learns\nto infer the return and argument types for functions from partially annotated\ncode bases by combining the natural language properties of code with\nprogramming language-level information. To validate predicted types, TypeWriter\ninvokes a gradual type checker with different combinations of the predicted\ntypes, while navigating the space of possible type combinations in a\nfeedback-directed manner. We implement the TypeWriter approach for Python and\nevaluate it on two code corpora: a multi-million line code base at Facebook and\na collection of 1,137 popular open-source projects. We show that TypeWriter’s\ntype predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5)\npredictions for return types, and 0.57 (0.80) for argument types, which clearly\noutperforms prior type prediction models. By combining predictions with\nsearch-based validation, TypeWriter can fully annotate between 14% to 44% of\nthe files in a randomly selected corpus, while ensuring type correctness. A\ncomparison with a static type inference tool shows that TypeWriter adds many\nmore non-trivial types. TypeWriter currently suggests types to developers at\nFacebook and several thousands of types have already been accepted with minimal\nchanges.</p>\n", "tags": ["Datasets","Llm For Code"] },
{"key": "prakash2016neural", "citations": "229", "year": "2016", "title":"Neural Paraphrase Generation With Stacked Residual LSTM Networks", "abstract": "<p>In this paper, we propose a novel neural approach for paraphrase generation.\nConventional para- phrase generation methods either leverage hand-written rules\nand thesauri-based alignments, or use statistical machine learning principles.\nTo the best of our knowledge, this work is the first to explore deep learning\nmodels for paraphrase generation. Our primary contribution is a stacked\nresidual LSTM network, where we add residual connections between LSTM layers.\nThis allows for efficient training of deep LSTMs. We evaluate our model and\nother state-of-the-art deep learning models on three different datasets: PPDB,\nWikiAnswers and MSCOCO. Evaluation results demonstrate that our model\noutperforms sequence to sequence, attention-based and bi- directional LSTM\nmodels on BLEU, METEOR, TER and an embedding-based sentence similarity metric.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "prates2018assessing", "citations": "358", "year": "2019", "title":"Assessing Gender Bias In Machine Translation -- A Case Study With Google Translate", "abstract": "<p>Recently there has been a growing concern about machine bias, where trained\nstatistical models grow to reflect controversial societal asymmetries, such as\ngender or racial bias. A significant number of AI tools have recently been\nsuggested to be harmfully biased towards some minority, with reports of racist\ncriminal behavior predictors, Iphone X failing to differentiate between two\nAsian people and Google photos’ mistakenly classifying black people as\ngorillas. Although a systematic study of such biases can be difficult, we\nbelieve that automated translation tools can be exploited through gender\nneutral languages to yield a window into the phenomenon of gender bias in AI.\n  In this paper, we start with a comprehensive list of job positions from the\nU.S. Bureau of Labor Statistics (BLS) and used it to build sentences in\nconstructions like “He/She is an Engineer” in 12 different gender neutral\nlanguages such as Hungarian, Chinese, Yoruba, and several others. We translate\nthese sentences into English using the Google Translate API, and collect\nstatistics about the frequency of female, male and gender-neutral pronouns in\nthe translated output. We show that GT exhibits a strong tendency towards male\ndefaults, in particular for fields linked to unbalanced gender distribution\nsuch as STEM jobs. We ran these statistics against BLS’ data for the frequency\nof female participation in each job position, showing that GT fails to\nreproduce a real-world distribution of female workers. We provide experimental\nevidence that even if one does not expect in principle a 50:50 pronominal\ngender distribution, GT yields male defaults much more frequently than what\nwould be expected from demographic data alone.\n  We are hopeful that this work will ignite a debate about the need to augment\ncurrent statistical translation tools with debiasing techniques which can\nalready be found in the scientific literature.</p>\n", "tags": ["Applications","Ethics & Fairness","Tools"] },
{"key": "prather2023robots", "citations": "170", "year": "2023", "title":"The Robots Are Here: Navigating The Generative AI Revolution In Computing Education", "abstract": "<p>Recent advancements in artificial intelligence (AI) are fundamentally\nreshaping computing, with large language models (LLMs) now effectively being\nable to generate and interpret source code and natural language instructions.\nThese emergent capabilities have sparked urgent questions in the computing\neducation community around how educators should adapt their pedagogy to address\nthe challenges and to leverage the opportunities presented by this new\ntechnology. In this working group report, we undertake a comprehensive\nexploration of LLMs in the context of computing education and make five\nsignificant contributions. First, we provide a detailed review of the\nliterature on LLMs in computing education and synthesise findings from 71\nprimary articles. Second, we report the findings of a survey of computing\nstudents and instructors from across 20 countries, capturing prevailing\nattitudes towards LLMs and their use in computing education contexts. Third, to\nunderstand how pedagogy is already changing, we offer insights collected from\nin-depth interviews with 22 computing educators from five continents who have\nalready adapted their curricula and assessments. Fourth, we use the ACM Code of\nEthics to frame a discussion of ethical issues raised by the use of large\nlanguage models in computing education, and we provide concrete advice for\npolicy makers, educators, and students. Finally, we benchmark the performance\nof LLMs on various computing education datasets, and highlight the extent to\nwhich the capabilities of current models are rapidly improving. Our aim is that\nthis report will serve as a focal point for both researchers and practitioners\nwho are exploring, adapting, using, and evaluating LLMs and LLM-based tools in\ncomputing classrooms.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Has Code","Survey Paper","Tools"] },
{"key": "preece2018stakeholders", "citations": "71", "year": "2018", "title":"Stakeholders In Explainable AI", "abstract": "<p>There is general consensus that it is important for artificial intelligence\n(AI) and machine learning systems to be explainable and/or interpretable.\nHowever, there is no general consensus over what is meant by ‘explainable’ and\n‘interpretable’. In this paper, we argue that this lack of consensus is due to\nthere being several distinct stakeholder communities. We note that, while the\nconcerns of the individual communities are broadly compatible, they are not\nidentical, which gives rise to different intents and requirements for\nexplainability/interpretability. We use the software engineering distinction\nbetween validation and verification, and the epistemological distinctions\nbetween knowns/unknowns, to tease apart the concerns of the stakeholder\ncommunities and highlight the areas where their foci overlap or diverge. It is\nnot the purpose of the authors of this paper to ‘take sides’ - we count\nourselves as members, to varying degrees, of multiple communities - but rather\nto help disambiguate what stakeholders mean when they ask ‘Why?’ of an AI.</p>\n", "tags": [] },
{"key": "press2016using", "citations": "635", "year": "2017", "title":"Using The Output Embedding To Improve Language Models", "abstract": "<p>We study the topmost weight matrix of neural network language models. We show\nthat this matrix constitutes a valid word embedding. When training language\nmodels, we recommend tying the input embedding and this output embedding. We\nanalyze the resulting update rules and show that the tied embedding evolves in\na more similar way to the output embedding than to the input embedding in the\nuntied model. We also offer a new method of regularizing the output embedding.\nOur methods lead to a significant reduction in perplexity, as we are able to\nshow on a variety of neural network language models. Finally, we show that\nweight tying can reduce the size of neural translation models to less than half\nof their original size without harming their performance.</p>\n", "tags": ["NAACL","Training Techniques"] },
{"key": "press2017language", "citations": "90", "year": "2017", "title":"Language Generation With Recurrent Generative Adversarial Networks Without Pre-training", "abstract": "<p>Generative Adversarial Networks (GANs) have shown great promise recently in\nimage generation. Training GANs for language generation has proven to be more\ndifficult, because of the non-differentiable nature of generating text with\nrecurrent neural networks. Consequently, past work has either resorted to\npre-training with maximum-likelihood or used convolutional networks for\ngeneration. In this work, we show that recurrent neural networks can be trained\nto generate text with GANs from scratch using curriculum learning, by slowly\nteaching the model to generate sequences of increasing and variable length. We\nempirically show that our approach vastly improves the quality of generated\nsequences compared to a convolutional baseline.</p>\n", "tags": ["Training Techniques"] },
{"key": "press2019improving", "citations": "66", "year": "2020", "title":"Improving Transformer Models By Reordering Their Sublayers", "abstract": "<p>Multilayer transformer networks consist of interleaved self-attention and\nfeedforward sublayers. Could ordering the sublayers in a different pattern lead\nto better performance? We generate randomly ordered transformers and train them\nwith the language modeling objective. We observe that some of these models are\nable to achieve better performance than the interleaved baseline, and that\nthose successful variants tend to have more self-attention at the bottom and\nmore feedforward sublayers at the top. We propose a new transformer pattern\nthat adheres to this property, the sandwich transformer, and show that it\nimproves perplexity on multiple word-level and character-level language\nmodeling benchmarks, at no cost in parameters, memory, or training time.\nHowever, the sandwich reordering pattern does not guarantee performance gains\nacross every task, as we demonstrate on machine translation models. Instead, we\nsuggest that further exploration of task-specific sublayer reorderings is\nneeded in order to unlock additional gains.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "pruksachatkun2020jiant", "citations": "75", "year": "2020", "title":"Jiant: A Software Toolkit For Research On General-purpose Text Understanding Models", "abstract": "<p>We introduce jiant, an open source toolkit for conducting multitask and\ntransfer learning experiments on English NLU tasks. jiant enables modular and\nconfiguration-driven experimentation with state-of-the-art models and\nimplements a broad set of tasks for probing, transfer learning, and multitask\ntraining experiments. jiant implements over 50 NLU tasks, including all GLUE\nand SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published\nperformance on a variety of tasks and models, including BERT and RoBERTa. jiant\nis available at https://jiant.info.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "pruthi2019learning", "citations": "155", "year": "2020", "title":"Learning To Deceive With Attention-based Explanations", "abstract": "<p>Attention mechanisms are ubiquitous components in neural architectures\napplied to natural language processing. In addition to yielding gains in\npredictive accuracy, attention weights are often claimed to confer\ninterpretability, purportedly useful both for providing insights to\npractitioners and for explaining why a model makes its decisions to\nstakeholders. We call the latter use of attention mechanisms into question by\ndemonstrating a simple method for training models to produce deceptive\nattention masks. Our method diminishes the total weight assigned to designated\nimpermissible tokens, even when the models can be shown to nevertheless rely on\nthese features to drive predictions. Across multiple models and tasks, our\napproach manipulates attention weights while paying surprisingly little cost in\naccuracy. Through a human study, we show that our manipulated attention-based\nexplanations deceive people into thinking that predictions from a model biased\nagainst gender minorities do not rely on the gender. Consequently, our results\ncast doubt on attention’s reliability as a tool for auditing algorithms in the\ncontext of fairness and accountability.</p>\n", "tags": ["Ethics & Fairness","Model Architecture","Training Techniques"] },
{"key": "pryzant2023automatic", "citations": "65", "year": "2023", "title":"Automatic Prompt Optimization With \"gradient Descent\" And Beam Search", "abstract": "<p>Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language “gradients” that criticize the\ncurrent prompt. The gradients are then “propagated” into the prompt by editing\nthe prompt in the opposite semantic direction of the gradient. These gradient\ndescent steps are guided by a beam search and bandit selection procedure which\nsignificantly improves algorithmic efficiency. Preliminary results across three\nbenchmark NLP tasks and the novel problem of LLM jailbreak detection suggest\nthat Automatic Prompt Optimization can outperform prior prompt editing\ntechniques and improve an initial prompt’s performance by up to 31%, by using\ndata to rewrite vague task descriptions into more precise annotation\ninstructions.</p>\n", "tags": ["Datasets","EMNLP","Efficiency","Evaluation","Prompting","Tools","Training Techniques"] },
{"key": "pu2016sk_p", "citations": "121", "year": "2016", "title":"Sk_p: A Neural Program Corrector For Moocs", "abstract": "<p>We present a novel technique for automatic program correction in MOOCs,\ncapable of fixing both syntactic and semantic errors without manual, problem\nspecific correction strategies. Given an incorrect student program, it\ngenerates candidate programs from a distribution of likely corrections, and\nchecks each candidate for correctness against a test suite.\n  The key observation is that in MOOCs many programs share similar code\nfragments, and the seq2seq neural network model, used in the natural-language\nprocessing task of machine translation, can be modified and trained to recover\nthese fragments.\n  Experiment shows our scheme can correct 29% of all incorrect submissions and\nout-performs state of the art approach which requires manual, problem specific\ncorrection strategies.</p>\n", "tags": ["Applications"] },
{"key": "puduppully2018data", "citations": "273", "year": "2019", "title":"Data-to-text Generation With Content Selection And Planning", "abstract": "<p>Recent advances in data-to-text generation have led to the use of large-scale\ndatasets and neural network models which are trained end-to-end, without\nexplicitly modeling what to say and in what order. In this work, we present a\nneural network architecture which incorporates content selection and planning\nwithout sacrificing end-to-end training. We decompose the generation task into\ntwo stages. Given a corpus of data records (paired with descriptive documents),\nwe first generate a content plan highlighting which information should be\nmentioned and in which order and then generate the document while taking the\ncontent plan into account. Automatic and human-based evaluation experiments\nshow that our model outperforms strong baselines improving the state-of-the-art\non the recently released RotoWire dataset.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "puduppully2019data", "citations": "121", "year": "2019", "title":"Data-to-text Generation With Entity Modeling", "abstract": "<p>Recent approaches to data-to-text generation have shown great promise thanks\nto the use of large-scale datasets and the application of neural network\narchitectures which are trained end-to-end. These models rely on representation\nlearning to select content appropriately, structure it coherently, and\nverbalize it grammatically, treating entities as nothing more than vocabulary\ntokens. In this work we propose an entity-centric neural architecture for\ndata-to-text generation. Our model creates entity-specific representations\nwhich are dynamically updated. Text is generated conditioned on the data input\nand entity memory representations using hierarchical attention at each time\nstep. We present experiments on the RotoWire benchmark and a (five times\nlarger) new dataset on the baseball domain which we create. Our results show\nthat the proposed model outperforms competitive baselines in automatic and\nhuman evaluation.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "puri2020training", "citations": "113", "year": "2020", "title":"Training Question Answering Models From Synthetic Data", "abstract": "<p>Question and answer generation is a data augmentation method that aims to\nimprove question answering (QA) models given the limited amount of human\nlabeled data. However, a considerable gap remains between synthetic and\nhuman-generated question-answer pairs. This work aims to narrow this gap by\ntaking advantage of large language models and explores several factors such as\nmodel size, quality of pretrained models, scale of data synthesized, and\nalgorithmic choices. On the SQuAD1.1 question answering task, we achieve higher\naccuracy using solely synthetic questions and answers than when using the\nSQuAD1.1 training set questions alone. Removing access to real Wikipedia data,\nwe synthesize questions and answers from a synthetic corpus generated by an 8.3\nbillion parameter GPT-2 model. With no access to human supervision and only\naccess to other models, we are able to train state of the art question\nanswering networks on entirely model-generated data that achieve 88.4 Exact\nMatch (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our\nmethodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to\nprior work using synthetic data.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "pursnani2023performance", "citations": "69", "year": "2023", "title":"Performance Of Chatgpt On The US Fundamentals Of Engineering Exam: Comprehensive Assessment Of Proficiency And Potential Implications For Professional Environmental Engineering Practice", "abstract": "<p>In recent years, advancements in artificial intelligence (AI) have led to the\ndevelopment of large language models like GPT-4, demonstrating potential\napplications in various fields, including education. This study investigates\nthe feasibility and effectiveness of using ChatGPT, a GPT-4 based model, in\nachieving satisfactory performance on the Fundamentals of Engineering (FE)\nEnvironmental Exam. This study further shows a significant improvement in the\nmodel’s accuracy when answering FE exam questions through noninvasive prompt\nmodifications, substantiating the utility of prompt modification as a viable\napproach to enhance AI performance in educational contexts. Furthermore, the\nfindings reflect remarkable improvements in mathematical capabilities across\nsuccessive iterations of ChatGPT models, showcasing their potential in solving\ncomplex engineering problems. Our paper also explores future research\ndirections, emphasizing the importance of addressing AI challenges in\neducation, enhancing accessibility and inclusion for diverse student\npopulations, and developing AI-resistant exam questions to maintain examination\nintegrity. By evaluating the performance of ChatGPT in the context of the FE\nEnvironmental Exam, this study contributes valuable insights into the potential\napplications and limitations of large language models in educational settings.\nAs AI continues to evolve, these findings offer a foundation for further\nresearch into the responsible and effective integration of AI models across\nvarious disciplines, ultimately optimizing the learning experience and\nimproving student outcomes.</p>\n", "tags": ["Applications","Model Architecture","Prompting"] },
{"key": "páez2020pragmatic", "citations": "85", "year": "2019", "title":"The Pragmatic Turn In Explainable Artificial Intelligence (XAI)", "abstract": "<p>In this paper I argue that the search for explainable models and\ninterpretable decisions in AI must be reformulated in terms of the broader\nproject of offering a pragmatic and naturalistic account of understanding in\nAI. Intuitively, the purpose of providing an explanation of a model or a\ndecision is to make it understandable to its stakeholders. But without a\nprevious grasp of what it means to say that an agent understands a model or a\ndecision, the explanatory strategies will lack a well-defined goal. Aside from\nproviding a clearer objective for XAI, focusing on understanding also allows us\nto relax the factivity condition on explanation, which is impossible to fulfill\nin many machine learning models, and to focus instead on the pragmatic\nconditions that determine the best fit between a model and the methods and\ndevices deployed to understand it. After an examination of the different types\nof understanding discussed in the philosophical and psychological literature, I\nconclude that interpretative or approximation models not only provide the best\nway to achieve the objectual understanding of a machine learning model, but are\nalso a necessary condition to achieve post-hoc interpretability. This\nconclusion is partly based on the shortcomings of the purely functionalist\napproach to post-hoc interpretability that seems to be predominant in most\nrecent literature.</p>\n", "tags": ["Agentic"] },
{"key": "qi2018when", "citations": "349", "year": "2018", "title":"When And Why Are Pre-trained Word Embeddings Useful For Neural Machine Translation?", "abstract": "<p>The performance of Neural Machine Translation (NMT) systems often suffers in\nlow-resource scenarios where sufficiently large-scale parallel corpora cannot\nbe obtained. Pre-trained word embeddings have proven to be invaluable for\nimproving performance in natural language analysis tasks, which often suffer\nfrom paucity of data. However, their utility for NMT has not been extensively\nexplored. In this work, we perform five sets of experiments that analyze when\nwe can expect pre-trained word embeddings to help in NMT tasks. We show that\nsuch embeddings can be surprisingly effective in some cases – providing gains\nof up to 20 BLEU points in the most favorable setting.</p>\n", "tags": ["NAACL"] },
{"key": "qi2019reverie", "citations": "220", "year": "2020", "title":"REVERIE: Remote Embodied Visual Referring Expression In Real Indoor Environments", "abstract": "<p>One of the long-term challenges of robotics is to enable robots to interact\nwith humans in the visual world via natural language, as humans are visual\nanimals that communicate through language. Overcoming this challenge requires\nthe ability to perform a wide variety of complex tasks in response to\nmultifarious instructions from humans. In the hope that it might drive progress\ntowards more flexible and powerful human interactions with robots, we propose a\ndataset of varied and complex robot tasks, described in natural language, in\nterms of objects visible in a large set of real images. Given an instruction,\nsuccess requires navigating through a previously-unseen environment to identify\nan object. This represents a practical challenge, but one that closely reflects\none of the core visual problems in robotics. Several state-of-the-art\nvision-and-language navigation, and referring-expression models are tested to\nverify the difficulty of this new task, but none of them show promising results\nbecause there are many fundamental differences between our task and previous\nones. A novel Interactive Navigator-Pointer model is also proposed that\nprovides a strong baseline on the task. The proposed model especially achieves\nthe best performance on the unseen test split, but still leaves substantial\nroom for improvement compared to the human performance.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "qi2019two", "citations": "144", "year": "2020", "title":"Two Causal Principles For Improving Visual Dialog", "abstract": "<p>This paper unravels the design tricks adopted by us, the champion team\nMReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for\nimproving Visual Dialog (VisDial). By “improving”, we mean that they can\npromote almost every existing VisDial model to the state-of-the-art performance\non the leader-board. Such a major improvement is only due to our careful\ninspection on the causality behind the model and data, finding that the\ncommunity has overlooked two causalities in VisDial. Intuitively, Principle 1\nsuggests: we should remove the direct input of the dialog history to the answer\nmodel, otherwise a harmful shortcut bias will be introduced; Principle 2 says:\nthere is an unobserved confounder for history, question, and answer, leading to\nspurious correlations from training data. In particular, to remove the\nconfounder suggested in Principle 2, we propose several causal intervention\nalgorithms, which make the training fundamentally different from the\ntraditional likelihood estimation. Note that the two principles are\nmodel-agnostic, so they are applicable in any VisDial model. The code is\navailable at https://github.com/simpleshinobu/visdial-principles.</p>\n", "tags": ["CVPR","Has Code","Training Techniques"] },
{"key": "qi2020imagebert", "citations": "157", "year": "2020", "title":"Imagebert: Cross-modal Pre-training With Large-scale Weak-supervised Image-text Data", "abstract": "<p>In this paper, we introduce a new vision-language pre-trained model –\nImageBERT – for image-text joint embedding. Our model is a Transformer-based\nmodel, which takes different modalities as input and models the relationship\nbetween them. The model is pre-trained on four tasks simultaneously: Masked\nLanguage Modeling (MLM), Masked Object Classification (MOC), Masked Region\nFeature Regression (MRFR), and Image Text Matching (ITM). To further enhance\nthe pre-training quality, we have collected a Large-scale weAk-supervised\nImage-Text (LAIT) dataset from Web. We first pre-train the model on this\ndataset, then conduct a second stage pre-training on Conceptual Captions and\nSBU Captions. Our experiments show that multi-stage pre-training strategy\noutperforms single-stage pre-training. We also fine-tune and evaluate our\npre-trained ImageBERT model on image retrieval and text retrieval tasks, and\nachieve new state-of-the-art results on both MSCOCO and Flickr30k datasets.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "qi2020object", "citations": "77", "year": "2020", "title":"Object-and-action Aware Model For Visual Language Navigation", "abstract": "<p>Vision-and-Language Navigation (VLN) is unique in that it requires turning\nrelatively general natural-language instructions into robot agent actions, on\nthe basis of the visible environment. This requires to extract value from two\nvery different types of natural-language information. The first is object\ndescription (e.g., ‘table’, ‘door’), each presenting as a tip for the agent to\ndetermine the next action by finding the item visible in the environment, and\nthe second is action specification (e.g., ‘go straight’, ‘turn left’) which\nallows the robot to directly predict the next movements without relying on\nvisual perceptions. However, most existing methods pay few attention to\ndistinguish these information from each other during instruction encoding and\nmix together the matching between textual object/action encoding and visual\nperception/orientation features of candidate viewpoints. In this paper, we\npropose an Object-and-Action Aware Model (OAAM) that processes these two\ndifferent forms of natural language based instruction separately. This enables\neach process to match object-centered/action-centered instruction to their own\ncounterpart visual perception/action orientation flexibly. However, one\nside-issue caused by above solution is that an object mentioned in instructions\nmay be observed in the direction of two or more candidate viewpoints, thus the\nOAAM may not predict the viewpoint on the shortest path as the next action. To\nhandle this problem, we design a simple but effective path loss to penalize\ntrajectories deviating from the ground truth path. Experimental results\ndemonstrate the effectiveness of the proposed model and path loss, and the\nsuperiority of their combination with a 50% SPL score on the R2R dataset and a\n40% CLS score on the R4R dataset in unseen environments, outperforming the\nprevious state-of-the-art.</p>\n", "tags": ["Agentic","Datasets","Model Architecture"] },
{"key": "qi2020stanza", "citations": "1275", "year": "2020", "title":"Stanza: A Python Natural Language Processing Toolkit For Many Human Languages", "abstract": "<p>We introduce Stanza, an open-source Python natural language processing\ntoolkit supporting 66 human languages. Compared to existing widely used\ntoolkits, Stanza features a language-agnostic fully neural pipeline for text\nanalysis, including tokenization, multi-word token expansion, lemmatization,\npart-of-speech and morphological feature tagging, dependency parsing, and named\nentity recognition. We have trained Stanza on a total of 112 datasets,\nincluding the Universal Dependencies treebanks and other multilingual corpora,\nand show that the same neural architecture generalizes well and achieves\ncompetitive performance on all languages tested. Additionally, Stanza includes\na native Python interface to the widely used Java Stanford CoreNLP software,\nwhich further extends its functionality to cover other tasks such as\ncoreference resolution and relation extraction. Source code, documentation, and\npretrained models for 66 languages are available at\nhttps://stanfordnlp.github.io/stanza.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Tools"] },
{"key": "qi2021mind", "citations": "74", "year": "2021", "title":"Mind The Style Of Text! Adversarial And Backdoor Attacks Based On Text Style Transfer", "abstract": "<p>Adversarial attacks and backdoor attacks are two common security threats that\nhang over deep learning. Both of them harness task-irrelevant features of data\nin their implementation. Text style is a feature that is naturally irrelevant\nto most NLP tasks, and thus suitable for adversarial and backdoor attacks. In\nthis paper, we make the first attempt to conduct adversarial and backdoor\nattacks based on text style transfer, which is aimed at altering the style of a\nsentence while preserving its meaning. We design an adversarial attack method\nand a backdoor attack method, and conduct extensive experiments to evaluate\nthem. Experimental results show that popular NLP models are vulnerable to both\nadversarial and backdoor attacks based on text style transfer – the attack\nsuccess rates can exceed 90% without much effort. It reflects the limited\nability of NLP models to handle the feature of text style that has not been\nwidely realized. In addition, the style transfer-based adversarial and backdoor\nattack methods show superiority to baselines in many aspects. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/StyleAttack.</p>\n", "tags": ["EMNLP","Has Code","Security"] },
{"key": "qian2018leveraging", "citations": "93", "year": "2018", "title":"Leveraging Intra-user And Inter-user Representation Learning For Automated Hate Speech Detection", "abstract": "<p>Hate speech detection is a critical, yet challenging problem in Natural\nLanguage Processing (NLP). Despite the existence of numerous studies dedicated\nto the development of NLP hate speech detection approaches, the accuracy is\nstill poor. The central problem is that social media posts are short and noisy,\nand most existing hate speech detection solutions take each post as an isolated\ninput instance, which is likely to yield high false positive and negative\nrates. In this paper, we radically improve automated hate speech detection by\npresenting a novel model that leverages intra-user and inter-user\nrepresentation learning for robust hate speech detection on Twitter. In\naddition to the target Tweet, we collect and analyze the user’s historical\nposts to model intra-user Tweet representations. To suppress the noise in a\nsingle Tweet, we also model the similar Tweets posted by all other users with\nreinforced inter-user representation learning techniques. Experimentally, we\nshow that leveraging these two representations can significantly improve the\nf-score of a strong bidirectional LSTM baseline model by 10.1%.</p>\n", "tags": ["Model Architecture","NAACL"] },
{"key": "qian2019domain", "citations": "138", "year": "2019", "title":"Domain Adaptive Dialog Generation Via Meta Learning", "abstract": "<p>Domain adaptation is an essential task in dialog system building because\nthere are so many new dialog tasks created for different needs every day.\nCollecting and annotating training data for these new tasks is costly since it\ninvolves real user interactions. We propose a domain adaptive dialog generation\nmethod based on meta-learning (DAML). DAML is an end-to-end trainable dialog\nsystem model that learns from multiple rich-resource tasks and then adapts to\nnew domains with minimal training samples. We train a dialog system model using\nmultiple rich-resource single-domain dialog data by applying the model-agnostic\nmeta-learning algorithm to dialog domain. The model is capable of learning a\ncompetitive dialog system on a new domain with only a few training examples in\nan efficient manner. The two-step gradient updates in DAML enable the model to\nlearn general features across multiple tasks. We evaluate our method on a\nsimulated dialog dataset and achieve state-of-the-art performance, which is\ngeneralizable to new tasks.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "qian2020glancing", "citations": "90", "year": "2021", "title":"Glancing Transformer For Non-autoregressive Neural Machine Translation", "abstract": "<p>Recent work on non-autoregressive neural machine translation (NAT) aims at\nimproving the efficiency by parallel decoding without sacrificing the quality.\nHowever, existing NAT methods are either inferior to Transformer or require\nmultiple decoding passes, leading to reduced speedup. We propose the Glancing\nLanguage Model (GLM), a method to learn word interdependency for single-pass\nparallel generation models. With GLM, we develop Glancing Transformer (GLAT)\nfor machine translation. With only single-pass parallel decoding, GLAT is able\nto generate high-quality translation with 8-15 times speedup. Experiments on\nmultiple WMT language directions show that GLAT outperforms all previous single\npass non-autoregressive methods, and is nearly comparable to Transformer,\nreducing the gap to 0.25-0.9 BLEU points.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "qian2021speech", "citations": "241", "year": "2019", "title":"Speech-language Pre-training For End-to-end Spoken Language Understanding", "abstract": "<p>End-to-end (E2E) spoken language understanding (SLU) can infer semantics\ndirectly from speech signal without cascading an automatic speech recognizer\n(ASR) with a natural language understanding (NLU) module. However, paired\nutterance recordings and corresponding semantics may not always be available or\nsufficient to train an E2E SLU model in a real production environment. In this\npaper, we propose to unify a well-optimized E2E ASR encoder (speech) and a\npre-trained language model encoder (language) into a transformer decoder. The\nunified speech-language pre-trained model (SLP) is continually enhanced on\nlimited labeled data from a target domain by using a conditional masked\nlanguage model (MLM) objective, and thus can effectively generate a sequence of\nintent, slot type, and slot value for given input speech in the inference. The\nexperimental results on two public corpora show that our approach to E2E SLU is\nsuperior to the conventional cascaded method. It also outperforms the present\nstate-of-the-art approaches to E2E SLU with much less paired data.</p>\n", "tags": ["INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "qiang2019lexical", "citations": "69", "year": "2020", "title":"Lexical Simplification With Pretrained Encoders", "abstract": "<p>Lexical simplification (LS) aims to replace complex words in a given sentence\nwith their simpler alternatives of equivalent meaning. Recently unsupervised\nlexical simplification approaches only rely on the complex word itself\nregardless of the given sentence to generate candidate substitutions, which\nwill inevitably produce a large number of spurious candidates. We present a\nsimple LS approach that makes use of the Bidirectional Encoder Representations\nfrom Transformers (BERT) which can consider both the given sentence and the\ncomplex word during generating candidate substitutions for the complex word.\nSpecifically, we mask the complex word of the original sentence for feeding\ninto the BERT to predict the masked token. The predicted results will be used\nas candidate substitutions. Despite being entirely unsupervised, experimental\nresults show that our approach obtains obvious improvement compared with these\nbaselines leveraging linguistic databases and parallel corpus, outperforming\nthe state-of-the-art by more than 12 Accuracy points on three well-known\nbenchmarks.</p>\n", "tags": ["AAAI","Datasets","Model Architecture"] },
{"key": "qiao2017exploring", "citations": "84", "year": "2018", "title":"Exploring Human-like Attention Supervision In Visual Question Answering", "abstract": "<p>Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA.</p>\n", "tags": ["AAAI","Datasets","Model Architecture","Training Techniques"] },
{"key": "qiao2019mirrorgan", "citations": "554", "year": "2019", "title":"Mirrorgan: Learning Text-to-image Generation By Redescription", "abstract": "<p>Generating an image from a given text description has two goals: visual\nrealism and semantic consistency. Although significant progress has been made\nin generating high-quality and visually realistic images using generative\nadversarial networks, guaranteeing semantic consistency between the text\ndescription and visual content remains very challenging. In this paper, we\naddress this problem by proposing a novel global-local attentive and\nsemantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN\nexploits the idea of learning text-to-image generation by redescription and\nconsists of three modules: a semantic text embedding module (STEM), a\nglobal-local collaborative attentive module for cascaded image generation\n(GLAM), and a semantic text regeneration and alignment module (STREAM). STEM\ngenerates word- and sentence-level embeddings. GLAM has a cascaded architecture\nfor generating target images from coarse to fine scales, leveraging both local\nword attention and global sentence attention to progressively enhance the\ndiversity and semantic consistency of the generated images. STREAM seeks to\nregenerate the text description from the generated image, which semantically\naligns with the given text description. Thorough experiments on two public\nbenchmark datasets demonstrate the superiority of MirrorGAN over other\nrepresentative state-of-the-art methods.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "qiao2019understanding", "citations": "153", "year": "2019", "title":"Understanding The Behaviors Of BERT In Ranking", "abstract": "<p>This paper studies the performances and behaviors of BERT in ranking tasks.\nWe explore several different ways to leverage the pre-trained BERT and\nfine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web\nTrack ad hoc document ranking. Experimental results on MS MARCO demonstrate the\nstrong effectiveness of BERT in question-answering focused passage ranking\ntasks, as well as the fact that BERT is a strong interaction-based seq2seq\nmatching model. Experimental results on TREC show the gaps between the BERT\npre-trained on surrounding contexts and the needs of ad hoc document ranking.\nAnalyses illustrate how BERT allocates its attentions between query-document\ntokens in its Transformer layers, how it prefers semantic matches between\nparaphrase tokens, and how that differs with the soft match patterns learned by\na click-trained neural ranker.</p>\n", "tags": ["Model Architecture"] },
{"key": "qiao2022reasoning", "citations": "74", "year": "2023", "title":"Reasoning With Language Model Prompting: A Survey", "abstract": "<p>Reasoning, as an essential ability for complex problem-solving, can provide\nback-end support for various real-world applications, such as medical\ndiagnosis, negotiation, etc. This paper provides a comprehensive survey of\ncutting-edge research on reasoning with language model prompting. We introduce\nresearch works with comparisons and summaries and provide systematic resources\nto help beginners. We also discuss the potential reasons for emerging such\nreasoning abilities and highlight future research directions. Resources are\navailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updated\nperiodically).</p>\n", "tags": ["Applications","Prompting","Survey Paper"] },
{"key": "qin2019conversing", "citations": "107", "year": "2019", "title":"Conversing By Reading: Contentful Neural Conversation With On-demand Machine Reading", "abstract": "<p>Although neural conversation models are effective in learning how to produce\nfluent responses, their primary challenge lies in knowing what to say to make\nthe conversation contentful and non-vacuous. We present a new end-to-end\napproach to contentful neural conversation that jointly models response\ngeneration and on-demand machine reading. The key idea is to provide the\nconversation model with relevant long-form text on the fly as a source of\nexternal knowledge. The model performs QA-style reading comprehension on this\ntext in response to each conversational turn, thereby allowing for more focused\nintegration of external knowledge than has been possible in prior approaches.\nTo support further research on knowledge-grounded conversation, we introduce a\nnew large-scale conversation dataset grounded in external web pages (2.8M\nturns, 7.4M sentences of grounding). Both human evaluation and automated\nmetrics show that our approach results in more contentful responses compared to\na variety of previous methods, improving both the informativeness and diversity\nof generated output.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "qin2019counterfactual", "citations": "106", "year": "2019", "title":"Counterfactual Story Reasoning And Generation", "abstract": "<p>Counterfactual reasoning requires predicting how alternative events, contrary\nto what actually happened, might have resulted in different outcomes. Despite\nbeing considered a necessary component of AI-complete systems, few resources\nhave been developed for evaluating counterfactual reasoning in narratives.\n  In this paper, we propose Counterfactual Story Rewriting: given an original\nstory and an intervening counterfactual event, the task is to minimally revise\nthe story to make it compatible with the given counterfactual event. Solving\nthis task will require deep understanding of causal narrative chains and\ncounterfactual invariance, and integration of such story reasoning capabilities\ninto conditional language generation models.\n  We present TimeTravel, a new dataset of 29,849 counterfactual rewritings,\neach with the original story, a counterfactual event, and human-generated\nrevision of the original story compatible with the counterfactual event.\nAdditionally, we include 80,115 counterfactual “branches” without a rewritten\nstoryline to support future work on semi- or un-supervised approaches to\ncounterfactual story rewriting.\n  Finally, we evaluate the counterfactual rewriting capacities of several\ncompetitive baselines based on pretrained language models, and assess whether\ncommon overlap and model-based automatic metrics for text generation correlate\nwell with human scores for counterfactual rewriting.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "qin2019stack", "citations": "291", "year": "2019", "title":"A Stack-propagation Framework With Token-level Intent Detection For Spoken Language Understanding", "abstract": "<p>Intent detection and slot filling are two main tasks for building a spoken\nlanguage understanding (SLU) system. The two tasks are closely tied and the\nslots often highly depend on the intent. In this paper, we propose a novel\nframework for SLU to better incorporate the intent information, which further\nguides the slot filling. In our framework, we adopt a joint model with\nStack-Propagation which can directly use the intent information as input for\nslot filling, thus to capture the intent semantic knowledge. In addition, to\nfurther alleviate the error propagation, we perform the token-level intent\ndetection for the Stack-Propagation framework. Experiments on two publicly\ndatasets show that our model achieves the state-of-the-art performance and\noutperforms other previous methods by a large margin. Finally, we use the\nBidirectional Encoder Representation from Transformer (BERT) model in our\nframework, which further boost our performance in SLU task.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Tools"] },
{"key": "qin2020agif", "citations": "101", "year": "2020", "title":"AGIF: An Adaptive Graph-interactive Framework For Joint Multiple Intent Detection And Slot Filling", "abstract": "<p>In real-world scenarios, users usually have multiple intents in the same\nutterance. Unfortunately, most spoken language understanding (SLU) models\neither mainly focused on the single intent scenario, or simply incorporated an\noverall intent context vector for all tokens, ignoring the fine-grained\nmultiple intents information integration for token-level slot prediction. In\nthis paper, we propose an Adaptive Graph-Interactive Framework (AGIF) for joint\nmultiple intent detection and slot filling, where we introduce an intent-slot\ngraph interaction layer to model the strong correlation between the slot and\nintents. Such an interaction layer is applied to each token adaptively, which\nhas the advantage to automatically extract the relevant intents information,\nmaking a fine-grained intent information integration for the token-level slot\nprediction. Experimental results on three multi-intent datasets show that our\nframework obtains substantial improvement and achieves the state-of-the-art\nperformance. In addition, our framework achieves new state-of-the-art\nperformance on two single-intent datasets.</p>\n", "tags": ["Datasets","EMNLP","Tools"] },
{"key": "qin2020co", "citations": "123", "year": "2021", "title":"A Co-interactive Transformer For Joint Slot Filling And Intent Detection", "abstract": "<p>Intent detection and slot filling are two main tasks for building a spoken\nlanguage understanding (SLU) system. The two tasks are closely related and the\ninformation of one task can be utilized in the other task. Previous studies\neither model the two tasks separately or only consider the single information\nflow from intent to slot. None of the prior approaches model the bidirectional\nconnection between the two tasks simultaneously. In this paper, we propose a\nCo-Interactive Transformer to consider the cross-impact between the two tasks.\nInstead of adopting the self-attention mechanism in vanilla Transformer, we\npropose a co-interactive module to consider the cross-impact by building a\nbidirectional connection between the two related tasks. In addition, the\nproposed co-interactive module can be stacked to incrementally enhance each\nother with mutual features. The experimental results on two public datasets\n(SNIPS and ATIS) show that our model achieves the state-of-the-art performance\nwith considerable improvements (+3.4% and +0.9% on overall acc). Extensive\nexperiments empirically verify that our model successfully captures the mutual\ninteraction knowledge.</p>\n", "tags": ["Datasets","ICASSP","Model Architecture"] },
{"key": "qin2020cosda", "citations": "118", "year": "2020", "title":"Cosda-ml: Multi-lingual Code-switching Data Augmentation For Zero-shot Cross-lingual NLP", "abstract": "<p>Multi-lingual contextualized embeddings, such as multilingual-BERT (mBERT),\nhave shown success in a variety of zero-shot cross-lingual tasks. However,\nthese models are limited by having inconsistent contextualized representations\nof subwords across different languages. Existing work addresses this issue by\nbilingual projection and fine-tuning technique. We propose a data augmentation\nframework to generate multi-lingual code-switching data to fine-tune mBERT,\nwhich encourages model to align representations from source and multiple target\nlanguages once by mixing their context information. Compared with the existing\nwork, our method does not rely on bilingual sentences for training, and\nrequires only one training process for multiple target languages. Experimental\nresults on five tasks with 19 languages show that our method leads to\nsignificantly improved performances for all the tasks compared with mBERT.</p>\n", "tags": ["Fine-Tuning","IJCAI","Model Architecture","Tools","Training Techniques"] },
{"key": "qin2020erica", "citations": "83", "year": "2021", "title":"ERICA: Improving Entity And Relation Understanding For Pre-trained Language Models Via Contrastive Learning", "abstract": "<p>Pre-trained Language Models (PLMs) have shown superior performance on various\ndownstream Natural Language Processing (NLP) tasks. However, conventional\npre-training objectives do not explicitly model relational facts in text, which\nare crucial for textual understanding. To address this issue, we propose a\nnovel contrastive learning framework ERICA to obtain a deep understanding of\nthe entities and their relations in text. Specifically, we define two novel\npre-training tasks to better understand entities and relations: (1) the entity\ndiscrimination task to distinguish which tail entity can be inferred by the\ngiven head entity and relation; (2) the relation discrimination task to\ndistinguish whether two relations are close or not semantically, which involves\ncomplex relational reasoning. Experimental results demonstrate that ERICA can\nimprove typical PLMs (BERT and RoBERTa) on several language understanding\ntasks, including relation extraction, entity typing and question answering,\nespecially under low-resource settings.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "qin2021learning", "citations": "293", "year": "2021", "title":"Learning How To Ask: Querying Lms With Mixtures Of Soft Prompts", "abstract": "<p>Natural-language prompts have recently been used to coax pretrained language\nmodels into performing other AI tasks, using a fill-in-the-blank paradigm\n(Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,\n2020). For example, language models retain factual knowledge from their\ntraining corpora that can be extracted by asking them to “fill in the blank” in\na sentential prompt. However, where does this prompt come from? We explore the\nidea of learning prompts by gradient descent – either fine-tuning prompts\ntaken from previous work, or starting from random initialization. Our prompts\nconsist of “soft words,” i.e., continuous vectors that are not necessarily word\ntype embeddings from the language model. Furthermore, for each task, we\noptimize a mixture of prompts, learning which prompts are most effective and\nhow to ensemble them. Across multiple English LMs and tasks, our approach\nhugely outperforms previous methods, showing that the implicit factual\nknowledge in language models was previously underestimated. Moreover, this\nknowledge is cheap to elicit: random initialization is nearly as good as\ninformed initialization.</p>\n", "tags": ["Few-Shot","Fine-Tuning","NAACL","Prompting","Training Techniques"] },
{"key": "qin2023is", "citations": "345", "year": "2023", "title":"Is Chatgpt A General-purpose Natural Language Processing Task Solver?", "abstract": "<p>Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot – i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "qin2023large", "citations": "66", "year": "2024", "title":"Large Language Models Are Effective Text Rankers With Pairwise Ranking Prompting", "abstract": "<p>Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, researchers have found it difficult to outperform fine-tuned\nbaseline rankers on benchmark datasets. We analyze pointwise and listwise\nranking prompts used by existing methods and argue that off-the-shelf LLMs do\nnot fully understand these challenging ranking formulations. In this paper, we\npropose to significantly reduce the burden on LLMs by using a new technique\ncalled Pairwise Ranking Prompting (PRP). Our results are the first in the\nliterature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&amp;2020, PRP\nbased on the Flan-UL2 model with 20B parameters performs favorably with the\nprevious best approach in the literature, which is based on the blackbox\ncommercial GPT-4 that has 50x (estimated) model size, while outperforming other\nLLM-based solutions, such as InstructGPT which has 175B parameters, by over 10%\nfor all ranking metrics. By using the same prompt template on seven BEIR tasks,\nPRP outperforms supervised baselines and outperforms the blackbox commercial\nChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on\naverage NDCG@10. Furthermore, we propose several variants of PRP to improve\nefficiency and show that it is possible to achieve competitive results even\nwith linear complexity.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Model Architecture","NAACL","Prompting"] },
{"key": "qinghao2023mplug", "citations": "130", "year": "2023", "title":"Mplug-owl: Modularization Empowers Large Language Models With Multimodality", "abstract": "<p>Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl’s impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "qinkai2023codegeex", "citations": "85", "year": "2023", "title":"Codegeex: A Pre-trained Model For Code Generation With Multilingual Benchmarking On Humaneval-x", "abstract": "<p>Large pre-trained code generation models, such as OpenAI Codex, can generate\nsyntax- and function-correct code, making the coding of programmers more\nproductive and our pursuit of artificial general intelligence closer. In this\npaper, we introduce CodeGeeX, a multilingual model with 13 billion parameters\nfor code generation. CodeGeeX is pre-trained on 850 billion tokens of 23\nprogramming languages as of June 2022. Our extensive experiments suggest that\nCodeGeeX outperforms multilingual code models of similar scale for both the\ntasks of code generation and translation on HumanEval-X. Building upon\nHumanEval (Python only), we develop the HumanEval-X benchmark for evaluating\nmultilingual models by hand-writing the solutions in C++, Java, JavaScript, and\nGo. In addition, we build CodeGeeX-based extensions on Visual Studio Code,\nJetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of\nthousands of active users per week. Our user study demonstrates that CodeGeeX\ncan help to increase coding efficiency for 83.4% of its users. Finally,\nCodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code,\nmodel weights (the version of 850B tokens), API, extensions, and HumanEval-X at\nhttps://github.com/THUDM/CodeGeeX.</p>\n", "tags": ["Evaluation","Has Code","KDD","Llm For Code","Tools"] },
{"key": "qinyuan2021crossfit", "citations": "96", "year": "2021", "title":"Crossfit: A Few-shot Learning Challenge For Cross-task Generalization In NLP", "abstract": "<p>Humans can learn a new language task efficiently with only few examples, by\nleveraging their knowledge obtained when learning prior tasks. In this paper,\nwe explore whether and how such cross-task generalization ability can be\nacquired, and further applied to build better few-shot learners across diverse\nNLP tasks. We introduce CrossFit, a problem setup for studying cross-task\ngeneralization ability, which standardizes seen/unseen task partitions, data\naccess during different learning stages, and the evaluation protocols. To\ninstantiate different seen/unseen task partitions in CrossFit and facilitate\nin-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse\nfew-shot NLP tasks created from open-access NLP datasets and converted to a\nunified text-to-text format. Our analysis reveals that the few-shot learning\nability on unseen tasks can be improved via an upstream learning stage using a\nset of seen tasks. We also observe that the selection of upstream learning\ntasks can significantly influence few-shot performance on unseen tasks, asking\nfurther analysis on task similarity and transferability.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Few-Shot","Has Code"] },
{"key": "qiu2019blockwise", "citations": "142", "year": "2020", "title":"Blockwise Self-attention For Long Document Understanding", "abstract": "<p>We present BlockBERT, a lightweight and efficient BERT model for better\nmodeling long-distance dependencies. Our model extends BERT by introducing\nsparse block structures into the attention matrix to reduce both memory\nconsumption and training/inference time, which also enables attention heads to\ncapture either short- or long-range contextual information. We conduct\nexperiments on language model pre-training and several benchmark question\nanswering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1%\nless memory and 12.0-25.1% less time to learn the model. During testing,\nBlockBERT saves 27.8% inference time, while having comparable and sometimes\nbetter prediction accuracy, compared to an advanced BERT-based model, RoBERTa.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture","Training Techniques"] },
{"key": "qiu2021contrastive", "citations": "259", "year": "2022", "title":"Contrastive Learning For Representation Degeneration Problem In Sequential Recommendation", "abstract": "<p>Recent advancements of sequential deep learning models such as Transformer\nand BERT have significantly facilitated the sequential recommendation. However,\naccording to our study, the distribution of item embeddings generated by these\nmodels tends to degenerate into an anisotropic shape, which may result in high\nsemantic similarities among embeddings. In this paper, both empirical and\ntheoretical investigations of this representation degeneration problem are\nfirst provided, based on which a novel recommender model DuoRec is proposed to\nimprove the item embeddings distribution. Specifically, in light of the\nuniformity property of contrastive learning, a contrastive regularization is\ndesigned for DuoRec to reshape the distribution of sequence representations.\nGiven the convention that the recommendation task is performed by measuring the\nsimilarity between sequence representations and item embeddings in the same\nspace via dot product, the regularization can be implicitly applied to the item\nembedding distribution. Existing contrastive learning methods mainly rely on\ndata level augmentation for user-item interaction sequences through item\ncropping, masking, or reordering and can hardly provide semantically consistent\naugmentation samples. In DuoRec, a model-level augmentation is proposed based\non Dropout to enable better semantic preserving. Furthermore, a novel sampling\nstrategy is developed, where sequences having the same target item are chosen\nhard positive samples. Extensive experiments conducted on five datasets\ndemonstrate the superior performance of the proposed DuoRec model compared with\nbaseline methods. Visualization results of the learned representations validate\nthat DuoRec can largely alleviate the representation degeneration problem.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "qu2019attentive", "citations": "90", "year": "2019", "title":"Attentive History Selection For Conversational Question Answering", "abstract": "<p>Conversational question answering (ConvQA) is a simplified but concrete\nsetting of conversational search. One of its major challenges is to leverage\nthe conversation history to understand and answer the current question. In this\nwork, we propose a novel solution for ConvQA that involves three aspects.\nFirst, we propose a positional history answer embedding method to encode\nconversation history with position information using BERT in a natural way.\nBERT is a powerful technique for text representation. Second, we design a\nhistory attention mechanism (HAM) to conduct a “soft selection” for\nconversation histories. This method attends to history turns with different\nweights based on how helpful they are on answering the current question. Third,\nin addition to handling conversation history, we take advantage of multi-task\nlearning (MTL) to do answer prediction along with another essential\nconversation task (dialog act prediction) using a uniform model architecture.\nMTL is able to learn more expressive and generic representations to improve the\nperformance of ConvQA. We demonstrate the effectiveness of our model with\nextensive experimental evaluations on QuAC, a large-scale ConvQA dataset. We\nshow that position information plays an important role in conversation history\nmodeling. We also visualize the history attention and provide new insights into\nconversation history understanding.</p>\n", "tags": ["CIKM","Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "qu2019bert", "citations": "206", "year": "2019", "title":"BERT With History Answer Embedding For Conversational Question Answering", "abstract": "<p>Conversational search is an emerging topic in the information retrieval\ncommunity. One of the major challenges to multi-turn conversational search is\nto model the conversation history to answer the current question. Existing\nmethods either prepend history turns to the current question or use complicated\nattention mechanisms to model the history. We propose a conceptually simple yet\nhighly effective approach referred to as history answer embedding. It enables\nseamless integration of conversation history into a conversational question\nanswering (ConvQA) model built on BERT (Bidirectional Encoder Representations\nfrom Transformers). We first explain our view that ConvQA is a simplified but\nconcrete setting of conversational search, and then we provide a general\nframework to solve ConvQA. We further demonstrate the effectiveness of our\napproach under this framework. Finally, we analyze the impact of different\nnumbers of history turns under different settings to provide new insights into\nconversation history modeling in ConvQA.</p>\n", "tags": ["Model Architecture","SIGIR"] },
{"key": "qu2020open", "citations": "137", "year": "2020", "title":"Open-retrieval Conversational Question Answering", "abstract": "<p>Conversational search is one of the ultimate goals of information retrieval.\nRecent research approaches conversational search by simplified settings of\nresponse ranking and conversational question answering, where an answer is\neither selected from a given candidate set or extracted from a given passage.\nThese simplifications neglect the fundamental role of retrieval in\nconversational search. To address this limitation, we introduce an\nopen-retrieval conversational question answering (ORConvQA) setting, where we\nlearn to retrieve evidence from a large collection before extracting answers,\nas a further step towards building functional conversational search systems. We\ncreate a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an\nend-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader\nthat are all based on Transformers. Our extensive experiments on OR-QuAC\ndemonstrate that a learnable retriever is crucial for ORConvQA. We further show\nthat our system can make a substantial improvement when we enable history\nmodeling in all system components. Moreover, we show that the reranker\ncomponent contributes to the model performance by providing a regularization\neffect. Finally, further in-depth analyses are performed to provide new\ninsights into ORConvQA.</p>\n", "tags": ["Datasets","Retrieval Systems","SIGIR"] },
{"key": "qu2020rocketqa", "citations": "296", "year": "2021", "title":"Rocketqa: An Optimized Training Approach To Dense Passage Retrieval For Open-domain Question Answering", "abstract": "<p>In open-domain question answering, dense passage retrieval has become a new\nparadigm to retrieve relevant passages for finding answers. Typically, the\ndual-encoder architecture is adopted to learn dense representations of\nquestions and passages for semantic matching. However, it is difficult to\neffectively train a dual-encoder due to the challenges including the\ndiscrepancy between training and inference, the existence of unlabeled\npositives and limited training data. To address these challenges, we propose an\noptimized training approach, called RocketQA, to improving dense passage\nretrieval. We make three major technical contributions in RocketQA, namely\ncross-batch negatives, denoised hard negatives and data augmentation. The\nexperiment results show that RocketQA significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions. We also conduct\nextensive experiments to examine the effectiveness of the three strategies in\nRocketQA. Besides, we demonstrate that the performance of end-to-end QA can be\nimproved based on our RocketQA retriever.</p>\n", "tags": ["Model Architecture","NAACL","Retrieval Systems","Training Techniques"] },
{"key": "quadrana2017personalizing", "citations": "659", "year": "2017", "title":"Personalizing Session-based Recommendations With Hierarchical Recurrent Neural Networks", "abstract": "<p>Session-based recommendations are highly relevant in many modern on-line\nservices (e.g. e-commerce, video streaming) and recommendation settings.\nRecently, Recurrent Neural Networks have been shown to perform very well in\nsession-based settings. While in many session-based recommendation domains user\nidentifiers are hard to come by, there are also domains in which user profiles\nare readily available. We propose a seamless way to personalize RNN models with\ncross-session information transfer and devise a Hierarchical RNN model that\nrelays end evolves latent hidden states of the RNNs across user sessions.\nResults on two industry datasets show large improvements over the session-only\nRNNs.</p>\n", "tags": ["Datasets"] },
{"key": "r2020towards", "citations": "89", "year": "2019", "title":"Towards Automatic Face-to-face Translation", "abstract": "<p>In light of the recent breakthroughs in automatic machine translation\nsystems, we propose a novel approach that we term as “Face-to-Face\nTranslation”. As today’s digital communication becomes increasingly visual, we\nargue that there is a need for systems that can automatically translate a video\nof a person speaking in language A into a target language B with realistic lip\nsynchronization. In this work, we create an automatic pipeline for this problem\nand demonstrate its impact on multiple real-world applications. First, we build\na working speech-to-speech translation system by bringing together multiple\nexisting modules from speech and language. We then move towards “Face-to-Face\nTranslation” by incorporating a novel visual module, LipGAN for generating\nrealistic talking faces from the translated audio. Quantitative evaluation of\nLipGAN on the standard LRW test set shows that it significantly outperforms\nexisting approaches across all standard metrics. We also subject our\nFace-to-Face Translation pipeline, to multiple human evaluations and show that\nit can significantly improve the overall user experience for consuming and\ninteracting with multimodal content across languages. Code, models and demo\nvideo are made publicly available.\n  Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0\n  Code and models: https://github.com/Rudrabha/LipGAN</p>\n", "tags": ["Applications","Evaluation","Has Code"] },
{"key": "rabinovich2016personalized", "citations": "107", "year": "2017", "title":"Personalized Machine Translation: Preserving Original Author Traits", "abstract": "<p>The language that we produce reflects our personality, and various personal\nand demographic characteristics can be detected in natural language texts. We\nfocus on one particular personal trait of the author, gender, and study how it\nis manifested in original texts and in translations. We show that author’s\ngender has a powerful, clear signal in originals texts, but this signal is\nobfuscated in human and machine translation. We then propose simple\ndomain-adaptation techniques that help retain the original gender traits in the\ntranslation, without harming the quality of the translation, thereby creating\nmore personalized machine translation systems.</p>\n", "tags": ["NAACL"] },
{"key": "radford2021learning", "citations": "4132", "year": "2021", "title":"Learning Transferable Visual Models From Natural Language Supervision", "abstract": "<p>State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.</p>\n", "tags": ["Datasets","Has Code","Training Techniques"] },
{"key": "radziwill2017evaluating", "citations": "196", "year": "2017", "title":"Evaluating Quality Of Chatbots And Intelligent Conversational Agents", "abstract": "<p>Chatbots are one class of intelligent, conversational software agents\nactivated by natural language input (which can be in the form of text, voice,\nor both). They provide conversational output in response, and if commanded, can\nsometimes also execute tasks. Although chatbot technologies have existed since\nthe 1960s and have influenced user interface development in games since the\nearly 1980s, chatbots are now easier to train and implement. This is due to\nplentiful open source code, widely available development platforms, and\nimplementation options via Software as a Service (SaaS). In addition to\nenhancing customer experiences and supporting learning, chatbots can also be\nused to engineer social harm - that is, to spread rumors and misinformation, or\nattack people for posting their thoughts and opinions online. This paper\npresents a literature review of quality issues and attributes as they relate to\nthe contemporary issue of chatbot development and implementation. Finally,\nquality assessment approaches are reviewed, and a quality assessment method\nbased on these attributes and the Analytic Hierarchy Process (AHP) is proposed\nand examined.</p>\n", "tags": ["Evaluation","Has Code"] },
{"key": "rae2016scaling", "citations": "88", "year": "2016", "title":"Scaling Memory-augmented Neural Networks With Sparse Reads And Writes", "abstract": "<p>Neural networks augmented with external memory have the ability to learn\nalgorithmic solutions to complex tasks. These models appear promising for\napplications such as language modeling and machine translation. However, they\nscale poorly in both space and time as the amount of memory grows — limiting\ntheir applicability to real-world domains. Here, we present an end-to-end\ndifferentiable memory access scheme, which we call Sparse Access Memory (SAM),\nthat retains the representational power of the original approaches whilst\ntraining efficiently with very large memories. We show that SAM achieves\nasymptotic lower bounds in space and time complexity, and find that an\nimplementation runs \\(1,!000\\times\\) faster and with \\(3,!000\\times\\) less\nphysical memory than non-sparse models. SAM learns with comparable data\nefficiency to existing models on a range of synthetic tasks and one-shot\nOmniglot character recognition, and can scale to tasks requiring \\(100,!000\\)s\nof time steps and memories. As well, we show how our approach can be adapted\nfor models that maintain temporal associations between memories, as with the\nrecently introduced Differentiable Neural Computer.</p>\n", "tags": ["Applications","Efficiency","Memory & Context","Training Techniques"] },
{"key": "rae2019compressive", "citations": "146", "year": "2019", "title":"Compressive Transformers For Long-range Sequence Modelling", "abstract": "<p>We present the Compressive Transformer, an attentive sequence model which\ncompresses past memories for long-range sequence learning. We find the\nCompressive Transformer obtains state-of-the-art language modelling results in\nthe WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc\nrespectively. We also find it can model high-frequency speech effectively and\ncan be used as a memory mechanism for RL, demonstrated on an object matching\ntask. To promote the domain of long-range sequence learning, we propose a new\nopen-vocabulary language modelling benchmark derived from books, PG-19.</p>\n", "tags": ["Datasets","Evaluation","Memory & Context","Model Architecture"] },
{"key": "rafael2023direct", "citations": "129", "year": "2023", "title":"Direct Preference Optimization: Your Language Model Is Secretly A Reward Model", "abstract": "<p>While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.</p>\n", "tags": ["Agentic","Efficiency","Fine-Tuning","Reinforcement Learning","Training Techniques"] },
{"key": "raganato2020fixed", "citations": "65", "year": "2020", "title":"Fixed Encoder Self-attention Patterns In Transformer-based Machine Translation", "abstract": "<p>Transformer-based models have brought a radical change to neural machine\ntranslation. A key feature of the Transformer architecture is the so-called\nmulti-head attention mechanism, which allows the model to focus simultaneously\non different parts of the input. However, recent works have shown that most\nattention heads learn simple, and often redundant, positional patterns. In this\npaper, we propose to replace all but one attention head of each encoder layer\nwith simple fixed – non-learnable – attentive patterns that are solely based\non position and do not require any external knowledge. Our experiments with\ndifferent data sizes and multiple language pairs show that fixing the attention\nheads on the encoder side of the Transformer at training time does not impact\nthe translation quality and even increases BLEU scores by up to 3 points in\nlow-resource scenarios.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "rahimi2019massively", "citations": "183", "year": "2019", "title":"Massively Multilingual Transfer For NER", "abstract": "<p>In cross-lingual transfer, NLP models over one or more source languages are\napplied to a low-resource target language. While most prior work has used a\nsingle source model or a few carefully selected models, here we consider a\n`massive’ setting with many such models. This setting raises the problem of\npoor transfer, particularly from distant languages. We propose two techniques\nfor modulating the transfer, suitable for zero-shot or few-shot learning,\nrespectively. Evaluating on named entity recognition, we show that our\ntechniques are much more effective than strong baselines, including standard\nensembling, and our unsupervised method rivals oracle selection of the single\nbest individual model.</p>\n", "tags": ["Few-Shot"] },
{"key": "rahman2019integrating", "citations": "409", "year": "2020", "title":"Integrating Multimodal Information In Large Pretrained Transformers", "abstract": "<p>Recent Transformer-based contextual word representations, including BERT and\nXLNet, have shown state-of-the-art performance in multiple disciplines within\nNLP. Fine-tuning the trained contextual models on task-specific datasets has\nbeen the key to achieving superior performance downstream. While fine-tuning\nthese pre-trained models is straightforward for lexical applications\n(applications with only language modality), it is not trivial for multimodal\nlanguage (a growing area in NLP focused on modeling face-to-face\ncommunication). Pre-trained models don’t have the necessary components to\naccept two extra modalities of vision and acoustic. In this paper, we proposed\nan attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG\nallows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.\nIt does so by generating a shift to internal representation of BERT and XLNet;\na shift that is conditioned on the visual and acoustic modalities. In our\nexperiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for\nmultimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly\nboosts the sentiment analysis performance over previous baselines as well as\nlanguage-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet\nachieves human-level multimodal sentiment analysis performance for the first\ntime in the NLP community.</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "rahman2020improved", "citations": "60", "year": "2021", "title":"An Improved Attention For Visual Question Answering", "abstract": "<p>We consider the problem of Visual Question Answering (VQA). Given an image\nand a free-form, open-ended, question, expressed in natural language, the goal\nof VQA system is to provide accurate answer to this question with respect to\nthe image. The task is challenging because it requires simultaneous and\nintricate understanding of both visual and textual information. Attention,\nwhich captures intra- and inter-modal dependencies, has emerged as perhaps the\nmost widely used mechanism for addressing these challenges. In this paper, we\npropose an improved attention-based architecture to solve VQA. We incorporate\nan Attention on Attention (AoA) module within encoder-decoder framework, which\nis able to determine the relation between attention results and queries.\nAttention module generates weighted average for each query. On the other hand,\nAoA module first generates an information vector and an attention gate using\nattention results and current context; and then adds another attention to\ngenerate final attended information by multiplying the two. We also propose\nmultimodal fusion module to combine both visual and textual information. The\ngoal of this fusion module is to dynamically decide how much information should\nbe considered from each modality. Extensive experiments on VQA-v2 benchmark\ndataset show that our method achieves the state-of-the-art performance.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "raj2023dreambooth3d", "citations": "90", "year": "2023", "title":"Dreambooth3d: Subject-driven Text-to-3d Generation", "abstract": "<p>We present DreamBooth3D, an approach to personalize text-to-3D generative\nmodels from as few as 3-6 casually captured images of a subject. Our approach\ncombines recent advances in personalizing text-to-image models (DreamBooth)\nwith text-to-3D generation (DreamFusion). We find that naively combining these\nmethods fails to yield satisfactory subject-specific 3D assets due to\npersonalized text-to-image models overfitting to the input viewpoints of the\nsubject. We overcome this through a 3-stage optimization strategy where we\njointly leverage the 3D consistency of neural radiance fields together with the\npersonalization capability of text-to-image models. Our method can produce\nhigh-quality, subject-specific 3D assets with text-driven modifications such as\nnovel poses, colors and attributes that are not seen in any of the input images\nof the subject.</p>\n", "tags": ["ICCV"] },
{"key": "rajani2019explain", "citations": "365", "year": "2019", "title":"Explain Yourself! Leveraging Language Models For Commonsense Reasoning", "abstract": "<p>Deep learning models perform poorly on tasks that require commonsense\nreasoning, which often necessitates some form of world-knowledge or reasoning\nover information not immediately present in the input. We collect human\nexplanations for commonsense reasoning in the form of natural language\nsequences and highlighted annotations in a new dataset called Common Sense\nExplanations (CoS-E). We use CoS-E to train language models to automatically\ngenerate explanations that can be used during training and inference in a novel\nCommonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the\nstate-of-the-art by 10% on the challenging CommonsenseQA task. We further study\ncommonsense reasoning in DNNs using both human and auto-generated explanations\nincluding transfer to out-of-domain tasks. Empirical results indicate that we\ncan effectively leverage language models for commonsense reasoning.</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "rajbhandari2019zero", "citations": "474", "year": "2020", "title":"Zero: Memory Optimizations Toward Training Trillion Parameter Models", "abstract": "<p>Large deep learning models offer significant accuracy gains, but training\nbillions to trillions of parameters is challenging. Existing solutions such as\ndata and model parallelisms exhibit fundamental limitations to fit these models\ninto limited device memory, while obtaining computation, communication and\ndevelopment efficiency. We develop a novel solution, Zero Redundancy Optimizer\n(ZeRO), to optimize memory, vastly improving training speed while increasing\nthe model size that can be efficiently trained. ZeRO eliminates memory\nredundancies in data- and model-parallel training while retaining low\ncommunication volume and high computational granularity, allowing us to scale\nthe model size proportional to the number of devices with sustained high\nefficiency. Our analysis on memory requirements and communication volume\ndemonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters\nusing today’s hardware.\n  We implement and evaluate ZeRO: it trains large models of over 100B parameter\nwith super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops.\nThis represents an 8x increase in model size and 10x increase in achievable\nperformance over state-of-the-art. In terms of usability, ZeRO can train large\nmodels of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B)\nwithout requiring model parallelism which is harder for scientists to apply.\nLast but not the least, researchers have used the system breakthroughs of ZeRO\nto create the world’s largest language model (Turing-NLG, 17B parameters) with\nrecord breaking accuracy.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "rajeswar2017adversarial", "citations": "120", "year": "2017", "title":"Adversarial Generation Of Natural Language", "abstract": "<p>Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.</p>\n", "tags": ["Datasets","Model Architecture","Security"] },
{"key": "rajpurkar2018know", "citations": "132", "year": "2018", "title":"Know What You Don't Know: Unanswerable Questions For Squad", "abstract": "<p>Extractive reading comprehension systems can often locate the correct answer\nto a question in a context document, but they also tend to make unreliable\nguesses on questions for which the correct answer is not stated in the context.\nExisting datasets either focus exclusively on answerable questions, or use\nautomatically generated unanswerable questions that are easy to identify. To\naddress these weaknesses, we present SQuAD 2.0, the latest version of the\nStanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD\ndata with over 50,000 unanswerable questions written adversarially by\ncrowdworkers to look similar to answerable ones. To do well on SQuAD 2.0,\nsystems must not only answer questions when possible, but also determine when\nno answer is supported by the paragraph and abstain from answering. SQuAD 2.0\nis a challenging natural language understanding task for existing models: a\nstrong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on\nSQuAD 2.0.</p>\n", "tags": ["Datasets"] },
{"key": "ram2018conversational", "citations": "222", "year": "2018", "title":"Conversational AI: The Science Behind The Alexa Prize", "abstract": "<p>Conversational agents are exploding in popularity. However, much work remains\nin the area of social conversation as well as free-form conversation over a\nbroad range of domains and topics. To advance the state of the art in\nconversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar\nuniversity competition where sixteen selected university teams were challenged\nto build conversational agents, known as socialbots, to converse coherently and\nengagingly with humans on popular topics such as Sports, Politics,\nEntertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers\nthe academic community a unique opportunity to perform research with a live\nsystem used by millions of users. The competition provided university teams\nwith real user conversational data at scale, along with the user-provided\nratings and feedback augmented with annotations by the Alexa team. This enabled\nteams to effectively iterate and make improvements throughout the competition\nwhile being evaluated in real-time through live user interactions. To build\ntheir socialbots, university teams combined state-of-the-art techniques with\nnovel strategies in the areas of Natural Language Understanding, Context\nModeling, Dialog Management, Response Generation, and Knowledge Acquisition. To\nsupport the efforts of participating teams, the Alexa Prize team made\nsignificant scientific and engineering investments to build and improve\nConversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice\nUser Experience, and tools for traffic management and scalability. This paper\noutlines the advances created by the university teams as well as the Alexa\nPrize team to achieve the common goal of solving the problem of Conversational\nAI.</p>\n", "tags": ["Evaluation","Tools"] },
{"key": "ram2021few", "citations": "69", "year": "2021", "title":"Few-shot Question Answering By Pretraining Span Selection", "abstract": "<p>In several question answering benchmarks, pretrained models have reached\nhuman parity through fine-tuning on an order of 100,000 annotated questions and\nanswers. We explore the more realistic few-shot setting, where only a few\nhundred training examples are available, and observe that standard models\nperform poorly, highlighting the discrepancy between current pretraining\nobjectives and question answering. We propose a new pretraining scheme tailored\nfor question answering: recurring span selection. Given a passage with multiple\nsets of recurring spans, we mask in each set all recurring spans but one, and\nask the model to select the correct span in the passage for each masked span.\nMasked spans are replaced with a special token, viewed as a question\nrepresentation, that is later used during fine-tuning to select the answer\nspan. The resulting model obtains surprisingly good results on multiple\nbenchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while\nmaintaining competitive performance in the high-resource setting.</p>\n", "tags": ["Few-Shot","Fine-Tuning","Training Techniques"] },
{"key": "ram2023context", "citations": "142", "year": "2023", "title":"In-context Retrieval-augmented Language Models", "abstract": "<p>Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, were shown to significantly improve language modeling performance.\nIn addition, they can mitigate the problem of factually inaccurate text\ngeneration and provide natural source attribution mechanism. Existing RALM\napproaches focus on modifying the LM architecture in order to facilitate the\nincorporation of external information, significantly complicating deployment.\nThis paper considers a simple alternative, which we dub In-Context RALM:\nleaving the LM architecture unchanged and prepending grounding documents to the\ninput, without any further training of the LM. We show that In-Context RALM\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\nthe document retrieval and ranking mechanism can be specialized to the RALM\nsetting to further boost performance. We conclude that In-Context RALM has\nconsiderable potential to increase the prevalence of LM grounding, particularly\nin settings where a pretrained LM must be used without modification or even via\nAPI access.</p>\n", "tags": ["Datasets","Model Architecture","RAG","TACL","Tools","Training Techniques"] },
{"key": "ramachandran2016unsupervised", "citations": "275", "year": "2017", "title":"Unsupervised Pretraining For Sequence To Sequence Learning", "abstract": "<p>This work presents a general unsupervised learning method to improve the\naccuracy of sequence to sequence (seq2seq) models. In our method, the weights\nof the encoder and decoder of a seq2seq model are initialized with the\npretrained weights of two language models and then fine-tuned with labeled\ndata. We apply this method to challenging benchmarks in machine translation and\nabstractive summarization and find that it significantly improves the\nsubsequent supervised models. Our main result is that pretraining improves the\ngeneralization of seq2seq models. We achieve state-of-the art results on the\nWMT English\\(\\rightarrow\\)German task, surpassing a range of methods using both\nphrase-based machine translation and neural machine translation. Our method\nachieves a significant improvement of 1.3 BLEU from the previous best models on\nboth WMT’14 and WMT’15 English\\(\\rightarrow\\)German. We also conduct human\nevaluations on abstractive summarization and find that our method outperforms a\npurely supervised learning baseline in a statistically significant manner.</p>\n", "tags": ["EMNLP"] },
{"key": "ramakrishnan2018overcoming", "citations": "132", "year": "2018", "title":"Overcoming Language Priors In Visual Question Answering With Adversarial Regularization", "abstract": "<p>Modern Visual Question Answering (VQA) models have been shown to rely heavily\non superficial correlations between question and answer words learned during\ntraining such as overwhelmingly reporting the type of room as kitchen or the\nsport being played as tennis, irrespective of the image. Most alarmingly, this\nshortcoming is often not well reflected during evaluation because the same\nstrong priors exist in test distributions; however, a VQA system that fails to\nground questions in image content would likely perform poorly in real-world\nsettings. In this work, we present a novel regularization scheme for VQA that\nreduces this effect. We introduce a question-only model that takes as input the\nquestion encoding from the VQA model and must leverage language biases in order\nto succeed. We then pose training as an adversarial game between the VQA model\nand this question-only adversary – discouraging the VQA model from capturing\nlanguage biases in its question encoding. Further,we leverage this\nquestion-only model to estimate the increase in model confidence after\nconsidering the image, which we maximize explicitly to encourage visual\ngrounding. Our approach is a model agnostic training procedure and simple to\nimplement. We show empirically that it can improve performance significantly on\na bias-sensitive split of the VQA dataset for multiple base models – achieving\nstate-of-the-art on this task. Further, on standard VQA tasks, our approach\nshows significantly less drop in accuracy compared to existing bias-reducing\nVQA models.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Training Techniques"] },
{"key": "ramesh2021zero", "citations": "1075", "year": "2021", "title":"Zero-shot Text-to-image Generation", "abstract": "<p>Text-to-image generation has traditionally focused on finding better modeling\nassumptions for training on a fixed dataset. These assumptions might involve\ncomplex architectures, auxiliary losses, or side information such as object\npart labels or segmentation masks supplied during training. We describe a\nsimple approach for this task based on a transformer that autoregressively\nmodels the text and image tokens as a single stream of data. With sufficient\ndata and scale, our approach is competitive with previous domain-specific\nmodels when evaluated in a zero-shot fashion.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "ramesh2022hierarchical", "citations": "1958", "year": "2022", "title":"Hierarchical Text-conditional Image Generation With CLIP Latents", "abstract": "<p>Contrastive models like CLIP have been shown to learn robust representations\nof images that capture both semantics and style. To leverage these\nrepresentations for image generation, we propose a two-stage model: a prior\nthat generates a CLIP image embedding given a text caption, and a decoder that\ngenerates an image conditioned on the image embedding. We show that explicitly\ngenerating image representations improves image diversity with minimal loss in\nphotorealism and caption similarity. Our decoders conditioned on image\nrepresentations can also produce variations of an image that preserve both its\nsemantics and style, while varying the non-essential details absent from the\nimage representation. Moreover, the joint embedding space of CLIP enables\nlanguage-guided image manipulations in a zero-shot fashion. We use diffusion\nmodels for the decoder and experiment with both autoregressive and diffusion\nmodels for the prior, finding that the latter are computationally more\nefficient and produce higher-quality samples.</p>\n", "tags": [] },
{"key": "ranathunga2021neural", "citations": "171", "year": "2022", "title":"Neural Machine Translation For Low-resource Languages: A Survey", "abstract": "<p>Neural Machine Translation (NMT) has seen a tremendous spurt of growth in\nless than ten years, and has already entered a mature phase. While considered\nas the most widely used solution for Machine Translation, its performance on\nlow-resource language pairs still remains sub-optimal compared to the\nhigh-resource counterparts, due to the unavailability of large parallel\ncorpora. Therefore, the implementation of NMT techniques for low-resource\nlanguage pairs has been receiving the spotlight in the recent NMT research\narena, thus leading to a substantial amount of research reported on this topic.\nThis paper presents a detailed survey of research advancements in low-resource\nlanguage NMT (LRL-NMT), along with a quantitative analysis aimed at identifying\nthe most popular solutions. Based on our findings from reviewing previous work,\nthis survey paper provides a set of guidelines to select the possible NMT\ntechnique for a given LRL data setting. It also presents a holistic view of the\nLRL-NMT research landscape and provides a list of recommendations to further\nenhance the research efforts on LRL-NMT.</p>\n", "tags": ["Survey Paper","Tools"] },
{"key": "rao2018learning", "citations": "149", "year": "2018", "title":"Learning To Ask Good Questions: Ranking Clarification Questions Using Neural Expected Value Of Perfect Information", "abstract": "<p>Inquiry is fundamental to communication, and machines cannot effectively\ncollaborate with humans unless they can ask questions. In this work, we build a\nneural network model for the task of ranking clarification questions. Our model\nis inspired by the idea of expected value of perfect information: a good\nquestion is one whose expected answer will be useful. We study this problem\nusing data from StackExchange, a plentiful online resource in which people\nroutinely ask clarifying questions to posts so that they can better offer\nassistance to the original poster. We create a dataset of clarification\nquestions consisting of ~77K posts paired with a clarification question (and\nanswer) from three domains of StackExchange: askubuntu, unix and superuser. We\nevaluate our model on 500 samples of this dataset against expert human\njudgments and demonstrate significant improvements over controlled baselines.</p>\n", "tags": ["Datasets"] },
{"key": "rao2021denseclip", "citations": "347", "year": "2022", "title":"Denseclip: Language-guided Dense Prediction With Context-aware Prompting", "abstract": "<p>Recent progress has shown that large-scale pre-training using contrastive\nimage-text pairs can be a promising alternative for high-quality visual\nrepresentation learning from natural language supervision. Benefiting from a\nbroader source of supervision, this new paradigm exhibits impressive\ntransferability to downstream classification tasks and datasets. However, the\nproblem of transferring the knowledge learned from image-text pairs to more\ncomplex dense prediction tasks has barely been visited. In this work, we\npresent a new framework for dense prediction by implicitly and explicitly\nleveraging the pre-trained knowledge from CLIP. Specifically, we convert the\noriginal image-text matching problem in CLIP to a pixel-text matching problem\nand use the pixel-text score maps to guide the learning of dense prediction\nmodels. By further using the contextual information from the image to prompt\nthe language model, we are able to facilitate our model to better exploit the\npre-trained knowledge. Our method is model-agnostic, which can be applied to\narbitrary dense prediction systems and various pre-trained visual backbones\nincluding both CLIP models and ImageNet pre-trained models. Extensive\nexperiments demonstrate the superior performance of our methods on semantic\nsegmentation, object detection, and instance segmentation tasks. Code is\navailable at https://github.com/raoyongming/DenseCLIP</p>\n", "tags": ["CVPR","Datasets","Has Code","Prompting","Tools","Training Techniques"] },
{"key": "rasheed2022fine", "citations": "75", "year": "2023", "title":"Fine-tuned CLIP Models Are Efficient Video Learners", "abstract": "<p>Large-scale multi-modal training with image-text pairs imparts strong\ngeneralization to CLIP model. Since training on a similar scale for videos is\ninfeasible, recent approaches focus on the effective transfer of image-based\nCLIP to the video domain. In this pursuit, new parametric modules are added to\nlearn temporal information and inter-frame relationships which require\nmeticulous design efforts. Furthermore, when the resulting models are learned\non videos, they tend to overfit on the given task distribution and lack in\ngeneralization aspect. This begs the following question: How to effectively\ntransfer image-level CLIP representations to videos? In this work, we show that\na simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to\nbridge the domain gap from images to videos. Our qualitative analysis\nillustrates that the frame-level processing from CLIP image-encoder followed by\nfeature pooling and similarity matching with corresponding text embeddings\nhelps in implicitly modeling the temporal cues within ViFi-CLIP. Such\nfine-tuning helps the model to focus on scene dynamics, moving objects and\ninter-object relationships. For low-data regimes where full fine-tuning is not\nviable, we propose a `bridge and prompt’ approach that first uses fine-tuning\nto bridge the domain gap and then learns prompts on language and vision side to\nadapt CLIP representations. We extensively evaluate this simple yet strong\nbaseline on zero-shot, base-to-novel generalization, few-shot and fully\nsupervised settings across five video benchmarks. Our code is available at\nhttps://github.com/muzairkhattak/ViFi-CLIP.</p>\n", "tags": ["CVPR","Few-Shot","Fine-Tuning","Has Code","Prompting","Training Techniques"] },
{"key": "rashkin2018modeling", "citations": "82", "year": "2018", "title":"Modeling Naive Psychology Of Characters In Simple Commonsense Stories", "abstract": "<p>Understanding a narrative requires reading between the lines and reasoning\nabout the unspoken but obvious implications about events and people’s mental\nstates - a capability that is trivial for humans but remarkably hard for\nmachines. To facilitate research addressing this challenge, we introduce a new\nannotation framework to explain naive psychology of story characters as\nfully-specified chains of mental states with respect to motivations and\nemotional reactions. Our work presents a new large-scale dataset with rich\nlow-level annotations and establishes baseline performance on several new\ntasks, suggesting avenues for future research.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "rashkin2018towards", "citations": "603", "year": "2019", "title":"Towards Empathetic Open-domain Conversation Models: A New Benchmark And Dataset", "abstract": "<p>One challenge for dialogue agents is recognizing feelings in the conversation\npartner and replying accordingly, a key communicative skill. While it is\nstraightforward for humans to recognize and acknowledge others’ feelings in a\nconversation, this is a significant challenge for AI systems due to the paucity\nof suitable publicly-available datasets for training and evaluation. This work\nproposes a new benchmark for empathetic dialogue generation and\nEmpatheticDialogues, a novel dataset of 25k conversations grounded in emotional\nsituations. Our experiments indicate that dialogue models that use our dataset\nare perceived to be more empathetic by human evaluators, compared to models\nmerely trained on large-scale Internet conversation data. We also present\nempirical comparisons of dialogue model adaptations for empathetic responding,\nleveraging existing models or datasets without requiring lengthy re-training of\nthe full model.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "rashkin2020plotmachines", "citations": "98", "year": "2020", "title":"Plotmachines: Outline-conditioned Generation With Dynamic Plot State Tracking", "abstract": "<p>We propose the task of outline-conditioned story generation: given an outline\nas a set of phrases that describe key characters and events to appear in a\nstory, the task is to generate a coherent narrative that is consistent with the\nprovided outline. This task is challenging as the input only provides a rough\nsketch of the plot, and thus, models need to generate a story by interweaving\nthe key points provided in the outline. This requires the model to keep track\nof the dynamic states of the latent plot, conditioning on the input outline\nwhile generating the full story. We present PlotMachines, a neural narrative\nmodel that learns to transform an outline into a coherent story by tracking the\ndynamic plot states. In addition, we enrich PlotMachines with high-level\ndiscourse structure so that the model can learn different writing styles\ncorresponding to different parts of the narrative. Comprehensive experiments\nover three fiction and non-fiction datasets demonstrate that large-scale\nlanguage models, such as GPT-2 and Grover, despite their impressive generation\nperformance, are not sufficient in generating coherent narratives for the given\noutline, and dynamic plot state tracking is important for composing narratives\nwith tighter, more consistent plots.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "rashkin2021increasing", "citations": "63", "year": "2021", "title":"Increasing Faithfulness In Knowledge-grounded Dialogue With Controllable Features", "abstract": "<p>Knowledge-grounded dialogue systems are intended to convey information that\nis based on evidence provided in a given source text. We discuss the challenges\nof training a generative neural dialogue model for such systems that is\ncontrolled to stay faithful to the evidence. Existing datasets contain a mix of\nconversational responses that are faithful to selected evidence as well as more\nsubjective or chit-chat style responses. We propose different evaluation\nmeasures to disentangle these different styles of responses by quantifying the\ninformativeness and objectivity. At training time, additional inputs based on\nthese evaluation measures are given to the dialogue model. At generation time,\nthese additional inputs act as stylistic controls that encourage the model to\ngenerate responses that are faithful to the provided evidence. We also\ninvestigate the usage of additional controls at decoding time using resampling\ntechniques. In addition to automatic metrics, we perform a human evaluation\nstudy where raters judge the output of these controlled generation models to be\ngenerally more objective and faithful to the evidence compared to baseline\ndialogue systems.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Training Techniques"] },
{"key": "rastogi2018multi", "citations": "65", "year": "2018", "title":"Multi-task Learning For Joint Language Understanding And Dialogue State Tracking", "abstract": "<p>This paper presents a novel approach for multi-task learning of language\nunderstanding (LU) and dialogue state tracking (DST) in task-oriented dialogue\nsystems. Multi-task training enables the sharing of the neural network layers\nresponsible for encoding the user utterance for both LU and DST and improves\nperformance while reducing the number of network parameters. In our proposed\nframework, DST operates on a set of candidate values for each slot that has\nbeen mentioned so far. These candidate sets are generated using LU slot\nannotations for the current user utterance, dialogue acts corresponding to the\npreceding system utterance and the dialogue state estimated for the previous\nturn, enabling DST to handle slots with a large or unbounded set of possible\nvalues and deal with slot values not seen during training. Furthermore, to\nbridge the gap between training and inference, we investigate the use of\nscheduled sampling on LU output for the current user utterance as well as the\nDST output for the preceding turn.</p>\n", "tags": ["Tools","Training Techniques"] },
{"key": "rastogi2019towards", "citations": "457", "year": "2020", "title":"Towards Scalable Multi-domain Conversational Agents: The Schema-guided Dialogue Dataset", "abstract": "<p>Virtual assistants such as Google Assistant, Alexa and Siri provide a\nconversational interface to a large number of services and APIs spanning\nmultiple domains. Such systems need to support an ever-increasing number of\nservices with possibly overlapping functionality. Furthermore, some of these\nservices have little to no training data available. Existing public datasets\nfor task-oriented dialogue do not sufficiently capture these challenges since\nthey cover few domains and assume a single static ontology per domain. In this\nwork, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing\nover 16k multi-domain conversations spanning 16 domains. Our dataset exceeds\nthe existing task-oriented dialogue corpora in scale, while also highlighting\nthe challenges associated with building large-scale virtual assistants. It\nprovides a challenging testbed for a number of tasks including language\nunderstanding, slot filling, dialogue state tracking and response generation.\nAlong the same lines, we present a schema-guided paradigm for task-oriented\ndialogue, in which predictions are made over a dynamic set of intents and\nslots, provided as input, using their natural language descriptions. This\nallows a single dialogue system to easily support a large number of services\nand facilitates simple integration of new services without requiring additional\ntraining data. Building upon the proposed paradigm, we release a model for\ndialogue state tracking capable of zero-shot generalization to new APIs, while\nremaining competitive in the regular setting.</p>\n", "tags": ["AAAI","Datasets","Dialogue & Multi Turn","Training Techniques"] },
{"key": "raunak2021curious", "citations": "91", "year": "2021", "title":"The Curious Case Of Hallucinations In Neural Machine Translation", "abstract": "<p>In this work, we study hallucinations in Neural Machine Translation (NMT),\nwhich lie at an extreme end on the spectrum of NMT pathologies. Firstly, we\nconnect the phenomenon of hallucinations under source perturbation to the\nLong-Tail theory of Feldman (2020), and present an empirically validated\nhypothesis that explains hallucinations under source perturbation. Secondly, we\nconsider hallucinations under corpus-level noise (without any source\nperturbation) and demonstrate that two prominent types of natural\nhallucinations (detached and oscillatory outputs) could be generated and\nexplained through specific corpus-level noise patterns. Finally, we elucidate\nthe phenomenon of hallucination amplification in popular data-generation\nprocesses such as Backtranslation and sequence-level Knowledge Distillation.</p>\n", "tags": ["Datasets","Efficiency","NAACL"] },
{"key": "ravaut2022summareranker", "citations": "60", "year": "2022", "title":"Summareranker: A Multi-task Mixture-of-experts Re-ranking Framework For Abstractive Summarization", "abstract": "<p>Sequence-to-sequence neural networks have recently achieved great success in\nabstractive summarization, especially through fine-tuning large pre-trained\nlanguage models on the downstream dataset. These models are typically decoded\nwith beam search to generate a unique summary. However, the search space is\nvery large, and with the exposure bias, such decoding is not optimal. In this\npaper, we show that it is possible to directly train a second-stage model\nperforming re-ranking on a set of summary candidates. Our mixture-of-experts\nSummaReranker learns to select a better candidate and consistently improves the\nperformance of the base model. With a base PEGASUS, we push ROUGE scores by\n5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34%\non Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and\ncheckpoints will be available at https://github.com/ntunlp/SummaReranker.</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Tools","Training Techniques"] },
{"key": "ravichander2019equate", "citations": "74", "year": "2019", "title":"EQUATE: A Benchmark Evaluation Framework For Quantitative Reasoning In Natural Language Inference", "abstract": "<p>Quantitative reasoning is a higher-order reasoning skill that any intelligent\nnatural language understanding system can reasonably be expected to handle. We\npresent EQUATE (Evaluating Quantitative Understanding Aptitude in Textual\nEntailment), a new framework for quantitative reasoning in textual entailment.\nWe benchmark the performance of 9 published NLI models on EQUATE, and find that\non average, state-of-the-art methods do not achieve an absolute improvement\nover a majority-class baseline, suggesting that they do not implicitly learn to\nreason with quantities. We establish a new baseline Q-REAS that manipulates\nquantities symbolically. In comparison to the best performing NLI model, it\nachieves success on numerical reasoning tests (+24.2%), but has limited verbal\nreasoning capabilities (-8.1%). We hope our evaluation framework will support\nthe development of models of quantitative reasoning in language understanding.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Tools"] },
{"key": "ravichander2019question", "citations": "89", "year": "2019", "title":"Question Answering For Privacy Policies: Combining Computational And Legal Perspectives", "abstract": "<p>Privacy policies are long and complex documents that are difficult for users\nto read and understand, and yet, they have legal effects on how user data is\ncollected, managed and used. Ideally, we would like to empower users to inform\nthemselves about issues that matter to them, and enable them to selectively\nexplore those issues. We present PrivacyQA, a corpus consisting of 1750\nquestions about the privacy policies of mobile applications, and over 3500\nexpert annotations of relevant answers. We observe that a strong neural\nbaseline underperforms human performance by almost 0.3 F1 on PrivacyQA,\nsuggesting considerable room for improvement for future systems. Further, we\nuse this dataset to shed light on challenges to question answerability, with\ndomain-general implications for any question answering system. The PrivacyQA\ncorpus offers a challenging corpus for question answering, with genuine\nreal-world utility.</p>\n", "tags": ["Applications","EMNLP","Privacy"] },
{"key": "ravichander2020probing", "citations": "67", "year": "2021", "title":"Probing The Probing Paradigm: Does Probing Accuracy Entail Task Relevance?", "abstract": "<p>Although neural models have achieved impressive results on several NLP\nbenchmarks, little is understood about the mechanisms they use to perform\nlanguage tasks. Thus, much recent attention has been devoted to analyzing the\nsentence representations learned by neural encoders, through the lens of\n`probing’ tasks. However, to what extent was the information encoded in\nsentence representations, as discovered through a probe, actually used by the\nmodel to perform its task? In this work, we examine this probing paradigm\nthrough a case study in Natural Language Inference, showing that models can\nlearn to encode linguistic properties even if they are not needed for the task\non which the model was trained. We further identify that pretrained word\nembeddings play a considerable role in encoding these properties rather than\nthe training task itself, highlighting the importance of careful controls when\ndesigning probing experiments. Finally, through a set of controlled synthetic\ntasks, we demonstrate models can encode these properties considerably above\nchance-level even when distributed in the data as random noise, calling into\nquestion the interpretation of absolute claims on probing tasks.</p>\n", "tags": ["EACL","Model Architecture","NAACL","Training Techniques"] },
{"key": "rawte2023survey", "citations": "63", "year": "2023", "title":"A Survey Of Hallucination In Large Foundation Models", "abstract": "<p>Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large’’ Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.</p>\n", "tags": ["Evaluation","Survey Paper"] },
{"key": "razniewski2021language", "citations": "1631", "year": "2019", "title":"Language Models As Or For Knowledge Bases", "abstract": "<p>Pre-trained language models (LMs) have recently gained attention for their\npotential as an alternative to (or proxy for) explicit knowledge bases (KBs).\nIn this position paper, we examine this hypothesis, identify strengths and\nlimitations of both LMs and KBs, and discuss the complementary nature of the\ntwo paradigms. In particular, we offer qualitative arguments that latent LMs\nare not suitable as a substitute for explicit KBs, but could play a major role\nfor augmenting and curating KBs.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "rebuffi2021fixing", "citations": "100", "year": "2021", "title":"Fixing Data Augmentation To Improve Adversarial Robustness", "abstract": "<p>Adversarial training suffers from robust overfitting, a phenomenon where the\nrobust test accuracy starts to decrease during training. In this paper, we\nfocus on both heuristics-driven and data-driven augmentations as a means to\nreduce robust overfitting. First, we demonstrate that, contrary to previous\nfindings, when combined with model weight averaging, data augmentation can\nsignificantly boost robust accuracy. Second, we explore how state-of-the-art\ngenerative models can be leveraged to artificially increase the size of the\ntraining set and further improve adversarial robustness. Finally, we evaluate\nour approach on CIFAR-10 against \\(\\ell_\\infty\\) and \\(ℓ₂\\) norm-bounded\nperturbations of size \\(\\epsilon = 8/255\\) and \\(\\epsilon = 128/255\\),\nrespectively. We show large absolute improvements of +7.06% and +5.88% in\nrobust accuracy compared to previous state-of-the-art methods. In particular,\nagainst \\(\\ell_\\infty\\) norm-bounded perturbations of size \\(\\epsilon = 8/255\\),\nour model reaches 64.20% robust accuracy without using any external data,\nbeating most prior works that use external data.</p>\n", "tags": ["Security","Training Techniques"] },
{"key": "reddy2018coqa", "citations": "949", "year": "2019", "title":"Coqa: A Conversational Question Answering Challenge", "abstract": "<p>Humans gather information by engaging in conversations involving a series of\ninterconnected questions and answers. For machines to assist in information\ngathering, it is therefore essential to enable them to answer conversational\nquestions. We introduce CoQA, a novel dataset for building Conversational\nQuestion Answering systems. Our dataset contains 127k questions with answers,\nobtained from 8k conversations about text passages from seven diverse domains.\nThe questions are conversational, and the answers are free-form text with their\ncorresponding evidence highlighted in the passage. We analyze CoQA in depth and\nshow that conversational questions have challenging phenomena not present in\nexisting reading comprehension datasets, e.g., coreference and pragmatic\nreasoning. We evaluate strong conversational and reading comprehension models\non CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points\nbehind human performance (88.8%), indicating there is ample room for\nimprovement. We launch CoQA as a challenge to the community at\nhttp://stanfordnlp.github.io/coqa/</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Has Code","TACL"] },
{"key": "reddy2018multi", "citations": "66", "year": "2019", "title":"Multi-level Memory For Task Oriented Dialogs", "abstract": "<p>Recent end-to-end task oriented dialog systems use memory architectures to\nincorporate external knowledge in their dialogs. Current work makes simplifying\nassumptions about the structure of the knowledge base, such as the use of\ntriples to represent knowledge, and combines dialog utterances (context) as\nwell as knowledge base (KB) results as part of the same memory. This causes an\nexplosion in the memory size, and makes the reasoning over memory harder. In\naddition, such a memory design forces hierarchical properties of the data to be\nfit into a triple structure of memory. This requires the memory reader to infer\nrelationships across otherwise connected attributes. In this paper we relax the\nstrong assumptions made by existing architectures and separate memories used\nfor modeling dialog context and KB results. Instead of using triples to store\nKB results, we introduce a novel multi-level memory architecture consisting of\ncells for each query and their corresponding results. The multi-level memory\nfirst addresses queries, followed by results and finally each key-value pair\nwithin a result. We conduct detailed experiments on three publicly available\ntask oriented dialog data sets and we find that our method conclusively\noutperforms current state-of-the-art models. We report a 15-25% increase in\nboth entity F1 and BLEU scores.</p>\n", "tags": ["Dialogue & Multi Turn","Memory & Context","Model Architecture"] },
{"key": "rei2020comet", "citations": "448", "year": "2020", "title":"COMET: A Neural Framework For MT Evaluation", "abstract": "<p>We present COMET, a neural framework for training multilingual machine\ntranslation evaluation models which obtains new state-of-the-art levels of\ncorrelation with human judgements. Our framework leverages recent breakthroughs\nin cross-lingual pretrained language modeling resulting in highly multilingual\nand adaptable MT evaluation models that exploit information from both the\nsource input and a target-language reference translation in order to more\naccurately predict MT quality. To showcase our framework, we train three models\nwith different types of human judgements: Direct Assessments, Human-mediated\nTranslation Edit Rate and Multidimensional Quality Metrics. Our models achieve\nnew state-of-the-art performance on the WMT 2019 Metrics shared task and\ndemonstrate robustness to high-performing systems.</p>\n", "tags": ["EMNLP","Evaluation","Tools"] },
{"key": "reif2021recipe", "citations": "77", "year": "2022", "title":"A Recipe For Arbitrary Text Style Transfer With Large Language Models", "abstract": "<p>In this paper, we leverage large language models (LMs) to perform zero-shot\ntext style transfer. We present a prompting method that we call augmented\nzero-shot learning, which frames style transfer as a sentence rewriting task\nand requires only a natural language instruction, without model fine-tuning or\nexemplars in the target style. Augmented zero-shot learning is simple and\ndemonstrates promising results not just on standard style transfer tasks such\nas sentiment, but also on arbitrary transformations such as “make this\nmelodramatic” or “insert a metaphor.”</p>\n", "tags": ["Fine-Tuning","Prompting","Training Techniques"] },
{"key": "reimers2019sentence", "citations": "7625", "year": "2019", "title":"Sentence-bert: Sentence Embeddings Using Siamese Bert-networks", "abstract": "<p>BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new\nstate-of-the-art performance on sentence-pair regression tasks like semantic\ntextual similarity (STS). However, it requires that both sentences are fed into\nthe network, which causes a massive computational overhead: Finding the most\nsimilar pair in a collection of 10,000 sentences requires about 50 million\ninference computations (~65 hours) with BERT. The construction of BERT makes it\nunsuitable for semantic similarity search as well as for unsupervised tasks\nlike clustering.\n  In this publication, we present Sentence-BERT (SBERT), a modification of the\npretrained BERT network that use siamese and triplet network structures to\nderive semantically meaningful sentence embeddings that can be compared using\ncosine-similarity. This reduces the effort for finding the most similar pair\nfrom 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while\nmaintaining the accuracy from BERT.\n  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning\ntasks, where it outperforms other state-of-the-art sentence embeddings methods.</p>\n", "tags": ["EMNLP","Fine-Tuning","Model Architecture"] },
{"key": "reimers2020making", "citations": "619", "year": "2020", "title":"Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation", "abstract": "<p>We present an easy and efficient method to extend existing sentence embedding\nmodels to new languages. This allows to create multilingual versions from\npreviously monolingual models. The training is based on the idea that a\ntranslated sentence should be mapped to the same location in the vector space\nas the original sentence. We use the original (monolingual) model to generate\nsentence embeddings for the source language and then train a new system on\ntranslated sentences to mimic the original model. Compared to other methods for\ntraining multilingual sentence embeddings, this approach has several\nadvantages: It is easy to extend existing models with relatively few samples to\nnew languages, it is easier to ensure desired properties for the vector space,\nand the hardware requirements for training is lower. We demonstrate the\neffectiveness of our approach for 50+ languages from various language families.\nCode to extend sentence embeddings models to more than 400 languages is\npublicly available.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "reiss2023testing", "citations": "67", "year": "2023", "title":"Testing The Reliability Of Chatgpt For Text Annotation And Classification: A Cautionary Remark", "abstract": "<p>Recent studies have demonstrated promising potential of ChatGPT for various\ntext annotation and classification tasks. However, ChatGPT is non-deterministic\nwhich means that, as with human coders, identical input can lead to different\noutputs. Given this, it seems appropriate to test the reliability of ChatGPT.\nTherefore, this study investigates the consistency of ChatGPT’s zero-shot\ncapabilities for text annotation and classification, focusing on different\nmodel parameters, prompt variations, and repetitions of identical inputs. Based\non the real-world classification task of differentiating website texts into\nnews and not news, results show that consistency in ChatGPT’s classification\noutput can fall short of scientific thresholds for reliability. For example,\neven minor wording alterations in prompts or repeating the identical input can\nlead to varying outputs. Although pooling outputs from multiple repetitions can\nimprove reliability, this study advises caution when using ChatGPT for\nzero-shot text annotation and underscores the need for thorough validation,\nsuch as comparison against human-annotated data. The unsupervised application\nof ChatGPT for text annotation and classification is not recommended.</p>\n", "tags": ["Prompting"] },
{"key": "ren2017deep", "citations": "303", "year": "2017", "title":"Deep Reinforcement Learning-based Image Captioning With Embedding Reward", "abstract": "<p>Image captioning is a challenging problem owing to the complexity in\nunderstanding the image content and diverse ways of describing it in natural\nlanguage. Recent advances in deep neural networks have substantially improved\nthe performance of this task. Most state-of-the-art approaches follow an\nencoder-decoder framework, which generates captions using a sequential\nrecurrent prediction model. However, in this paper, we introduce a novel\ndecision-making framework for image captioning. We utilize a “policy network”\nand a “value network” to collaboratively generate captions. The policy network\nserves as a local guidance by providing the confidence of predicting the next\nword according to the current state. Additionally, the value network serves as\na global and lookahead guidance by evaluating all possible extensions of the\ncurrent state. In essence, it adjusts the goal of predicting the correct words\ntowards the goal of generating captions similar to the ground truth captions.\nWe train both networks using an actor-critic reinforcement learning model, with\na novel reward defined by visual-semantic embedding. Extensive experiments and\nanalyses on the Microsoft COCO dataset show that the proposed framework\noutperforms state-of-the-art approaches across different evaluation metrics.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Reinforcement Learning","Tools"] },
{"key": "ren2018repeatnet", "citations": "237", "year": "2019", "title":"Repeatnet: A Repeat Aware Neural Recommendation Machine For Session-based Recommendation", "abstract": "<p>Recurrent neural networks for session-based recommendation have attracted a\nlot of attention recently because of their promising performance. repeat\nconsumption is a common phenomenon in many recommendation scenarios (e.g.,\ne-commerce, music, and TV program recommendations), where the same item is\nre-consumed repeatedly over time. However, no previous studies have emphasized\nrepeat consumption with neural networks. An effective neural approach is needed\nto decide when to perform repeat recommendation. In this paper, we incorporate\na repeat-explore mechanism into neural networks and propose a new model, called\nRepeatNet, with an encoder-decoder structure. RepeatNet integrates a regular\nneural recommendation approach in the decoder with a new repeat recommendation\nmechanism that can choose items from a user’s history and recommends them at\nthe right time. We report on extensive experiments on three benchmark datasets.\nRepeatNet outperforms state-of-the-art baselines on all three datasets in terms\nof MRR and Recall. Furthermore, as the dataset size and the repeat ratio\nincrease, the improvements of RepeatNet over the baselines also increase, which\ndemonstrates its advantage in handling repeat recommendation scenarios.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture"] },
{"key": "ren2019unsupervised", "citations": "62", "year": "2019", "title":"Unsupervised Neural Machine Translation With SMT As Posterior Regularization", "abstract": "<p>Without real bilingual corpus available, unsupervised Neural Machine\nTranslation (NMT) typically requires pseudo parallel data generated with the\nback-translation method for the model training. However, due to weak\nsupervision, the pseudo data inevitably contain noises and errors that will be\naccumulated and reinforced in the subsequent training process, leading to bad\ntranslation performance. To address this issue, we introduce phrase based\nStatistic Machine Translation (SMT) models which are robust to noisy data, as\nposterior regularizations to guide the training of unsupervised NMT models in\nthe iterative back-translation process. Our method starts from SMT models built\nwith pre-trained language models and word-level translation tables inferred\nfrom cross-lingual embeddings. Then SMT and NMT models are optimized jointly\nand boost each other incrementally in a unified EM framework. In this way, (1)\nthe negative effect caused by errors in the iterative back-translation process\ncan be alleviated timely by SMT filtering noises from its phrase tables;\nmeanwhile, (2) NMT can compensate for the deficiency of fluency inherent in\nSMT. Experiments conducted on en-fr and en-de translation tasks show that our\nmethod outperforms the strong baseline and achieves new state-of-the-art\nunsupervised machine translation performance.</p>\n", "tags": ["AAAI","Datasets","Tools","Training Techniques"] },
{"key": "ren2020sequential", "citations": "89", "year": "2020", "title":"Sequential Recommendation With Self-attentive Multi-adversarial Network", "abstract": "<p>Recently, deep learning has made significant progress in the task of\nsequential recommendation. Existing neural sequential recommenders typically\nadopt a generative way trained with Maximum Likelihood Estimation (MLE). When\ncontext information (called factor) is involved, it is difficult to analyze\nwhen and how each individual factor would affect the final recommendation\nperformance. For this purpose, we take a new perspective and introduce\nadversarial learning to sequential recommendation. In this paper, we present a\nMulti-Factor Generative Adversarial Network (MFGAN) for explicitly modeling the\neffect of context information on sequential recommendation. Specifically, our\nproposed MFGAN has two kinds of modules: a Transformer-based generator taking\nuser behavior sequences as input to recommend the possible next items, and\nmultiple factor-specific discriminators to evaluate the generated sub-sequence\nfrom the perspectives of different factors. To learn the parameters, we adopt\nthe classic policy gradient method, and utilize the reward signal of\ndiscriminators for guiding the learning of the generator. Our framework is\nflexible to incorporate multiple kinds of factor information, and is able to\ntrace how each factor contributes to the recommendation decision over time.\nExtensive experiments conducted on three real-world datasets demonstrate the\nsuperiority of our proposed model over the state-of-the-art methods, in terms\nof effectiveness and interpretability.</p>\n", "tags": ["Datasets","Model Architecture","Reinforcement Learning","SIGIR"] },
{"key": "ren2021rocketqav2", "citations": "133", "year": "2021", "title":"Rocketqav2: A Joint Training Method For Dense Passage Retrieval And Passage Re-ranking", "abstract": "<p>In various natural language processing tasks, passage retrieval and passage\nre-ranking are two key procedures in finding and ranking relevant information.\nSince both the two procedures contribute to the final performance, it is\nimportant to jointly optimize them in order to achieve mutual improvement. In\nthis paper, we propose a novel joint training approach for dense passage\nretrieval and passage re-ranking. A major contribution is that we introduce the\ndynamic listwise distillation, where we design a unified listwise training\napproach for both the retriever and the re-ranker. During the dynamic\ndistillation, the retriever and the re-ranker can be adaptively improved\naccording to each other’s relevance information. We also propose a hybrid data\naugmentation strategy to construct diverse training instances for listwise\ntraining approach. Extensive experiments show the effectiveness of our approach\non both MSMARCO and Natural Questions datasets. Our code is available at\nhttps://github.com/PaddlePaddle/RocketQA.</p>\n", "tags": ["EMNLP","Has Code","Retrieval Systems","Training Techniques"] },
{"key": "ren2023representation", "citations": "60", "year": "2024", "title":"Representation Learning With Large Language Models For Recommendation", "abstract": "<p>Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.</p>\n", "tags": ["Efficiency","Evaluation","Has Code","Model Architecture","Prompting","Tools"] },
{"key": "reynolds2021prompt", "citations": "540", "year": "2021", "title":"Prompt Programming For Large Language Models: Beyond The Few-shot Paradigm", "abstract": "<p>Prevailing methods for mapping large generative language models to supervised\ntasks may fail to sufficiently probe models’ novel capabilities. Using GPT-3 as\na case study, we show that 0-shot prompts can significantly outperform few-shot\nprompts. We suggest that the function of few-shot examples in these cases is\nbetter described as locating an already learned task rather than meta-learning.\nThis analysis motivates rethinking the role of prompts in controlling and\nevaluating powerful language models. In this work, we discuss methods of prompt\nprogramming, emphasizing the usefulness of considering prompts through the lens\nof natural language. We explore techniques for exploiting the capacity of\nnarratives and cultural anchors to encode nuanced intentions and techniques for\nencouraging deconstruction of a problem into components before producing a\nverdict. Informed by this more encompassing theory of prompt programming, we\nalso introduce the idea of a metaprompt that seeds the model to generate its\nown natural language prompts for a range of tasks. Finally, we discuss how\nthese more general methods of interacting with language models can be\nincorporated into existing and future benchmarks and practical applications.</p>\n", "tags": ["Applications","Few-Shot","Model Architecture","Prompting"] },
{"key": "ribeiro2019enhancing", "citations": "68", "year": "2019", "title":"Enhancing Amr-to-text Generation With Dual Graph Representations", "abstract": "<p>Generating text from graph-based data, such as Abstract Meaning\nRepresentation (AMR), is a challenging task due to the inherent difficulty in\nhow to properly encode the structure of a graph with labeled edges. To address\nthis difficulty, we propose a novel graph-to-sequence model that encodes\ndifferent but complementary perspectives of the structural information\ncontained in the AMR graph. The model learns parallel top-down and bottom-up\nrepresentations of nodes capturing contrasting views of the graph. We also\ninvestigate the use of different node message passing strategies, employing\ndifferent state-of-the-art graph encoders to compute node representations based\non incoming and outgoing perspectives. In our experiments, we demonstrate that\nthe dual graph representation leads to improvements in AMR-to-text generation,\nachieving state-of-the-art results on two AMR datasets.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "ribeiro2020beyond", "citations": "753", "year": "2020", "title":"Beyond Accuracy: Behavioral Testing Of NLP Models With Checklist", "abstract": "<p>Although measuring held-out accuracy has been the primary approach to\nevaluate generalization, it often overestimates the performance of NLP models,\nwhile alternative approaches for evaluating models either focus on individual\ntasks or on specific behaviors. Inspired by principles of behavioral testing in\nsoftware engineering, we introduce CheckList, a task-agnostic methodology for\ntesting NLP models. CheckList includes a matrix of general linguistic\ncapabilities and test types that facilitate comprehensive test ideation, as\nwell as a software tool to generate a large and diverse number of test cases\nquickly. We illustrate the utility of CheckList with tests for three tasks,\nidentifying critical failures in both commercial and state-of-art models. In a\nuser study, a team responsible for a commercial sentiment analysis model found\nnew and actionable bugs in an extensively tested model. In another user study,\nNLP practitioners with CheckList created twice as many tests, and found almost\nthree times as many bugs as users without it.</p>\n", "tags": ["Evaluation Frameworks","Evaluation"] },
{"key": "ribeiro2020investigating", "citations": "126", "year": "2021", "title":"Investigating Pretrained Language Models For Graph-to-text Generation", "abstract": "<p>Graph-to-text generation aims to generate fluent texts from graph-based data.\nIn this paper, we investigate two recently proposed pretrained language models\n(PLMs) and analyze the impact of different task-adaptive pretraining strategies\nfor PLMs in graph-to-text generation. We present a study across three graph\ndomains: meaning representations, Wikipedia knowledge graphs (KGs) and\nscientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art\nresults and that task-adaptive pretraining strategies improve their performance\neven further. In particular, we report new state-of-the-art BLEU scores of\n49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative\nimprovement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,\nwe identify possible reasons for the PLMs’ success on graph-to-text tasks. We\nfind evidence that their knowledge about true facts helps them perform well\neven when the input graph representation is reduced to a simple bag of node and\nedge labels.</p>\n", "tags": ["Datasets"] },
{"key": "ribeiro2020modeling", "citations": "64", "year": "2020", "title":"Modeling Global And Local Node Contexts For Text Generation From Knowledge Graphs", "abstract": "<p>Recent graph-to-text models generate text from graph-based data using either\nglobal or local aggregation to learn node representations. Global node encoding\nallows explicit communication between two distant nodes, thereby neglecting\ngraph topology as all nodes are directly connected. In contrast, local node\nencoding considers the relations between neighbor nodes capturing the graph\nstructure, but it can fail to capture long-range relations. In this work, we\ngather both encoding strategies, proposing novel neural models which encode an\ninput graph combining both global and local node contexts, in order to learn\nbetter contextualized node embeddings. In our experiments, we demonstrate that\nour approaches lead to significant improvements on two graph-to-text datasets\nachieving BLEU scores of 18.01 on AGENDA dataset, and 63.69 on the WebNLG\ndataset for seen categories, outperforming state-of-the-art models by 3.7 and\n3.1 points, respectively.</p>\n", "tags": ["Datasets","TACL"] },
{"key": "richardson2019probing", "citations": "137", "year": "2020", "title":"Probing Natural Language Inference Models Through Semantic Fragments", "abstract": "<p>Do state-of-the-art models for language understanding already have, or can\nthey easily learn, abilities such as boolean coordination, quantification,\nconditionals, comparatives, and monotonicity reasoning (i.e., reasoning about\nword substitutions in sentential contexts)? While such phenomena are involved\nin natural language inference (NLI) and go beyond basic linguistic\nunderstanding, it is unclear the extent to which they are captured in existing\nNLI benchmarks and effectively learned by models. To investigate this, we\npropose the use of semantic fragments—systematically generated datasets that\neach target a different semantic phenomenon—for probing, and efficiently\nimproving, such capabilities of linguistic models. This approach to creating\nchallenge datasets allows direct control over the semantic diversity and\ncomplexity of the targeted linguistic phenomena, and results in a more precise\ncharacterization of a model’s linguistic behavior. Our experiments, using a\nlibrary of 8 such semantic fragments, reveal two remarkable findings: (a)\nState-of-the-art models, including BERT, that are pre-trained on existing NLI\nbenchmark datasets perform poorly on these new fragments, even though the\nphenomena probed here are central to the NLI task. (b) On the other hand, with\nonly a few minutes of additional fine-tuning—with a carefully selected\nlearning rate and a novel variation of “inoculation”—a BERT-based model can\nmaster all of these logic and monotonicity fragments while retaining its\nperformance on established NLI benchmarks.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "rietzler2019adapt", "citations": "126", "year": "2019", "title":"Adapt Or Get Left Behind: Domain Adaptation Through BERT Language Model Finetuning For Aspect-target Sentiment Classification", "abstract": "<p>Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based\nSentiment Analysis (ABSA), which has many applications e.g. in e-commerce,\nwhere data and insights from reviews can be leveraged to create value for\nbusinesses and customers. Recently, deep transfer-learning methods have been\napplied successfully to a myriad of Natural Language Processing (NLP) tasks,\nincluding ATSC. Building on top of the prominent BERT language model, we\napproach ATSC using a two-step procedure: self-supervised domain-specific BERT\nlanguage model finetuning, followed by supervised task-specific finetuning. Our\nfindings on how to best exploit domain-specific language model finetuning\nenable us to produce new state-of-the-art performance on the SemEval 2014 Task\n4 restaurants dataset. In addition, to explore the real-world robustness of our\nmodels, we perform cross-domain evaluation. We show that a cross-domain adapted\nBERT language model performs significantly better than strong baseline models\nlike vanilla BERT-base and XLNet-base. Finally, we conduct a case study to\ninterpret model prediction errors.</p>\n", "tags": ["Applications","Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "rogers2020primer", "citations": "1280", "year": "2020", "title":"A Primer In Bertology: What We Know About How BERT Works", "abstract": "<p>Transformer-based models have pushed state of the art in many areas of NLP,\nbut our understanding of what is behind their success is still limited. This\npaper is the first survey of over 150 studies of the popular BERT model. We\nreview the current state of knowledge about how BERT works, what kind of\ninformation it learns and how it is represented, common modifications to its\ntraining objectives and architecture, the overparameterization issue and\napproaches to compression. We then outline directions for future research.</p>\n", "tags": ["Model Architecture","Survey Paper","TACL","Training Techniques"] },
{"key": "rogers2021qa", "citations": "117", "year": "2022", "title":"QA Dataset Explosion: A Taxonomy Of NLP Resources For Question Answering And Reading Comprehension", "abstract": "<p>Alongside huge volumes of research on deep learning models in NLP in the\nrecent years, there has been also much work on benchmark datasets needed to\ntrack modeling progress. Question answering and reading comprehension have been\nparticularly prolific in this regard, with over 80 new datasets appearing in\nthe past two years. This study is the largest survey of the field to date. We\nprovide an overview of the various formats and domains of the current\nresources, highlighting the current lacunae for future work. We further discuss\nthe current classifications of “skills” that question answering/reading\ncomprehension systems are supposed to acquire, and propose a new taxonomy. The\nsupplementary materials survey the current multilingual resources and\nmonolingual resources for languages other than English, and we discuss the\nimplications of over-focusing on English. The study is aimed at both\npractitioners looking for pointers to the wealth of existing data, and at\nresearchers working on new resources.</p>\n", "tags": ["Datasets","Evaluation","Survey Paper"] },
{"key": "rohrbach2018object", "citations": "254", "year": "2018", "title":"Object Hallucination In Image Captioning", "abstract": "<p>Despite continuously improving performance, contemporary image captioning\nmodels are prone to “hallucinating” objects that are not actually in a scene.\nOne problem is that standard metrics only measure similarity to ground truth\ncaptions and may not fully capture image relevance. In this work, we propose a\nnew image relevance metric to evaluate current models with veridical visual\nlabels and assess their rate of object hallucination. We analyze how captioning\nmodel architectures and learning objectives contribute to object hallucination,\nexplore when hallucination is likely due to image misclassification or language\npriors, and assess how well current sentence metrics capture object\nhallucination. We investigate these questions on the standard image captioning\nbenchmark, MSCOCO, using a diverse set of models. Our analysis yields several\ninteresting findings, including that models which score best on standard\nsentence metrics do not always have lower hallucination and that models which\nhallucinate more tend to make errors driven by language priors.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "roller2020recipes", "citations": "598", "year": "2021", "title":"Recipes For Building An Open-domain Chatbot", "abstract": "<p>Building open-domain chatbots is a challenging area for machine learning\nresearch. While prior work has shown that scaling neural models in the number\nof parameters and the size of the data they are trained on gives improved\nresults, we show that other ingredients are important for a high-performing\nchatbot. Good conversation requires a number of skills that an expert\nconversationalist blends in a seamless way: providing engaging talking points\nand listening to their partners, and displaying knowledge, empathy and\npersonality appropriately, while maintaining a consistent persona. We show that\nlarge scale models can learn these skills when given appropriate training data\nand choice of generation strategy. We build variants of these recipes with 90M,\n2.7B and 9.4B parameter models, and make our models and code publicly\navailable. Human evaluations show our best models are superior to existing\napproaches in multi-turn dialogue in terms of engagingness and humanness\nmeasurements. We then discuss the limitations of this work by analyzing failure\ncases of our models.</p>\n", "tags": ["Dialogue & Multi Turn","Training Techniques"] },
{"key": "rolnick2018experience", "citations": "355", "year": "2018", "title":"Experience Replay For Continual Learning", "abstract": "<p>Continual learning is the problem of learning new tasks or knowledge while\nprotecting old knowledge and ideally generalizing from old experience to learn\nnew tasks faster. Neural networks trained by stochastic gradient descent often\ndegrade on old tasks when trained successively on new tasks with different data\ndistributions. This phenomenon, referred to as catastrophic forgetting, is\nconsidered a major hurdle to learning with non-stationary data or sequences of\nnew tasks, and prevents networks from continually accumulating knowledge and\nskills. We examine this issue in the context of reinforcement learning, in a\nsetting where an agent is exposed to tasks in a sequence. Unlike most other\nwork, we do not provide an explicit indication to the model of task boundaries,\nwhich is the most general circumstance for a learning agent exposed to\ncontinuous experience. While various methods to counteract catastrophic\nforgetting have recently been proposed, we explore a straightforward, general,\nand seemingly overlooked solution - that of using experience replay buffers for\nall past events - with a mixture of on- and off-policy learning, leveraging\nbehavioral cloning. We show that this strategy can still learn new tasks\nquickly yet can substantially reduce catastrophic forgetting in both Atari and\nDMLab domains, even matching the performance of methods that require task\nidentities. When buffer storage is constrained, we confirm that a simple\nmechanism for randomly discarding data allows a limited size buffer to perform\nalmost as well as an unbounded one.</p>\n", "tags": ["Agentic","Reinforcement Learning","Training Techniques"] },
{"key": "romal2022lamda", "citations": "616", "year": "2022", "title":"Lamda: Language Models For Dialog Applications", "abstract": "<p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\nof Transformer-based neural language models specialized for dialog, which have\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\nand web text. While model scaling alone can improve quality, it shows less\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\nwith annotated data and enabling the model to consult external knowledge\nsources can lead to significant improvements towards the two key challenges of\nsafety and factual grounding. The first challenge, safety, involves ensuring\nthat the model’s responses are consistent with a set of human values, such as\npreventing harmful suggestions and unfair bias. We quantify safety using a\nmetric based on an illustrative set of human values, and we find that filtering\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\ncrowdworker-annotated data offers a promising approach to improving model\nsafety. The second challenge, factual grounding, involves enabling the model to\nconsult external knowledge sources, such as an information retrieval system, a\nlanguage translator, and a calculator. We quantify factuality using a\ngroundedness metric, and we find that our approach enables the model to\ngenerate responses grounded in known sources, rather than responses that merely\nsound plausible. Finally, we explore the use of LaMDA in the domains of\neducation and content recommendations, and analyze their helpfulness and role\nconsistency.</p>\n", "tags": ["Applications","Dialogue & Multi Turn","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "ronan2011natural", "citations": "5334", "year": "2011", "title":"Natural Language Processing (almost) From Scratch", "abstract": "<p>We propose a unified neural network architecture and learning algorithm that\ncan be applied to various natural language processing tasks including:\npart-of-speech tagging, chunking, named entity recognition, and semantic role\nlabeling. This versatility is achieved by trying to avoid task-specific\nengineering and therefore disregarding a lot of prior knowledge. Instead of\nexploiting man-made input features carefully optimized for each task, our\nsystem learns internal representations on the basis of vast amounts of mostly\nunlabeled training data. This work is then used as a basis for building a\nfreely available tagging system with good performance and minimal computational\nrequirements.</p>\n", "tags": ["Model Architecture","RAG","Training Techniques"] },
{"key": "rong2022towards", "citations": "80", "year": "2023", "title":"Towards Human-centered Explainable AI: A Survey Of User Studies For Model Explanations", "abstract": "<p>Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI\nresearch. A better understanding of the needs of XAI users, as well as\nhuman-centered evaluations of explainable models are both a necessity and a\nchallenge. In this paper, we explore how HCI and AI researchers conduct user\nstudies in XAI applications based on a systematic literature review. After\nidentifying and thoroughly analyzing 97core papers with human-based XAI\nevaluations over the past five years, we categorize them along the measured\ncharacteristics of explanatory methods, namely trust, understanding, usability,\nand human-AI collaboration performance. Our research shows that XAI is\nspreading more rapidly in certain application domains, such as recommender\nsystems than in others, but that user evaluations are still rather sparse and\nincorporate hardly any insights from cognitive or social sciences. Based on a\ncomprehensive discussion of best practices, i.e., common models, design\nchoices, and measures in user studies, we propose practical guidelines on\ndesigning and conducting user studies for XAI researchers and practitioners.\nLastly, this survey also highlights several open research directions,\nparticularly linking psychological science and human-centered XAI.</p>\n", "tags": ["Survey Paper"] },
{"key": "ross2020explaining", "citations": "79", "year": "2021", "title":"Explaining NLP Models Via Minimal Contrastive Editing (mice)", "abstract": "<p>Humans have been shown to give contrastive explanations, which explain why an\nobserved event happened rather than some other counterfactual event (the\ncontrast case). Despite the influential role that contrastivity plays in how\nhumans explain, this property is largely missing from current methods for\nexplaining NLP models. We present Minimal Contrastive Editing (MiCE), a method\nfor producing contrastive explanations of model predictions in the form of\nedits to inputs that change model outputs to the contrast case. Our experiments\nacross three tasks–binary sentiment classification, topic classification, and\nmultiple-choice question answering–show that MiCE is able to produce edits\nthat are not only contrastive, but also minimal and fluent, consistent with\nhuman contrastive edits. We demonstrate how MiCE edits can be used for two use\ncases in NLP system development–debugging incorrect model outputs and\nuncovering dataset artifacts–and thereby illustrate that producing contrastive\nexplanations is a promising research direction for model interpretability.</p>\n", "tags": ["Datasets"] },
{"key": "ross2022galactica", "citations": "212", "year": "2022", "title":"Galactica: A Large Language Model For Science", "abstract": "<p>Information overload is a major obstacle to scientific progress. The\nexplosive growth in scientific literature and data has made it ever harder to\ndiscover useful insights in a large mass of information. Today scientific\nknowledge is accessed through search engines, but they are unable to organize\nscientific knowledge alone. In this paper we introduce Galactica: a large\nlanguage model that can store, combine and reason about scientific knowledge.\nWe train on a large scientific corpus of papers, reference material, knowledge\nbases and many other sources. We outperform existing models on a range of\nscientific tasks. On technical knowledge probes such as LaTeX equations,\nGalactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also\nperforms well on reasoning, outperforming Chinchilla on mathematical MMLU by\n41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It\nalso sets a new state-of-the-art on downstream tasks such as PubMedQA and\nMedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general\ncorpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these\nresults demonstrate the potential for language models as a new interface for\nscience. We open source the model for the benefit of the scientific community.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Model Architecture"] },
{"key": "rossenbach2019generating", "citations": "66", "year": "2020", "title":"Generating Synthetic Audio Data For Attention-based Speech Recognition Systems", "abstract": "<p>Recent advances in text-to-speech (TTS) led to the development of flexible\nmulti-speaker end-to-end TTS systems. We extend state-of-the-art\nattention-based automatic speech recognition (ASR) systems with synthetic audio\ngenerated by a TTS system trained only on the ASR corpora itself. ASR and TTS\nsystems are built separately to show that text-only data can be used to enhance\nexisting end-to-end ASR systems without the necessity of parameter or\narchitecture changes. We compare our method with language model integration of\nthe same text data and with simple data augmentation methods like SpecAugment\nand show that performance improvements are mostly independent. We achieve\nimprovements of up to 33% relative in word-error-rate (WER) over a strong\nbaseline with data-augmentation in a low-resource environment\n(LibriSpeech-100h), closing the gap to a comparable oracle experiment by more\nthan 50%. We also show improvements of up to 5% relative WER over our most\nrecent ASR baseline on LibriSpeech-960h.</p>\n", "tags": ["ICASSP","Model Architecture"] },
{"key": "rothe2019leveraging", "citations": "384", "year": "2020", "title":"Leveraging Pre-trained Checkpoints For Sequence Generation Tasks", "abstract": "<p>Unsupervised pre-training of large neural models has recently revolutionized\nNatural Language Processing. By warm-starting from the publicly released\ncheckpoints, NLP practitioners have pushed the state-of-the-art on multiple\nbenchmarks while saving significant amounts of compute time. So far the focus\nhas been mainly on the Natural Language Understanding tasks. In this paper, we\ndemonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\ndeveloped a Transformer-based sequence-to-sequence model that is compatible\nwith publicly available pre-trained BERT, GPT-2 and RoBERTa checkpoints and\nconducted an extensive empirical study on the utility of initializing our\nmodel, both encoder and decoder, with these checkpoints. Our models result in\nnew state-of-the-art results on Machine Translation, Text Summarization,\nSentence Splitting, and Sentence Fusion.</p>\n", "tags": ["Model Architecture","TACL","Training Techniques"] },
{"key": "rothe2021simple", "citations": "99", "year": "2021", "title":"A Simple Recipe For Multilingual Grammatical Error Correction", "abstract": "<p>This paper presents a simple recipe to train state-of-the-art multilingual\nGrammatical Error Correction (GEC) models. We achieve this by first proposing a\nlanguage-agnostic method to generate a large number of synthetic examples. The\nsecond ingredient is to use large-scale multilingual language models (up to 11B\nparameters). Once fine-tuned on language-specific supervised sets we surpass\nthe previous state-of-the-art results on GEC benchmarks in four languages:\nEnglish, Czech, German and Russian. Having established a new set of baselines\nfor GEC, we make our results easily reproducible and accessible by releasing a\ncLang-8 dataset. It is produced by using our best model, which we call gT5, to\nclean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly\nsimplifies typical GEC training pipelines composed of multiple fine-tuning\nstages – we demonstrate that performing a single fine-tuning step on cLang-8\nwith the off-the-shelf language models yields further accuracy improvements\nover an already top-performing gT5 model for English.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "roy2019unsupervised", "citations": "68", "year": "2019", "title":"Unsupervised Paraphrasing Without Translation", "abstract": "<p>Paraphrasing exemplifies the ability to abstract semantic content from\nsurface forms. Recent work on automatic paraphrasing is dominated by methods\nleveraging Machine Translation (MT) as an intermediate step. This contrasts\nwith humans, who can paraphrase without being bilingual. This work proposes to\nlearn paraphrasing models from an unlabeled monolingual corpus only. To that\nend, we propose a residual variant of vector-quantized variational\nauto-encoder.\n  We compare with MT-based approaches on paraphrase identification, generation,\nand training augmentation. Monolingual paraphrasing outperforms unsupervised\ntranslation in all settings. Comparisons with supervised translation are more\nmixed: monolingual paraphrasing is interesting for identification and\naugmentation; supervised translation is superior for generation.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "roy2020efficient", "citations": "377", "year": "2021", "title":"Efficient Content-based Sparse Attention With Routing Transformers", "abstract": "<p>Self-attention has recently been adopted for a wide range of sequence\nmodeling problems. Despite its effectiveness, self-attention suffers from\nquadratic compute and memory requirements with respect to sequence length.\nSuccessful approaches to reduce this complexity focused on attending to local\nsliding windows or a small set of locations independent of content. Our work\nproposes to learn dynamic sparse attention patterns that avoid allocating\ncomputation and memory to attend to content unrelated to the query of interest.\nThis work builds upon two lines of research: it combines the modeling\nflexibility of prior work on content-based sparse attention with the efficiency\ngains from approaches based on local, temporal sparse attention. Our model, the\nRouting Transformer, endows self-attention with a sparse routing module based\non online k-means while reducing the overall complexity of attention to\n\\(O\\left(n^{1.5}d\\right)\\) from \\(O\\left(n^2d\\right)\\) for sequence length \\(n\\) and\nhidden dimension \\(d\\). We show that our model outperforms comparable sparse\nattention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity)\nas well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while\nusing fewer self-attention layers. Additionally, we set a new state-of-the-art\non the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with\na 22 layer Routing Transformer model trained on sequences of length 8192.</p>\n", "tags": ["Efficiency","Memory & Context","Model Architecture","TACL"] },
{"key": "rozière2023code", "citations": "259", "year": "2023", "title":"Code Llama: Open Foundation Models For Code", "abstract": "<p>We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are\ntrained on sequences of 16k tokens and show improvements on inputs with up to\n100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants\nsupport infilling based on surrounding content. Code Llama reaches\nstate-of-the-art performance among open models on several code benchmarks, with\nscores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code\nLlama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our\nmodels outperform every other publicly available model on MultiPL-E. We release\nCode Llama under a permissive license that allows for both research and\ncommercial use.</p>\n", "tags": ["Applications","Has Code","Llm For Code"] },
{"key": "ruan2021dae", "citations": "90", "year": "2021", "title":"DAE-GAN: Dynamic Aspect-aware GAN For Text-to-image Synthesis", "abstract": "<p>Text-to-image synthesis refers to generating an image from a given text\ndescription, the key goal of which lies in photo realism and semantic\nconsistency. Previous methods usually generate an initial image with sentence\nembedding and then refine it with fine-grained word embedding. Despite the\nsignificant progress, the ‘aspect’ information (e.g., red eyes) contained in\nthe text, referring to several words rather than a word that depicts ‘a\nparticular part or feature of something’, is often ignored, which is highly\nhelpful for synthesizing image details. How to make better utilization of\naspect information in text-to-image synthesis still remains an unresolved\nchallenge. To address this problem, in this paper, we propose a Dynamic\nAspect-awarE GAN (DAE-GAN) that represents text information comprehensively\nfrom multiple granularities, including sentence-level, word-level, and\naspect-level. Moreover, inspired by human learning behaviors, we develop a\nnovel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an\nAttended Global Refinement (AGR) module and an Aspect-aware Local Refinement\n(ALR) module are alternately employed. AGR utilizes word-level embedding to\nglobally enhance the previously generated image, while ALR dynamically employs\naspect-level embedding to refine image details from a local perspective.\nFinally, a corresponding matching loss function is designed to ensure the\ntext-image semantic consistency at different levels. Extensive experiments on\ntwo well-studied and publicly available datasets (i.e., CUB-200 and COCO)\ndemonstrate the superiority and rationality of our method.</p>\n", "tags": ["Datasets","ICCV"] },
{"key": "ruder2021xtreme", "citations": "100", "year": "2021", "title":"XTREME-R: Towards More Challenging And Nuanced Multilingual Evaluation", "abstract": "<p>Machine learning has brought striking advances in multilingual natural\nlanguage processing capabilities over the past year. For example, the latest\ntechniques have improved the state-of-the-art performance on the XTREME\nmultilingual benchmark by more than 13 points. While a sizeable gap to\nhuman-level performance remains, improvements have been easier to achieve in\nsome tasks than in others. This paper analyzes the current state of\ncross-lingual transfer learning and summarizes some lessons learned. In order\nto catalyze meaningful progress, we extend XTREME to XTREME-R, which consists\nof an improved set of ten natural language understanding tasks, including\nchallenging language-agnostic retrieval tasks, and covers 50 typologically\ndiverse languages. In addition, we provide a massively multilingual diagnostic\nsuite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities\nthrough an interactive public leaderboard to gain a better understanding of\nsuch models. The leaderboard and code for XTREME-R will be made available at\nhttps://sites.research.google/xtreme and\nhttps://github.com/google-research/xtreme respectively.</p>\n", "tags": ["EMNLP","Evaluation","Has Code"] },
{"key": "ruiz2022dreambooth", "citations": "1125", "year": "2023", "title":"Dreambooth: Fine Tuning Text-to-image Diffusion Models For Subject-driven Generation", "abstract": "<p>Large text-to-image models achieved a remarkable leap in the evolution of AI,\nenabling high-quality and diverse synthesis of images from a given text prompt.\nHowever, these models lack the ability to mimic the appearance of subjects in a\ngiven reference set and synthesize novel renditions of them in different\ncontexts. In this work, we present a new approach for “personalization” of\ntext-to-image diffusion models. Given as input just a few images of a subject,\nwe fine-tune a pretrained text-to-image model such that it learns to bind a\nunique identifier with that specific subject. Once the subject is embedded in\nthe output domain of the model, the unique identifier can be used to synthesize\nnovel photorealistic images of the subject contextualized in different scenes.\nBy leveraging the semantic prior embedded in the model with a new autogenous\nclass-specific prior preservation loss, our technique enables synthesizing the\nsubject in diverse scenes, poses, views and lighting conditions that do not\nappear in the reference images. We apply our technique to several\npreviously-unassailable tasks, including subject recontextualization,\ntext-guided view synthesis, and artistic rendering, all while preserving the\nsubject’s key features. We also provide a new dataset and evaluation protocol\nfor this new task of subject-driven generation. Project page:\nhttps://dreambooth.github.io/</p>\n", "tags": ["CVPR","Datasets","Evaluation","Has Code","Prompting"] },
{"key": "ruocco2017inter", "citations": "72", "year": "2017", "title":"Inter-session Modeling For Session-based Recommendation", "abstract": "<p>In recent years, research has been done on applying Recurrent Neural Networks\n(RNNs) as recommender systems. Results have been promising, especially in the\nsession-based setting where RNNs have been shown to outperform state-of-the-art\nmodels. In many of these experiments, the RNN could potentially improve the\nrecommendations by utilizing information about the user’s past sessions, in\naddition to its own interactions in the current session. A problem for\nsession-based recommendation, is how to produce accurate recommendations at the\nstart of a session, before the system has learned much about the user’s current\ninterests. We propose a novel approach that extends a RNN recommender to be\nable to process the user’s recent sessions, in order to improve\nrecommendations. This is done by using a second RNN to learn from recent\nsessions, and predict the user’s interest in the current session. By feeding\nthis information to the original RNN, it is able to improve its\nrecommendations. Our experiments on two different datasets show that the\nproposed approach can significantly improve recommendations throughout the\nsessions, compared to a single RNN working only on the current session. The\nproposed model especially improves recommendations at the start of sessions,\nand is therefore able to deal with the cold start problem within sessions.</p>\n", "tags": ["Datasets"] },
{"key": "rust2020how", "citations": "139", "year": "2021", "title":"How Good Is Your Tokenizer? On The Monolingual Performance Of Multilingual Language Models", "abstract": "<p>In this work, we provide a systematic and comprehensive empirical comparison\nof pretrained multilingual language models versus their monolingual\ncounterparts with regard to their monolingual task performance. We study a set\nof nine typologically diverse languages with readily available pretrained\nmonolingual models on a set of five diverse monolingual downstream tasks. We\nfirst aim to establish, via fair and controlled comparisons, if a gap between\nthe multilingual and the corresponding monolingual representation of that\nlanguage exists, and subsequently investigate the reason for any performance\ndifference. To disentangle conflating factors, we train new monolingual models\non the same data, with monolingually and multilingually trained tokenizers. We\nfind that while the pretraining data size is an important factor, a designated\nmonolingual tokenizer plays an equally important role in the downstream\nperformance. Our results show that languages that are adequately represented in\nthe multilingual model’s vocabulary exhibit negligible performance decreases\nover their monolingual counterparts. We further find that replacing the\noriginal multilingual tokenizer with the specialized monolingual tokenizer\nimproves the downstream performance of the multilingual model for almost every\ntask and language.</p>\n", "tags": [] },
{"key": "rylan2023are", "citations": "95", "year": "2023", "title":"Are Emergent Abilities Of Large Language Models A Mirage?", "abstract": "<p>Recent work claims that large language models display emergent abilities,\nabilities not present in smaller-scale models that are present in larger-scale\nmodels. What makes emergent abilities intriguing is two-fold: their sharpness,\ntransitioning seemingly instantaneously from not present to present, and their\nunpredictability, appearing at seemingly unforeseeable model scales. Here, we\npresent an alternative explanation for emergent abilities: that for a\nparticular task and model family, when analyzing fixed model outputs, emergent\nabilities appear due to the researcher’s choice of metric rather than due to\nfundamental changes in model behavior with scale. Specifically, nonlinear or\ndiscontinuous metrics produce apparent emergent abilities, whereas linear or\ncontinuous metrics produce smooth, continuous predictable changes in model\nperformance. We present our alternative explanation in a simple mathematical\nmodel, then test it in three complementary ways: we (1) make, test and confirm\nthree predictions on the effect of metric choice using the InstructGPT/GPT-3\nfamily on tasks with claimed emergent abilities; (2) make, test and confirm two\npredictions about metric choices in a meta-analysis of emergent abilities on\nBIG-Bench; and (3) show to choose metrics to produce never-before-seen\nseemingly emergent abilities in multiple vision tasks across diverse deep\nnetworks. Via all three analyses, we provide evidence that alleged emergent\nabilities evaporate with different metrics or with better statistics, and may\nnot be a fundamental property of scaling AI models.</p>\n", "tags": ["Emergent Abilities","Evaluation Frameworks","Evaluation","Model Architecture","Survey Paper"] },
{"key": "rücklé2020adapterdrop", "citations": "128", "year": "2021", "title":"Adapterdrop: On The Efficiency Of Adapters In Transformers", "abstract": "<p>Massively pre-trained transformer models are computationally expensive to\nfine-tune, slow for inference, and have large storage requirements. Recent\napproaches tackle these shortcomings by training smaller models, dynamically\nreducing the model size, and by training light-weight adapters. In this paper,\nwe propose AdapterDrop, removing adapters from lower transformer layers during\ntraining and inference, which incorporates concepts from all three directions.\nWe show that AdapterDrop can dynamically reduce the computational overhead when\nperforming inference over multiple tasks simultaneously, with minimal decrease\nin task performances. We further prune adapters from AdapterFusion, which\nimproves the inference efficiency while maintaining the task performances\nentirely.</p>\n", "tags": ["EMNLP","Efficiency","Model Architecture","Training Techniques"] },
{"key": "s2024text", "citations": "495", "year": "2009", "title":"Text To Speech Synthesis", "abstract": "<p>Text-to-speech (TTS) synthesis is a technology that converts written text\ninto spoken words, enabling a natural and accessible means of communication.\nThis abstract explores the key aspects of TTS synthesis, encompassing its\nunderlying technologies, applications, and implications for various sectors.\nThe technology utilizes advanced algorithms and linguistic models to convert\ntextual information into life like speech, allowing for enhanced user\nexperiences in diverse contexts such as accessibility tools, navigation\nsystems, and virtual assistants. The abstract delves into the challenges and\nadvancements in TTS synthesis, including considerations for naturalness,\nmultilingual support, and emotional expression in synthesized speech.</p>\n", "tags": ["Applications","Tools"] },
{"key": "sabour2021cem", "citations": "99", "year": "2022", "title":"CEM: Commonsense-aware Empathetic Response Generation", "abstract": "<p>A key trait of daily conversations between individuals is the ability to\nexpress empathy towards others, and exploring ways to implement empathy is a\ncrucial step towards human-like dialogue systems. Previous approaches on this\ntopic mainly focus on detecting and utilizing the user’s emotion for generating\nempathetic responses. However, since empathy includes both aspects of affection\nand cognition, we argue that in addition to identifying the user’s emotion,\ncognitive understanding of the user’s situation should also be considered. To\nthis end, we propose a novel approach for empathetic response generation, which\nleverages commonsense to draw more information about the user’s situation and\nuses this additional information to further enhance the empathy expression in\ngenerated responses. We evaluate our approach on EmpatheticDialogues, which is\na widely-used benchmark dataset for empathetic response generation. Empirical\nresults demonstrate that our approach outperforms the baseline models in both\nautomatic and human evaluations and can generate more informative and\nempathetic responses.</p>\n", "tags": ["AAAI","Datasets","Dialogue & Multi Turn","Evaluation"] },
{"key": "sachan2018parameter", "citations": "114", "year": "2018", "title":"Parameter Sharing Methods For Multilingual Self-attentional Translation Models", "abstract": "<p>In multilingual neural machine translation, it has been shown that sharing a\nsingle translation model between multiple languages can achieve competitive\nperformance, sometimes even leading to performance gains over bilingually\ntrained models. However, these improvements are not uniform; often multilingual\nparameter sharing results in a decrease in accuracy due to translation models\nnot being able to accommodate different languages in their limited parameter\nspace. In this work, we examine parameter sharing techniques that strike a\nhappy medium between full sharing and individual training, specifically\nfocusing on the self-attentional Transformer model. We find that the full\nparameter sharing approach leads to increases in BLEU scores mainly when the\ntarget languages are from a similar language family. However, even in the case\nwhere target languages are from different families where full parameter sharing\nleads to a noticeable drop in BLEU scores, our proposed methods for partial\nsharing of parameters can lead to substantial improvements in translation\naccuracy.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "sadasivan2023can", "citations": "120", "year": "2023", "title":"Can Ai-generated Text Be Reliably Detected?", "abstract": "<p>Large Language Models (LLMs) perform impressively well in various\napplications. However, the potential for misuse of these models in activities\nsuch as plagiarism, generating fake news, and spamming has raised concern about\ntheir responsible use. Consequently, the reliable detection of AI-generated\ntext has become a critical area of research. AI text detectors have shown to be\neffective under their specific settings. In this paper, we stress-test the\nrobustness of these AI text detectors in the presence of an attacker. We\nintroduce recursive paraphrasing attack to stress test a wide range of\ndetection schemes, including the ones using the watermarking as well as neural\nnetwork-based detectors, zero shot classifiers, and retrieval-based detectors.\nOur experiments conducted on passages, each approximately 300 tokens long,\nreveal the varying sensitivities of these detectors to our attacks. Our\nfindings indicate that while our recursive paraphrasing method can\nsignificantly reduce detection rates, it only slightly degrades text quality in\nmany cases, highlighting potential vulnerabilities in current detection systems\nin the presence of an attacker. Additionally, we investigate the susceptibility\nof watermarked LLMs to spoofing attacks aimed at misclassifying human-written\ntext as AI-generated. We demonstrate that an attacker can infer hidden AI text\nsignatures without white-box access to the detection method, potentially\nleading to reputational risks for LLM developers. Finally, we provide a\ntheoretical framework connecting the AUROC of the best possible detector to the\nTotal Variation distance between human and AI text distributions. This analysis\noffers insights into the fundamental challenges of reliable detection as\nlanguage models continue to advance. Our code is publicly available at\nhttps://github.com/vinusankars/Reliability-of-AI-text-detectors.</p>\n", "tags": ["Applications","Has Code","Security","Tools"] },
{"key": "saha2017towards", "citations": "108", "year": "2018", "title":"Towards Building Large Scale Multimodal Domain-aware Conversation Systems", "abstract": "<p>While multimodal conversation agents are gaining importance in several\ndomains such as retail, travel etc., deep learning research in this area has\nbeen limited primarily due to the lack of availability of large-scale, open\nchatlogs. To overcome this bottleneck, in this paper we introduce the task of\nmultimodal, domain-aware conversations, and propose the MMD benchmark dataset.\nThis dataset was gathered by working in close coordination with large number of\ndomain experts in the retail domain. These experts suggested various\nconversations flows and dialog states which are typically seen in multimodal\nconversations in the fashion domain. Keeping these flows and states in mind, we\ncreated a dataset consisting of over 150K conversation sessions between\nshoppers and sales agents, with the help of in-house annotators using a\nsemi-automated manually intense iterative process. With this dataset, we\npropose 5 new sub-tasks for multimodal conversations along with their\nevaluation methodology. We also propose two multimodal neural models in the\nencode-attend-decode paradigm and demonstrate their performance on two of the\nsub-tasks, namely text response generation and best image response selection.\nThese experiments serve to establish baseline performance and open new research\ndirections for each of these sub-tasks. Further, for each of the sub-tasks, we\npresent a `per-state evaluation’ of 9 most significant dialog states, which\nwould enable more focused research into understanding the challenges and\ncomplexities involved in each of these states.</p>\n", "tags": ["AAAI","Datasets","Evaluation"] },
{"key": "saha2018complex", "citations": "171", "year": "2018", "title":"Complex Sequential Question Answering: Towards Learning To Converse Over Linked Question Answer Pairs With A Knowledge Graph", "abstract": "<p>While conversing with chatbots, humans typically tend to ask many questions,\na significant portion of which can be answered by referring to large-scale\nknowledge graphs (KG). While Question Answering (QA) and dialog systems have\nbeen studied independently, there is a need to study them closely to evaluate\nsuch real-world scenarios faced by bots involving both these tasks. Towards\nthis end, we introduce the task of Complex Sequential QA which combines the two\ntasks of (i) answering factual questions through complex inferencing over a\nrealistic-sized KG of millions of entities, and (ii) learning to converse\nthrough a series of coherently linked QA pairs. Through a labor intensive\nsemi-automatic process, involving in-house and crowdsourced workers, we created\na dataset containing around 200K dialogs with a total of 1.6M turns. Further,\nunlike existing large scale QA datasets which contain simple questions that can\nbe answered from a single tuple, the questions in our dialogs require a larger\nsubgraph of the KG. Specifically, our dataset has questions which require\nlogical, quantitative, and comparative reasoning as well as their combinations.\nThis calls for models which can: (i) parse complex natural language questions,\n(ii) use conversation context to resolve coreferences and ellipsis in\nutterances, (iii) ask for clarifications for ambiguous queries, and finally\n(iv) retrieve relevant subgraphs of the KG to answer such questions. However,\nour experiments with a combination of state of the art dialog and QA models\nshow that they clearly do not achieve the above objectives and are inadequate\nfor dealing with such complex real world settings. We believe that this new\ndataset coupled with the limitations of existing models as reported in this\npaper should encourage further research in Complex Sequential QA.</p>\n", "tags": ["AAAI","Datasets","Dialogue & Multi Turn"] },
{"key": "saha2018duorc", "citations": "130", "year": "2018", "title":"Duorc: Towards Complex Language Understanding With Paraphrased Reading Comprehension", "abstract": "<p>We propose DuoRC, a novel dataset for Reading Comprehension (RC) that\nmotivates several new challenges for neural approaches in language\nunderstanding beyond those offered by existing RC datasets. DuoRC contains\n186,089 unique question-answer pairs created from a collection of 7680 pairs of\nmovie plots where each pair in the collection reflects two versions of the same\nmovie - one from Wikipedia and the other from IMDb - written by two different\nauthors. We asked crowdsourced workers to create questions from one version of\nthe plot and a different set of workers to extract or synthesize answers from\nthe other version. This unique characteristic of DuoRC where questions and\nanswers are created from different versions of a document narrating the same\nunderlying story, ensures by design, that there is very little lexical overlap\nbetween the questions created from one version and the segments containing the\nanswer in the other version. Further, since the two versions have different\nlevels of plot detail, narration style, vocabulary, etc., answering questions\nfrom the second version requires deeper language understanding and\nincorporating external background knowledge. Additionally, the narrative style\nof passages arising from movie plots (as opposed to typical descriptive\npassages in existing datasets) exhibits the need to perform complex reasoning\nover events across multiple sentences. Indeed, we observe that state-of-the-art\nneural RC models which have achieved near human performance on the SQuAD\ndataset, even when coupled with traditional NLP techniques to address the\nchallenges presented in DuoRC exhibit very poor performance (F1 score of 37.42%\non DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research\navenues wherein DuoRC could complement other RC datasets to explore novel\nneural approaches for studying language understanding.</p>\n", "tags": ["Datasets"] },
{"key": "saharia2020non", "citations": "120", "year": "2020", "title":"Non-autoregressive Machine Translation With Latent Alignments", "abstract": "<p>This paper presents two strong methods, CTC and Imputer, for\nnon-autoregressive machine translation that model latent alignments with\ndynamic programming. We revisit CTC for machine translation and demonstrate\nthat a simple CTC model can achieve state-of-the-art for single-step\nnon-autoregressive machine translation, contrary to what prior work indicates.\nIn addition, we adapt the Imputer model for non-autoregressive machine\ntranslation and demonstrate that Imputer with just 4 generation steps can match\nthe performance of an autoregressive Transformer baseline. Our latent alignment\nmodels are simpler than many existing non-autoregressive translation baselines;\nfor example, we do not require target length prediction or re-scoring with an\nautoregressive model. On the competitive WMT’14 En\\(\\rightarrow\\)De task, our CTC\nmodel achieves 25.7 BLEU with a single generation step, while Imputer achieves\n27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This\ncompares favourably to the autoregressive Transformer baseline at 27.8 BLEU.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "saharia2022photorealistic", "citations": "1488", "year": "2022", "title":"Photorealistic Text-to-image Diffusion Models With Deep Language Understanding", "abstract": "<p>We present Imagen, a text-to-image diffusion model with an unprecedented\ndegree of photorealism and a deep level of language understanding. Imagen\nbuilds on the power of large transformer language models in understanding text\nand hinges on the strength of diffusion models in high-fidelity image\ngeneration. Our key discovery is that generic large language models (e.g. T5),\npretrained on text-only corpora, are surprisingly effective at encoding text\nfor image synthesis: increasing the size of the language model in Imagen boosts\nboth sample fidelity and image-text alignment much more than increasing the\nsize of the image diffusion model. Imagen achieves a new state-of-the-art FID\nscore of 7.27 on the COCO dataset, without ever training on COCO, and human\nraters find Imagen samples to be on par with the COCO data itself in image-text\nalignment. To assess text-to-image models in greater depth, we introduce\nDrawBench, a comprehensive and challenging benchmark for text-to-image models.\nWith DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP,\nLatent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen\nover other models in side-by-side comparisons, both in terms of sample quality\nand image-text alignment. See https://imagen.research.google/ for an overview\nof the results.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "sahoo2024systematic", "citations": "87", "year": "2024", "title":"A Systematic Survey Of Prompt Engineering In Large Language Models: Techniques And Applications", "abstract": "<p>Prompt engineering has emerged as an indispensable technique for extending\nthe capabilities of large language models (LLMs) and vision-language models\n(VLMs). This approach leverages task-specific instructions, known as prompts,\nto enhance model efficacy without modifying the core model parameters. Rather\nthan updating the model parameters, prompts allow seamless integration of\npre-trained models into downstream tasks by eliciting desired model behaviors\nsolely based on the given prompt. Prompts can be natural language instructions\nthat provide context to guide the model or learned vector representations that\nactivate relevant knowledge. This burgeoning field has enabled success across\nvarious applications, from question-answering to commonsense reasoning.\nHowever, there remains a lack of systematic organization and understanding of\nthe diverse prompt engineering methods and techniques. This survey paper\naddresses the gap by providing a structured overview of recent advancements in\nprompt engineering, categorized by application area. For each prompting\napproach, we provide a summary detailing the prompting methodology, its\napplications, the models involved, and the datasets utilized. We also delve\ninto the strengths and limitations of each approach and include a taxonomy\ndiagram and table summarizing datasets, models, and critical points of each\nprompting technique. This systematic analysis enables a better understanding of\nthis rapidly developing field and facilitates future research by illuminating\nopen challenges and opportunities for prompt engineering.</p>\n", "tags": ["Applications","Datasets","Prompting","Survey Paper"] },
{"key": "saito2016dualnet", "citations": "60", "year": "2017", "title":"Dualnet: Domain-invariant Network For Visual Question Answering", "abstract": "<p>Visual question answering (VQA) task not only bridges the gap between images\nand language, but also requires that specific contents within the image are\nunderstood as indicated by linguistic context of the question, in order to\ngenerate the accurate answers. Thus, it is critical to build an efficient\nembedding of images and texts. We implement DualNet, which fully takes\nadvantage of discriminative power of both image and textual features by\nseparately performing two operations. Building an ensemble of DualNet further\nboosts the performance. Contrary to common belief, our method proved effective\nin both real images and abstract scenes, in spite of significantly different\nproperties of respective domain. Our method was able to outperform previous\nstate-of-the-art methods in real images category even without explicitly\nemploying attention mechanism, and also outperformed our own state-of-the-art\nmethod in abstract scenes category, which recently won the first place in VQA\nChallenge 2016.</p>\n", "tags": ["Model Architecture"] },
{"key": "sakaguchi2019winogrande", "citations": "404", "year": "2020", "title":"Winogrande: An Adversarial Winograd Schema Challenge At Scale", "abstract": "<p>The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011),\na benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun\nresolution problems originally designed to be unsolvable for statistical models\nthat rely on selectional preferences or word associations. However, recent\nadvances in neural language models have already reached around 90% accuracy on\nvariants of WSC. This raises an important question whether these models have\ntruly acquired robust commonsense capabilities or whether they rely on spurious\nbiases in the datasets that lead to an overestimation of the true capabilities\nof machine commonsense. To investigate this question, we introduce WinoGrande,\na large-scale dataset of 44k problems, inspired by the original WSC design, but\nadjusted to improve both the scale and the hardness of the dataset. The key\nsteps of the dataset construction consist of (1) a carefully designed\ncrowdsourcing procedure, followed by (2) systematic bias reduction using a\nnovel AfLite algorithm that generalizes human-detectable word associations to\nmachine-detectable embedding associations. The best state-of-the-art methods on\nWinoGrande achieve 59.4-79.1%, which are 15-35% below human performance of\n94.0%, depending on the amount of the training data allowed. Furthermore, we\nestablish new state-of-the-art results on five related benchmarks - WSC\n(90.1%), DPR (93.1%), COPA (90.6%), KnowRef (85.6%), and Winogender (97.1%).\nThese results have dual implications: on one hand, they demonstrate the\neffectiveness of WinoGrande when used as a resource for transfer learning. On\nthe other hand, they raise a concern that we are likely to be overestimating\nthe true capabilities of machine commonsense across all these benchmarks. We\nemphasize the importance of algorithmic bias reduction in existing and future\nbenchmarks to mitigate such overestimation.</p>\n", "tags": ["AAAI","Datasets","Ethics & Fairness","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "salazar2019masked", "citations": "325", "year": "2020", "title":"Masked Language Model Scoring", "abstract": "<p>Pretrained masked language models (MLMs) require finetuning for most NLP\ntasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood\nscores (PLLs), which are computed by masking tokens one by one. We show that\nPLLs outperform scores from autoregressive language models like GPT-2 in a\nvariety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an\nend-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on\nstate-of-the-art baselines for low-resource translation pairs, with further\ngains from domain adaptation. We attribute this success to PLL’s unsupervised\nexpression of linguistic acceptability without a left-to-right bias, greatly\nimproving on scores from GPT-2 (+10 points on island effects, NPI licensing in\nBLiMP). One can finetune MLMs to give scores without masking, enabling\ncomputation in a single inference pass. In all, PLLs and their associated\npseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of\npretrained MLMs; e.g., we use a single cross-lingual model to rescore\ntranslations in multiple languages. We release our library for language model\nscoring at https://github.com/awslabs/mlm-scoring.</p>\n", "tags": ["Ethics & Fairness","Fine-Tuning","Has Code","Model Architecture","Tools"] },
{"key": "salesky2021multilingual", "citations": "75", "year": "2021", "title":"The Multilingual Tedx Corpus For Speech Recognition And Translation", "abstract": "<p>We present the Multilingual TEDx corpus, built to support speech recognition\n(ASR) and speech translation (ST) research across many non-English source\nlanguages. The corpus is a collection of audio recordings from TEDx talks in 8\nsource languages. We segment transcripts into sentences and align them to the\nsource-language audio and target-language translations. The corpus is released\nalong with open-sourced code enabling extension to new talks and languages as\nthey become available. Our corpus creation methodology can be applied to more\nlanguages than previous work, and creates multi-way parallel evaluation sets.\nWe provide baselines in multiple ASR and ST settings, including multilingual\nmodels to improve translation performance for low-resource language pairs.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH"] },
{"key": "sanh2021multitask", "citations": "468", "year": "2021", "title":"Multitask Prompted Training Enables Zero-shot Task Generalization", "abstract": "<p>Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models’ pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Evaluation","Has Code","Training Techniques"] },
{"key": "sankar2019do", "citations": "115", "year": "2019", "title":"Do Neural Dialog Systems Use The Conversation History Effectively? An Empirical Study", "abstract": "<p>Neural generative models have been become increasingly popular when building\nconversational agents. They offer flexibility, can be easily adapted to new\ndomains, and require minimal domain engineering. A common criticism of these\nsystems is that they seldom understand or use the available dialog history\neffectively. In this paper, we take an empirical approach to understanding how\nthese models use the available dialog history by studying the sensitivity of\nthe models to artificially introduced unnatural changes or perturbations to\ntheir context at test time. We experiment with 10 different types of\nperturbations on 4 multi-turn dialog datasets and find that commonly used\nneural dialog architectures like recurrent and transformer-based seq2seq models\nare rarely sensitive to most perturbations such as missing or reordering\nutterances, shuffling words, etc. Also, by open-sourcing our code, we believe\nthat it will serve as a useful diagnostic tool for evaluating dialog systems in\nthe future.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "santhanam2021colbertv2", "citations": "163", "year": "2022", "title":"Colbertv2: Effective And Efficient Retrieval Via Lightweight Late Interaction", "abstract": "<p>Neural information retrieval (IR) has greatly advanced search and other\nknowledge-intensive language tasks. While many neural IR methods encode queries\nand documents into single-vector representations, late interaction models\nproduce multi-vector representations at the granularity of each token and\ndecompose relevance modeling into scalable token-level computations. This\ndecomposition has been shown to make late interaction more effective, but it\ninflates the space footprint of these models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\ncompression mechanism with a denoised supervision strategy to simultaneously\nimprove the quality and space footprint of late interaction. We evaluate\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\nquality within and outside the training domain while reducing the space\nfootprint of late interaction models by 6–10\\(\\times\\).</p>\n", "tags": ["NAACL","Retrieval Systems","Training Techniques"] },
{"key": "santoro2018relational", "citations": "139", "year": "2018", "title":"Relational Recurrent Neural Networks", "abstract": "<p>Memory-based neural networks model temporal data by leveraging an ability to\nremember information for long periods. It is unclear, however, whether they\nalso have an ability to perform complex relational reasoning with the\ninformation they remember. Here, we first confirm our intuitions that standard\nmemory architectures may struggle at tasks that heavily involve an\nunderstanding of the ways in which entities are connected – i.e., tasks\ninvolving relational reasoning. We then improve upon these deficits by using a\nnew memory module – a \\textit{Relational Memory Core} (RMC) – which employs\nmulti-head dot product attention to allow memories to interact. Finally, we\ntest the RMC on a suite of tasks that may profit from more capable relational\nreasoning across sequential information, and show large gains in RL domains\n(e.g. Mini PacMan), program evaluation, and language modeling, achieving\nstate-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord\ndatasets.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Reinforcement Learning","Time Series"] },
{"key": "santos2016attentive", "citations": "341", "year": "2016", "title":"Attentive Pooling Networks", "abstract": "<p>In this work, we propose Attentive Pooling (AP), a two-way attention\nmechanism for discriminative model training. In the context of pair-wise\nranking or classification with neural networks, AP enables the pooling layer to\nbe aware of the current input pair, in a way that information from the two\ninput items can directly influence the computation of each other’s\nrepresentations. Along with such representations of the paired inputs, AP\njointly learns a similarity measure over projected segments (e.g. trigrams) of\nthe pair, and subsequently, derives the corresponding attention vector for each\ninput to guide the pooling. Our two-way attention mechanism is a general\nframework independent of the underlying representation learning, and it has\nbeen applied to both convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) in our studies. The empirical results, from three very\ndifferent benchmark tasks of question answering/answer selection, demonstrate\nthat our proposed models outperform a variety of strong baselines and achieve\nstate-of-the-art performance in all the benchmarks.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "santurkar2023whose", "citations": "73", "year": "2023", "title":"Whose Opinions Do Language Models Reflect?", "abstract": "<p>Language models (LMs) are increasingly being used in open-ended contexts,\nwhere the opinions reflected by LMs in response to subjective queries can have\na profound impact, both on user satisfaction, as well as shaping the views of\nsociety at large. In this work, we put forth a quantitative framework to\ninvestigate the opinions reflected by LMs – by leveraging high-quality public\nopinion polls and their associated human responses. Using this framework, we\ncreate OpinionsQA, a new dataset for evaluating the alignment of LM opinions\nwith those of 60 US demographic groups over topics ranging from abortion to\nautomation. Across topics, we find substantial misalignment between the views\nreflected by current LMs and those of US demographic groups: on par with the\nDemocrat-Republican divide on climate change. Notably, this misalignment\npersists even after explicitly steering the LMs towards particular demographic\ngroups. Our analysis not only confirms prior observations about the\nleft-leaning tendencies of some human feedback-tuned LMs, but also surfaces\ngroups whose opinions are poorly reflected by current LMs (e.g., 65+ and\nwidowed individuals). Our code and data are available at\nhttps://github.com/tatsu-lab/opinions_qa.</p>\n", "tags": ["Datasets","Has Code","Tools"] },
{"key": "sap2018atomic", "citations": "720", "year": "2019", "title":"ATOMIC: An Atlas Of Machine Commonsense For If-then Reasoning", "abstract": "<p>We present ATOMIC, an atlas of everyday commonsense reasoning, organized\nthrough 877k textual descriptions of inferential knowledge. Compared to\nexisting resources that center around taxonomic knowledge, ATOMIC focuses on\ninferential knowledge organized as typed if-then relations with variables\n(e.g., “if X pays Y a compliment, then Y will likely return the compliment”).\nWe propose nine if-then relation types to distinguish causes vs. effects,\nagents vs. themes, voluntary vs. involuntary events, and actions vs. mental\nstates. By generatively training on the rich inferential knowledge described in\nATOMIC, we show that neural models can acquire simple commonsense capabilities\nand reason about previously unseen events. Experimental results demonstrate\nthat multitask models that incorporate the hierarchical structure of if-then\nrelation types lead to more accurate inference compared to models trained in\nisolation, as measured by both automatic and human evaluation.</p>\n", "tags": ["AAAI","Evaluation","Training Techniques"] },
{"key": "sap2022neural", "citations": "70", "year": "2022", "title":"Neural Theory-of-mind? On The Limits Of Social Intelligence In Large Lms", "abstract": "<p>Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today’s largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models’ ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n  In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "sariyildiz2020learning", "citations": "99", "year": "2020", "title":"Learning Visual Representations With Caption Annotations", "abstract": "<p>Pretraining general-purpose visual features has become a crucial part of\ntackling many computer vision tasks. While one can learn such features on the\nextensively-annotated ImageNet dataset, recent approaches have looked at ways\nto allow for noisy, fewer, or even no annotations to perform such pretraining.\nStarting from the observation that captioned images are easily crawlable, we\nargue that this overlooked source of information can be exploited to supervise\nthe training of visual representations. To do so, motivated by the recent\nprogresses in language models, we introduce {\\em image-conditioned masked\nlanguage modeling} (ICMLM) – a proxy task to learn visual representations over\nimage-caption pairs. ICMLM consists in predicting masked words in captions by\nrelying on visual cues. To tackle this task, we propose hybrid models, with\ndedicated visual and textual encoders, and we show that the visual\nrepresentations learned as a by-product of solving this task transfer well to a\nvariety of target tasks. Our experiments confirm that image captions can be\nleveraged to inject global and localized semantic information into visual\nrepresentations. Project website: https://europe.naverlabs.com/icmlm.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "sarsa2022automatic", "citations": "299", "year": "2022", "title":"Automatic Generation Of Programming Exercises And Code Explanations Using Large Language Models", "abstract": "<p>This article explores the natural language generation capabilities of large\nlanguage models with application to the production of two types of learning\nresources common in programming courses. Using OpenAI Codex as the large\nlanguage model, we create programming exercises (including sample solutions and\ntest cases) and code explanations, assessing these qualitatively and\nquantitatively. Our results suggest that the majority of the automatically\ngenerated content is both novel and sensible, and in some cases ready to use as\nis. When creating exercises we find that it is remarkably easy to influence\nboth the programming concepts and the contextual themes they contain, simply by\nsupplying keywords as input to the model. Our analysis suggests that there is\nsignificant value in massive generative machine learning models as a tool for\ninstructors, although there remains a need for some oversight to ensure the\nquality of the generated content before it is delivered to students. We further\ndiscuss the implications of OpenAI Codex and similar tools for introductory\nprogramming education and highlight future research streams that have the\npotential to improve the quality of the educational experience for both\nteachers and students alike.</p>\n", "tags": ["Tools"] },
{"key": "saunders2020progressive", "citations": "84", "year": "2020", "title":"Progressive Transformers For End-to-end Sign Language Production", "abstract": "<p>The goal of automatic Sign Language Production (SLP) is to translate spoken\nlanguage to a continuous stream of sign language video at a level comparable to\na human translator. If this was achievable, then it would revolutionise Deaf\nhearing communications. Previous work on predominantly isolated SLP has shown\nthe need for architectures that are better suited to the continuous domain of\nfull sign sequences.\n  In this paper, we propose Progressive Transformers, a novel architecture that\ncan translate from discrete spoken language sentences to continuous 3D skeleton\npose outputs representing sign language. We present two model configurations,\nan end-to-end network that produces sign direct from text and a stacked network\nthat utilises a gloss intermediary.\n  Our transformer network architecture introduces a counter that enables\ncontinuous sequence generation at training and inference. We also provide\nseveral data augmentation processes to overcome the problem of drift and\nimprove the performance of SLP models. We propose a back translation evaluation\nmechanism for SLP, presenting benchmark quantitative results on the challenging\nRWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future\nresearch.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "saunders2020reducing", "citations": "107", "year": "2020", "title":"Reducing Gender Bias In Neural Machine Translation As A Domain Adaptation Problem", "abstract": "<p>Training data for NLP tasks often exhibits gender bias in that fewer\nsentences refer to women than to men. In Neural Machine Translation (NMT)\ngender bias has been shown to reduce translation quality, particularly when the\ntarget language has grammatical gender. The recent WinoMT challenge set allows\nus to measure this effect directly (Stanovsky et al, 2019).\n  Ideally we would reduce system bias by simply debiasing all data prior to\ntraining, but achieving this effectively is itself a challenge. Rather than\nattempt to create a <code class=\"language-plaintext highlighter-rouge\">balanced' dataset, we use transfer learning on a small set\nof trusted, gender-balanced examples. This approach gives strong and consistent\nimprovements in gender debiasing with much less computational cost than\ntraining from scratch.\n  A known pitfall of transfer learning on new domains is </code>catastrophic\nforgetting’, which we address both in adaptation and in inference. During\nadaptation we show that Elastic Weight Consolidation allows a performance\ntrade-off between general translation quality and bias reduction. During\ninference we propose a lattice-rescoring scheme which outperforms all systems\nevaluated in Stanovsky et al (2019) on WinoMT with no degradation of general\ntest set BLEU, and we show this scheme can be applied to remove gender bias in\nthe output of <code class=\"language-plaintext highlighter-rouge\">black box</code> online commercial MT systems. We demonstrate our\napproach translating from English into three languages with varied linguistic\nproperties and data availability.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "savage2023diagnostic", "citations": "102", "year": "2024", "title":"Diagnostic Reasoning Prompts Reveal The Potential For Large Language Model Interpretability In Medicine", "abstract": "<p>One of the major barriers to using large language models (LLMs) in medicine\nis the perception they use uninterpretable methods to make clinical decisions\nthat are inherently different from the cognitive processes of clinicians. In\nthis manuscript we develop novel diagnostic reasoning prompts to study whether\nLLMs can perform clinical reasoning to accurately form a diagnosis. We find\nthat GPT4 can be prompted to mimic the common clinical reasoning processes of\nclinicians without sacrificing diagnostic accuracy. This is significant because\nan LLM that can use clinical reasoning to provide an interpretable rationale\noffers physicians a means to evaluate whether LLMs can be trusted for patient\ncare. Novel prompting methods have the potential to expose the black box of\nLLMs, bringing them one step closer to safe and effective use in medicine.</p>\n", "tags": ["Prompting"] },
{"key": "savelka2023can", "citations": "82", "year": "2023", "title":"Can GPT-4 Support Analysis Of Textual Data In Tasks Requiring Highly Specialized Domain Expertise?", "abstract": "<p>We evaluated the capability of generative pre-trained transformers~(GPT-4) in\nanalysis of textual data in tasks that require highly specialized domain\nexpertise. Specifically, we focused on the task of analyzing court opinions to\ninterpret legal concepts. We found that GPT-4, prompted with annotation\nguidelines, performs on par with well-trained law student annotators. We\nobserved that, with a relatively minor decrease in performance, GPT-4 can\nperform batch predictions leading to significant cost reductions. However,\nemploying chain-of-thought prompting did not lead to noticeably improved\nperformance on this task. Further, we demonstrated how to analyze GPT-4’s\npredictions to identify and mitigate deficiencies in annotation guidelines, and\nsubsequently improve the performance of the model. Finally, we observed that\nthe model is quite brittle, as small formatting related changes in the prompt\nhad a high impact on the predictions. These findings can be leveraged by\nresearchers and practitioners who engage in semantic/pragmatic annotations of\ntexts in the context of the tasks requiring highly specialized domain\nexpertise.</p>\n", "tags": ["Applications","Model Architecture","Prompting"] },
{"key": "savelka2023thrilled", "citations": "88", "year": "2023", "title":"Thrilled By Your Progress! Large Language Models (GPT-4) No Longer Struggle To Pass Assessments In Higher Education Programming Courses", "abstract": "<p>This paper studies recent developments in large language models’ (LLM)\nabilities to pass assessments in introductory and intermediate Python\nprogramming courses at the postsecondary level. The emergence of ChatGPT\nresulted in heated debates of its potential uses (e.g., exercise generation,\ncode explanation) as well as misuses in programming classes (e.g., cheating).\nRecent studies show that while the technology performs surprisingly well on\ndiverse sets of assessment instruments employed in typical programming classes\nthe performance is usually not sufficient to pass the courses. The release of\nGPT-4 largely emphasized notable improvements in the capabilities related to\nhandling assessments originally designed for human test-takers. This study is\nthe necessary analysis in the context of this ongoing transition towards mature\ngenerative AI systems. Specifically, we report the performance of GPT-4,\ncomparing it to the previous generations of GPT models, on three Python courses\nwith assessments ranging from simple multiple-choice questions (no code\ninvolved) to complex programming projects with code bases distributed into\nmultiple files (599 exercises overall). Additionally, we analyze the\nassessments that were not handled well by GPT-4 to understand the current\nlimitations of the model, as well as its capabilities to leverage feedback\nprovided by an auto-grader. We found that the GPT models evolved from\ncompletely failing the typical programming class’ assessments (the original\nGPT-3) to confidently passing the courses with no human involvement (GPT-4).\nWhile we identified certain limitations in GPT-4’s handling of MCQs and coding\nexercises, the rate of improvement across the recent generations of GPT models\nstrongly suggests their potential to handle almost any type of assessment\nwidely used in higher education programming courses. These findings could be\nleveraged by educators and institutions to adapt the design of programming\nassessments as well as to fuel the necessary discussions into how programming\nclasses should be updated to reflect the recent technological developments.\nThis study provides evidence that programming instructors need to prepare for a\nworld in which there is an easy-to-use widely accessible technology that can be\nutilized by learners to collect passing scores, with no effort whatsoever, on\nwhat today counts as viable programming knowledge and skills assessments.</p>\n", "tags": ["Model Architecture"] },
{"key": "savoldi2021gender", "citations": "126", "year": "2021", "title":"Gender Bias In Machine Translation", "abstract": "<p>Machine translation (MT) technology has facilitated our daily tasks by\nproviding accessible shortcuts for gathering, elaborating and communicating\ninformation. However, it can suffer from biases that harm users and society at\nlarge. As a relatively new field of inquiry, gender bias in MT still lacks\ninternal cohesion, which advocates for a unified framework to ease future\nresearch. To this end, we: i) critically review current conceptualizations of\nbias in light of theoretical insights from related disciplines, ii) summarize\nprevious analyses aimed at assessing gender bias in MT, iii) discuss the\nmitigating strategies proposed so far, and iv) point toward potential\ndirections for future work.</p>\n", "tags": ["Ethics & Fairness","TACL"] },
{"key": "saxena2022sequence", "citations": "84", "year": "2022", "title":"Sequence-to-sequence Knowledge Graph Completion And Question Answering", "abstract": "<p>Knowledge graph embedding (KGE) models represent each entity and relation of\na knowledge graph (KG) with low-dimensional embedding vectors. These methods\nhave recently been applied to KG link prediction and question answering over\nincomplete KGs (KGQA). KGEs typically create an embedding for each entity in\nthe graph, which results in large model sizes on real-world graphs with\nmillions of entities. For downstream tasks these atomic entity representations\noften need to be integrated into a multi stage pipeline, limiting their\nutility. We show that an off-the-shelf encoder-decoder Transformer model can\nserve as a scalable and versatile KGE model obtaining state-of-the-art results\nfor KG link prediction and incomplete KG question answering. We achieve this by\nposing KG link prediction as a sequence-to-sequence task and exchange the\ntriple scoring approach taken by prior KGE methods with autoregressive\ndecoding. Such a simple but powerful method reduces the model size up to 98%\ncompared to conventional KGE models while keeping inference time tractable.\nAfter finetuning this model on the task of KGQA over incomplete KGs, our\napproach outperforms baselines on multiple large-scale datasets without\nextensive hyperparameter tuning.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "scao2021how", "citations": "165", "year": "2021", "title":"How Many Data Points Is A Prompt Worth?", "abstract": "<p>When fine-tuning pretrained models for classification, researchers either use\na generic model head or a task-specific prompt for prediction. Proponents of\nprompting have argued that prompts provide a method for injecting task-specific\nguidance, which is beneficial in low-data regimes. We aim to quantify this\nbenefit through rigorous testing of prompts in a fair setting: comparing\nprompted and head-based fine-tuning in equal conditions across many tasks and\ndata sizes. By controlling for many sources of advantage, we find that\nprompting does indeed provide a benefit, and that this benefit can be\nquantified per task. Results show that prompting is often worth 100s of data\npoints on average across classification tasks.</p>\n", "tags": ["Fine-Tuning","NAACL","Prompting","Training Techniques"] },
{"key": "schick2019rare", "citations": "88", "year": "2020", "title":"Rare Words: A Major Problem For Contextualized Embeddings And How To Fix It By Attentive Mimicking", "abstract": "<p>Pretraining deep neural network architectures with a language modeling\nobjective has brought large improvements for many natural language processing\ntasks. Exemplified by BERT, a recently proposed such architecture, we\ndemonstrate that despite being trained on huge amounts of data, deep language\nmodels still struggle to understand rare words. To fix this problem, we adapt\nAttentive Mimicking, a method that was designed to explicitly learn embeddings\nfor rare words, to deep language models. In order to make this possible, we\nintroduce one-token approximation, a procedure that enables us to use Attentive\nMimicking even when the underlying language model uses subword-based\ntokenization, i.e., it does not assign embeddings to all words. To evaluate our\nmethod, we create a novel dataset that tests the ability of language models to\ncapture semantic properties of words without any task-specific fine-tuning.\nUsing this dataset, we show that adding our adapted version of Attentive\nMimicking to BERT does indeed substantially improve its understanding of rare\nwords.</p>\n", "tags": ["AAAI","Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "schick2020few", "citations": "77", "year": "2020", "title":"Few-shot Text Generation With Pattern-exploiting Training", "abstract": "<p>Providing pretrained language models with simple task descriptions in natural\nlanguage enables them to solve some tasks in a fully unsupervised fashion.\nMoreover, when combined with regular learning from examples, this idea yields\nimpressive few-shot results for a wide range of text classification tasks. It\nis also a promising direction to improve data efficiency in generative\nsettings, but there are several challenges to using a combination of task\ndescriptions and example-based learning for text generation. In particular, it\nis crucial to find task descriptions that are easy to understand for the\npretrained model and to ensure that it actually makes good use of them;\nfurthermore, effective measures against overfitting have to be implemented. In\nthis paper, we show how these challenges can be tackled: We introduce GenPET, a\nmethod for text generation that is based on pattern-exploiting training, a\nrecent approach for combining textual instructions with supervised learning\nthat only works for classification tasks. On several summarization and headline\ngeneration datasets, GenPET gives consistent improvements over strong baselines\nin few-shot settings.</p>\n", "tags": ["Datasets","Efficiency","Few-Shot","Training Techniques"] },
{"key": "schick2021generating", "citations": "100", "year": "2021", "title":"Generating Datasets With Pretrained Language Models", "abstract": "<p>To obtain high-quality sentence embeddings from pretrained language models\n(PLMs), they must either be augmented with additional pretraining objectives or\nfinetuned on a large set of labeled text pairs. While the latter approach\ntypically outperforms the former, it requires great human effort to generate\nsuitable datasets of sufficient size. In this paper, we show how PLMs can be\nleveraged to obtain high-quality sentence embeddings without the need for\nlabeled data, finetuning or modifications to the pretraining objective: We\nutilize the generative abilities of large and high-performing PLMs to generate\nentire datasets of labeled text pairs from scratch, which we then use for\nfinetuning much smaller and more efficient models. Our fully unsupervised\napproach outperforms strong baselines on several semantic textual similarity\ndatasets.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "schick2023toolformer", "citations": "237", "year": "2023", "title":"Toolformer: Language Models Can Teach Themselves To Use Tools", "abstract": "<p>Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&amp;A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.</p>\n", "tags": ["Prompting","Tools","Training Techniques"] },
{"key": "schmidt2019generalization", "citations": "65", "year": "2019", "title":"Generalization In Generation: A Closer Look At Exposure Bias", "abstract": "<p>Exposure bias refers to the train-test discrepancy that seemingly arises when\nan autoregressive generative model uses only ground-truth contexts at training\ntime but generated ones at test time. We separate the contributions of the\nmodel and the learning framework to clarify the debate on consequences and\nreview proposed counter-measures. In this light, we argue that generalization\nis the underlying property to address and propose unconditional generation as\nits fundamental benchmark. Finally, we combine latent variable modeling with a\nrecent formulation of exploration in reinforcement learning to obtain a\nrigorous handling of true and generated contexts. Results on language modeling\nand variational sentence auto-encoding confirm the model’s generalization\ncapability.</p>\n", "tags": ["Ethics & Fairness","Evaluation","Training Techniques"] },
{"key": "scholak2021picard", "citations": "165", "year": "2021", "title":"PICARD: Parsing Incrementally For Constrained Auto-regressive Decoding From Language Models", "abstract": "<p>Large pre-trained language models for textual data have an unconstrained\noutput space; at each decoding step, they can produce any of 10,000s of\nsub-word tokens. When fine-tuned to target constrained formal languages like\nSQL, these models often generate invalid code, rendering it unusable. We\npropose PICARD (code and trained models available at\nhttps://github.com/ElementAI/picard), a method for constraining auto-regressive\ndecoders of language models through incremental parsing. PICARD helps to find\nvalid output sequences by rejecting inadmissible tokens at each decoding step.\nOn the challenging Spider and CoSQL text-to-SQL translation tasks, we show that\nPICARD transforms fine-tuned T5 models with passable performance into\nstate-of-the-art solutions.</p>\n", "tags": ["EMNLP","Has Code"] },
{"key": "schramowski2021large", "citations": "202", "year": "2022", "title":"Large Pre-trained Language Models Contain Human-like Biases Of What Is Right And Wrong To Do", "abstract": "<p>Artificial writing is permeating our lives due to recent advances in\nlarge-scale, transformer-based language models (LMs) such as BERT, its\nvariants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning\nthem for specific tasks, researchers have extended state of the art for many\nNLP tasks and shown that they capture not only linguistic knowledge but also\nretain general knowledge implicitly present in the data. Unfortunately, LMs\ntrained on unfiltered text corpora suffer from degenerated and biased\nbehaviour. While this is well established, we show that recent LMs also contain\nhuman-like biases of what is right and wrong to do, some form of ethical and\nmoral norms of the society – they bring a “moral direction” to surface. That\nis, we show that these norms can be captured geometrically by a direction,\nwhich can be computed, e.g., by a PCA, in the embedding space, reflecting well\nthe agreement of phrases to social norms implicitly expressed in the training\ntexts and providing a path for attenuating or even preventing toxic\ndegeneration in LMs. Being able to rate the (non-)normativity of arbitrary\nphrases without explicitly training the LM for this task, we demonstrate the\ncapabilities of the “moral direction” for guiding (even other) LMs towards\nproducing normative text and showcase it on RealToxicityPrompts testbed,\npreventing the neural toxic degeneration in GPT-2.</p>\n", "tags": ["Ethics & Fairness","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "schramowski2022safe", "citations": "73", "year": "2023", "title":"Safe Latent Diffusion: Mitigating Inappropriate Degeneration In Diffusion Models", "abstract": "<p>Text-conditioned image generation models have recently achieved astonishing\nresults in image quality and text alignment and are consequently employed in a\nfast-growing number of applications. Since they are highly data-driven, relying\non billion-sized datasets randomly scraped from the internet, they also suffer,\nas we demonstrate, from degenerated and biased human behavior. In turn, they\nmay even reinforce such biases. To help combat these undesired side effects, we\npresent safe latent diffusion (SLD). Specifically, to measure the inappropriate\ndegeneration due to unfiltered and imbalanced training sets, we establish a\nnovel image generation test bed-inappropriate image prompts (I2P)-containing\ndedicated, real-world image-to-text prompts covering concepts such as nudity\nand violence. As our exhaustive empirical evaluation demonstrates, the\nintroduced SLD removes and suppresses inappropriate image parts during the\ndiffusion process, with no additional training required and no adverse effect\non overall image quality or text alignment.</p>\n", "tags": ["Applications","CVPR","Datasets","Evaluation","Training Techniques"] },
{"key": "schuhmann2022laion", "citations": "638", "year": "2022", "title":"LAION-5B: An Open Large-scale Dataset For Training Next Generation Image-text Models", "abstract": "<p>Groundbreaking language-vision architectures like CLIP and DALL-E proved the\nutility of training on large amounts of noisy image-text data, without relying\non expensive accurate labels used in standard vision unimodal supervised\nlearning. The resulting models showed capabilities of strong text-guided image\ngeneration and transfer to downstream tasks, while performing remarkably at\nzero-shot classification with noteworthy out-of-distribution robustness. Since\nthen, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and\nImagen made further improvements. Studying the training and capabilities of\nsuch models requires datasets containing billions of image-text pairs. Until\nnow, no datasets of this size have been made openly available for the broader\nresearch community. To address this problem and democratize research on\nlarge-scale multi-modal models, we present LAION-5B - a dataset consisting of\n5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English\nlanguage. We show successful replication and fine-tuning of foundational models\nlike CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further\nexperiments enabled with an openly available dataset of this scale.\nAdditionally we provide several nearest neighbor indices, an improved\nweb-interface for dataset exploration and subset generation, and detection\nscores for watermark, NSFW, and toxic content detection. Announcement page\nhttps://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "schulz2018multi", "citations": "65", "year": "2018", "title":"Multi-task Learning For Argumentation Mining In Low-resource Settings", "abstract": "<p>We investigate whether and where multi-task learning (MTL) can improve\nperformance on NLP problems related to argumentation mining (AM), in particular\nargument component identification. Our results show that MTL performs\nparticularly well (and better than single-task learning) when little training\ndata is available for the main task, a common scenario in AM. Our findings\nchallenge previous assumptions that conceptualizations across AM datasets are\ndivergent and that MTL is difficult for semantic or higher-level tasks.</p>\n", "tags": ["Datasets","NAACL","Training Techniques"] },
{"key": "schuster2018cross", "citations": "255", "year": "2019", "title":"Cross-lingual Transfer Learning For Multilingual Task Oriented Dialog", "abstract": "<p>One of the first steps in the utterance interpretation pipeline of many\ntask-oriented conversational AI systems is to identify user intents and the\ncorresponding slots. Since data collection for machine learning models for this\ntask is time-consuming, it is desirable to make use of existing data in a\nhigh-resource language to train models in low-resource languages. However,\ndevelopment of such models has largely been hindered by the lack of\nmultilingual training data. In this paper, we present a new data set of 57k\nannotated utterances in English (43k), Spanish (8.6k) and Thai (5k) across the\ndomains weather, alarm, and reminder. We use this data set to evaluate three\ndifferent cross-lingual transfer methods: (1) translating the training data,\n(2) using cross-lingual pre-trained embeddings, and (3) a novel method of using\na multilingual machine translation encoder as contextual word representations.\nWe find that given several hundred training examples in the the target\nlanguage, the latter two methods outperform translating the training data.\nFurther, in very low-resource settings, multilingual contextual word\nrepresentations give better results than using cross-lingual static embeddings.\nWe also compare the cross-lingual methods to using monolingual resources in the\nform of contextual ELMo representations and find that given just small amounts\nof target language data, this method outperforms all cross-lingual methods,\nwhich highlights the need for more sophisticated cross-lingual methods.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "schuster2019limitations", "citations": "90", "year": "2020", "title":"The Limitations Of Stylometry For Detecting Machine-generated Fake News", "abstract": "<p>Recent developments in neural language models (LMs) have raised concerns\nabout their potential misuse for automatically spreading misinformation. In\nlight of these concerns, several studies have proposed to detect\nmachine-generated fake news by capturing their stylistic differences from\nhuman-written text. These approaches, broadly termed stylometry, have found\nsuccess in source attribution and misinformation detection in human-written\ntexts. However, in this work, we show that stylometry is limited against\nmachine-generated misinformation. While humans speak differently when trying to\ndeceive, LMs generate stylistically consistent text, regardless of underlying\nmotive. Thus, though stylometry can successfully prevent impersonation by\nidentifying text provenance, it fails to distinguish legitimate LM applications\nfrom those that introduce false information. We create two benchmarks\ndemonstrating the stylistic similarity between malicious and legitimate uses of\nLMs, employed in auto-completion and editing-assistance settings. Our findings\nhighlight the need for non-stylometry approaches in detecting machine-generated\nmisinformation, and open up the discussion on the desired evaluation\nbenchmarks.</p>\n", "tags": ["Applications","Evaluation"] },
{"key": "schwaller2017found", "citations": "422", "year": "2018", "title":"\"found In Translation\": Predicting Outcomes Of Complex Organic Chemistry Reactions Using Neural Sequence-to-sequence Models", "abstract": "<p>There is an intuitive analogy of an organic chemist’s understanding of a\ncompound and a language speaker’s understanding of a word. Consequently, it is\npossible to introduce the basic concepts and analyze potential impacts of\nlinguistic analysis to the world of organic chemistry. In this work, we cast\nthe reaction prediction task as a translation problem by introducing a\ntemplate-free sequence-to-sequence model, trained end-to-end and fully\ndata-driven. We propose a novel way of tokenization, which is arbitrarily\nextensible with reaction information. With this approach, we demonstrate\nresults superior to the state-of-the-art solution by a significant margin on\nthe top-1 accuracy. Specifically, our approach achieves an accuracy of 80.1%\nwithout relying on auxiliary knowledge such as reaction templates. Also, 66.4%\naccuracy is reached on a larger and noisier dataset.</p>\n", "tags": ["Datasets"] },
{"key": "schwartz2019factor", "citations": "109", "year": "2019", "title":"Factor Graph Attention", "abstract": "<p>Dialog is an effective way to exchange information, but subtle details and\nnuances are extremely important. While significant progress has paved a path to\naddress visual dialog with algorithms, details and nuances remain a challenge.\nAttention mechanisms have demonstrated compelling results to extract details in\nvisual question answering and also provide a convincing framework for visual\ndialog due to their interpretability and effectiveness. However, the many data\nutilities that accompany visual dialog challenge existing attention techniques.\nWe address this issue and develop a general attention mechanism for visual\ndialog which operates on any number of data utilities. To this end, we design a\nfactor graph based attention mechanism which combines any number of utility\nrepresentations. We illustrate the applicability of the proposed approach on\nthe challenging and recently introduced VisDial datasets, outperforming recent\nstate-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on\nMRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.</p>\n", "tags": ["CVPR","Model Architecture","Tools"] },
{"key": "schwenk2019ccmatrix", "citations": "75", "year": "2021", "title":"Ccmatrix: Mining Billions Of High-quality Parallel Sentences On The WEB", "abstract": "<p>We show that margin-based bitext mining in a multilingual sentence space can\nbe applied to monolingual corpora of billions of sentences. We are using ten\nsnapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7\nbillion unique sentences. Using one unified approach for 38 languages, we were\nable to mine 4.5 billions parallel sentences, out of which 661 million are\naligned with English. 20 language pairs have more then 30 million parallel\nsentences, 112 more then 10 million, and most more than one million, including\ndirect alignments between many European or Asian languages.\n  To evaluate the quality of the mined bitexts, we train NMT systems for most\nof the language pairs and evaluate them on TED, WMT and WAT test sets. Using\nour mined bitexts only and no human translated parallel data, we achieve a new\nstate-of-the-art for a single system on the WMT’19 test set for translation\nbetween English and German, Russian and Chinese, as well as German/French. In\nparticular, our English/German system outperforms the best single one by close\nto 4 BLEU points and is almost on pair with best WMT’19 evaluation system which\nuses system combination and back-translation. We also achieve excellent results\nfor distant languages pairs like Russian/Japanese, outperforming the best\nsubmission at the 2019 workshop on Asian Translation (WAT).</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "schwenk2019wikimatrix", "citations": "152", "year": "2019", "title":"Wikimatrix: Mining 135M Parallel Sentences In 1620 Language Pairs From Wikipedia", "abstract": "<p>We present an approach based on multilingual sentence embeddings to\nautomatically extract parallel sentences from the content of Wikipedia articles\nin 85 languages, including several dialects or low-resource languages. We do\nnot limit the the extraction process to alignments with English, but\nsystematically consider all possible language pairs. In total, we are able to\nextract 135M parallel sentences for 1620 different language pairs, out of which\nonly 34M are aligned with English. This corpus of parallel sentences is freely\navailable at\nhttps://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix. To get\nan indication on the quality of the extracted bitexts, we train neural MT\nbaseline systems on the mined data only for 1886 languages pairs, and evaluate\nthem on the TED corpus, achieving strong BLEU scores for many language pairs.\nThe WikiMatrix bitexts seem to be particularly interesting to train MT systems\nbetween distant languages without the need to pivot through English.</p>\n", "tags": ["Datasets","Has Code"] },
{"key": "schwenk2022okvqa", "citations": "107", "year": "2022", "title":"A-OKVQA: A Benchmark For Visual Question Answering Using World Knowledge", "abstract": "<p>The Visual Question Answering (VQA) task aspires to provide a meaningful\ntestbed for the development of AI models that can jointly reason over visual\nand natural language inputs. Despite a proliferation of VQA datasets, this goal\nis hindered by a set of common limitations. These include a reliance on\nrelatively simplistic questions that are repetitive in both concepts and\nlinguistic structure, little world knowledge needed outside of the paired\nimage, and limited reasoning required to arrive at the correct answer. We\nintroduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about\n25K questions requiring a broad base of commonsense and world knowledge to\nanswer. In contrast to the existing knowledge-based VQA datasets, the questions\ngenerally cannot be answered by simply querying a knowledge base, and instead\nrequire some form of commonsense reasoning about the scene depicted in the\nimage. We demonstrate the potential of this new dataset through a detailed\nanalysis of its contents and baseline performance measurements over a variety\nof state-of-the-art vision-language models. Project page:\nhttp://a-okvqa.allenai.org/</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "schäfer2023empirical", "citations": "89", "year": "2023", "title":"An Empirical Evaluation Of Using Large Language Models For Automated Unit Test Generation", "abstract": "<p>Unit tests play a key role in ensuring the correctness of software. However,\nmanually creating unit tests is a laborious task, motivating the need for\nautomation. Large Language Models (LLMs) have recently been applied to this\nproblem, utilizing additional training or few-shot learning on examples of\nexisting tests. This paper presents a large-scale empirical evaluation on the\neffectiveness of LLMs for automated unit test generation without additional\ntraining or manual effort, providing the LLM with the signature and\nimplementation of the function under test, along with usage examples extracted\nfrom documentation. We also attempt to repair failed generated tests by\nre-prompting the model with the failing test and error message. We implement\nour approach in TestPilot, a test generation tool for JavaScript that\nautomatically generates unit tests for all API functions in an npm package. We\nevaluate TestPilot using OpenAI’s gpt3.5-turbo LLM on 25 npm packages with a\ntotal of 1,684 API functions. The generated tests achieve a median statement\ncoverage of 70.2% and branch coverage of 52.8%, significantly improving on\nNessie, a recent feedback-directed JavaScript test generation technique, which\nachieves only 51.3% statement coverage and 25.6% branch coverage. We also find\nthat 92.8% of TestPilot’s generated tests have no more than 50% similarity with\nexisting tests (as measured by normalized edit distance), with none of them\nbeing exact copies. Finally, we run TestPilot with two additional LLMs,\nOpenAI’s older code-cushman-002 LLM and the open LLM StarCoder. Overall, we\nobserved similar results with the former (68.2% median statement coverage), and\nsomewhat worse results with the latter (54.0% median statement coverage),\nsuggesting that the effectiveness of the approach is influenced by the size and\ntraining set of the LLM, but does not fundamentally depend on the specific\nmodel.</p>\n", "tags": ["Evaluation","Few-Shot","Llm For Code","Prompting","Tools","Training Techniques"] },
{"key": "scialom2021questeval", "citations": "119", "year": "2021", "title":"Questeval: Summarization Asks For Fact-based Evaluation", "abstract": "<p>Summarization evaluation remains an open research problem: current metrics\nsuch as ROUGE are known to be limited and to correlate poorly with human\njudgments. To alleviate this issue, recent work has proposed evaluation metrics\nwhich rely on question answering models to assess whether a summary contains\nall the relevant information in its source document. Though promising, the\nproposed approaches have so far failed to correlate better than ROUGE with\nhuman judgments.\n  In this paper, we extend previous approaches and propose a unified framework,\nnamed QuestEval. In contrast to established metrics such as ROUGE or BERTScore,\nQuestEval does not require any ground-truth reference. Nonetheless, QuestEval\nsubstantially improves the correlation with human judgments over four\nevaluation dimensions (consistency, coherence, fluency, and relevance), as\nshown in the extensive experiments we report.</p>\n", "tags": ["EMNLP","Evaluation","Tools"] },
{"key": "sciavolino2021simple", "citations": "69", "year": "2021", "title":"Simple Entity-centric Questions Challenge Dense Retrievers", "abstract": "<p>Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., “Where was Arve Furset born?”), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.</p>\n", "tags": ["EMNLP","Retrieval Systems","Training Techniques"] },
{"key": "see2016compression", "citations": "165", "year": "2016", "title":"Compression Of Neural Machine Translation Models Via Pruning", "abstract": "<p>Neural Machine Translation (NMT), like many other deep learning domains,\ntypically suffers from over-parameterization, resulting in large storage sizes.\nThis paper examines three simple magnitude-based pruning schemes to compress\nNMT models, namely class-blind, class-uniform, and class-distribution, which\ndiffer in terms of how pruning thresholds are computed for the different\nclasses of weights in the NMT architecture. We demonstrate the efficacy of\nweight pruning as a compression technique for a state-of-the-art NMT system. We\nshow that an NMT model with over 200 million parameters can be pruned by 40%\nwith very little performance loss as measured on the WMT’14 English-German\ntranslation task. This sheds light on the distribution of redundancy in the NMT\narchitecture. Our main result is that with retraining, we can recover and even\nsurpass the original performance with an 80%-pruned model.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "see2017get", "citations": "3783", "year": "2017", "title":"Get To The Point: Summarization With Pointer-generator Networks", "abstract": "<p>Neural sequence-to-sequence models have provided a viable new approach for\nabstractive text summarization (meaning they are not restricted to simply\nselecting and rearranging passages from the original text). However, these\nmodels have two shortcomings: they are liable to reproduce factual details\ninaccurately, and they tend to repeat themselves. In this work we propose a\nnovel architecture that augments the standard sequence-to-sequence attentional\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\nthat can copy words from the source text via pointing, which aids accurate\nreproduction of information, while retaining the ability to produce novel words\nthrough the generator. Second, we use coverage to keep track of what has been\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\nMail summarization task, outperforming the current abstractive state-of-the-art\nby at least 2 ROUGE points.</p>\n", "tags": ["Model Architecture"] },
{"key": "see2019do", "citations": "153", "year": "2019", "title":"Do Massively Pretrained Language Models Make Better Storytellers?", "abstract": "<p>Large neural language models trained on massive amounts of text have emerged\nas a formidable strategy for Natural Language Understanding tasks. However, the\nstrength of these models as Natural Language Generators is less clear. Though\nanecdotal evidence suggests that these models generate better quality text,\nthere has been no detailed study characterizing their generation abilities. In\nthis work, we compare the performance of an extensively pretrained model,\nOpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story\ngeneration model (Fan et al., 2018). By evaluating the generated text across a\nwide variety of automatic metrics, we characterize the ways in which pretrained\nmodels do, and do not, make better storytellers. We find that although GPT2-117\nconditions more strongly on context, is more sensitive to ordering of events,\nand uses more unusual words, it is just as likely to produce repetitive and\nunder-diverse text when using likelihood-maximizing decoding algorithms.</p>\n", "tags": ["Evaluation"] },
{"key": "see2019what", "citations": "241", "year": "2019", "title":"What Makes A Good Conversation? How Controllable Attributes Affect Human Judgments", "abstract": "<p>A good conversation requires balance – between simplicity and detail;\nstaying on topic and changing it; asking questions and answering them. Although\ndialogue agents are commonly evaluated via human judgments of overall quality,\nthe relationship between quality and these individual factors is less\nwell-studied. In this work, we examine two controllable neural text generation\nmethods, conditional training and weighted decoding, in order to control four\nimportant attributes for chitchat dialogue: repetition, specificity,\nresponse-relatedness and question-asking. We conduct a large-scale human\nevaluation to measure the effect of these control parameters on multi-turn\ninteractive conversations on the PersonaChat task. We provide a detailed\nanalysis of their relationship to high-level aspects of conversation, and show\nthat by controlling combinations of these variables our models obtain clear\nimprovements in human quality judgments.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "segal2019simple", "citations": "72", "year": "2020", "title":"A Simple And Effective Model For Answering Multi-span Questions", "abstract": "<p>Models for reading comprehension (RC) commonly restrict their output space to\nthe set of all single contiguous spans from the input, in order to alleviate\nthe learning problem and avoid the need for a model that generates text\nexplicitly. However, forcing an answer to be a single span can be restrictive,\nand some recent datasets also include multi-span questions, i.e., questions\nwhose answer is a set of non-contiguous spans in the text. Naturally, models\nthat return single spans cannot answer these questions. In this work, we\npropose a simple architecture for answering multi-span questions by casting the\ntask as a sequence tagging problem, namely, predicting for each input token\nwhether it should be part of the output or not. Our model substantially\nimproves performance on span extraction questions from DROP and Quoref by 9.9\nand 5.5 EM points respectively.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "sejnowski2022large", "citations": "118", "year": "2023", "title":"Large Language Models And The Reverse Turing Test", "abstract": "<p>Large Language Models (LLMs) have been transformative. They are pre-trained\nfoundational models that are self-supervised and can be adapted with fine\ntuning to a wide range of natural language tasks, each of which previously\nwould have required a separate network model. This is one step closer to the\nextraordinary versatility of human language. GPT-3 and more recently LaMDA can\ncarry on dialogs with humans on many topics after minimal priming with a few\nexamples. However, there has been a wide range of reactions and debate on\nwhether these LLMs understand what they are saying or exhibit signs of\nintelligence. This high variance is exhibited in three interviews with LLMs\nreaching wildly different conclusions. A new possibility was uncovered that\ncould explain this divergence. What appears to be intelligence in LLMs may in\nfact be a mirror that reflects the intelligence of the interviewer, a\nremarkable twist that could be considered a Reverse Turing Test. If so, then by\nstudying interviews we may be learning more about the intelligence and beliefs\nof the interviewer than the intelligence of the LLMs. As LLMs become more\ncapable they may transform the way we interact with machines and how they\ninteract with each other. Increasingly, LLMs are being coupled with\nsensorimotor devices. LLMs can talk the talk, but can they walk the walk? A\nroad map for achieving artificial general autonomy is outlined with seven major\nimprovements inspired by brain systems. LLMs could be used to uncover new\ninsights into brain function by downloading brain data during natural\nbehaviors.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "sellam2020bleurt", "citations": "804", "year": "2020", "title":"BLEURT: Learning Robust Metrics For Text Generation", "abstract": "<p>Text generation has made significant advances in the last few years. Yet,\nevaluation metrics have lagged behind, as the most popular choices (e.g., BLEU\nand ROUGE) may correlate poorly with human judgments. We propose BLEURT, a\nlearned evaluation metric based on BERT that can model human judgments with a\nfew thousand possibly biased training examples. A key aspect of our approach is\na novel pre-training scheme that uses millions of synthetic examples to help\nthe model generalize. BLEURT provides state-of-the-art results on the last\nthree years of the WMT Metrics shared task and the WebNLG Competition dataset.\nIn contrast to a vanilla BERT-based approach, it yields superior results even\nwhen the training data is scarce and out-of-distribution.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "selvaraju2019taking", "citations": "228", "year": "2019", "title":"Taking A HINT: Leveraging Explanations To Make Vision And Language Models More Grounded", "abstract": "<p>Many vision and language models suffer from poor visual grounding - often\nfalling back on easy-to-learn language priors rather than basing their\ndecisions on visual concepts in the image. In this work, we propose a generic\napproach called Human Importance-aware Network Tuning (HINT) that effectively\nleverages human demonstrations to improve visual grounding. HINT encourages\ndeep networks to be sensitive to the same input regions as humans. Our approach\noptimizes the alignment between human attention maps and gradient-based network\nimportances - ensuring that models learn not just to look at but rather rely on\nvisual concepts that humans found relevant for a task when making predictions.\nWe apply HINT to Visual Question Answering and Image Captioning tasks,\noutperforming top approaches on splits that penalize over-reliance on language\npriors (VQA-CP and robust captioning) using human attention demonstrations for\njust 6% of the training data.</p>\n", "tags": ["ICCV","Model Architecture","Training Techniques"] },
{"key": "semeniuta2017hybrid", "citations": "222", "year": "2017", "title":"A Hybrid Convolutional Variational Autoencoder For Text Generation", "abstract": "<p>In this paper we explore the effect of architectural choices on learning a\nVariational Autoencoder (VAE) for text generation. In contrast to the\npreviously introduced VAE model for text where both the encoder and decoder are\nRNNs, we propose a novel hybrid architecture that blends fully feed-forward\nconvolutional and deconvolutional components with a recurrent language model.\nOur architecture exhibits several attractive properties such as faster run time\nand convergence, ability to better handle long sequences and, more importantly,\nit helps to avoid some of the major difficulties posed by training VAE models\non textual data.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "semeniuta2018accurate", "citations": "83", "year": "2018", "title":"On Accurate Evaluation Of Gans For Language Generation", "abstract": "<p>Generative Adversarial Networks (GANs) are a promising approach to language\ngeneration. The latest works introducing novel GAN models for language\ngeneration use n-gram based metrics for evaluation and only report single\nscores of the best run. In this paper, we argue that this often misrepresents\nthe true picture and does not tell the full story, as GAN models can be\nextremely sensitive to the random initialization and small deviations from the\nbest hyperparameter choice. In particular, we demonstrate that the previously\nused BLEU score is not sensitive to semantic deterioration of generated texts\nand propose alternative metrics that better capture the quality and diversity\nof the generated samples. We also conduct a set of experiments comparing a\nnumber of GAN models for text with a conventional Language Model (LM) and find\nthat neither of the considered models performs convincingly better than the LM.</p>\n", "tags": ["Evaluation"] },
{"key": "sennrich2016edinburgh", "citations": "471", "year": "2016", "title":"Edinburgh Neural Machine Translation Systems For WMT 16", "abstract": "<p>We participated in the WMT 2016 shared news translation task by building\nneural translation systems for four language pairs, each trained in both\ndirections: English&lt;-&gt;Czech, English&lt;-&gt;German, English&lt;-&gt;Romanian and\nEnglish&lt;-&gt;Russian. Our systems are based on an attentional encoder-decoder,\nusing BPE subword segmentation for open-vocabulary translation with a fixed\nvocabulary. We experimented with using automatic back-translations of the\nmonolingual News corpus as additional training data, pervasive dropout, and\ntarget-bidirectional models. All reported methods give substantial\nimprovements, and we see improvements of 4.3–11.2 BLEU over our baseline\nsystems. In the human evaluation, our systems were the (tied) best constrained\nsystem for 7 out of 8 translation directions in which we participated.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "sennrich2016how", "citations": "91", "year": "2017", "title":"How Grammatical Is Character-level Neural Machine Translation? Assessing MT Quality With Contrastive Translation Pairs", "abstract": "<p>Analysing translation quality in regards to specific linguistic phenomena has\nhistorically been difficult and time-consuming. Neural machine translation has\nthe attractive property that it can produce scores for arbitrary translations,\nand we propose a novel method to assess how well NMT systems model specific\nlinguistic phenomena such as agreement over long distances, the production of\nnovel words, and the faithful translation of polarity. The core idea is that we\nmeasure whether a reference translation is more probable under a NMT model than\na contrastive translation which introduces a specific type of error. We present\nLingEval97, a large-scale data set of 97000 contrastive translation pairs based\non the WMT English-&gt;German translation task, with errors automatically created\nwith simple rules. We report results for a number of systems, and find that\nrecently introduced character-level NMT systems perform better at\ntransliteration than models with byte-pair encoding (BPE) segmentation, but\nperform more poorly at morphosyntactic agreement, and translating discontiguous\nunits of meaning.</p>\n", "tags": ["NAACL"] },
{"key": "sennrich2016linguistic", "citations": "349", "year": "2016", "title":"Linguistic Input Features Improve Neural Machine Translation", "abstract": "<p>Neural machine translation has recently achieved impressive results, while\nusing little in the way of external linguistic information. In this paper we\nshow that the strong learning capability of neural MT models does not make\nlinguistic features redundant; they can be easily incorporated to provide\nfurther improvements in performance. We generalize the embedding layer of the\nencoder in the attentional encoder–decoder architecture to support the\ninclusion of arbitrary features, in addition to the baseline word feature. We\nadd morphological features, part-of-speech tags, and syntactic dependency\nlabels as input features to English&lt;-&gt;German, and English-&gt;Romanian neural\nmachine translation systems. In experiments on WMT16 training and test sets, we\nfind that linguistic input features improve model quality according to three\nmetrics: perplexity, BLEU and CHRF3. An open-source implementation of our\nneural MT system is available, as are sample files and configurations.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "sennrich2017nematus", "citations": "350", "year": "2017", "title":"Nematus: A Toolkit For Neural Machine Translation", "abstract": "<p>We present Nematus, a toolkit for Neural Machine Translation. The toolkit\nprioritizes high translation accuracy, usability, and extensibility. Nematus\nhas been used to build top-performing submissions to shared translation tasks\nat WMT and IWSLT, and has been used to train systems for production\nenvironments.</p>\n", "tags": ["Tools"] },
{"key": "sennrich2017university", "citations": "161", "year": "2017", "title":"The University Of Edinburgh's Neural MT Systems For WMT17", "abstract": "<p>This paper describes the University of Edinburgh’s submissions to the WMT17\nshared news translation and biomedical translation tasks. We participated in 12\ntranslation directions for news, translating between English and Czech, German,\nLatvian, Russian, Turkish and Chinese. For the biomedical task we submitted\nsystems for English to Czech, German, Polish and Romanian. Our systems are\nneural machine translation systems trained with Nematus, an attentional\nencoder-decoder. We follow our setup from last year and build BPE-based models\nwith parallel and back-translated monolingual training data. Novelties this\nyear include the use of deep architectures, layer normalization, and more\ncompact models due to weight tying and improvements in BPE segmentations. We\nperform extensive ablative experiments, reporting on the effectivenes of layer\nnormalization, deep architectures, and different ensembling techniques.</p>\n", "tags": ["Training Techniques"] },
{"key": "sennrich2019revisiting", "citations": "166", "year": "2019", "title":"Revisiting Low-resource Neural Machine Translation: A Case Study", "abstract": "<p>It has been shown that the performance of neural machine translation (NMT)\ndrops starkly in low-resource conditions, underperforming phrase-based\nstatistical machine translation (PBSMT) and requiring large amounts of\nauxiliary data to achieve competitive results. In this paper, we re-assess the\nvalidity of these results, arguing that they are the result of lack of system\nadaptation to low-resource settings. We discuss some pitfalls to be aware of\nwhen training low-resource NMT systems, and recent techniques that have shown\nto be especially helpful in low-resource settings, resulting in a set of best\npractices for low-resource NMT. In our experiments on German–English with\ndifferent amounts of IWSLT14 training data, we show that, without the use of\nany auxiliary monolingual or multilingual data, an optimized NMT system can\noutperform PBSMT with far less data than previously claimed. We also apply\nthese techniques to a low-resource Korean-English dataset, surpassing\npreviously reported results by 4 BLEU.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "seo2016bidirectional", "citations": "1348", "year": "2016", "title":"Bidirectional Attention Flow For Machine Comprehension", "abstract": "<p>Machine comprehension (MC), answering a query about a given context\nparagraph, requires modeling complex interactions between the context and the\nquery. Recently, attention mechanisms have been successfully extended to MC.\nTypically these methods use attention to focus on a small portion of the\ncontext and summarize it with a fixed-size vector, couple attentions\ntemporally, and/or often form a uni-directional attention. In this paper we\nintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage\nhierarchical process that represents the context at different levels of\ngranularity and uses bi-directional attention flow mechanism to obtain a\nquery-aware context representation without early summarization. Our\nexperimental evaluations show that our model achieves the state-of-the-art\nresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze\ntest.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "seo2016query", "citations": "66", "year": "2016", "title":"Query-reduction Networks For Question Answering", "abstract": "<p>In this paper, we study the problem of question answering when reasoning over\nmultiple facts is required. We propose Query-Reduction Network (QRN), a variant\nof Recurrent Neural Network (RNN) that effectively handles both short-term\n(local) and long-term (global) sequential dependencies to reason over multiple\nfacts. QRN considers the context sentences as a sequence of state-changing\ntriggers, and reduces the original query to a more informed query as it\nobserves each trigger (context sentence) through time. Our experiments show\nthat QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and\nin a real goal-oriented dialog dataset. In addition, QRN formulation allows\nparallelization on RNN’s time axis, saving an order of magnitude in time\ncomplexity for training and inference.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "seo2017visual", "citations": "91", "year": "2017", "title":"Visual Reference Resolution Using Attention Memory For Visual Dialog", "abstract": "<p>Visual dialog is a task of answering a series of inter-dependent questions\ngiven an input image, and often requires to resolve visual references among the\nquestions. This problem is different from visual question answering (VQA),\nwhich relies on spatial attention (a.k.a. visual grounding) estimated from an\nimage and question pair. We propose a novel attention mechanism that exploits\nvisual attentions in the past to resolve the current reference in the visual\ndialog scenario. The proposed model is equipped with an associative attention\nmemory storing a sequence of previous (attention, key) pairs. From this memory,\nthe model retrieves the previous attention, taking into account recency, which\nis most relevant for the current question, in order to resolve potentially\nambiguous references. The model then merges the retrieved attention with a\ntentative one to obtain the final attention for the current question;\nspecifically, we use dynamic parameter prediction to combine the two attentions\nconditioned on the question. Through extensive experiments on a new synthetic\nvisual dialog dataset, we show that our model significantly outperforms the\nstate-of-the-art (by ~16 % points) in situations, where visual reference\nresolution plays an important role. Moreover, the proposed model achieves\nsuperior performance (~ 2 % points improvement) in the Visual Dialog dataset,\ndespite having significantly fewer parameters than the baselines.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Memory & Context","Model Architecture"] },
{"key": "seo2019real", "citations": "143", "year": "2019", "title":"Real-time Open-domain Question Answering With Dense-sparse Phrase Index", "abstract": "<p>Existing open-domain question answering (QA) models are not suitable for\nreal-time usage because they need to process several long documents on-demand\nfor every input query. In this paper, we introduce the query-agnostic indexable\nrepresentation of document phrases that can drastically speed up open-domain QA\nand also allows us to reach long-tail targets. In particular, our dense-sparse\nphrase encoding effectively captures syntactic, semantic, and lexical\ninformation of the phrases and eliminates the pipeline filtering of context\ndocuments. Leveraging optimization strategies, our model can be trained in a\nsingle 4-GPU server and serve entire Wikipedia (up to 60 billion phrases) under\n2TB with CPUs only. Our experiments on SQuAD-Open show that our model is more\naccurate than DrQA (Chen et al., 2017) with 6000x reduced computational cost,\nwhich translates into at least 58x faster end-to-end inference benchmark on\nCPUs.</p>\n", "tags": ["Datasets","Efficiency","Evaluation"] },
{"key": "seo2022end", "citations": "119", "year": "2022", "title":"End-to-end Generative Pretraining For Multimodal Video Captioning", "abstract": "<p>Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective – we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.</p>\n", "tags": ["CVPR","Model Architecture","Tools"] },
{"key": "serapiogarcía2023personality", "citations": "64", "year": "2023", "title":"Personality Traits In Large Language Models", "abstract": "<p>The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling the generation of coherent and contextually\nrelevant human-like text. As LLMs increasingly powerconversational agents used\nby the general public world-wide, the synthetic personality traits embedded in\nthese models, by virtue of training on large amounts of human data, is becoming\nincreasingly important. Since personality is a key factor determining the\neffectiveness of communication, we present a novel and comprehensive\npsychometrically valid and reliable methodology for administering and\nvalidating personality tests on widely-used LLMs, as well as for shaping\npersonality in the generated text of such LLMs. Applying this method to 18\nLLMs, we found: 1) personality measurements in the outputs of some LLMs under\nspecific prompting configurations are reliable and valid; 2) evidence of\nreliability and validity of synthetic LLM personality is stronger for larger\nand instruction fine-tuned models; and 3) personality in LLM outputs can be\nshaped along desired dimensions to mimic specific human personality profiles.\nWe discuss the application and ethical implications of the measurement and\nshaping method, in particular regarding responsible AI.</p>\n", "tags": ["Ethics & Fairness","Prompting","Training Techniques"] },
{"key": "serban2016generating", "citations": "286", "year": "2016", "title":"Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-answer Corpus", "abstract": "<p>Over the past decade, large-scale supervised learning corpora have enabled\nmachine learning researchers to make substantial advances. However, to this\ndate, there are no large-scale question-answer corpora available. In this paper\nwe present the 30M Factoid Question-Answer Corpus, an enormous question answer\npair corpus produced by applying a novel neural network architecture on the\nknowledge base Freebase to transduce facts into natural language questions. The\nproduced question answer pairs are evaluated both by human evaluators and using\nautomatic evaluation metrics, including well-established machine translation\nand sentence similarity metrics. Across all evaluation criteria the\nquestion-generation model outperforms the competing template-based baseline.\nFurthermore, when presented to human evaluators, the generated questions appear\ncomparable in quality to real human-generated questions.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "serban2016generative", "citations": "64", "year": "2016", "title":"Generative Deep Neural Networks For Dialogue: A Short Review", "abstract": "<p>Researchers have recently started investigating deep neural networks for\ndialogue applications. In particular, generative sequence-to-sequence (Seq2Seq)\nmodels have shown promising results for unstructured tasks, such as word-level\ndialogue response generation. The hope is that such models will be able to\nleverage massive amounts of data to learn meaningful natural language\nrepresentations and response generation strategies, while requiring a minimum\namount of domain knowledge and hand-crafting. An important challenge is to\ndevelop models that can effectively incorporate dialogue context and generate\nmeaningful and diverse responses. In support of this goal, we review recently\nproposed models based on generative encoder-decoder neural network\narchitectures, and show that these models have better ability to incorporate\nlong-term dialogue history, to model uncertainty and ambiguity in dialogue, and\nto generate responses with high-level compositional structure.</p>\n", "tags": ["Applications","Dialogue & Multi Turn","Survey Paper"] },
{"key": "serban2016hierarchical", "citations": "732", "year": "2017", "title":"A Hierarchical Latent Variable Encoder-decoder Model For Generating Dialogues", "abstract": "<p>Sequential data often possesses a hierarchical structure with complex\ndependencies between subsequences, such as found between the utterances in a\ndialogue. In an effort to model this kind of generative process, we propose a\nneural network-based generative architecture, with latent stochastic variables\nthat span a variable number of time steps. We apply the proposed model to the\ntask of dialogue response generation and compare it with recent neural network\narchitectures. We evaluate the model performance through automatic evaluation\nmetrics and by carrying out a human evaluation. The experiments demonstrate\nthat our model improves upon recently proposed models and that the latent\nvariables facilitate the generation of long outputs and maintain the context.</p>\n", "tags": ["AAAI","Dialogue & Multi Turn","Evaluation","Model Architecture"] },
{"key": "serban2016multiresolution", "citations": "169", "year": "2017", "title":"Multiresolution Recurrent Neural Networks: An Application To Dialogue Response Generation", "abstract": "<p>We introduce the multiresolution recurrent neural network, which extends the\nsequence-to-sequence framework to model natural language generation as two\nparallel discrete stochastic processes: a sequence of high-level coarse tokens,\nand a sequence of natural language tokens. There are many ways to estimate or\nlearn the high-level coarse tokens, but we argue that a simple extraction\nprocedure is sufficient to capture a wealth of high-level discourse semantics.\nSuch procedure allows training the multiresolution recurrent neural network by\nmaximizing the exact joint log-likelihood over both sequences. In contrast to\nthe standard log- likelihood objective w.r.t. natural language tokens (word\nperplexity), optimizing the joint log-likelihood biases the model towards\nmodeling high-level abstractions. We apply the proposed model to the task of\ndialogue response generation in two challenging domains: the Ubuntu technical\nsupport domain, and Twitter conversations. On Ubuntu, the model outperforms\ncompeting approaches by a substantial margin, achieving state-of-the-art\nresults according to both automatic evaluation metrics and a human evaluation\nstudy. On Twitter, the model appears to generate more relevant and on-topic\nresponses according to automatic evaluation metrics. Finally, our experiments\ndemonstrate that the proposed model is more adept at overcoming the sparsity of\nnatural language and is better able to capture long-term structure.</p>\n", "tags": ["AAAI","Dialogue & Multi Turn","Evaluation","Training Techniques"] },
{"key": "serban2017deep", "citations": "209", "year": "2017", "title":"A Deep Reinforcement Learning Chatbot", "abstract": "<p>We present MILABOT: a deep reinforcement learning chatbot developed by the\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\ncompetition. MILABOT is capable of conversing with humans on popular small talk\ntopics through both speech and text. The system consists of an ensemble of\nnatural language generation and retrieval models, including template-based\nmodels, bag-of-words models, sequence-to-sequence neural network and latent\nvariable neural network models. By applying reinforcement learning to\ncrowdsourced data and real-world user interactions, the system has been trained\nto select an appropriate response from the models in its ensemble. The system\nhas been evaluated through A/B testing with real-world users, where it\nperformed significantly better than many competing systems. Due to its machine\nlearning architecture, the system is likely to improve with additional data.</p>\n", "tags": ["Agentic","Model Architecture","Reinforcement Learning"] },
{"key": "serrano2019is", "citations": "493", "year": "2019", "title":"Is Attention Interpretable?", "abstract": "<p>Attention mechanisms have recently boosted performance on a range of NLP\ntasks. Because attention layers explicitly weight input components’\nrepresentations, it is also often assumed that attention can be used to\nidentify information that models found important (e.g., specific contextualized\nword tokens). We test whether that assumption holds by manipulating attention\nweights in already-trained text classification models and analyzing the\nresulting differences in their predictions. While we observe some ways in which\nhigher attention weights correlate with greater impact on model predictions, we\nalso find many ways in which this does not hold, i.e., where gradient-based\nrankings of attention weights better predict their effects than their\nmagnitudes. We conclude that while attention noisily predicts input components’\noverall importance to a model, it is by no means a fail-safe indicator.</p>\n", "tags": ["Model Architecture"] },
{"key": "sha2017order", "citations": "104", "year": "2018", "title":"Order-planning Neural Text Generation From Structured Data", "abstract": "<p>Generating texts from structured data (e.g., a table) is important for\nvarious natural language processing tasks such as question answering and dialog\nsystems. In recent studies, researchers use neural language models and\nencoder-decoder frameworks for table-to-text generation. However, these neural\nnetwork-based approaches do not model the order of contents during text\ngeneration. When a human writes a summary based on a given table, he or she\nwould probably consider the content order before wording. In a biography, for\nexample, the nationality of a person is typically mentioned before occupation\nin a biography. In this paper, we propose an order-planning text generation\nmodel to capture the relationship between different fields and use such\nrelationship to make the generated text more fluent and smooth. We conducted\nexperiments on the WikiBio dataset and achieve significantly higher performance\nthan previous methods in terms of BLEU, ROUGE, and NIST scores.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "shah2018building", "citations": "177", "year": "2018", "title":"Building A Conversational Agent Overnight With Dialogue Self-play", "abstract": "<p>We propose Machines Talking To Machines (M2M), a framework combining\nautomation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents\nfor goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with\njust a task schema and an API client from the dialogue system developer, but it\nis also customizable to cater to task-specific interactions. Compared to the\nWizard-of-Oz approach for data collection, M2M achieves greater diversity and\ncoverage of salient dialogue flows while maintaining the naturalness of\nindividual utterances. In the first phase, a simulated user bot and a\ndomain-agnostic system bot converse to exhaustively generate dialogue\n“outlines”, i.e. sequences of template utterances and their semantic parses. In\nthe second phase, crowd workers provide contextual rewrites of the dialogues to\nmake the utterances more natural while preserving their meaning. The entire\nprocess can finish within a few hours. We propose a new corpus of 3,000\ndialogues spanning 2 domains collected with M2M, and present comparisons with\npopular dialogue datasets on the quality and diversity of the surface forms and\ndialogue flows.</p>\n", "tags": ["Agentic","Datasets","Dialogue & Multi Turn","Tools"] },
{"key": "shah2019cycle", "citations": "152", "year": "2019", "title":"Cycle-consistency For Robust Visual Question Answering", "abstract": "<p>Despite significant progress in Visual Question Answering over the years,\nrobustness of today’s VQA models leave much to be desired. We introduce a new\nevaluation protocol and associated dataset (VQA-Rephrasings) and show that\nstate-of-the-art VQA models are notoriously brittle to linguistic variations in\nquestions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k\nquestions spanning 40k images from the VQA v2.0 validation dataset. As a step\ntowards improving robustness of VQA models, we propose a model-agnostic\nframework that exploits cycle consistency. Specifically, we train a model to\nnot only answer a question, but also generate a question conditioned on the\nanswer, such that the answer predicted for the generated question is the same\nas the ground truth answer to the original question. Without the use of\nadditional annotations, we show that our approach is significantly more robust\nto linguistic variations than state-of-the-art VQA models, when evaluated on\nthe VQA-Rephrasings dataset. In addition, our approach outperforms\nstate-of-the-art approaches on the standard VQA and Visual Question Generation\ntasks on the challenging VQA v2.0 dataset.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Tools"] },
{"key": "shah2019robust", "citations": "86", "year": "2019", "title":"Robust Zero-shot Cross-domain Slot Filling With Example Values", "abstract": "<p>Task-oriented dialog systems increasingly rely on deep learning-based slot\nfilling models, usually needing extensive labeled training data for target\ndomains. Often, however, little to no target domain training data may be\navailable, or the training and target domain schemas may be misaligned, as is\ncommon for web forms on similar websites. Prior zero-shot slot filling models\nuse slot descriptions to learn concepts, but are not robust to misaligned\nschemas. We propose utilizing both the slot description and a small number of\nexamples of slot values, which may be easily available, to learn semantic\nrepresentations of slots which are transferable across domains and robust to\nmisaligned schemas. Our approach outperforms state-of-the-art models on two\nmulti-domain datasets, especially in the low-data setting.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "shan2017attention", "citations": "65", "year": "2018", "title":"Attention-based End-to-end Speech Recognition On Voice Search", "abstract": "<p>Recently, there has been a growing interest in end-to-end speech recognition\nthat directly transcribes speech to text without any predefined alignments. In\nthis paper, we explore the use of attention-based encoder-decoder model for\nMandarin speech recognition on a voice search task. Previous attempts have\nshown that applying attention-based encoder-decoder to Mandarin speech\nrecognition was quite difficult due to the logographic orthography of Mandarin,\nthe large vocabulary and the conditional dependency of the attention model. In\nthis paper, we use character embedding to deal with the large vocabulary.\nSeveral tricks are used for effective model training, including L2\nregularization, Gaussian weight noise and frame skipping. We compare two\nattention mechanisms and use attention smoothing to cover long context in the\nattention model. Taken together, these tricks allow us to finally achieve a\ncharacter error rate (CER) of 3.58% and a sentence error rate (SER) of 7.43% on\nthe MiTV voice search dataset. While together with a trigram language model,\nCER and SER reach 2.81% and 5.77%, respectively.</p>\n", "tags": ["Datasets","ICASSP","Memory & Context","Model Architecture","Training Techniques"] },
{"key": "shan2020contextual", "citations": "62", "year": "2020", "title":"A Contextual Hierarchical Attention Network With Adaptive Objective For Dialogue State Tracking", "abstract": "<p>Recent studies in dialogue state tracking (DST) leverage historical\ninformation to determine states which are generally represented as slot-value\npairs. However, most of them have limitations to efficiently exploit relevant\ncontext due to the lack of a powerful mechanism for modeling interactions\nbetween the slot and the dialogue history. Besides, existing methods usually\nignore the slot imbalance problem and treat all slots indiscriminately, which\nlimits the learning of hard slots and eventually hurts overall performance. In\nthis paper, we propose to enhance the DST through employing a contextual\nhierarchical attention network to not only discern relevant information at both\nword level and turn level but also learn contextual representations. We further\npropose an adaptive objective to alleviate the slot imbalance problem by\ndynamically adjust weights of different slots during training. Experimental\nresults show that our approach reaches 52.68% and 58.55% joint accuracy on\nMultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new\nstate-of-the-art performance with considerable improvements (+1.24% and\n+5.98%).</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture","Training Techniques"] },
{"key": "shanahan2022talking", "citations": "127", "year": "2024", "title":"Talking About Large Language Models", "abstract": "<p>Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as “knows”, “believes”, and\n“thinks”, when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.</p>\n", "tags": [] },
{"key": "shao2017generating", "citations": "194", "year": "2017", "title":"Generating High-quality And Informative Conversation Responses With Sequence-to-sequence Models", "abstract": "<p>Sequence-to-sequence models have been applied to the conversation response\ngeneration problem where the source sequence is the conversation history and\nthe target sequence is the response. Unlike translation, conversation\nresponding is inherently creative. The generation of long, informative,\ncoherent, and diverse responses remains a hard task. In this work, we focus on\nthe single turn setting. We add self-attention to the decoder to maintain\ncoherence in longer responses, and we propose a practical approach, called the\nglimpse-model, for scaling to large datasets. We introduce a stochastic\nbeam-search algorithm with segment-by-segment reranking which lets us inject\ndiversity earlier in the generation process. We trained on a combined data set\nof over 2.3B conversation messages mined from the web. In human evaluation\nstudies, our method produces longer responses overall, with a higher proportion\nrated as acceptable and excellent as length increases, compared to baseline\nsequence-to-sequence models with explicit length-promotion. A back-off strategy\nproduces better responses overall, in the full spectrum of lengths.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "shao2018drcd", "citations": "91", "year": "2018", "title":"DRCD: A Chinese Machine Reading Comprehension Dataset", "abstract": "<p>In this paper, we introduce DRCD (Delta Reading Comprehension Dataset), an\nopen domain traditional Chinese machine reading comprehension (MRC) dataset.\nThis dataset aimed to be a standard Chinese machine reading comprehension\ndataset, which can be a source dataset in transfer learning. The dataset\ncontains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions\ngenerated by annotators. We build a baseline model that achieves an F1 score of\n89.59%. F1 score of Human performance is 93.30%.</p>\n", "tags": ["Datasets","Fine-Tuning"] },
{"key": "shao2019long", "citations": "94", "year": "2019", "title":"Long And Diverse Text Generation With Planning-based Hierarchical Variational Model", "abstract": "<p>Existing neural methods for data-to-text generation are still struggling to\nproduce long and diverse texts: they are insufficient to model input data\ndynamically during generation, to capture inter-sentence coherence, or to\ngenerate diversified expressions. To address these issues, we propose a\nPlanning-based Hierarchical Variational Model (PHVM). Our model first plans a\nsequence of groups (each group is a subset of input items to be covered by a\nsentence) and then realizes each sentence conditioned on the planning result\nand the previously generated context, thereby decomposing long text generation\ninto dependent sentence generation sub-tasks. To capture expression diversity,\nwe devise a hierarchical latent structure where a global planning latent\nvariable models the diversity of reasonable planning and a sequence of local\nlatent variables controls sentence realization. Experiments show that our model\noutperforms state-of-the-art baselines in long and diverse text generation.</p>\n", "tags": ["EMNLP"] },
{"key": "shao2021cpt", "citations": "75", "year": "2024", "title":"CPT: A Pre-trained Unbalanced Transformer For Both Chinese Language Understanding And Generation", "abstract": "<p>In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed to utilize the shared knowledge\nbetween natural language understanding (NLU) and natural language generation\n(NLG) to boost the performance. CPT consists of three parts: a shared encoder,\nan understanding decoder, and a generation decoder. Two specific decoders with\na shared encoder are pre-trained with masked language modeling (MLM) and\ndenoising auto-encoding (DAE) tasks, respectively. With the partially shared\narchitecture and multi-task pre-training, CPT can (1) learn specific knowledge\nof both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that\nfully exploits the potential of the model. Moreover, the unbalanced Transformer\nsaves the computational and storage cost, which makes CPT competitive and\ngreatly accelerates the inference of text generation. Experimental results on a\nwide range of Chinese NLU and NLG tasks show the effectiveness of CPT.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "sharma2017relevance", "citations": "206", "year": "2017", "title":"Relevance Of Unsupervised Metrics In Task-oriented Dialogue For Evaluating Natural Language Generation", "abstract": "<p>Automated metrics such as BLEU are widely used in the machine translation\nliterature. They have also been used recently in the dialogue community for\nevaluating dialogue response generation. However, previous work in dialogue\nresponse generation has shown that these metrics do not correlate strongly with\nhuman judgment in the non task-oriented dialogue setting. Task-oriented\ndialogue responses are expressed on narrower domains and exhibit lower\ndiversity. It is thus reasonable to think that these automated metrics would\ncorrelate well with human judgment in the task-oriented setting where the\ngeneration task consists of translating dialogue acts into a sentence. We\nconduct an empirical study to confirm whether this is the case. Our findings\nindicate that these automated metrics have stronger correlation with human\njudgments in the task-oriented setting compared to what has been observed in\nthe non task-oriented setting. We also observe that these metrics correlate\neven better for datasets which provide multiple ground truth reference\nsentences. In addition, we show that some of the currently available corpora\nfor task-oriented language generation can be solved with simple models and\nadvocate for more challenging datasets.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "sharma2018chatpainter", "citations": "80", "year": "2018", "title":"Chatpainter: Improving Text To Image Generation Using Dialogue", "abstract": "<p>Synthesizing realistic images from text descriptions on a dataset like\nMicrosoft Common Objects in Context (MS COCO), where each image can contain\nseveral objects, is a challenging task. Prior work has used text captions to\ngenerate images. However, captions might not be informative enough to capture\nthe entire image and insufficient for the model to be able to understand which\nobjects in the images correspond to which words in the captions. We show that\nadding a dialogue that further describes the scene leads to significant\nimprovement in the inception score and in the quality of generated images on\nthe MS COCO dataset.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn"] },
{"key": "sharma2019entity", "citations": "69", "year": "2019", "title":"An Entity-driven Framework For Abstractive Summarization", "abstract": "<p>Abstractive summarization systems aim to produce more coherent and concise\nsummaries than their extractive counterparts. Popular neural models have\nachieved impressive results for single-document summarization, yet their\noutputs are often incoherent and unfaithful to the input. In this paper, we\nintroduce SENECA, a novel System for ENtity-drivEn Coherent Abstractive\nsummarization framework that leverages entity information to generate\ninformative and coherent abstracts. Our framework takes a two-step approach:\n(1) an entity-aware content selection module first identifies salient sentences\nfrom the input, then (2) an abstract generation module conducts cross-sentence\ninformation compression and abstraction to generate the final summary, which is\ntrained with rewards to promote coherence, conciseness, and clarity. The two\ncomponents are further connected using reinforcement learning. Automatic\nevaluation shows that our model significantly outperforms previous\nstate-of-the-art on ROUGE and our proposed coherence measures on New York Times\nand CNN/Daily Mail datasets. Human judges further rate our system summaries as\nmore informative and coherent than those by popular summarization models.</p>\n", "tags": ["EMNLP","Evaluation","Tools"] },
{"key": "sharma2020computational", "citations": "179", "year": "2020", "title":"A Computational Approach To Understanding Empathy Expressed In Text-based Mental Health Support", "abstract": "<p>Empathy is critical to successful mental health support. Empathy measurement\nhas predominantly occurred in synchronous, face-to-face settings, and may not\ntranslate to asynchronous, text-based contexts. Because millions of people use\ntext-based platforms for mental health support, understanding empathy in these\ncontexts is crucial. In this work, we present a computational approach to\nunderstanding how empathy is expressed in online mental health platforms. We\ndevelop a novel unifying theoretically-grounded framework for characterizing\nthe communication of empathy in text-based conversations. We collect and share\na corpus of 10k (post, response) pairs annotated using this empathy framework\nwith supporting evidence for annotations (rationales). We develop a multi-task\nRoBERTa-based bi-encoder model for identifying empathy in conversations and\nextracting rationales underlying its predictions. Experiments demonstrate that\nour approach can effectively identify empathic conversations. We further apply\nthis model to analyze 235k mental health interactions and show that users do\nnot self-learn empathy over time, revealing opportunities for empathy training\nand feedback.</p>\n", "tags": ["Datasets","EMNLP","Tools","Training Techniques"] },
{"key": "sharma2021towards", "citations": "120", "year": "2021", "title":"Towards Facilitating Empathic Conversations In Online Mental Health Support: A Reinforcement Learning Approach", "abstract": "<p>Online peer-to-peer support platforms enable conversations between millions\nof people who seek and provide mental health support. If successful, web-based\nmental health conversations could improve access to treatment and reduce the\nglobal disease burden. Psychologists have repeatedly demonstrated that empathy,\nthe ability to understand and feel the emotions and experiences of others, is a\nkey component leading to positive outcomes in supportive conversations.\nHowever, recent studies have shown that highly empathic conversations are rare\nin online mental health platforms.\n  In this paper, we work towards improving empathy in online mental health\nsupport conversations. We introduce a new task of empathic rewriting which aims\nto transform low-empathy conversational posts to higher empathy. Learning such\ntransformations is challenging and requires a deep understanding of empathy\nwhile maintaining conversation quality through text fluency and specificity to\nthe conversational context. Here we propose PARTNER, a deep reinforcement\nlearning agent that learns to make sentence-level edits to posts in order to\nincrease the expressed level of empathy while maintaining conversation quality.\nOur RL agent leverages a policy network, based on a transformer language model\nadapted from GPT-2, which performs the dual task of generating candidate\nempathic sentences and adding those sentences at appropriate positions. During\ntraining, we reward transformations that increase empathy in posts while\nmaintaining text fluency, context specificity and diversity. Through a\ncombination of automatic and human evaluation, we demonstrate that PARTNER\nsuccessfully generates more empathic, specific, and diverse responses and\noutperforms NLP methods from related tasks like style transfer and empathic\ndialogue generation. Our work has direct implications for facilitating empathic\nconversations on web-based platforms.</p>\n", "tags": ["Applications","Dialogue & Multi Turn","Evaluation"] },
{"key": "sharma2022human", "citations": "226", "year": "2023", "title":"Human-ai Collaboration Enables More Empathic Conversations In Text-based Peer-to-peer Mental Health Support", "abstract": "<p>Advances in artificial intelligence (AI) are enabling systems that augment\nand collaborate with humans to perform simple, mechanistic tasks like\nscheduling meetings and grammar-checking text. However, such Human-AI\ncollaboration poses challenges for more complex, creative tasks, such as\ncarrying out empathic conversations, due to difficulties of AI systems in\nunderstanding complex human emotions and the open-ended nature of these tasks.\nHere, we focus on peer-to-peer mental health support, a setting in which\nempathy is critical for success, and examine how AI can collaborate with humans\nto facilitate peer empathy during textual, online supportive conversations. We\ndevelop Hailey, an AI-in-the-loop agent that provides just-in-time feedback to\nhelp participants who provide support (peer supporters) respond more\nempathically to those seeking help (support seekers). We evaluate Hailey in a\nnon-clinical randomized controlled trial with real-world peer supporters on\nTalkLife (N=300), a large online peer-to-peer support platform. We show that\nour Human-AI collaboration approach leads to a 19.60% increase in\nconversational empathy between peers overall. Furthermore, we find a larger\n38.88% increase in empathy within the subsample of peer supporters who\nself-identify as experiencing difficulty providing support. We systematically\nanalyze the Human-AI collaboration patterns and find that peer supporters are\nable to use the AI feedback both directly and indirectly without becoming\noverly reliant on AI while reporting improved self-efficacy post-feedback. Our\nfindings demonstrate the potential of feedback-driven, AI-in-the-loop writing\nsystems to empower humans in open-ended, social, creative tasks such as\nempathic conversations.</p>\n", "tags": ["Agentic","Tools"] },
{"key": "shaw2018self", "citations": "1977", "year": "2018", "title":"Self-attention With Relative Position Representations", "abstract": "<p>Relying entirely on an attention mechanism, the Transformer introduced by\nVaswani et al. (2017) achieves state-of-the-art results for machine\ntranslation. In contrast to recurrent and convolutional neural networks, it\ndoes not explicitly model relative or absolute position information in its\nstructure. Instead, it requires adding representations of absolute positions to\nits inputs. In this work we present an alternative approach, extending the\nself-attention mechanism to efficiently consider representations of the\nrelative positions, or distances between sequence elements. On the WMT 2014\nEnglish-to-German and English-to-French translation tasks, this approach yields\nimprovements of 1.3 BLEU and 0.3 BLEU over absolute position representations,\nrespectively. Notably, we observe that combining relative and absolute position\nrepresentations yields no further improvement in translation quality. We\ndescribe an efficient implementation of our method and cast it as an instance\nof relation-aware self-attention mechanisms that can generalize to arbitrary\ngraph-labeled inputs.</p>\n", "tags": ["Model Architecture","NAACL"] },
{"key": "shaw2020compositional", "citations": "104", "year": "2021", "title":"Compositional Generalization And Natural Language Variation: Can A Semantic Parsing Approach Handle Both?", "abstract": "<p>Sequence-to-sequence models excel at handling natural language variation, but\nhave been shown to struggle with out-of-distribution compositional\ngeneralization. This has motivated new specialized architectures with stronger\ncompositional biases, but most of these approaches have only been evaluated on\nsynthetically-generated datasets, which are not representative of natural\nlanguage variation. In this work we ask: can we develop a semantic parsing\napproach that handles both natural language variation and compositional\ngeneralization? To better assess this capability, we propose new train and test\nsplits of non-synthetic datasets. We demonstrate that strong existing\napproaches do not perform well across a broad set of evaluations. We also\npropose NQG-T5, a hybrid model that combines a high-precision grammar-based\napproach with a pre-trained sequence-to-sequence model. It outperforms existing\napproaches across several compositional generalization challenges on\nnon-synthetic data, while also being competitive with the state-of-the-art on\nstandard evaluations. While still far from solving this problem, our study\nhighlights the importance of diverse evaluations and the open challenge of\nhandling both compositional generalization and natural language variation in\nsemantic parsing.</p>\n", "tags": ["Datasets"] },
{"key": "shazeer2017outrageously", "citations": "572", "year": "2017", "title":"Outrageously Large Neural Networks: The Sparsely-gated Mixture-of-experts Layer", "abstract": "<p>The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "shazeer2018adafactor", "citations": "290", "year": "2018", "title":"Adafactor: Adaptive Learning Rates With Sublinear Memory Cost", "abstract": "<p>In several recently proposed stochastic optimization methods (e.g. RMSProp,\nAdam, Adadelta), parameter updates are scaled by the inverse square roots of\nexponential moving averages of squared past gradients. Maintaining these\nper-parameter second-moment estimators requires memory equal to the number of\nparameters. For the case of neural network weight matrices, we propose\nmaintaining only the per-row and per-column sums of these moving averages, and\nestimating the per-parameter second moments based on these sums. We demonstrate\nempirically that this method produces similar results to the baseline.\nSecondly, we show that adaptive methods can produce larger-than-desired updates\nwhen the decay rate of the second moment accumulator is too slow. We propose\nupdate clipping and a gradually increasing decay rate scheme as remedies.\nCombining these methods and dropping momentum, we achieve comparable results to\nthe published Adam regime in training the Transformer model on the WMT 2014\nEnglish-German machine translation task, while using very little auxiliary\nstorage in the optimizer. Finally, we propose scaling the parameter updates\nbased on the scale of the parameters themselves.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "shekhar2017foil", "citations": "105", "year": "2017", "title":"FOIL It! Find One Mismatch Between Image And Language Caption", "abstract": "<p>In this paper, we aim to understand whether current language and vision\n(LaVi) models truly grasp the interaction between the two modalities. To this\nend, we propose an extension of the MSCOCO dataset, FOIL-COCO, which associates\nimages with both correct and “foil” captions, that is, descriptions of the\nimage that are highly similar to the original ones, but contain one single\nmistake (“foil word”). We show that current LaVi models fall into the traps of\nthis data and perform badly on three tasks: a) caption classification (correct\nvs. foil); b) foil word detection; c) foil word correction. Humans, in\ncontrast, have near-perfect performance on those tasks. We demonstrate that\nmerely utilising language cues is not enough to model FOIL-COCO and that it\nchallenges the state-of-the-art by requiring a fine-grained understanding of\nthe relation between text and image.</p>\n", "tags": ["Datasets"] },
{"key": "shen2016reasonet", "citations": "83", "year": "2016", "title":"Reasonet: Learning To Stop Reading In Machine Comprehension", "abstract": "<p>Teaching a computer to read and answer general questions pertaining to a\ndocument is a challenging yet unsolved problem. In this paper, we describe a\nnovel neural network architecture called the Reasoning Network (ReasoNet) for\nmachine comprehension tasks. ReasoNets make use of multiple turns to\neffectively exploit and then reason over the relation among queries, documents,\nand answers. Different from previous approaches using a fixed number of turns\nduring inference, ReasoNets introduce a termination state to relax this\nconstraint on the reasoning depth. With the use of reinforcement learning,\nReasoNets can dynamically determine whether to continue the comprehension\nprocess after digesting intermediate results, or to terminate reading when it\nconcludes that existing information is adequate to produce an answer. ReasoNets\nhave achieved exceptional performance in machine comprehension datasets,\nincluding unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset,\nand a structured Graph Reachability dataset.</p>\n", "tags": ["Agentic","Datasets","Model Architecture","Reinforcement Learning"] },
{"key": "shen2017conditional", "citations": "117", "year": "2017", "title":"A Conditional Variational Framework For Dialog Generation", "abstract": "<p>Deep latent variable models have been shown to facilitate the response\ngeneration for open-domain dialog systems. However, these latent variables are\nhighly randomized, leading to uncontrollable generated responses. In this\npaper, we propose a framework allowing conditional response generation based on\nspecific attributes. These attributes can be either manually assigned or\nautomatically detected. Moreover, the dialog states for both speakers are\nmodeled separately in order to reflect personal features. We validate this\nframework on two different scenarios, where the attribute refers to genericness\nand sentiment states respectively. The experiment result testified the\npotential of our model, where meaningful responses can be generated in\naccordance with the specified attributes.</p>\n", "tags": ["Dialogue & Multi Turn","Tools"] },
{"key": "shen2018improving", "citations": "94", "year": "2018", "title":"Improving Variational Encoder-decoders In Dialogue Generation", "abstract": "<p>Variational encoder-decoders (VEDs) have shown promising results in dialogue\ngeneration. However, the latent variable distributions are usually approximated\nby a much simpler model than the powerful RNN structure used for encoding and\ndecoding, yielding the KL-vanishing problem and inconsistent training\nobjective. In this paper, we separate the training step into two phases: The\nfirst phase learns to autoencode discrete texts into continuous embeddings,\nfrom which the second phase learns to generalize latent representations by\nreconstructing the encoded embedding. In this case, latent variables are\nsampled by transforming Gaussian noise through multi-layer perceptrons and are\ntrained with a separate VED model, which has the potential of realizing a much\nmore flexible distribution. We compare our model with current popular models\nand the experiment demonstrates substantial improvement in both metric-based\nand human evaluations.</p>\n", "tags": ["AAAI","Dialogue & Multi Turn","Training Techniques"] },
{"key": "shen2018ordered", "citations": "184", "year": "2018", "title":"Ordered Neurons: Integrating Tree Structures Into Recurrent Neural Networks", "abstract": "<p>Natural language is hierarchically structured: smaller units (e.g., phrases)\nare nested within larger units (e.g., clauses). When a larger constituent ends,\nall of the smaller constituents that are nested within it must also be closed.\nWhile the standard LSTM architecture allows different neurons to track\ninformation at different time scales, it does not have an explicit bias towards\nmodeling a hierarchy of constituents. This paper proposes to add such an\ninductive bias by ordering the neurons; a vector of master input and forget\ngates ensures that when a given neuron is updated, all the neurons that follow\nit in the ordering are also updated. Our novel recurrent architecture, ordered\nneurons LSTM (ON-LSTM), achieves good performance on four different tasks:\nlanguage modeling, unsupervised parsing, targeted syntactic evaluation, and\nlogical inference.</p>\n", "tags": ["Ethics & Fairness","Evaluation","Model Architecture"] },
{"key": "shen2019lingvo", "citations": "202", "year": "2019", "title":"Lingvo: A Modular And Scalable Framework For Sequence-to-sequence Modeling", "abstract": "<p>Lingvo is a Tensorflow framework offering a complete solution for\ncollaborative deep learning research, with a particular focus towards\nsequence-to-sequence models. Lingvo models are composed of modular building\nblocks that are flexible and easily extensible, and experiment configurations\nare centralized and highly customizable. Distributed training and quantized\ninference are supported directly within the framework, and it contains existing\nimplementations of a large number of utilities, helper functions, and the\nnewest research ideas. Lingvo has been used in collaboration by dozens of\nresearchers in more than 20 papers over the last two years. This document\noutlines the underlying design of Lingvo and serves as an introduction to the\nvarious pieces of the framework, while also offering examples of advanced\nfeatures that showcase the capabilities of the framework.</p>\n", "tags": ["Tools","Training Techniques"] },
{"key": "shen2019mixture", "citations": "78", "year": "2019", "title":"Mixture Models For Diverse Machine Translation: Tricks Of The Trade", "abstract": "<p>Mixture models trained via EM are among the simplest, most widely used and\nwell understood latent variable models in the machine learning literature.\nSurprisingly, these models have been hardly explored in text generation\napplications such as machine translation. In principle, they provide a latent\nvariable to control generation and produce a diverse set of hypotheses. In\npractice, however, mixture models are prone to degeneracies—often only one\ncomponent gets trained or the latent variable is simply ignored. We find that\ndisabling dropout noise in responsibility computation is critical to successful\ntraining. In addition, the design choices of parameterization, prior\ndistribution, hard versus soft EM and online versus offline assignment can\ndramatically affect model performance. We develop an evaluation protocol to\nassess both quality and diversity of generations against multiple references,\nand provide an extensive empirical study of several mixture model variants. Our\nanalysis shows that certain types of mixture models are more robust and offer\nthe best trade-off between translation quality and diversity compared to\nvariational models and diverse decoding approaches.\\footnote{Code to reproduce\nthe results in this paper is available at\nhttps://github.com/pytorch/fairseq}</p>\n", "tags": ["Applications","Evaluation","Has Code","Training Techniques"] },
{"key": "shen2019multi", "citations": "94", "year": "2019", "title":"Multi-task Learning For Conversational Question Answering Over A Large-scale Knowledge Base", "abstract": "<p>We consider the problem of conversational question answering over a\nlarge-scale knowledge base. To handle huge entity vocabulary of a large-scale\nknowledge base, recent neural semantic parsing based approaches usually\ndecompose the task into several subtasks and then solve them sequentially,\nwhich leads to following issues: 1) errors in earlier subtasks will be\npropagated and negatively affect downstream ones; and 2) each subtask cannot\nnaturally share supervision signals with others. To tackle these issues, we\npropose an innovative multi-task learning framework where a pointer-equipped\nsemantic parsing model is designed to resolve coreference in conversations, and\nnaturally empower joint learning with a novel type-aware entity detection\nmodel. The proposed framework thus enables shared supervisions and alleviates\nthe effect of error propagation. Experiments on a large-scale conversational\nquestion answering dataset containing 1.6M question answering pairs over 12.8M\nentities show that the proposed framework improves overall F1 score from 67% to\n79% compared with previous state-of-the-art work.</p>\n", "tags": ["Datasets","EMNLP","Tools"] },
{"key": "shen2020cdl", "citations": "80", "year": "2020", "title":"CDL: Curriculum Dual Learning For Emotion-controllable Response Generation", "abstract": "<p>Emotion-controllable response generation is an attractive and valuable task\nthat aims to make open-domain conversations more empathetic and engaging.\nExisting methods mainly enhance the emotion expression by adding regularization\nterms to standard cross-entropy loss and thus influence the training process.\nHowever, due to the lack of further consideration of content consistency, the\ncommon problem of response generation tasks, safe response, is intensified.\nBesides, query emotions that can help model the relationship between query and\nresponse are simply ignored in previous models, which would further hurt the\ncoherence. To alleviate these problems, we propose a novel framework named\nCurriculum Dual Learning (CDL) which extends the emotion-controllable response\ngeneration to a dual task to generate emotional responses and emotional queries\nalternatively. CDL utilizes two rewards focusing on emotion and content to\nimprove the duality. Additionally, it applies curriculum learning to gradually\ngenerate high-quality responses based on the difficulties of expressing various\nemotions. Experimental results show that CDL significantly outperforms the\nbaselines in terms of coherence, diversity, and relation to emotion factors.</p>\n", "tags": ["Tools","Training Techniques"] },
{"key": "shen2020dialogxl", "citations": "177", "year": "2021", "title":"Dialogxl: All-in-one Xlnet For Multi-party Conversation Emotion Recognition", "abstract": "<p>This paper presents our pioneering effort for emotion recognition in\nconversation (ERC) with pre-trained language models. Unlike regular documents,\nconversational utterances appear alternately from different parties and are\nusually organized as hierarchical structures in previous work. Such structures\nare not conducive to the application of pre-trained language models such as\nXLNet. To address this issue, we propose an all-in-one XLNet model, namely\nDialogXL, with enhanced memory to store longer historical context and\ndialog-aware self-attention to deal with the multi-party structures.\nSpecifically, we first modify the recurrence mechanism of XLNet from\nsegment-level to utterance-level in order to better model the conversational\ndata. Second, we introduce dialog-aware self-attention in replacement of the\nvanilla self-attention in XLNet to capture useful intra- and inter-speaker\ndependencies. Extensive experiments are conducted on four ERC benchmarks with\nmainstream models presented for comparison. The experimental results show that\nthe proposed model outperforms the baselines on all the datasets. Several other\nexperiments such as ablation study and error analysis are also conducted and\nthe results confirm the role of the critical modules of DialogXL.</p>\n", "tags": ["AAAI","Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "shen2020non", "citations": "73", "year": "2020", "title":"Non-attentive Tacotron: Robust And Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling", "abstract": "<p>This paper presents Non-Attentive Tacotron based on the Tacotron 2\ntext-to-speech model, replacing the attention mechanism with an explicit\nduration predictor. This improves robustness significantly as measured by\nunaligned duration ratio and word deletion rate, two metrics introduced in this\npaper for large-scale robustness evaluation using a pre-trained speech\nrecognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron\nachieves a 5-scale mean opinion score for naturalness of 4.41, slightly\noutperforming Tacotron 2. The duration predictor enables both utterance-wide\nand per-phoneme control of duration at inference time. When accurate target\ndurations are scarce or unavailable in the training data, we propose a method\nusing a fine-grained variational auto-encoder to train the duration predictor\nin a semi-supervised or unsupervised manner, with results almost as good as\nsupervised training.</p>\n", "tags": ["Evaluation","Model Architecture","Training Techniques"] },
{"key": "shen2020simple", "citations": "94", "year": "2020", "title":"A Simple But Tough-to-beat Data Augmentation Approach For Natural Language Understanding And Generation", "abstract": "<p>Adversarial training has been shown effective at endowing the learned\nrepresentations with stronger generalization ability. However, it typically\nrequires expensive computation to determine the direction of the injected\nperturbations. In this paper, we introduce a set of simple yet effective data\naugmentation strategies dubbed cutoff, where part of the information within an\ninput sentence is erased to yield its restricted views (during the fine-tuning\nstage). Notably, this process relies merely on stochastic sampling and thus\nadds little computational overhead. A Jensen-Shannon Divergence consistency\nloss is further utilized to incorporate these augmented samples into the\ntraining objective in a principled manner. To verify the effectiveness of the\nproposed strategies, we apply cutoff to both natural language understanding and\ngeneration problems. On the GLUE benchmark, it is demonstrated that cutoff, in\nspite of its simplicity, performs on par or better than several competitive\nadversarial-based approaches. We further extend cutoff to machine translation\nand observe significant gains in BLEU scores (based upon the Transformer Base\nmodel). Moreover, cutoff consistently outperforms adversarial training and\nachieves state-of-the-art results on the IWSLT2014 German-English dataset.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "shen2023hugginggpt", "citations": "205", "year": "2023", "title":"Hugginggpt: Solving AI Tasks With Chatgpt And Its Friends In Hugging Face", "abstract": "<p>Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence. While there are numerous AI models\navailable for various domains and modalities, they cannot handle complicated AI\ntasks autonomously. Considering large language models (LLMs) have exhibited\nexceptional abilities in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks, with language serving as a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards the\nrealization of artificial general intelligence.</p>\n", "tags": ["Agentic"] },
{"key": "sheng2019woman", "citations": "374", "year": "2019", "title":"The Woman Worked As A Babysitter: On Biases In Language Generation", "abstract": "<p>We present a systematic study of biases in natural language generation (NLG)\nby analyzing text generated from prompts that contain mentions of different\ndemographic groups. In this work, we introduce the notion of the regard towards\na demographic, use the varying levels of regard towards different demographics\nas a defining metric for bias in NLG, and analyze the extent to which sentiment\nscores are a relevant proxy metric for regard. To this end, we collect\nstrategically-generated text from language models and manually annotate the\ntext with both sentiment and regard scores. Additionally, we build an automatic\nregard classifier through transfer learning, so that we can analyze biases in\nunseen text. Together, these methods reveal the extent of the biased nature of\nlanguage model generations. Our analysis provides a study of biases in NLG,\nbias metrics and correlated human judgments, and empirical evidence on the\nusefulness of our annotated dataset.</p>\n", "tags": ["Datasets","EMNLP","Ethics & Fairness","Evaluation","Fine-Tuning"] },
{"key": "sheng2020towards", "citations": "85", "year": "2020", "title":"Towards Controllable Biases In Language Generation", "abstract": "<p>We present a general approach towards controllable societal biases in natural\nlanguage generation (NLG). Building upon the idea of adversarial triggers, we\ndevelop a method to induce societal biases in generated text when input prompts\ncontain mentions of specific demographic groups. We then analyze two scenarios:\n1) inducing negative biases for one demographic and positive biases for another\ndemographic, and 2) equalizing biases between demographics. The former scenario\nenables us to detect the types of biases present in the model. Specifically, we\nshow the effectiveness of our approach at facilitating bias analysis by finding\ntopics that correspond to demographic inequalities in generated text and\ncomparing the relative effectiveness of inducing biases for different\ndemographics. The second scenario is useful for mitigating biases in downstream\napplications such as dialogue generation. In our experiments, the mitigation\ntechnique proves to be effective at equalizing the amount of biases across\ndemographics while simultaneously generating less negatively biased text\noverall.</p>\n", "tags": ["Applications","EMNLP","Ethics & Fairness"] },
{"key": "sheng2021societal", "citations": "89", "year": "2021", "title":"Societal Biases In Language Generation: Progress And Challenges", "abstract": "<p>Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.</p>\n", "tags": ["Applications","Ethics & Fairness","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "shetty2017speaking", "citations": "241", "year": "2017", "title":"Speaking The Same Language: Matching Machine To Human Captions By Adversarial Training", "abstract": "<p>While strong progress has been made in image captioning over the last years,\nmachine and human captions are still quite distinct. A closer look reveals that\nthis is due to the deficiencies in the generated word distribution, vocabulary\nsize, and strong bias in the generators towards frequent captions. Furthermore,\nhumans – rightfully so – generate multiple, diverse captions, due to the\ninherent ambiguity in the captioning task which is not considered in today’s\nsystems.\n  To address these challenges, we change the training objective of the caption\ngenerator from reproducing groundtruth captions to generating a set of captions\nthat is indistinguishable from human generated captions. Instead of\nhandcrafting such a learning target, we employ adversarial training in\ncombination with an approximate Gumbel sampler to implicitly match the\ngenerated distribution to the human one. While our method achieves comparable\nperformance to the state-of-the-art in terms of the correctness of the\ncaptions, we generate a set of diverse captions, that are significantly less\nbiased and match the word statistics better in several aspects.</p>\n", "tags": ["Ethics & Fairness","ICCV","Training Techniques"] },
{"key": "shi2018explainable", "citations": "221", "year": "2019", "title":"Explainable And Explicit Visual Reasoning Over Scene Graphs", "abstract": "<p>We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs — objects as nodes and the pairwise relationships\nas edges — for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to “think”,\nregardless of what they “look”. As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.</p>\n", "tags": ["CVPR","Ethics & Fairness","Model Architecture"] },
{"key": "shi2018sentiment", "citations": "76", "year": "2018", "title":"Sentiment Adaptive End-to-end Dialog Systems", "abstract": "<p>End-to-end learning framework is useful for building dialog systems for its\nsimplicity in training and efficiency in model updating. However, current\nend-to-end approaches only consider user semantic inputs in learning and\nunder-utilize other user information. Therefore, we propose to include user\nsentiment obtained through multimodal information (acoustic, dialogic and\ntextual), in the end-to-end learning framework to make systems more\nuser-adaptive and effective. We incorporated user sentiment information in both\nsupervised and reinforcement learning settings. In both settings, adding\nsentiment information reduced the dialog length and improved the task success\nrate on a bus information search task. This work is the first attempt to\nincorporate multimodal user information in the adaptive end-to-end dialog\nsystem training framework and attained state-of-the-art performance.</p>\n", "tags": ["Dialogue & Multi Turn","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "shi2018toward", "citations": "86", "year": "2018", "title":"Toward Diverse Text Generation With Inverse Reinforcement Learning", "abstract": "<p>Text generation is a crucial task in NLP. Recently, several adversarial\ngenerative models have been proposed to improve the exposure bias problem in\ntext generation. Though these models gain great success, they still suffer from\nthe problems of reward sparsity and mode collapse. In order to address these\ntwo problems, in this paper, we employ inverse reinforcement learning (IRL) for\ntext generation. Specifically, the IRL framework learns a reward function on\ntraining data, and then an optimal policy to maximum the expected total reward.\nSimilar to the adversarial models, the reward and policy function in IRL are\noptimized alternately. Our method has two advantages: (1) the reward function\ncan produce more dense reward signals. (2) the generation policy, trained by\n“entropy regularized” policy gradient, encourages to generate more diversified\ntexts. Experiment results demonstrate that our proposed method can generate\nhigher quality texts than the previous methods.</p>\n", "tags": ["Ethics & Fairness","IJCAI","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "shi2019visually", "citations": "63", "year": "2019", "title":"Visually Grounded Neural Syntax Acquisition", "abstract": "<p>We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach\nfor learning syntactic representations and structures without any explicit\nsupervision. The model learns by looking at natural images and reading paired\ncaptions. VG-NSL generates constituency parse trees of texts, recursively\ncomposes representations for constituents, and matches them with images. We\ndefine concreteness of constituents by their matching scores with images, and\nuse it to guide the parsing of text. Experiments on the MSCOCO data set show\nthat VG-NSL outperforms various unsupervised parsing approaches that do not use\nvisual grounding, in terms of F1 scores against gold parse trees. We find that\nVGNSL is much more stable with respect to the choice of random initialization\nand the amount of training data. We also find that the concreteness acquired by\nVG-NSL correlates well with a similar measure defined by linguists. Finally, we\nalso apply VG-NSL to multiple languages in the Multi30K data set, showing that\nour model consistently outperforms prior unsupervised approaches.</p>\n", "tags": ["Training Techniques"] },
{"key": "shi2020emformer", "citations": "110", "year": "2021", "title":"Emformer: Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition", "abstract": "<p>This paper proposes an efficient memory transformer Emformer for low latency\nstreaming speech recognition. In Emformer, the long-range history context is\ndistilled into an augmented memory bank to reduce self-attention’s computation\ncomplexity. A cache mechanism saves the computation for the key and value in\nself-attention for the left context. Emformer applies a parallelized block\nprocessing in training to support low latency models. We carry out experiments\non benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets\nWER \\(2.50%\\) on test-clean and \\(5.62%\\) on test-other. Comparing with a strong\nbaseline augmented memory transformer (AM-TRF), Emformer gets \\(4.6\\) folds\ntraining speedup and \\(18%\\) relative real-time factor (RTF) reduction in\ndecoding with relative WER reduction \\(17%\\) on test-clean and \\(9%\\) on\ntest-other. For a low latency scenario with an average latency of 80 ms,\nEmformer achieves WER \\(3.01%\\) on test-clean and \\(7.09%\\) on test-other.\nComparing with the LSTM baseline with the same latency and model size, Emformer\ngets relative WER reduction \\(9%\\) and \\(16%\\) on test-clean and test-other,\nrespectively.</p>\n", "tags": ["Datasets","Evaluation","ICASSP","Memory & Context","Model Architecture","Training Techniques"] },
{"key": "shimaoka2016attentive", "citations": "95", "year": "2016", "title":"An Attentive Neural Architecture For Fine-grained Entity Type Classification", "abstract": "<p>In this work we propose a novel attention-based neural network model for the\ntask of fine-grained entity type classification that unlike previously proposed\nmodels recursively composes representations of entity mention contexts. Our\nmodel achieves state-of-the-art performance with 74.94% loose micro F1-score on\nthe well-established FIGER dataset, a relative improvement of 2.59%. We also\ninvestigate the behavior of the attention mechanism of our model and observe\nthat it can learn contextual linguistic expressions that indicate the\nfine-grained category memberships of an entity.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "shin2020autoprompt", "citations": "922", "year": "2020", "title":"Autoprompt: Eliciting Knowledge From Language Models With Automatically Generated Prompts", "abstract": "<p>The remarkable success of pretrained language models has motivated the study\nof what kinds of knowledge these models learn during pretraining. Reformulating\ntasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach\nfor gauging such knowledge, however, its usage is limited by the manual effort\nand guesswork required to write suitable prompts. To address this, we develop\nAutoPrompt, an automated method to create prompts for a diverse set of tasks,\nbased on a gradient-guided search. Using AutoPrompt, we show that masked\nlanguage models (MLMs) have an inherent capability to perform sentiment\nanalysis and natural language inference without additional parameters or\nfinetuning, sometimes achieving performance on par with recent state-of-the-art\nsupervised models. We also show that our prompts elicit more accurate factual\nknowledge from MLMs than the manually created prompts on the LAMA benchmark,\nand that MLMs can be used as relation extractors more effectively than\nsupervised relation extraction models. These results demonstrate that\nautomatically generated prompts are a viable parameter-free alternative to\nexisting probing methods, and as pretrained LMs become more sophisticated and\ncapable, potentially a replacement for finetuning.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "shin2020biomegatron", "citations": "104", "year": "2020", "title":"Biomegatron: Larger Biomedical Domain Language Model", "abstract": "<p>There has been an influx of biomedical domain-specific language models,\nshowing language models pre-trained on biomedical text perform better on\nbiomedical domain benchmarks than those trained on general domain text corpora\nsuch as Wikipedia and Books. Yet, most works do not study the factors affecting\neach domain language application deeply. Additionally, the study of model size\non domain-specific models has been mostly missing. We empirically study and\nevaluate several factors that can affect performance on domain language\napplications, such as the sub-word vocabulary set, model size, pre-training\ncorpus, and domain transfer. We show consistent improvements on benchmarks with\nour larger BioMegatron model trained on a larger domain corpus, contributing to\nour understanding of domain language model applications. We demonstrate\nnoticeable improvements over the previous state-of-the-art (SOTA) on standard\nbiomedical NLP benchmarks of named entity recognition, relation extraction, and\nquestion answering. Model checkpoints and code are available at\n[https://ngc.nvidia.com] and [https://github.com/NVIDIA/NeMo].</p>\n", "tags": ["Applications","Datasets","EMNLP","Has Code","Training Techniques"] },
{"key": "shin2021constrained", "citations": "120", "year": "2021", "title":"Constrained Language Models Yield Few-shot Semantic Parsers", "abstract": "<p>We explore the use of large pretrained language models as few-shot semantic\nparsers. The goal in semantic parsing is to generate a structured meaning\nrepresentation given a natural language input. However, language models are\ntrained to generate natural language. To bridge the gap, we use language models\nto paraphrase inputs into a controlled sublanguage resembling English that can\nbe automatically mapped to a target meaning representation. Our results\ndemonstrate that with only a small amount of data and very little code to\nconvert into English-like representations, our blueprint for rapidly\nbootstrapping semantic parsers leads to surprisingly effective performance on\nmultiple community tasks, greatly exceeding baseline methods also trained on\nthe same limited data.</p>\n", "tags": ["EMNLP","Few-Shot"] },
{"key": "shrestha2019answer", "citations": "80", "year": "2019", "title":"Answer Them All! Toward Universal Visual Question Answering Models", "abstract": "<p>Visual Question Answering (VQA) research is split into two camps: the first\nfocuses on VQA datasets that require natural image understanding and the second\nfocuses on synthetic datasets that test reasoning. A good VQA algorithm should\nbe capable of both, but only a few VQA algorithms are tested in this manner. We\ncompare five state-of-the-art VQA algorithms across eight VQA datasets covering\nboth domains. To make the comparison fair, all of the models are standardized\nas much as possible, e.g., they use the same visual features, answer\nvocabularies, etc. We find that methods do not generalize across the two\ndomains. To address this problem, we propose a new VQA algorithm that rivals or\nexceeds the state-of-the-art for both domains.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "shridhar2018interactive", "citations": "116", "year": "2018", "title":"Interactive Visual Grounding Of Referring Expressions For Human-robot Interaction", "abstract": "<p>This paper presents INGRESS, a robot system that follows human natural\nlanguage instructions to pick and place everyday objects. The core issue here\nis the grounding of referring expressions: infer objects and their\nrelationships from input images and language expressions. INGRESS allows for\nunconstrained object categories and unconstrained language expressions.\nFurther, it asks questions to disambiguate referring expressions interactively.\nTo achieve these, we take the approach of grounding by generation and propose a\ntwo-stage neural network model for grounding. The first stage uses a neural\nnetwork to generate visual descriptions of objects, compares them with the\ninput language expression, and identifies a set of candidate objects. The\nsecond stage uses another neural network to examine all pairwise relations\nbetween the candidates and infers the most likely referred object. The same\nneural networks are used for both grounding and question generation for\ndisambiguation. Experiments show that INGRESS outperformed a state-of-the-art\nmethod on the RefCOCO dataset and in robot experiments with humans.</p>\n", "tags": ["Datasets"] },
{"key": "shridhar2019alfred", "citations": "376", "year": "2020", "title":"ALFRED: A Benchmark For Interpreting Grounded Instructions For Everyday Tasks", "abstract": "<p>We present ALFRED (Action Learning From Realistic Environments and\nDirectives), a benchmark for learning a mapping from natural language\ninstructions and egocentric vision to sequences of actions for household tasks.\nALFRED includes long, compositional tasks with non-reversible state changes to\nshrink the gap between research benchmarks and real-world applications. ALFRED\nconsists of expert demonstrations in interactive visual environments for 25k\nnatural language directives. These directives contain both high-level goals\nlike “Rinse off a mug and place it in the coffee maker.” and low-level language\ninstructions like “Walk to the coffee maker on the right.” ALFRED tasks are\nmore complex in terms of sequence length, action space, and language than\nexisting vision-and-language task datasets. We show that a baseline model based\non recent embodied vision-and-language tasks performs poorly on ALFRED,\nsuggesting that there is significant room for developing innovative grounded\nvisual language understanding models with this benchmark.</p>\n", "tags": ["Applications","CVPR","Datasets","Evaluation"] },
{"key": "shu2017compressing", "citations": "95", "year": "2017", "title":"Compressing Word Embeddings Via Deep Compositional Code Learning", "abstract": "<p>Natural language processing (NLP) models often require a massive number of\nparameters for word embeddings, resulting in a large storage or memory\nfootprint. Deploying neural NLP models to mobile devices requires compressing\nthe word embeddings without any significant sacrifices in performance. For this\npurpose, we propose to construct the embeddings with few basis vectors. For\neach word, the composition of basis vectors is determined by a hash code. To\nmaximize the compression rate, we adopt the multi-codebook quantization\napproach instead of binary coding scheme. Each code is composed of multiple\ndiscrete numbers, such as (3, 2, 1, 8), where the value of each component is\nlimited to a fixed range. We propose to directly learn the discrete codes in an\nend-to-end neural network by applying the Gumbel-softmax trick. Experiments\nshow the compression rate achieves 98% in a sentiment analysis task and 94% ~\n99% in machine translation tasks without performance loss. In both tasks, the\nproposed method can improve the model performance by slightly lowering the\ncompression rate. Compared to other approaches such as character-level\nsegmentation, the proposed method is language-independent and does not require\nmodifications to the network architecture.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "shu2019latent", "citations": "106", "year": "2020", "title":"Latent-variable Non-autoregressive Neural Machine Translation With Deterministic Inference Using A Delta Posterior", "abstract": "<p>Although neural machine translation models reached high translation quality,\nthe autoregressive nature makes inference difficult to parallelize and leads to\nhigh translation latency. Inspired by recent refinement-based approaches, we\npropose LaNMT, a latent-variable non-autoregressive model with continuous\nlatent variables and deterministic inference procedure. In contrast to existing\napproaches, we use a deterministic inference algorithm to find the target\nsequence that maximizes the lowerbound to the log-probability. During\ninference, the length of translation automatically adapts itself. Our\nexperiments show that the lowerbound can be greatly increased by running the\ninference algorithm, resulting in significantly improved translation quality.\nOur proposed model closes the performance gap between non-autoregressive and\nautoregressive approaches on ASPEC Ja-En dataset with 8.6x faster decoding. On\nWMT’14 En-De dataset, our model narrows the gap with autoregressive baseline to\n2.0 BLEU points with 12.5x speedup. By decoding multiple initial latent\nvariables in parallel and rescore using a teacher model, the proposed model\nfurther brings the gap down to 1.0 BLEU point on WMT’14 En-De task with 6.8x\nspeedup.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "shu2022test", "citations": "75", "year": "2022", "title":"Test-time Prompt Tuning For Zero-shot Generalization In Vision-language Models", "abstract": "<p>Pre-trained vision-language models (e.g., CLIP) have shown promising\nzero-shot generalization in many downstream tasks with properly designed text\nprompts. Instead of relying on hand-engineered prompts, recent works learn\nprompts using the training data from downstream tasks. While effective,\ntraining on domain-specific data reduces a model’s generalization capability to\nunseen new domains. In this work, we propose test-time prompt tuning (TPT), a\nmethod that can learn adaptive prompts on the fly with a single test sample.\nFor image classification, TPT optimizes the prompt by minimizing the entropy\nwith confidence selection so that the model has consistent predictions across\ndifferent augmented views of each test sample. In evaluating generalization to\nnatural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP\nby 3.6% on average, surpassing previous prompt tuning approaches that require\nadditional task-specific training data. In evaluating cross-dataset\ngeneralization with unseen categories, TPT performs on par with the\nstate-of-the-art approaches that use additional training data. Project page:\nhttps://azshue.github.io/TPT.</p>\n", "tags": ["Datasets","Has Code","Prompting","Training Techniques"] },
{"key": "shukang2023survey", "citations": "125", "year": "2024", "title":"A Survey On Multimodal Large Language Models", "abstract": "<p>Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.</p>\n", "tags": ["Evaluation","Has Code","Llm For Code","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "shum2018eliza", "citations": "627", "year": "2018", "title":"From Eliza To Xiaoice: Challenges And Opportunities With Social Chatbots", "abstract": "<p>Conversational systems have come a long way since their inception in the\n1960s. After decades of research and development, we’ve seen progress from\nEliza and Parry in the 60’s and 70’s, to task-completion systems as in the\nDARPA Communicator program in the 2000s, to intelligent personal assistants\nsuch as Siri in the 2010s, to today’s social chatbots like XiaoIce. Social\nchatbots’ appeal lies not only in their ability to respond to users’ diverse\nrequests, but also in being able to establish an emotional connection with\nusers. The latter is done by satisfying users’ need for communication,\naffection, as well as social belonging. To further the advancement and adoption\nof social chatbots, their design must focus on user engagement and take both\nintellectual quotient (IQ) and emotional quotient (EQ) into account. Users\nshould want to engage with a social chatbot; as such, we define the success\nmetric for social chatbots as conversation-turns per session (CPS). Using\nXiaoIce as an illustrative example, we discuss key technologies in building\nsocial chatbots from core chat to visual awareness to skills. We also show how\nXiaoIce can dynamically recognize emotion and engage the user throughout long\nconversations with appropriate interpersonal responses. As we become the first\ngeneration of humans ever living with AI, we have a responsibility to design\nsocial chatbots to be both useful and empathetic, so they will become\nubiquitous and help society as a whole.</p>\n", "tags": [] },
{"key": "shumailov2023curse", "citations": "98", "year": "2023", "title":"The Curse Of Recursion: Training On Generated Data Makes Models Forget", "abstract": "<p>Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "shunyu2022react", "citations": "268", "year": "2022", "title":"React: Synergizing Reasoning And Acting In Language Models", "abstract": "<p>While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io</p>\n", "tags": ["Agentic","Has Code","Prompting","Reinforcement Learning","Tools"] },
{"key": "shunyu2023tree", "citations": "317", "year": "2023", "title":"Tree Of Thoughts: Deliberate Problem Solving With Large Language Models", "abstract": "<p>Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models’\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.</p>\n", "tags": ["Has Code","Model Architecture","Prompting","Tools"] },
{"key": "shuster2018engaging", "citations": "164", "year": "2019", "title":"Engaging Image Captioning Via Personality", "abstract": "<p>Standard image captioning tasks such as COCO and Flickr30k are factual,\nneutral in tone and (to a human) state the obvious (e.g., “a man playing a\nguitar”). While such tasks are useful to verify that a machine understands the\ncontent of an image, they are not engaging to humans as captions. With this in\nmind we define a new task, Personality-Captions, where the goal is to be as\nengaging to humans as possible by incorporating controllable style and\npersonality traits. We collect and release a large dataset of 201,858 of such\ncaptions conditioned over 215 possible traits. We build models that combine\nexisting work from (i) sentence representations (Mazare et al., 2018) with\nTransformers trained on 1.7 billion dialogue examples; and (ii) image\nrepresentations (Mahajan et al., 2018) with ResNets trained on 3.5 billion\nsocial media images. We obtain state-of-the-art performance on Flickr30k and\nCOCO, and strong performance on our new task. Finally, online evaluations\nvalidate that our task and models are engaging to humans, with our best model\nclose to human performance.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "shuster2018image", "citations": "71", "year": "2020", "title":"Image Chat: Engaging Grounded Conversations", "abstract": "<p>To achieve the long-term goal of machines being able to engage humans in\nconversation, our models should captivate the interest of their speaking\npartners. Communication grounded in images, whereby a dialogue is conducted\nbased on a given photo, is a setup naturally appealing to humans (Hu et al.,\n2014). In this work we study large-scale architectures and datasets for this\ngoal. We test a set of neural architectures using state-of-the-art image and\ntext representations, considering various ways to fuse the components. To test\nsuch models, we collect a dataset of grounded human-human conversations, where\nspeakers are asked to play roles given a provided emotional mood or style, as\nthe use of such traits is also a key factor in engagingness (Guo et al., 2019).\nOur dataset, Image-Chat, consists of 202k dialogues over 202k images using 215\npossible style traits. Automatic metrics and human evaluations of engagingness\nshow the efficacy of our approach; in particular, we obtain state-of-the-art\nperformance on the existing IGC task, and our best performing model is almost\non par with humans on the Image-Chat test set (preferred 47.7% of the time).</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "shuster2019dialogue", "citations": "69", "year": "2020", "title":"The Dialogue Dodecathlon: Open-domain Knowledge And Image Grounded Conversational Agents", "abstract": "<p>We introduce dodecaDialogue: a set of 12 tasks that measures if a\nconversational agent can communicate engagingly with personality and empathy,\nask questions, answer questions by utilizing knowledge resources, discuss\ntopics and situations, and perceive and converse about images. By multi-tasking\non such a broad large-scale set of data, we hope to both move towards and\nmeasure progress in producing a single unified agent that can perceive, reason\nand converse with humans in an open-domain setting. We show that such\nmulti-tasking improves over a BERT pre-trained baseline, largely due to\nmulti-tasking with very large dialogue datasets in a similar domain, and that\nthe multi-tasking in general provides gains to both text and image-based tasks\nusing several metrics in both the fine-tune and task transfer settings. We\nobtain state-of-the-art results on many of the tasks, providing a strong\nbaseline for this challenge.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "shuster2021retrieval", "citations": "286", "year": "2021", "title":"Retrieval Augmentation Reduces Hallucination In Conversation", "abstract": "<p>Despite showing increasingly human-like conversational abilities,\nstate-of-the-art dialogue models often suffer from factual incorrectness and\nhallucination of knowledge (Roller et al., 2020). In this work we explore the\nuse of neural-retrieval-in-the-loop architectures - recently shown to be\neffective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) -\nfor knowledge-grounded dialogue, a task that is arguably more challenging as it\nrequires querying based on complex multi-turn dialogue context and generating\nconversationally coherent responses. We study various types of architectures\nwith multiple components - retrievers, rankers, and encoder-decoders - with the\ngoal of maximizing knowledgeability while retaining conversational ability. We\ndemonstrate that our best models obtain state-of-the-art performance on two\nknowledge-grounded conversational tasks. The models exhibit open-domain\nconversational capabilities, generalize effectively to scenarios not within the\ntraining data, and, as verified by human evaluations, substantially reduce the\nwell-known problem of knowledge hallucination in state-of-the-art chatbots.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP","Training Techniques"] },
{"key": "shuster2022blenderbot", "citations": "87", "year": "2022", "title":"Blenderbot 3: A Deployed Conversational Agent That Continually Learns To Responsibly Engage", "abstract": "<p>We present BlenderBot 3, a 175B parameter dialogue model capable of\nopen-domain conversation with access to the internet and a long-term memory,\nand having been trained on a large number of user defined tasks. We release\nboth the model weights and code, and have also deployed the model on a public\nweb page to interact with organic users. This technical report describes how\nthe model was built (architecture, model and training scheme), and details of\nits deployment, including safety mechanisms. Human evaluations show its\nsuperiority to existing open-domain dialogue agents, including its predecessors\n(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for\ncontinual learning using the data collected from deployment, which will also be\npublicly released. The goal of this research program is thus to enable the\ncommunity to study ever-improving responsible agents that learn through\ninteraction.</p>\n", "tags": ["Agentic","Model Architecture","Training Techniques"] },
{"key": "shwartz2020unsupervised", "citations": "178", "year": "2020", "title":"Unsupervised Commonsense Question Answering With Self-talk", "abstract": "<p>Natural language understanding involves reading between the lines with\nimplicit background knowledge. Current systems either rely on pre-trained\nlanguage models as the sole implicit source of world knowledge, or resort to\nexternal knowledge bases (KBs) to incorporate additional relevant knowledge. We\npropose an unsupervised framework based on self-talk as a novel alternative to\nmultiple-choice commonsense tasks. Inspired by inquiry-based discovery learning\n(Bruner, 1961), our approach inquires language models with a number of\ninformation seeking questions such as “\\(\\textit{what is the definition of\n…}\\)” to discover additional background knowledge. Empirical results\ndemonstrate that the self-talk procedure substantially improves the performance\nof zero-shot language model baselines on four out of six commonsense\nbenchmarks, and competes with models that obtain knowledge from external KBs.\nWhile our approach improves performance on several benchmarks, the self-talk\ninduced knowledge even when leading to correct answers is not always seen as\nuseful by human judges, raising interesting questions about the inner-workings\nof pre-trained language models for commonsense reasoning.</p>\n", "tags": ["EMNLP","Tools"] },
{"key": "siddhant2019evaluating", "citations": "63", "year": "2020", "title":"Evaluating The Cross-lingual Effectiveness Of Massively Multilingual Neural Machine Translation", "abstract": "<p>The recently proposed massively multilingual neural machine translation (NMT)\nsystem has been shown to be capable of translating over 100 languages to and\nfrom English within a single model. Its improved translation performance on low\nresource languages hints at potential cross-lingual transfer capability for\ndownstream tasks. In this paper, we evaluate the cross-lingual effectiveness of\nrepresentations from the encoder of a massively multilingual NMT model on 5\ndownstream classification and sequence labeling tasks covering a diverse set of\nover 50 languages. We compare against a strong baseline, multilingual BERT\n(mBERT), in different cross-lingual transfer learning scenarios and show gains\nin zero-shot transfer in 4 out of these 5 tasks.</p>\n", "tags": ["AAAI","Fine-Tuning","Model Architecture"] },
{"key": "singer2022make", "citations": "242", "year": "2022", "title":"Make-a-video: Text-to-video Generation Without Text-video Data", "abstract": "<p>We propose Make-A-Video – an approach for directly translating the\ntremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video\n(T2V). Our intuition is simple: learn what the world looks like and how it is\ndescribed from paired text-image data, and learn how the world moves from\nunsupervised video footage. Make-A-Video has three advantages: (1) it\naccelerates training of the T2V model (it does not need to learn visual and\nmultimodal representations from scratch), (2) it does not require paired\ntext-video data, and (3) the generated videos inherit the vastness (diversity\nin aesthetic, fantastical depictions, etc.) of today’s image generation models.\nWe design a simple yet effective way to build on T2I models with novel and\neffective spatial-temporal modules. First, we decompose the full temporal U-Net\nand attention tensors and approximate them in space and time. Second, we design\na spatial temporal pipeline to generate high resolution and frame rate videos\nwith a video decoder, interpolation model and two super resolution models that\ncan enable various applications besides T2V. In all aspects, spatial and\ntemporal resolution, faithfulness to text, and quality, Make-A-Video sets the\nnew state-of-the-art in text-to-video generation, as determined by both\nqualitative and quantitative measures.</p>\n", "tags": ["Applications","Model Architecture","Training Techniques"] },
{"key": "singh2019towards", "citations": "437", "year": "2019", "title":"Towards VQA Models That Can Read", "abstract": "<p>Studies have shown that a dominant class of questions asked by visually\nimpaired users on images of their surroundings involves reading text in the\nimage. But today’s VQA models can not read! Our paper takes a first step\ntowards addressing this problem. First, we introduce a new “TextVQA” dataset to\nfacilitate progress on this important problem. Existing datasets either have a\nsmall proportion of questions about text (e.g., the VQA dataset) or are too\nsmall (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408\nimages that require reasoning about text to answer. Second, we introduce a\nnovel model architecture that reads text in the image, reasons about it in the\ncontext of the image and the question, and predicts an answer which might be a\ndeduction based on the text and the image or composed of the strings found in\nthe image. Consequently, we call our approach Look, Read, Reason &amp; Answer\n(LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on\nour TextVQA dataset. We find that the gap between human performance and machine\nperformance is significantly larger on TextVQA than on VQA 2.0, suggesting that\nTextVQA is well-suited to benchmark progress along directions complementary to\nVQA 2.0.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture"] },
{"key": "singh2019xlda", "citations": "68", "year": "2019", "title":"XLDA: Cross-lingual Data Augmentation For Natural Language Inference And Question Answering", "abstract": "<p>While natural language processing systems often focus on a single language,\nmultilingual transfer learning has the potential to improve performance,\nespecially for low-resource languages. We introduce XLDA, cross-lingual data\naugmentation, a method that replaces a segment of the input text with its\ntranslation in another language. XLDA enhances performance of all 14 tested\nlanguages of the cross-lingual natural language inference (XNLI) benchmark.\nWith improvements of up to \\(4.8%\\), training with XLDA achieves\nstate-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast\nto, and performs markedly better than, a more naive approach that aggregates\nexamples in various languages in a way that each example is solely in one\nlanguage. On the SQuAD question answering task, we see that XLDA provides a\n\\(1.0%\\) performance increase on the English evaluation set. Comprehensive\nexperiments suggest that most languages are effective as cross-lingual\naugmentors, that XLDA is robust to a wide range of translation quality, and\nthat XLDA is even more effective for randomly initialized models than for\npretrained models.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "singh2021flava", "citations": "315", "year": "2022", "title":"FLAVA: A Foundational Language And Vision Alignment Model", "abstract": "<p>State-of-the-art vision and vision-and-language models rely on large-scale\nvisio-linguistic pretraining for obtaining good performance on a variety of\ndownstream tasks. Generally, such models are often either cross-modal\n(contrastive) or multi-modal (with earlier fusion) but not both; and they often\nonly target specific modalities or tasks. A promising direction would be to use\na single holistic universal model, as a “foundation”, that targets all\nmodalities at once – a true vision and language foundation model should be\ngood at vision tasks, language tasks, and cross- and multi-modal vision and\nlanguage tasks. We introduce FLAVA as such a model and demonstrate impressive\nperformance on a wide range of 35 tasks spanning these target modalities.</p>\n", "tags": ["CVPR"] },
{"key": "singh2021nlp", "citations": "113", "year": "2021", "title":"The NLP Cookbook: Modern Recipes For Transformer Based Deep Learning Architectures", "abstract": "<p>In recent years, Natural Language Processing (NLP) models have achieved\nphenomenal success in linguistic and semantic tasks like text classification,\nmachine translation, cognitive dialogue systems, information retrieval via\nNatural Language Understanding (NLU), and Natural Language Generation (NLG).\nThis feat is primarily attributed due to the seminal Transformer architecture,\nleading to designs such as BERT, GPT (I, II, III), etc. Although these\nlarge-size models have achieved unprecedented performances, they come at high\ncomputational costs. Consequently, some of the recent NLP architectures have\nutilized concepts of transfer learning, pruning, quantization, and knowledge\ndistillation to achieve moderate model sizes while keeping nearly similar\nperformances as achieved by their predecessors. Additionally, to mitigate the\ndata size challenge raised by language models from a knowledge extraction\nperspective, Knowledge Retrievers have been built to extricate explicit data\ndocuments from a large corpus of databases with greater efficiency and\naccuracy. Recent research has also focused on superior inference by providing\nefficient attention to longer input sequences. In this paper, we summarize and\nexamine the current state-of-the-art (SOTA) NLP models that have been employed\nfor numerous NLP tasks for optimal performance and efficiency. We provide a\ndetailed understanding and functioning of the different architectures, a\ntaxonomy of NLP designs, comparative evaluations, and future directions in NLP.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture"] },
{"key": "singh2021textocr", "citations": "98", "year": "2021", "title":"Textocr: Towards Large-scale End-to-end Reasoning For Arbitrary-shaped Scene Text", "abstract": "<p>A crucial component for the scene text based reasoning required for TextVQA\nand TextCaps datasets involve detecting and recognizing text present in the\nimages using an optical character recognition (OCR) system. The current systems\nare crippled by the unavailability of ground truth text annotations for these\ndatasets as well as lack of scene text detection and recognition datasets on\nreal images disallowing the progress in the field of OCR and evaluation of\nscene text based reasoning in isolation from OCR systems. In this work, we\npropose TextOCR, an arbitrary-shaped scene text detection and recognition with\n900k annotated words collected on real images from TextVQA dataset. We show\nthat current state-of-the-art text-recognition (OCR) models fail to perform\nwell on TextOCR and that training on TextOCR helps achieve state-of-the-art\nperformance on multiple other OCR datasets as well. We use a TextOCR trained\nOCR model to create PixelM4C model which can do scene text based reasoning on\nan image in an end-to-end fashion, allowing us to revisit several design\nchoices to achieve new state-of-the-art performance on TextVQA dataset.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Training Techniques"] },
{"key": "singh2022progprompt", "citations": "225", "year": "2023", "title":"Progprompt: Generating Situated Robot Task Plans Using Large Language Models", "abstract": "<p>Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io</p>\n", "tags": ["Has Code","ICRA","Prompting"] },
{"key": "singhal2022large", "citations": "1582", "year": "2023", "title":"Large Language Models Encode Clinical Knowledge", "abstract": "<p>Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but the quality bar for medical\nand clinical applications is high. Today, attempts to assess models’ clinical\nknowledge typically rely on automated evaluations on limited benchmarks. There\nis no standard to evaluate model predictions and reasoning across a breadth of\ntasks. To address this, we present MultiMedQA, a benchmark combining six\nexisting open question answering datasets spanning professional medical exams,\nresearch, and consumer queries; and HealthSearchQA, a new free-response dataset\nof medical questions searched online. We propose a framework for human\nevaluation of model answers along multiple axes including factuality,\nprecision, possible harm, and bias. In addition, we evaluate PaLM (a\n540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on\nMultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves\nstate-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA,\nMedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US\nMedical License Exam questions), surpassing prior state-of-the-art by over 17%.\nHowever, human evaluation reveals key gaps in Flan-PaLM responses. To resolve\nthis we introduce instruction prompt tuning, a parameter-efficient approach for\naligning LLMs to new domains using a few exemplars. The resulting model,\nMed-PaLM, performs encouragingly, but remains inferior to clinicians. We show\nthat comprehension, recall of knowledge, and medical reasoning improve with\nmodel scale and instruction prompt tuning, suggesting the potential utility of\nLLMs in medicine. Our human evaluations reveal important limitations of today’s\nmodels, reinforcing the importance of both evaluation frameworks and method\ndevelopment in creating safe, helpful LLM models for clinical applications.</p>\n", "tags": ["Applications","Datasets","Ethics & Fairness","Evaluation Frameworks","Evaluation","Prompting","Tools"] },
{"key": "singhal2023towards", "citations": "212", "year": "2023", "title":"Towards Expert-level Medical Question Answering With Large Language Models", "abstract": "<p>Recent artificial intelligence (AI) systems have reached milestones in “grand\nchallenges” ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a “passing” score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models’ answers were compared to\nclinicians’ answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n&lt; 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p &lt; 0.001) on newly introduced datasets of 240 long-form\n“adversarial” questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.</p>\n", "tags": ["Applications","Datasets","Evaluation Frameworks","Evaluation","Prompting"] },
{"key": "sinha2019clutrr", "citations": "93", "year": "2019", "title":"CLUTRR: A Diagnostic Benchmark For Inductive Reasoning From Text", "abstract": "<p>The recent success of natural language understanding (NLU) systems has been\ntroubled by results highlighting the failure of these models to generalize in a\nsystematic and robust way. In this work, we introduce a diagnostic benchmark\nsuite, named CLUTRR, to clarify some key issues related to the robustness and\nsystematicity of NLU systems. Motivated by classic work on inductive logic\nprogramming, CLUTRR requires that an NLU system infer kinship relations between\ncharacters in short stories. Successful performance on this task requires both\nextracting relationships between entities, as well as inferring the logical\nrules governing these relationships. CLUTRR allows us to precisely measure a\nmodel’s ability for systematic generalization by evaluating on held-out\ncombinations of logical rules, and it allows us to evaluate a model’s\nrobustness by adding curated noise facts. Our empirical results highlight a\nsubstantial performance gap between state-of-the-art NLU models (e.g., BERT and\nMAC) and a graph neural network model that works directly with symbolic\ninputs—with the graph-based model exhibiting both stronger generalization and\ngreater robustness.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "sinha2020learning", "citations": "66", "year": "2020", "title":"Learning An Unreferenced Metric For Online Dialogue Evaluation", "abstract": "<p>Evaluating the quality of a dialogue interaction between two agents is a\ndifficult task, especially in open-domain chit-chat style dialogue. There have\nbeen recent efforts to develop automatic dialogue evaluation metrics, but most\nof them do not generalize to unseen datasets and/or need a human-generated\nreference response during inference, making it infeasible for online\nevaluation. Here, we propose an unreferenced automated evaluation metric that\nuses large pre-trained language models to extract latent representations of\nutterances, and leverages the temporal transitions that exist between them. We\nshow that our model achieves higher correlation with human annotations in an\nonline setting, while not requiring true responses for comparison during\ninference.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "sinha2020unnatural", "citations": "67", "year": "2021", "title":"Unnatural Language Inference", "abstract": "<p>Recent investigations into the inner-workings of state-of-the-art large-scale\npre-trained Transformer-based Natural Language Understanding (NLU) models\nindicate that they appear to know humanlike syntax, at least to some extent. We\nprovide novel evidence that complicates this claim: we find that\nstate-of-the-art Natural Language Inference (NLI) models assign the same labels\nto permuted examples as they do to the original, i.e. they are largely\ninvariant to random word-order permutations. This behavior notably differs from\nthat of humans; we struggle with ungrammatical sentences. To measure the\nseverity of this issue, we propose a suite of metrics and investigate which\nproperties of particular permutations lead models to be word-order invariant.\nIn the MNLI dataset, for example, we find almost all (98.7%) examples contain\nat least one permutation which elicits the gold label. Models are sometimes\neven able to assign gold labels to permutations that they originally failed to\npredict correctly. We provide a comprehensive empirical evaluation of this\nphenomenon, and further show that this issue exists for both Transformers and\npre-Transformer RNN / ConvNet based encoders, as well as across multiple\nlanguages (English and Mandarin Chinese). Our code and data are available at\nhttps://github.com/facebookresearch/unlu.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "sinha2021masked", "citations": "165", "year": "2021", "title":"Masked Language Modeling And The Distributional Hypothesis: Order Word Matters Pre-training For Little", "abstract": "<p>A possible explanation for the impressive performance of masked language\nmodel (MLM) pre-training is that such models have learned to represent the\nsyntactic structures prevalent in classical NLP pipelines. In this paper, we\npropose a different explanation: MLMs succeed on downstream tasks almost\nentirely due to their ability to model higher-order word co-occurrence\nstatistics. To demonstrate this, we pre-train MLMs on sentences with randomly\nshuffled word order, and show that these models still achieve high accuracy\nafter fine-tuning on many downstream tasks – including on tasks specifically\ndesigned to be challenging for models that ignore word order. Our models\nperform surprisingly well according to some parametric syntactic probes,\nindicating possible deficiencies in how we test representations for syntactic\ninformation. Overall, our results show that purely distributional information\nlargely explains the success of pre-training, and underscore the importance of\ncurating challenging evaluation datasets that require deeper linguistic\nknowledge.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "siriwardhana2022improving", "citations": "106", "year": "2023", "title":"Improving The Domain Adaptation Of Retrieval Augmented Generation (RAG) Models For Open Domain Question Answering", "abstract": "<p>Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain\nQuestion Answering (ODQA). RAG has only been trained and explored with a\nWikipedia-based external knowledge base and is not optimized for use in other\nspecialized domains such as healthcare and news. In this paper, we evaluate the\nimpact of joint training of the retriever and generator components of RAG for\nthe task of domain adaptation in ODQA. We propose \\textit{RAG-end2end}, an\nextension to RAG, that can adapt to a domain-specific knowledge base by\nupdating all components of the external knowledge base during training. In\naddition, we introduce an auxiliary training signal to inject more\ndomain-specific knowledge. This auxiliary signal forces \\textit{RAG-end2end} to\nreconstruct a given sentence by accessing the relevant information from the\nexternal knowledge base. Our novel contribution is unlike RAG, RAG-end2end does\njoint training of the retriever and generator for the end QA task and domain\nadaptation. We evaluate our approach with datasets from three domains:\nCOVID-19, News, and Conversations, and achieve significant performance\nimprovements compared to the original RAG model. Our work has been open-sourced\nthrough the Huggingface Transformers library, attesting to our work’s\ncredibility and technical consistency.</p>\n", "tags": ["Datasets","Fine-Tuning","RAG","Retrieval Systems","TACL","Tools","Training Techniques"] },
{"key": "skerryryan2018towards", "citations": "230", "year": "2018", "title":"Towards End-to-end Prosody Transfer For Expressive Speech Synthesis With Tacotron", "abstract": "<p>We present an extension to the Tacotron speech synthesis architecture that\nlearns a latent embedding space of prosody, derived from a reference acoustic\nrepresentation containing the desired prosody. We show that conditioning\nTacotron on this learned embedding space results in synthesized audio that\nmatches the prosody of the reference signal with fine time detail even when the\nreference and synthesis speakers are different. Additionally, we show that a\nreference prosody embedding can be used to synthesize text that is different\nfrom that of the reference utterance. We define several quantitative and\nsubjective metrics for evaluating prosody transfer, and report results with\naccompanying audio samples from single-speaker and 44-speaker Tacotron models\non a prosody transfer task.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "smilkov2016embedding", "citations": "152", "year": "2016", "title":"Embedding Projector: Interactive Visualization And Interpretation Of Embeddings", "abstract": "<p>Embeddings are ubiquitous in machine learning, appearing in recommender\nsystems, NLP, and many other applications. Researchers and developers often\nneed to explore the properties of a specific embedding, and one way to analyze\nembeddings is to visualize them. We present the Embedding Projector, a tool for\ninteractive visualization and interpretation of embeddings.</p>\n", "tags": ["Applications"] },
{"key": "smilkov2019tensorflow", "citations": "122", "year": "2019", "title":"Tensorflow.js: Machine Learning For The Web And Beyond", "abstract": "<p>TensorFlow.js is a library for building and executing machine learning\nalgorithms in JavaScript. TensorFlow.js models run in a web browser and in the\nNode.js environment. The library is part of the TensorFlow ecosystem, providing\na set of APIs that are compatible with those in Python, allowing models to be\nported between the Python and JavaScript ecosystems. TensorFlow.js has\nempowered a new set of developers from the extensive JavaScript community to\nbuild and deploy machine learning models and enabled new classes of on-device\ncomputation. This paper describes the design, API, and implementation of\nTensorFlow.js, and highlights some of the impactful use cases.</p>\n", "tags": ["Applications","Tools"] },
{"key": "smirnova2017contextual", "citations": "159", "year": "2017", "title":"Contextual Sequence Modeling For Recommendation With Recurrent Neural Networks", "abstract": "<p>Recommendations can greatly benefit from good representations of the user\nstate at recommendation time. Recent approaches that leverage Recurrent Neural\nNetworks (RNNs) for session-based recommendations have shown that Deep Learning\nmodels can provide useful user representations for recommendation. However,\ncurrent RNN modeling approaches summarize the user state by only taking into\naccount the sequence of items that the user has interacted with in the past,\nwithout taking into account other essential types of context information such\nas the associated types of user-item interactions, the time gaps between events\nand the time of day for each interaction. To address this, we propose a new\nclass of Contextual Recurrent Neural Networks for Recommendation (CRNNs) that\ncan take into account the contextual information both in the input and output\nlayers and modifying the behavior of the RNN by combining the context embedding\nwith the item embedding and more explicitly, in the model dynamics, by\nparametrizing the hidden unit transitions as a function of context information.\nWe compare our CRNNs approach with RNNs and non-sequential baselines and show\ngood improvements on the next event prediction task.</p>\n", "tags": ["Time Series"] },
{"key": "smith2020can", "citations": "167", "year": "2020", "title":"Can You Put It All Together: Evaluating Conversational Agents' Ability To Blend Skills", "abstract": "<p>Being engaging, knowledgeable, and empathetic are all desirable general\nqualities in a conversational agent. Previous work has introduced tasks and\ndatasets that aim to help agents to learn those qualities in isolation and\ngauge how well they can express them. But rather than being specialized in one\nsingle quality, a good open-domain conversational agent should be able to\nseamlessly blend them all into one cohesive conversational flow. In this work,\nwe investigate several ways to combine models trained towards isolated\ncapabilities, ranging from simple model aggregation schemes that require\nminimal additional training, to various forms of multi-task training that\nencompass several skills at all training stages. We further propose a new\ndataset, BlendedSkillTalk, to analyze how these capabilities would mesh\ntogether in a natural conversation, and compare the performance of different\narchitectures and training schemes. Our experiments show that multi-tasking\nover several tasks that focus on particular capabilities results in better\nblended conversation performance compared to models trained on a single skill,\nand that both unified or two-stage approaches perform well if they are\nconstructed to avoid unwanted bias in skill selection or are fine-tuned on our\nnew task.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Training Techniques"] },
{"key": "smith2022coda", "citations": "103", "year": "2023", "title":"Coda-prompt: Continual Decomposed Attention-based Prompting For Rehearsal-free Continual Learning", "abstract": "<p>Computer vision models suffer from a phenomenon known as catastrophic\nforgetting when learning novel concepts from continuously shifting training\ndata. Typical solutions for this continual learning problem require extensive\nrehearsal of previously seen data, which increases memory costs and may violate\ndata privacy. Recently, the emergence of large-scale pre-trained vision\ntransformer models has enabled prompting approaches as an alternative to\ndata-rehearsal. These approaches rely on a key-query mechanism to generate\nprompts and have been found to be highly resistant to catastrophic forgetting\nin the well-established rehearsal-free continual learning setting. However, the\nkey mechanism of these methods is not trained end-to-end with the task\nsequence. Our experiments show that this leads to a reduction in their\nplasticity, hence sacrificing new task accuracy, and inability to benefit from\nexpanded parameter capacity. We instead propose to learn a set of prompt\ncomponents which are assembled with input-conditioned weights to produce\ninput-conditioned prompts, resulting in a novel attention-based end-to-end\nkey-query scheme. Our experiments show that we outperform the current SOTA\nmethod DualPrompt on established benchmarks by as much as 4.5% in average final\naccuracy. We also outperform the state of art by as much as 4.4% accuracy on a\ncontinual learning benchmark which contains both class-incremental and\ndomain-incremental task shifts, corresponding to many practical settings. Our\ncode is available at https://github.com/GT-RIPL/CODA-Prompt</p>\n", "tags": ["CVPR","Datasets","Evaluation","Has Code","Model Architecture","Prompting","Training Techniques"] },
{"key": "so2019evolved", "citations": "206", "year": "2019", "title":"The Evolved Transformer", "abstract": "<p>Recent works have highlighted the strength of the Transformer architecture on\nsequence tasks while, at the same time, neural architecture search (NAS) has\nbegun to outperform human-designed models. Our goal is to apply NAS to search\nfor a better alternative to the Transformer. We first construct a large search\nspace inspired by the recent advances in feed-forward sequence models and then\nrun evolutionary architecture search with warm starting by seeding our initial\npopulation with the Transformer. To directly search on the computationally\nexpensive WMT 2014 English-German translation task, we develop the Progressive\nDynamic Hurdles method, which allows us to dynamically allocate more resources\nto more promising candidate models. The architecture found in our experiments\n– the Evolved Transformer – demonstrates consistent improvement over the\nTransformer on four well-established language tasks: WMT 2014 English-German,\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\non WMT’14 English-German; at smaller sizes, it achieves the same quality as the\noriginal “big” Transformer with 37.6% less parameters and outperforms the\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.</p>\n", "tags": ["Model Architecture"] },
{"key": "solaiman2019release", "citations": "253", "year": "2019", "title":"Release Strategies And The Social Impacts Of Language Models", "abstract": "<p>Large language models have a range of beneficial uses: they can assist in\nprose, poetry, and programming; analyze dataset biases; and more. However,\ntheir flexibility and generative capabilities also raise misuse concerns. This\nreport discusses OpenAI’s work related to the release of its GPT-2 language\nmodel. It discusses staged release, which allows time between model releases to\nconduct risk and benefit analyses as model sizes increased. It also discusses\nongoing partnership-based research and provides recommendations for better\ncoordination and responsible publication in AI.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "solaiman2021process", "citations": "74", "year": "2021", "title":"Process For Adapting Language Models To Society (PALMS) With Values-targeted Datasets", "abstract": "<p>Language models can generate harmful and biased outputs and exhibit\nundesirable behavior according to a given cultural context. We propose a\nProcess for Adapting Language Models to Society (PALMS) with Values-Targeted\nDatasets, an iterative process to significantly change model behavior by\ncrafting and fine-tuning on a dataset that reflects a predetermined set of\ntarget values. We evaluate our process using three metrics: quantitative\nmetrics with human evaluations that score output adherence to a target value,\ntoxicity scoring on outputs; and qualitative metrics analyzing the most common\nword associated with a given social category. Through each iteration, we add\nadditional training dataset examples based on observed shortcomings from\nevaluations. PALMS performs significantly better on all metrics compared to\nbaseline and control models for a broad range of GPT-3 language model sizes\nwithout compromising capability integrity. We find that the effectiveness of\nPALMS increases with model size. We show that significantly adjusting language\nmodel behavior is feasible with a small, hand-curated dataset.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "song2016two", "citations": "89", "year": "2016", "title":"Two Are Better Than One: An Ensemble Of Retrieval- And Generation-based Dialog Systems", "abstract": "<p>Open-domain human-computer conversation has attracted much attention in the\nfield of NLP. Contrary to rule- or template-based domain-specific dialog\nsystems, open-domain conversation usually requires data-driven approaches,\nwhich can be roughly divided into two categories: retrieval-based and\ngeneration-based systems. Retrieval systems search a user-issued utterance\n(called a query) in a large database, and return a reply that best matches the\nquery. Generative approaches, typically based on recurrent neural networks\n(RNNs), can synthesize new replies, but they suffer from the problem of\ngenerating short, meaningless utterances. In this paper, we propose a novel\nensemble of retrieval-based and generation-based dialog systems in the open\ndomain. In our approach, the retrieved candidate, in addition to the original\nquery, is fed to an RNN-based reply generator, so that the neural model is\naware of more information. The generated reply is then fed back as a new\ncandidate for post-reranking. Experimental results show that such ensemble\noutperforms each single part of it by a large margin.</p>\n", "tags": ["Dialogue & Multi Turn","Model Architecture","Retrieval Systems"] },
{"key": "song2017hierarchical", "citations": "162", "year": "2017", "title":"Hierarchical LSTM With Adjusted Temporal Attention For Video Captioning", "abstract": "<p>Recent progress has been made in using attention based encoder-decoder\nframework for video captioning. However, most existing decoders apply the\nattention mechanism to every generated word including both visual words (e.g.,\n“gun” and “shooting”) and non-visual words (e.g. “the”, “a”). However, these\nnon-visual words can be easily predicted using natural language model without\nconsidering visual signals or attention. Imposing attention mechanism on\nnon-visual words could mislead and decrease the overall performance of video\ncaptioning. To address this issue, we propose a hierarchical LSTM with adjusted\ntemporal attention (hLSTMat) approach for video captioning. Specifically, the\nproposed framework utilizes the temporal attention for selecting specific\nframes to predict the related words, while the adjusted temporal attention is\nfor deciding whether to depend on the visual information or the language\ncontext information. Also, a hierarchical LSTMs is designed to simultaneously\nconsider both low-level visual information and high-level language context\ninformation to support the video caption generation. To demonstrate the\neffectiveness of our proposed framework, we test our method on two prevalent\ndatasets: MSVD and MSR-VTT, and experimental results show that our approach\noutperforms the state-of-the-art methods on both two datasets.</p>\n", "tags": ["Datasets","IJCAI","Model Architecture"] },
{"key": "song2018auditing", "citations": "165", "year": "2019", "title":"Auditing Data Provenance In Text-generation Models", "abstract": "<p>To help enforce data-protection regulations such as GDPR and detect\nunauthorized uses of personal data, we develop a new <em>model auditing</em>\ntechnique that helps users check if their data was used to train a machine\nlearning model. We focus on auditing deep-learning models that generate\nnatural-language text, including word prediction and dialog generation. These\nmodels are at the core of popular online services and are often trained on\npersonal data such as users’ messages, searches, chats, and comments.\n  We design and evaluate a black-box auditing method that can detect, with very\nfew queries to a model, if a particular user’s texts were used to train it\n(among thousands of other users). We empirically show that our method can\nsuccessfully audit well-generalized models that are not overfitted to the\ntraining data. We also analyze how text-generation models memorize word\nsequences and explain why this memorization makes them amenable to auditing.</p>\n", "tags": ["KDD","Training Techniques"] },
{"key": "song2018exploring", "citations": "84", "year": "2018", "title":"Exploring Graph-structured Passage Representation For Multi-hop Reading Comprehension With Graph Neural Networks", "abstract": "<p>Multi-hop reading comprehension focuses on one type of factoid question,\nwhere a system needs to properly integrate multiple pieces of evidence to\ncorrectly answer a question. Previous work approximates global evidence with\nlocal coreference information, encoding coreference chains with DAG-styled GRU\nlayers within a gated-attention reader. However, coreference is limited in\nproviding information for rich inference. We introduce a new method for better\nconnecting global evidence, which forms more complex graphs compared to DAGs.\nTo perform evidence integration on our graphs, we investigate two recent graph\nneural networks, namely graph convolutional network (GCN) and graph recurrent\nnetwork (GRN). Experiments on two standard datasets show that richer global\ninformation leads to better answers. Our method performs better than all\npublished results on these datasets.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "song2018graph", "citations": "201", "year": "2018", "title":"A Graph-to-sequence Model For Amr-to-text Generation", "abstract": "<p>The problem of AMR-to-text generation is to recover a text representing the\nsame meaning as an input AMR graph. The current state-of-the-art method uses a\nsequence-to-sequence model, leveraging LSTM for encoding a linearized AMR\nstructure. Although being able to model non-local semantic information, a\nsequence LSTM can lose information from the AMR graph structure, and thus faces\nchallenges with large graphs, which result in long sequences. We introduce a\nneural graph-to-sequence model, using a novel LSTM structure for directly\nencoding graph-level semantics. On a standard benchmark, our model shows\nsuperior results to existing methods in the literature.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "song2018hierarchical", "citations": "242", "year": "2019", "title":"Hierarchical Lstms With Adaptive Attention For Visual Captioning", "abstract": "<p>Recent progress has been made in using attention based encoder-decoder\nframework for image and video captioning. Most existing decoders apply the\nattention mechanism to every generated word including both visual words (e.g.,\n“gun” and “shooting”) and non-visual words (e.g. “the”, “a”). However, these\nnon-visual words can be easily predicted using natural language model without\nconsidering visual signals or attention. Imposing attention mechanism on\nnon-visual words could mislead and decrease the overall performance of visual\ncaptioning. Furthermore, the hierarchy of LSTMs enables more complex\nrepresentation of visual data, capturing information at different scales. To\naddress these issues, we propose a hierarchical LSTM with adaptive attention\n(hLSTMat) approach for image and video captioning. Specifically, the proposed\nframework utilizes the spatial or temporal attention for selecting specific\nregions or frames to predict the related words, while the adaptive attention is\nfor deciding whether to depend on the visual information or the language\ncontext information. Also, a hierarchical LSTMs is designed to simultaneously\nconsider both low-level visual information and high-level language context\ninformation to support the caption generation. We initially design our hLSTMat\nfor video captioning task. Then, we further refine it and apply it to image\ncaptioning task. To demonstrate the effectiveness of our proposed framework, we\ntest our method on both video and image captioning tasks. Experimental results\nshow that our approach achieves the state-of-the-art performance for most of\nthe evaluation metrics on both tasks. The effect of important components is\nalso well exploited in the ablation study.</p>\n", "tags": ["Evaluation","Model Architecture","Tools"] },
{"key": "song2019exploiting", "citations": "120", "year": "2019", "title":"Exploiting Persona Information For Diverse Generation Of Conversational Responses", "abstract": "<p>In human conversations, due to their personalities in mind, people can easily\ncarry out and maintain the conversations. Giving conversational context with\npersona information to a chatbot, how to exploit the information to generate\ndiverse and sustainable conversations is still a non-trivial task. Previous\nwork on persona-based conversational models successfully make use of predefined\npersona information and have shown great promise in delivering more realistic\nresponses. And they all learn with the assumption that given a source input,\nthere is only one target response. However, in human conversations, there are\nmassive appropriate responses to a given input message. In this paper, we\npropose a memory-augmented architecture to exploit persona information from\ncontext and incorporate a conditional variational autoencoder model together to\ngenerate diverse and sustainable conversations. We evaluate the proposed model\non a benchmark persona-chat dataset. Both automatic and human evaluations show\nthat our model can deliver more diverse and more engaging persona-based\nresponses than baseline approaches.</p>\n", "tags": ["Datasets","Evaluation","IJCAI","Model Architecture"] },
{"key": "song2019mass", "citations": "524", "year": "2019", "title":"MASS: Masked Sequence To Sequence Pre-training For Language Generation", "abstract": "<p>Pre-training and fine-tuning, e.g., BERT, have achieved great success in\nlanguage understanding by transferring knowledge from rich-resource\npre-training task to the low/zero-resource downstream tasks. Inspired by the\nsuccess of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for\nthe encoder-decoder based language generation tasks. MASS adopts the\nencoder-decoder framework to reconstruct a sentence fragment given the\nremaining part of the sentence: its encoder takes a sentence with randomly\nmasked fragment (several consecutive tokens) as input, and its decoder tries to\npredict this masked fragment. In this way, MASS can jointly train the encoder\nand decoder to develop the capability of representation extraction and language\nmodeling. By further fine-tuning on a variety of zero/low-resource language\ngeneration tasks, including neural machine translation, text summarization and\nconversational response generation (3 tasks and totally 8 datasets), MASS\nachieves significant improvements over the baselines without pre-training or\nwith other pre-training methods. Specially, we achieve the state-of-the-art\naccuracy (37.5 in terms of BLEU score) on the unsupervised English-French\ntranslation, even beating the early attention-based supervised model.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "song2019semantic", "citations": "104", "year": "2019", "title":"Semantic Neural Machine Translation Using AMR", "abstract": "<p>It is intuitive that semantic representations can be useful for machine\ntranslation, mainly because they can help in enforcing meaning preservation and\nhandling data sparsity (many sentences correspond to one meaning) of machine\ntranslation models. On the other hand, little work has been done on leveraging\nsemantics for neural machine translation (NMT). In this work, we study the\nusefulness of AMR (short for abstract meaning representation) on NMT.\nExperiments on a standard English-to-German dataset show that incorporating AMR\nas additional knowledge can significantly improve a strong attention-based\nsequence-to-sequence neural translation model.</p>\n", "tags": ["Datasets","Model Architecture","TACL"] },
{"key": "song2020mpnet", "citations": "419", "year": "2020", "title":"Mpnet: Masked And Permuted Pre-training For Language Understanding", "abstract": "<p>BERT adopts masked language modeling (MLM) for pre-training and is one of the\nmost successful pre-training models. Since BERT neglects dependency among\npredicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full\nposition information of a sentence and thus suffers from position discrepancy\nbetween pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids\ntheir limitations. MPNet leverages the dependency among predicted tokens\nthrough permuted language modeling (vs. MLM in BERT), and takes auxiliary\nposition information as input to make the model see a full sentence and thus\nreducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a\nlarge-scale dataset (over 160GB text corpora) and fine-tune on a variety of\ndown-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet\noutperforms MLM and PLM by a large margin, and achieves better results on these\ntasks compared with previous state-of-the-art pre-trained methods (e.g., BERT,\nXLNet, RoBERTa) under the same model setting. The code and the pre-trained\nmodels are available at: https://github.com/microsoft/MPNet.</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "song2020towards", "citations": "84", "year": "2020", "title":"Towards Automated Neural Interaction Discovery For Click-through Rate Prediction", "abstract": "<p>Click-Through Rate (CTR) prediction is one of the most important machine\nlearning tasks in recommender systems, driving personalized experience for\nbillions of consumers. Neural architecture search (NAS), as an emerging field,\nhas demonstrated its capabilities in discovering powerful neural network\narchitectures, which motivates us to explore its potential for CTR predictions.\nDue to 1) diverse unstructured feature interactions, 2) heterogeneous feature\nspace, and 3) high data volume and intrinsic data randomness, it is challenging\nto construct, search, and compare different architectures effectively for\nrecommendation models. To address these challenges, we propose an automated\ninteraction architecture discovering framework for CTR prediction named\nAutoCTR. Via modularizing simple yet representative interactions as virtual\nbuilding blocks and wiring them into a space of direct acyclic graphs, AutoCTR\nperforms evolutionary architecture exploration with learning-to-rank guidance\nat the architecture level and achieves acceleration using low-fidelity model.\nEmpirical analysis demonstrates the effectiveness of AutoCTR on different\ndatasets comparing to human-crafted architectures. The discovered architecture\nalso enjoys generalizability and transferability among different datasets.</p>\n", "tags": ["Datasets","KDD","Model Architecture","Tools"] },
{"key": "song2021bob", "citations": "81", "year": "2021", "title":"Bob: BERT Over BERT For Training Persona-based Dialogue Models From Limited Personalized Data", "abstract": "<p>Maintaining consistent personas is essential for dialogue agents. Although\ntremendous advancements have been brought, the limited-scale of annotated\npersona-dense data are still barriers towards training robust and consistent\npersona-based dialogue models. In this work, we show how the challenges can be\naddressed by disentangling persona-based dialogue generation into two sub-tasks\nwith a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a\nBERT-based encoder and two BERT-based decoders, where one decoder is for\nresponse generation, and another is for consistency understanding. In\nparticular, to learn the ability of consistency understanding from large-scale\nnon-dialogue inference data, we train the second decoder in an unlikelihood\nmanner. Under different limited data settings, both automatic and human\nevaluations demonstrate that the proposed model outperforms strong baselines in\nresponse quality and persona consistency.</p>\n", "tags": ["Dialogue & Multi Turn","Model Architecture","Training Techniques"] },
{"key": "song2022clip", "citations": "69", "year": "2022", "title":"CLIP Models Are Few-shot Learners: Empirical Studies On VQA And Visual Entailment", "abstract": "<p>CLIP has shown a remarkable zero-shot capability on a wide range of vision\ntasks. Previously, CLIP is only regarded as a powerful visual encoder. However,\nafter being pre-trained by language supervision from a large amount of\nimage-caption pairs, CLIP itself should also have acquired some few-shot\nabilities for vision-language tasks. In this work, we empirically show that\nCLIP can be a strong vision-language few-shot learner by leveraging the power\nof language. We first evaluate CLIP’s zero-shot performance on a typical visual\nquestion answering task and demonstrate a zero-shot cross-modality transfer\ncapability of CLIP on the visual entailment task. Then we propose a\nparameter-efficient fine-tuning strategy to boost the few-shot performance on\nthe vqa task. We achieve competitive zero/few-shot results on the visual\nquestion answering and visual entailment tasks without introducing any\nadditional pre-training procedure.</p>\n", "tags": ["Few-Shot","Fine-Tuning","Training Techniques"] },
{"key": "song2022llm", "citations": "143", "year": "2023", "title":"Llm-planner: Few-shot Grounded Planning For Embodied Agents With Large Language Models", "abstract": "<p>This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner</p>\n", "tags": ["Datasets","Few-Shot","Has Code","ICCV","Training Techniques"] },
{"key": "sorensen2022information", "citations": "81", "year": "2022", "title":"An Information-theoretic Approach To Prompt Engineering Without Ground Truth Labels", "abstract": "<p>Pre-trained language models derive substantial linguistic and factual\nknowledge from the massive corpora on which they are trained, and prompt\nengineering seeks to align these models to specific tasks. Unfortunately,\nexisting prompt engineering methods require significant amounts of labeled\ndata, access to model parameters, or both. We introduce a new method for\nselecting prompt templates \\textit{without labeled examples} and\n\\textit{without direct access to the model}. Specifically, over a set of\ncandidate templates, we choose the template that maximizes the mutual\ninformation between the input and the corresponding model output. Across 8\ndatasets representing 7 distinct NLP tasks, we show that when a template has\nhigh mutual information, it also has high accuracy on the task. On the largest\nmodel, selecting prompts with our method gets 90% of the way from the average\nprompt accuracy to the best prompt accuracy and requires no ground truth\nlabels.</p>\n", "tags": ["Datasets","Prompting"] },
{"key": "sperber2017neural", "citations": "74", "year": "2017", "title":"Neural Lattice-to-sequence Models For Uncertain Inputs", "abstract": "<p>The input to a neural sequence-to-sequence model is often determined by an\nup-stream system, e.g. a word segmenter, part of speech tagger, or speech\nrecognizer. These up-stream models are potentially error-prone. Representing\ninputs through word lattices allows making this uncertainty explicit by\ncapturing alternative sequences and their posterior probabilities in a compact\nform. In this work, we extend the TreeLSTM (Tai et al., 2015) into a\nLatticeLSTM that is able to consume word lattices, and can be used as encoder\nin an attentional encoder-decoder model. We integrate lattice posterior scores\ninto this architecture by extending the TreeLSTM’s child-sum and forget gates\nand introducing a bias term into the attention mechanism. We experiment with\nspeech translation lattices and report consistent improvements over baselines\nthat translate either the 1-best hypothesis or the lattice without posterior\nscores.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "sperber2019attention", "citations": "106", "year": "2019", "title":"Attention-passing Models For Robust And Data-efficient End-to-end Speech Translation", "abstract": "<p>Speech translation has traditionally been approached through cascaded models\nconsisting of a speech recognizer trained on a corpus of transcribed speech,\nand a machine translation system trained on parallel texts. Several recent\nworks have shown the feasibility of collapsing the cascade into a single,\ndirect model that can be trained in an end-to-end fashion on a corpus of\ntranslated speech. However, experiments are inconclusive on whether the cascade\nor the direct model is stronger, and have only been conducted under the\nunrealistic assumption that both are trained on equal amounts of data, ignoring\nother available speech recognition and machine translation corpora.\n  In this paper, we demonstrate that direct speech translation models require\nmore data to perform well than cascaded models, and while they allow including\nauxiliary data through multi-task training, they are poor at exploiting such\ndata, putting them at a severe disadvantage. As a remedy, we propose the use of\nend-to-end trainable models with two attention mechanisms, the first\nestablishing source speech to source text alignments, the second modeling\nsource to target text alignment. We show that such models naturally decompose\ninto multi-task-trainable recognition and translation tasks and propose an\nattention-passing technique that alleviates error propagation issues in a\nprevious formulation of a model with two attention stages. Our proposed model\noutperforms all examined baselines and is able to exploit auxiliary training\ndata much more effectively than direct attentional models.</p>\n", "tags": ["Datasets","Model Architecture","TACL","Training Techniques"] },
{"key": "sperber2020speech", "citations": "83", "year": "2020", "title":"Speech Translation And The End-to-end Promise: Taking Stock Of Where We Are", "abstract": "<p>Over its three decade history, speech translation has experienced several\nshifts in its primary research themes; moving from loosely coupled cascades of\nspeech recognition and machine translation, to exploring questions of tight\ncoupling, and finally to end-to-end models that have recently attracted much\nattention. This paper provides a brief survey of these developments, along with\na discussion of the main challenges of traditional approaches which stem from\ncommitting to intermediate representations from the speech recognizer, and from\ntraining cascaded models separately towards different objectives.\n  Recent end-to-end modeling techniques promise a principled way of overcoming\nthese issues by allowing joint training of all model components and removing\nthe need for explicit intermediate representations. However, a closer look\nreveals that many end-to-end models fall short of solving these issues, due to\ncompromises made to address data scarcity. This paper provides a unifying\ncategorization and nomenclature that covers both traditional and recent\napproaches and that may help researchers by highlighting both trade-offs and\nopen research questions.</p>\n", "tags": ["Model Architecture","Survey Paper","Training Techniques"] },
{"key": "srinivasan2021wit", "citations": "130", "year": "2021", "title":"WIT: Wikipedia-based Image Text Dataset For Multimodal Multilingual Machine Learning", "abstract": "<p>The milestone improvements brought about by deep representation learning and\npre-training techniques have led to large performance gains across downstream\nNLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large\nhigh-quality visio-linguistic datasets for learning complementary information\n(across image and text modalities). In this paper, we introduce the\nWikipedia-based Image Text (WIT) Dataset\n(https://github.com/google-research-datasets/wit) to better facilitate\nmultimodal, multilingual learning. WIT is composed of a curated set of 37.6\nmillion entity rich image-text examples with 11.5 million unique images across\n108 Wikipedia languages. Its size enables WIT to be used as a pretraining\ndataset for multimodal models, as we show when applied to downstream tasks such\nas image-text retrieval. WIT has four main and unique advantages. First, WIT is\nthe largest multimodal dataset by the number of image-text examples by 3x (at\nthe time of writing). Second, WIT is massively multilingual (first of its kind)\nwith coverage over 100+ languages (each of which has at least 12K examples) and\nprovides cross-lingual texts for many images. Third, WIT represents a more\ndiverse set of concepts and real world entities relative to what previous\ndatasets cover. Lastly, WIT provides a very challenging real-world test set, as\nwe empirically illustrate using an image-text retrieval task as an example.</p>\n", "tags": ["Datasets","Evaluation","Has Code","SIGIR","Training Techniques"] },
{"key": "sriram2017cold", "citations": "249", "year": "2018", "title":"Cold Fusion: Training Seq2seq Models Together With Language Models", "abstract": "<p>Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks\nwhich involve generating natural language sentences such as machine\ntranslation, image captioning and speech recognition. Performance has further\nbeen improved by leveraging unlabeled data, often in the form of a language\nmodel. In this work, we present the Cold Fusion method, which leverages a\npre-trained language model during training, and show its effectiveness on the\nspeech recognition task. We show that Seq2Seq models with Cold Fusion are able\nto better utilize language information enjoying i) faster convergence and\nbetter generalization, and ii) almost complete transfer to a new domain while\nusing less than 10% of the labeled training data.</p>\n", "tags": ["INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "srivastava2022beyond", "citations": "422", "year": "2022", "title":"Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models", "abstract": "<p>Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit “breakthrough” behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.</p>\n", "tags": ["Datasets","Ethics & Fairness","Evaluation Frameworks","Evaluation","Model Architecture","Prompting"] },
{"key": "stahlberg2016syntactically", "citations": "80", "year": "2016", "title":"Syntactically Guided Neural Machine Translation", "abstract": "<p>We investigate the use of hierarchical phrase-based SMT lattices in\nend-to-end neural machine translation (NMT). Weight pushing transforms the\nHiero scores for complete translation hypotheses, with the full translation\ngrammar score and full n-gram language model score, into posteriors compatible\nwith NMT predictive probabilities. With a slightly modified NMT beam-search\ndecoder we find gains over both Hiero and NMT decoding alone, with practical\nadvantages in extending NMT to very large input and output vocabularies.</p>\n", "tags": [] },
{"key": "stahlberg2018simple", "citations": "65", "year": "2018", "title":"Simple Fusion: Return Of The Language Model", "abstract": "<p>Neural Machine Translation (NMT) typically leverages monolingual data in\ntraining through backtranslation. We investigate an alternative simple method\nto use monolingual data for NMT training: We combine the scores of a\npre-trained and fixed language model (LM) with the scores of a translation\nmodel (TM) while the TM is trained from scratch. To achieve that, we train the\ntranslation model to predict the residual probability of the training data\nadded to the prediction of the LM. This enables the TM to focus its capacity on\nmodeling the source sentence since it can rely on the LM for fluency. We show\nthat our method outperforms previous approaches to integrate LMs into NMT while\nthe architecture is simpler as it does not require gating networks to balance\nTM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test\nsets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top\nof ensembles without LM. We compare our method with alternative ways to utilize\nmonolingual data such as backtranslation, shallow fusion, and cold fusion.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "stahlberg2019nmt", "citations": "121", "year": "2019", "title":"On NMT Search Errors And Model Errors: Cat Got Your Tongue?", "abstract": "<p>We report on search errors and model errors in neural machine translation\n(NMT). We present an exact inference procedure for neural sequence models based\non a combination of beam search and depth-first search. We use our exact search\nto find the global best model scores under a Transformer base model for the\nentire WMT15 English-German test set. Surprisingly, beam search fails to find\nthese global best model scores in most cases, even with a very large beam size\nof 100. For more than 50% of the sentences, the model in fact assigns its\nglobal best score to the empty translation, revealing a massive failure of\nneural models in properly accounting for adequacy. We show by constraining\nsearch with a minimum translation length that at the root of the problem of\nempty translations lies an inherent bias towards shorter translations. We\nconclude that vanilla NMT in its current form requires just the right amount of\nbeam search errors, which, from a modelling perspective, is a highly\nunsatisfactory conclusion indeed, as the model often prefers an empty\ntranslation.</p>\n", "tags": ["EMNLP","Evaluation","Model Architecture"] },
{"key": "stanton2018predicting", "citations": "109", "year": "2018", "title":"Predicting Expressive Speaking Style From Text In End-to-end Speech Synthesis", "abstract": "<p>Global Style Tokens (GSTs) are a recently-proposed method to learn latent\ndisentangled representations of high-dimensional data. GSTs can be used within\nTacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to\nuncover expressive factors of variation in speaking style. In this work, we\nintroduce the Text-Predicted Global Style Token (TP-GST) architecture, which\ntreats GST combination weights or style embeddings as “virtual” speaking style\nlabels within Tacotron. TP-GST learns to predict stylistic renderings from text\nalone, requiring neither explicit labels during training nor auxiliary inputs\nfor inference. We show that, when trained on a dataset of expressive speech,\nour system generates audio with more pitch and energy variation than two\nstate-of-the-art baseline models. We further demonstrate that TP-GSTs can\nsynthesize speech with background noise removed, and corroborate these analyses\nwith positive results on human-rated listener preference audiobook tasks.\nFinally, we demonstrate that multi-speaker TP-GST models successfully factorize\nspeaker identity and speaking style. We provide a website with audio samples\nfor each of our findings.</p>\n", "tags": ["Datasets","Model Architecture","SLT","Training Techniques"] },
{"key": "stern2019insertion", "citations": "160", "year": "2019", "title":"Insertion Transformer: Flexible Sequence Generation Via Insertion Operations", "abstract": "<p>We present the Insertion Transformer, an iterative, partially autoregressive\nmodel for sequence generation based on insertion operations. Unlike typical\nautoregressive models which rely on a fixed, often left-to-right ordering of\nthe output, our approach accommodates arbitrary orderings by allowing for\ntokens to be inserted anywhere in the sequence during decoding. This\nflexibility confers a number of advantages: for instance, not only can our\nmodel be trained to follow specific orderings such as left-to-right generation\nor a binary tree traversal, but it can also be trained to maximize entropy over\nall valid insertions for robustness. In addition, our model seamlessly\naccommodates both fully autoregressive generation (one insertion at a time) and\npartially autoregressive generation (simultaneous insertions at multiple\nlocations). We validate our approach by analyzing its performance on the WMT\n2014 English-German machine translation task under various settings for\ntraining and decoding. We find that the Insertion Transformer outperforms many\nprior non-autoregressive approaches to translation at comparable or better\nlevels of parallelism, and successfully recovers the performance of the\noriginal Transformer while requiring only logarithmically many iterations\nduring decoding.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "stickland2019bert", "citations": "117", "year": "2019", "title":"BERT And Pals: Projected Attention Layers For Efficient Adaptation In Multi-task Learning", "abstract": "<p>Multi-task learning shares information between related tasks, sometimes\nreducing the number of parameters required. State-of-the-art results across\nmultiple natural language understanding tasks in the GLUE benchmark have\npreviously used transfer from a single large task: unsupervised pre-training\nwith BERT, where a separate BERT model was fine-tuned for each task. We explore\nmulti-task approaches that share a single BERT model with a small number of\nadditional task-specific parameters. Using new adaptation modules, PALs or\n`projected attention layers’, we match the performance of separately fine-tuned\nmodels on the GLUE benchmark with roughly 7 times fewer parameters, and obtain\nstate-of-the-art results on the Recognizing Textual Entailment dataset.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "stiennon2020learning", "citations": "357", "year": "2020", "title":"Learning To Summarize From Human Feedback", "abstract": "<p>As language models become more powerful, training and evaluation are\nincreasingly bottlenecked by the data and metrics used for a particular task.\nFor example, summarization models are often trained to predict human reference\nsummaries and evaluated using ROUGE, but both of these metrics are rough\nproxies for what we really care about – summary quality. In this work, we show\nthat it is possible to significantly improve summary quality by training a\nmodel to optimize for human preferences. We collect a large, high-quality\ndataset of human comparisons between summaries, train a model to predict the\nhuman-preferred summary, and use that model as a reward function to fine-tune a\nsummarization policy using reinforcement learning. We apply our method to a\nversion of the TL;DR dataset of Reddit posts and find that our models\nsignificantly outperform both human reference summaries and much larger models\nfine-tuned with supervised learning alone. Our models also transfer to CNN/DM\nnews articles, producing summaries nearly as good as the human reference\nwithout any news-specific fine-tuning. We conduct extensive analyses to\nunderstand our human feedback dataset and fine-tuned models We establish that\nour reward model generalizes to new datasets, and that optimizing our reward\nmodel results in better summaries than optimizing ROUGE according to humans. We\nhope the evidence from our paper motivates machine learning researchers to pay\ncloser attention to how their training loss affects the model behavior they\nactually want.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "stoian2019analyzing", "citations": "63", "year": "2020", "title":"Analyzing ASR Pretraining For Low-resource Speech-to-text Translation", "abstract": "<p>Previous work has shown that for low-resource source languages, automatic\nspeech-to-text translation (AST) can be improved by pretraining an end-to-end\nmodel on automatic speech recognition (ASR) data from a high-resource language.\nHowever, it is not clear what factors –e.g., language relatedness or size of\nthe pretraining data– yield the biggest improvements, or whether pretraining\ncan be effectively combined with other methods such as data augmentation. Here,\nwe experiment with pretraining on datasets of varying sizes, including\nlanguages related and unrelated to the AST source language. We find that the\nbest predictor of final AST performance is the word error rate of the\npretrained ASR model, and that differences in ASR/AST performance correlate\nwith how phonetic information is encoded in the later RNN layers of our model.\nWe also show that pretraining and data augmentation yield complementary\nbenefits for AST.</p>\n", "tags": ["Datasets","ICASSP"] },
{"key": "stone2022artificial", "citations": "96", "year": "2016", "title":"Artificial Intelligence And Life In 2030: The One Hundred Year Study On Artificial Intelligence", "abstract": "<p>In September 2016, Stanford’s “One Hundred Year Study on Artificial\nIntelligence” project (AI100) issued the first report of its planned long-term\nperiodic assessment of artificial intelligence (AI) and its impact on society.\nIt was written by a panel of 17 study authors, each of whom is deeply rooted in\nAI research, chaired by Peter Stone of the University of Texas at Austin. The\nreport, entitled “Artificial Intelligence and Life in 2030,” examines eight\ndomains of typical urban settings on which AI is likely to have impact over the\ncoming years: transportation, home and service robots, healthcare, education,\npublic safety and security, low-resource communities, employment and workplace,\nand entertainment. It aims to provide the general public with a scientifically\nand technologically accurate portrayal of the current state of AI and its\npotential and to help guide decisions in industry and governments, as well as\nto inform research and development in the field. The charge for this report was\ngiven to the panel by the AI100 Standing Committee, chaired by Barbara Grosz of\nHarvard University.</p>\n", "tags": ["Security"] },
{"key": "strobelt2018seq2seq", "citations": "218", "year": "2018", "title":"Seq2seq-vis: A Visual Debugging Tool For Sequence-to-sequence Models", "abstract": "<p>Neural Sequence-to-Sequence models have proven to be accurate and robust for\nmany sequence prediction tasks, and have become the standard approach for\nautomatic translation of text. The models work in a five stage blackbox process\nthat involves encoding a source sequence to a vector space and then decoding\nout to a new target sequence. This process is now standard, but like many deep\nlearning methods remains quite difficult to understand or debug. In this work,\nwe present a visual analysis tool that allows interaction with a trained\nsequence-to-sequence model through each stage of the translation process. The\naim is to identify which patterns have been learned and to detect model errors.\nWe demonstrate the utility of our tool through several real-world large-scale\nsequence-to-sequence use cases.</p>\n", "tags": ["Applications","Tools"] },
{"key": "strobelt2022interactive", "citations": "118", "year": "2022", "title":"Interactive And Visual Prompt Engineering For Ad-hoc Task Adaptation With Large Language Models", "abstract": "<p>State-of-the-art neural language models can now be used to solve ad-hoc\nlanguage tasks through zero-shot prompting without the need for supervised\ntraining. This approach has gained popularity in recent years, and researchers\nhave demonstrated prompts that achieve strong accuracy on specific NLP tasks.\nHowever, finding a prompt for new tasks requires experimentation. Different\nprompt templates with different wording choices lead to significant accuracy\ndifferences. PromptIDE allows users to experiment with prompt variations,\nvisualize prompt performance, and iteratively optimize prompts. We developed a\nworkflow that allows users to first focus on model feedback using small data\nbefore moving on to a large data regime that allows empirical grounding of\npromising prompts using quantitative measures of the task. The tool then allows\neasy deployment of the newly created ad-hoc models. We demonstrate the utility\nof PromptIDE (demo at http://prompt.vizhub.ai) and our workflow using several\nreal-world use cases.</p>\n", "tags": ["Applications","Prompting","Training Techniques"] },
{"key": "strub2017end", "citations": "94", "year": "2017", "title":"End-to-end Optimization Of Goal-driven And Visually Grounded Dialogue Systems", "abstract": "<p>End-to-end design of dialogue systems has recently become a popular research\ntopic thanks to powerful tools such as encoder-decoder architectures for\nsequence-to-sequence learning. Yet, most current approaches cast human-machine\ndialogue management as a supervised learning problem, aiming at predicting the\nnext utterance of a participant given the full history of the dialogue. This\nvision is too simplistic to render the intrinsic planning problem inherent to\ndialogue as well as its grounded nature, making the context of a dialogue\nlarger than the sole history. This is why only chit-chat and question answering\ntasks have been addressed so far using end-to-end architectures. In this paper,\nwe introduce a Deep Reinforcement Learning method to optimize visually grounded\ntask-oriented dialogues, based on the policy gradient algorithm. This approach\nis tested on a dataset of 120k dialogues collected through Mechanical Turk and\nprovides encouraging results at solving both the problem of generating natural\ndialogues and the task of discovering a specific object in a complex picture.</p>\n", "tags": ["Dialogue & Multi Turn","Efficiency","IJCAI","Reinforcement Learning"] },
{"key": "su2016lattice", "citations": "65", "year": "2016", "title":"Lattice-based Recurrent Neural Network Encoders For Neural Machine Translation", "abstract": "<p>Neural machine translation (NMT) heavily relies on word-level modelling to\nlearn semantic representations of input sentences. However, for languages\nwithout natural word delimiters (e.g., Chinese) where input sentences have to\nbe tokenized first, conventional NMT is confronted with two issues: 1) it is\ndifficult to find an optimal tokenization granularity for source sentence\nmodelling, and 2) errors in 1-best tokenizations may propagate to the encoder\nof NMT. To handle these issues, we propose word-lattice based Recurrent Neural\nNetwork (RNN) encoders for NMT, which generalize the standard RNN to word\nlattice topology. The proposed encoders take as input a word lattice that\ncompactly encodes multiple tokenizations, and learn to generate new hidden\nstates from arbitrarily many inputs and hidden states in preceding time steps.\nAs such, the word-lattice based encoders not only alleviate the negative impact\nof tokenization errors but also are more expressive and flexible to embed input\nsentences. Experiment results on Chinese-English translation demonstrate the\nsuperiorities of the proposed encoders over the conventional encoder.</p>\n", "tags": [] },
{"key": "su2016line", "citations": "102", "year": "2016", "title":"On-line Active Reward Learning For Policy Optimisation In Spoken Dialogue Systems", "abstract": "<p>The ability to compute an accurate reward function is essential for\noptimising a dialogue policy via reinforcement learning. In real-world\napplications, using explicit user feedback as the reward signal is often\nunreliable and costly to collect. This problem can be mitigated if the user’s\nintent is known in advance or data is available to pre-train a task success\npredictor off-line. In practice neither of these apply for most real world\napplications. Here we propose an on-line learning framework whereby the\ndialogue policy is jointly trained alongside the reward model via active\nlearning with a Gaussian process model. This Gaussian process operates on a\ncontinuous space dialogue representation generated in an unsupervised fashion\nusing a recurrent neural network encoder-decoder. The experimental results\ndemonstrate that the proposed framework is able to significantly reduce data\nannotation costs and mitigate noisy user feedback in dialogue policy learning.</p>\n", "tags": ["Applications","Dialogue & Multi Turn","Reinforcement Learning"] },
{"key": "su2017cross", "citations": "84", "year": "2017", "title":"Cross-domain Semantic Parsing Via Paraphrasing", "abstract": "<p>Existing studies on semantic parsing mainly focus on the in-domain setting.\nWe formulate cross-domain semantic parsing as a domain adaptation problem:\ntrain a semantic parser on some source domains and then adapt it to the target\ndomain. Due to the diversity of logical forms in different domains, this\nproblem presents unique and intriguing challenges. By converting logical forms\ninto canonical utterances in natural language, we reduce semantic parsing to\nparaphrasing, and develop an attentive sequence-to-sequence paraphrase model\nthat is general and flexible to adapt to different domains. We discover two\nproblems, small micro variance and large macro variance, of pre-trained word\nembeddings that hinder their direct use in neural networks, and propose\nstandardization techniques as a remedy. On the popular Overnight dataset, which\ncontains eight domains, we show that both cross-domain training and\nstandardized pre-trained word embeddings can bring significant improvement.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "su2017sample", "citations": "123", "year": "2017", "title":"Sample-efficient Actor-critic Reinforcement Learning With Supervised Data For Dialogue Management", "abstract": "<p>Deep reinforcement learning (RL) methods have significant potential for\ndialogue policy optimisation. However, they suffer from a poor performance in\nthe early stages of learning. This is especially problematic for on-line\nlearning with real users. Two approaches are introduced to tackle this problem.\nFirstly, to speed up the learning process, two sample-efficient neural networks\nalgorithms: trust region actor-critic with experience replay (TRACER) and\nepisodic natural actor-critic with experience replay (eNACER) are presented.\nFor TRACER, the trust region helps to control the learning step size and avoid\ncatastrophic model changes. For eNACER, the natural gradient identifies the\nsteepest ascent direction in policy space to speed up the convergence. Both\nmodels employ off-policy learning with experience replay to improve\nsample-efficiency. Secondly, to mitigate the cold start issue, a corpus of\ndemonstration data is utilised to pre-train the models prior to on-line\nreinforcement learning. Combining these two approaches, we demonstrate a\npractical approach to learn deep RL-based dialogue policies and demonstrate\ntheir effectiveness in a task-oriented information seeking domain.</p>\n", "tags": ["Agentic","Datasets","Efficiency","Reinforcement Learning"] },
{"key": "su2018extended", "citations": "159", "year": "2019", "title":"On Extended Long Short-term Memory And Dependent Bidirectional Recurrent Neural Network", "abstract": "<p>In this work, we first analyze the memory behavior in three recurrent neural\nnetworks (RNN) cells; namely, the simple RNN (SRN), the long short-term memory\n(LSTM) and the gated recurrent unit (GRU), where the memory is defined as a\nfunction that maps previous elements in a sequence to the current output. Our\nstudy shows that all three of them suffer rapid memory decay. Then, to\nalleviate this effect, we introduce trainable scaling factors that act like an\nattention mechanism to adjust memory decay adaptively. The new design is called\nthe extended LSTM (ELSTM). Finally, to design a system that is robust to\nprevious erroneous predictions, we propose a dependent bidirectional recurrent\nneural network (DBRNN). Extensive experiments are conducted on different\nlanguage tasks to demonstrate the superiority of the proposed ELSTM and DBRNN\nsolutions. The ELTSM has achieved up to 30% increase in the labeled attachment\nscore (LAS) as compared to LSTM and GRU in the dependency parsing (DP) task.\nOur models also outperform other state-of-the-art models such as bi-attention\nand convolutional sequence to sequence (convseq2seq) by close to 10% in the\nLAS. The code is released as an open source\n(https://github.com/yuanhangsu/ELSTM-DBRNN)</p>\n", "tags": ["Has Code","Memory & Context","Model Architecture"] },
{"key": "su2018learning", "citations": "62", "year": "2018", "title":"Learning Visual Knowledge Memory Networks For Visual Question Answering", "abstract": "<p>Visual question answering (VQA) requires joint comprehension of images and\nnatural language questions, where many questions can’t be directly or clearly\nanswered from visual content but require reasoning from structured human\nknowledge with confirmation from visual content. This paper proposes visual\nknowledge memory network (VKMN) to address this issue, which seamlessly\nincorporates structured human knowledge and deep visual features into memory\nnetworks in an end-to-end learning framework. Comparing to existing methods for\nleveraging external knowledge for supporting VQA, this paper stresses more on\ntwo missing mechanisms. First is the mechanism for integrating visual contents\nwith knowledge facts. VKMN handles this issue by embedding knowledge triples\n(subject, relation, target) and deep visual features jointly into the visual\nknowledge features. Second is the mechanism for handling multiple knowledge\nfacts expanding from question and answer pairs. VKMN stores joint embedding\nusing key-value pair structure in the memory networks so that it is easy to\nhandle multiple facts. Experiments show that the proposed method achieves\npromising results on both VQA v1.0 and v2.0 benchmarks, while outperforms\nstate-of-the-art methods on the knowledge-reasoning related questions.</p>\n", "tags": ["CVPR","Tools"] },
{"key": "su2018unsupervised", "citations": "70", "year": "2019", "title":"Unsupervised Multi-modal Neural Machine Translation", "abstract": "<p>Unsupervised neural machine translation (UNMT) has recently achieved\nremarkable results with only large monolingual corpora in each language.\nHowever, the uncertainty of associating target with source sentences makes UNMT\ntheoretically an ill-posed problem. This work investigates the possibility of\nutilizing images for disambiguation to improve the performance of UNMT. Our\nassumption is intuitively based on the invariant property of image, i.e., the\ndescription of the same visual content by different languages should be\napproximately similar. We propose an unsupervised multi-modal machine\ntranslation (UMNMT) framework based on the language translation cycle\nconsistency loss conditional on the image, targeting to learn the bidirectional\nmulti-modal translation simultaneously. Through an alternate training between\nmulti-modal and uni-modal, our inference model can translate with or without\nthe image. On the widely used Multi30K dataset, the experimental results of our\napproach are significantly better than those of the text-only UNMT on the 2016\ntest dataset.</p>\n", "tags": ["CVPR","Datasets","Tools","Training Techniques"] },
{"key": "su2018variational", "citations": "78", "year": "2018", "title":"Variational Recurrent Neural Machine Translation", "abstract": "<p>Partially inspired by successful applications of variational recurrent neural\nnetworks, we propose a novel variational recurrent neural machine translation\n(VRNMT) model in this paper. Different from the variational NMT, VRNMT\nintroduces a series of latent random variables to model the translation\nprocedure of a sentence in a generative way, instead of a single latent\nvariable. Specifically, the latent random variables are included into the\nhidden states of the NMT decoder with elements from the variational\nautoencoder. In this way, these variables are recurrently generated, which\nenables them to further capture strong and complex dependencies among the\noutput translations at different timesteps. In order to deal with the\nchallenges in performing efficient posterior inference and large-scale training\nduring the incorporation of latent variables, we build a neural posterior\napproximator, and equip it with a reparameterization technique to estimate the\nvariational lower bound. Experiments on Chinese-English and English-German\ntranslation tasks demonstrate that the proposed model achieves significant\nimprovements over both the conventional and variational NMT models.</p>\n", "tags": ["AAAI","Applications","Training Techniques"] },
{"key": "su2019improving", "citations": "103", "year": "2019", "title":"Improving Multi-turn Dialogue Modelling With Utterance Rewriter", "abstract": "<p>Recent research has made impressive progress in single-turn dialogue\nmodelling. In the multi-turn setting, however, current models are still far\nfrom satisfactory. One major challenge is the frequently occurred coreference\nand information omission in our daily conversation, making it hard for machines\nto understand the real intention. In this paper, we propose rewriting the human\nutterance as a pre-process to help multi-turn dialgoue modelling. Each\nutterance is first rewritten to recover all coreferred and omitted information.\nThe next processing steps are then performed based on the rewritten utterance.\nTo properly train the utterance rewriter, we collect a new dataset with human\nannotations and introduce a Transformer-based utterance rewriting architecture\nusing the pointer network. We show the proposed architecture achieves\nremarkably good performance on the utterance rewriting task. The trained\nutterance rewriter can be easily integrated into online chatbots and brings\ngeneral improvement over different domains.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "su2019vl", "citations": "714", "year": "2019", "title":"VL-BERT: Pre-training Of Generic Visual-linguistic Representations", "abstract": "<p>We introduce a new pre-trainable generic representation for visual-linguistic\ntasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the\nsimple yet powerful Transformer model as the backbone, and extends it to take\nboth visual and linguistic embedded features as input. In it, each element of\nthe input is either of a word from the input sentence, or a region-of-interest\n(RoI) from the input image. It is designed to fit for most of the\nvisual-linguistic downstream tasks. To better exploit the generic\nrepresentation, we pre-train VL-BERT on the massive-scale Conceptual Captions\ndataset, together with text-only corpus. Extensive empirical analysis\ndemonstrates that the pre-training procedure can better align the\nvisual-linguistic clues and benefit the downstream tasks, such as visual\ncommonsense reasoning, visual question answering and referring expression\ncomprehension. It is worth noting that VL-BERT achieved the first place of\nsingle model on the leaderboard of the VCR benchmark. Code is released at\nhttps://github.com/jackroos/VL-BERT.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "su2021roformer", "citations": "503", "year": "2023", "title":"Roformer: Enhanced Transformer With Rotary Position Embedding", "abstract": "<p>Position encoding recently has shown effective in the transformer\narchitecture. It enables valuable supervision for dependency modeling between\nelements at different positions of the sequence. In this paper, we first\ninvestigate various methods to integrate positional information into the\nlearning process of transformer-based language models. Then, we propose a novel\nmethod named Rotary Position Embedding(RoPE) to effectively leverage the\npositional information. Specifically, the proposed RoPE encodes the absolute\nposition with a rotation matrix and meanwhile incorporates the explicit\nrelative position dependency in self-attention formulation. Notably, RoPE\nenables valuable properties, including the flexibility of sequence length,\ndecaying inter-token dependency with increasing relative distances, and the\ncapability of equipping the linear self-attention with relative position\nencoding. Finally, we evaluate the enhanced transformer with rotary position\nembedding, also called RoFormer, on various long text classification benchmark\ndatasets. Our experiments show that it consistently overcomes its alternatives.\nFurthermore, we provide a theoretical analysis to explain some experimental\nresults. RoFormer is already integrated into Huggingface:\nhttps://huggingface.co/docs/transformers/model_doc/roformer.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "su2022contrastive", "citations": "80", "year": "2022", "title":"Contrastive Search Is What You Need For Neural Text Generation", "abstract": "<p>Generating text with autoregressive language models (LMs) is of great\nimportance to many natural language processing (NLP) applications. Previous\nsolutions for this task often produce text that contains degenerative\nexpressions or lacks semantic consistency. Recently, Su et al. introduced a new\ndecoding method, contrastive search, based on the isotropic representation\nspace of the language model and obtained new state of the art on various\nbenchmarks. Additionally, Su et al. argued that the representations of\nautoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also\nshared by previous studies. Therefore, to ensure the language model follows an\nisotropic distribution, Su et al. proposed a contrastive learning scheme,\nSimCTG, which calibrates the language model’s representations through\nadditional training.\n  In this study, we first answer the question: “Are autoregressive LMs really\nanisotropic?”. To this end, we extensively evaluate the isotropy of LMs across\n16 major languages. Surprisingly, we find that the anisotropic problem only\nexists in the two specific English GPT-2-small/medium models. On the other\nhand, all other evaluated LMs are naturally isotropic which is in contrast to\nthe conclusion drawn by previous studies. Based on our findings, we further\nassess the contrastive search decoding method using off-the-shelf LMs on four\ngeneration tasks across 16 languages. Our experimental results demonstrate that\ncontrastive search significantly outperforms previous decoding methods without\nany additional training. More notably, on 12 out of the 16 evaluated languages,\ncontrastive search performs comparably with human-level performances as judged\nby human evaluations. Our code and other related resources are publicly\navailable at https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need.</p>\n", "tags": ["Applications","Has Code","Model Architecture","Training Techniques"] },
{"key": "subramanian2018learning", "citations": "181", "year": "2018", "title":"Learning General Purpose Distributed Sentence Representations Via Large Scale Multi-task Learning", "abstract": "<p>A lot of the recent success in natural language processing (NLP) has been\ndriven by distributed vector representations of words trained on large amounts\nof text in an unsupervised manner. These representations are typically used as\ngeneral purpose features for words across a range of NLP problems. However,\nextending this success to learning representations of sequences of words, such\nas sentences, remains an open problem. Recent work has explored unsupervised as\nwell as supervised learning techniques with different training objectives to\nlearn general purpose fixed-length sentence representations. In this work, we\npresent a simple, effective multi-task learning framework for sentence\nrepresentations that combines the inductive biases of diverse training\nobjectives in a single model. We train this model on several data sources with\nmultiple training objectives on over 100 million sentences. Extensive\nexperiments demonstrate that sharing a single recurrent sentence encoder across\nweakly related tasks leads to consistent improvements over previous methods. We\npresent substantial improvements in the context of transfer learning and\nlow-resource settings using our learned general-purpose representations.</p>\n", "tags": ["Fine-Tuning","Tools","Training Techniques"] },
{"key": "subramanian2019extractive", "citations": "174", "year": "2020", "title":"On Extractive And Abstractive Neural Document Summarization With Transformer Language Models", "abstract": "<p>We present a method to produce abstractive summaries of long documents that\nexceed several thousand words via neural abstractive summarization. We perform\na simple extractive step before generating a summary, which is then used to\ncondition the transformer language model on relevant information before being\ntasked with generating a summary. We show that this extractive step\nsignificantly improves summarization results. We also show that this approach\nproduces more abstractive summaries compared to prior work that employs a copy\nmechanism while still achieving higher rouge scores. Note: The abstract above\nwas not written by the authors, it was generated by one of the models presented\nin this paper.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "sugawara2018what", "citations": "110", "year": "2018", "title":"What Makes Reading Comprehension Questions Easier?", "abstract": "<p>A challenge in creating a dataset for machine reading comprehension (MRC) is\nto collect questions that require a sophisticated understanding of language to\nanswer beyond using superficial cues. In this work, we investigate what makes\nquestions easier across recent 12 MRC datasets with three question styles\n(answer extraction, description, and multiple choice). We propose to employ\nsimple heuristics to split each dataset into easy and hard subsets and examine\nthe performance of two baseline models for each of the subsets. We then\nmanually annotate questions sampled from each subset with both validity and\nrequisite reasoning skills to investigate which skills explain the difference\nbetween easy and hard questions. From this study, we observed that (i) the\nbaseline performances for the hard subsets remarkably degrade compared to those\nof entire datasets, (ii) hard questions require knowledge inference and\nmultiple-sentence reasoning in comparison with easy questions, and (iii)\nmultiple-choice questions tend to require a broader range of reasoning skills\nthan answer extraction and description questions. These results suggest that\none might overestimate recent advances in MRC.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "sugawara2019assessing", "citations": "75", "year": "2020", "title":"Assessing The Benchmarking Capacity Of Machine Reading Comprehension Datasets", "abstract": "<p>Existing analysis work in machine reading comprehension (MRC) is largely\nconcerned with evaluating the capabilities of systems. However, the\ncapabilities of datasets are not assessed for benchmarking language\nunderstanding precisely. We propose a semi-automated, ablation-based\nmethodology for this challenge; By checking whether questions can be solved\neven after removing features associated with a skill requisite for language\nunderstanding, we evaluate to what degree the questions do not require the\nskill. Experiments on 10 datasets (e.g., CoQA, SQuAD v2.0, and RACE) with a\nstrong baseline model show that, for example, the relative scores of a baseline\nmodel provided with content words only and with shuffled sentence words in the\ncontext are on average 89.2% and 78.5% of the original score, respectively.\nThese results suggest that most of the questions already answered correctly by\nthe model do not necessarily require grammatical and complex reasoning. For\nprecise benchmarking, MRC datasets will need to take extra care in their design\nto ensure that questions can correctly evaluate the intended skills.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "suh2023sensecape", "citations": "65", "year": "2023", "title":"Sensecape: Enabling Multilevel Exploration And Sensemaking With Large Language Models", "abstract": "<p>People are increasingly turning to large language models (LLMs) for complex\ninformation tasks like academic research or planning a move to another city.\nHowever, while they often require working in a nonlinear manner – e.g., to\narrange information spatially to organize and make sense of it, current\ninterfaces for interacting with LLMs are generally linear to support\nconversational interaction. To address this limitation and explore how we can\nsupport LLM-powered exploration and sensemaking, we developed Sensecape, an\ninteractive system designed to support complex information tasks with an LLM by\nenabling users to (1) manage the complexity of information through multilevel\nabstraction and (2) seamlessly switch between foraging and sensemaking. Our\nwithin-subject user study reveals that Sensecape empowers users to explore more\ntopics and structure their knowledge hierarchically, thanks to the\nexternalization of levels of abstraction. We contribute implications for\nLLM-based workflows and interfaces for information tasks.</p>\n", "tags": [] },
{"key": "suhr2018corpus", "citations": "400", "year": "2019", "title":"A Corpus For Reasoning About Natural Language Grounded In Photographs", "abstract": "<p>We introduce a new dataset for joint reasoning about natural language and\nimages, with a focus on semantic diversity, compositionality, and visual\nreasoning challenges. The data contains 107,292 examples of English sentences\npaired with web photographs. The task is to determine whether a natural\nlanguage caption is true about a pair of photographs. We crowdsource the data\nusing sets of visually rich images and a compare-and-contrast task to elicit\nlinguistically diverse language. Qualitative analysis shows the data requires\ncompositional joint reasoning, including about quantities, comparisons, and\nrelations. Evaluation using state-of-the-art visual reasoning methods shows the\ndata presents a strong challenge.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "suhr2018learning", "citations": "95", "year": "2018", "title":"Learning To Map Context-dependent Sentences To Executable Formal Queries", "abstract": "<p>We propose a context-dependent model to map utterances within an interaction\nto executable formal queries. To incorporate interaction history, the model\nmaintains an interaction-level encoder that updates after each turn, and can\ncopy sub-sequences of previously predicted queries during generation. Our\napproach combines implicit and explicit modeling of references between\nutterances. We evaluate our model on the ATIS flight planning interactions, and\ndemonstrate the benefits of modeling context and explicit references.</p>\n", "tags": ["NAACL"] },
{"key": "sukhbaatar2019adaptive", "citations": "286", "year": "2019", "title":"Adaptive Attention Span In Transformers", "abstract": "<p>We propose a novel self-attention mechanism that can learn its optimal\nattention span. This allows us to extend significantly the maximum context size\nused in Transformer, while maintaining control over their memory footprint and\ncomputational time. We show the effectiveness of our approach on the task of\ncharacter level language modeling, where we achieve state-of-the-art\nperformances on text8 and enwiki8 by using a maximum context of 8k characters.</p>\n", "tags": ["Memory & Context","Model Architecture"] },
{"key": "sulubacak2019multimodal", "citations": "65", "year": "2020", "title":"Multimodal Machine Translation Through Visuals And Speech", "abstract": "<p>Multimodal machine translation involves drawing information from more than\none modality, based on the assumption that the additional modalities will\ncontain useful alternative views of the input data. The most prominent tasks in\nthis area are spoken language translation, image-guided translation, and\nvideo-guided translation, which exploit audio and visual modalities,\nrespectively. These tasks are distinguished from their monolingual counterparts\nof speech recognition, image captioning, and video captioning by the\nrequirement of models to generate outputs in a different language. This survey\nreviews the major data resources for these tasks, the evaluation campaigns\nconcentrated around them, the state of the art in end-to-end and pipeline\napproaches, and also the challenges in performance evaluation. The paper\nconcludes with a discussion of directions for future research in these areas:\nthe need for more expansive and challenging datasets, for targeted evaluations\nof model performance, and for multimodality in both the input and output space.</p>\n", "tags": ["Datasets","Evaluation","Survey Paper"] },
{"key": "sun2018conversational", "citations": "342", "year": "2018", "title":"Conversational Recommender System", "abstract": "<p>A personalized conversational sales agent could have much commercial\npotential. E-commerce companies such as Amazon, eBay, JD, Alibaba etc. are\npiloting such kind of agents with their users. However, the research on this\ntopic is very limited and existing solutions are either based on single round\nadhoc search engine or traditional multi round dialog system. They usually only\nutilize user inputs in the current session, ignoring users’ long term\npreferences. On the other hand, it is well known that sales conversion rate can\nbe greatly improved based on recommender systems, which learn user preferences\nbased on past purchasing behavior and optimize business oriented metrics such\nas conversion rate or expected revenue. In this work, we propose to integrate\nresearch in dialog systems and recommender systems into a novel and unified\ndeep reinforcement learning framework to build a personalized conversational\nrecommendation agent that optimizes a per session based utility function.</p>\n", "tags": ["Evaluation","Reinforcement Learning","SIGIR","Tools"] },
{"key": "sun2018grammar", "citations": "106", "year": "2019", "title":"A Grammar-based Structural CNN Decoder For Code Generation", "abstract": "<p>Code generation maps a program description to executable source code in a\nprogramming language. Existing approaches mainly rely on a recurrent neural\nnetwork (RNN) as the decoder. However, we find that a program contains\nsignificantly more tokens than a natural language sentence, and thus it may be\ninappropriate for RNN to capture such a long sequence. In this paper, we\npropose a grammar-based structural convolutional neural network (CNN) for code\ngeneration. Our model generates a program by predicting the grammar rules of\nthe programming language; we design several CNN modules, including the\ntree-based convolution and pre-order convolution, whose information is further\naggregated by dedicated attentive pooling layers. Experimental results on the\nHearthStone benchmark dataset show that our CNN code generator significantly\noutperforms the previous state-of-the-art method by 5 percentage points;\nadditional experiments on several semantic parsing tasks demonstrate the\nrobustness of our model. We also conduct in-depth ablation test to better\nunderstand each component of our model.</p>\n", "tags": ["AAAI","Evaluation","Llm For Code"] },
{"key": "sun2018improving", "citations": "129", "year": "2019", "title":"Improving Machine Reading Comprehension With General Reading Strategies", "abstract": "<p>Reading strategies have been shown to improve comprehension levels,\nespecially for readers lacking adequate prior knowledge. Just as the process of\nknowledge accumulation is time-consuming for human readers, it is\nresource-demanding to impart rich general domain knowledge into a deep language\nmodel via pre-training. Inspired by reading strategies identified in cognitive\nscience, and given limited computational resources – just a pre-trained model\nand a fixed number of training instances – we propose three general strategies\naimed to improve non-extractive machine reading comprehension (MRC): (i) BACK\nAND FORTH READING that considers both the original and reverse order of an\ninput sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text\nembedding of tokens that are relevant to the question and candidate answers,\nand (iii) SELF-ASSESSMENT that generates practice questions and candidate\nanswers directly from the text in an unsupervised manner.\n  By fine-tuning a pre-trained language model (Radford et al., 2018) with our\nproposed strategies on the largest general domain multiple-choice MRC dataset\nRACE, we obtain a 5.8% absolute increase in accuracy over the previous best\nresult achieved by the same pre-trained model fine-tuned on RACE without the\nuse of strategies. We further fine-tune the resulting model on a target MRC\ntask, leading to an absolute improvement of 6.2% in average accuracy over\nprevious state-of-the-art approaches on six representative non-extractive MRC\ndatasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018\nTask 11, ROCStories, and MultiRC). These results demonstrate the effectiveness\nof our proposed strategies and the versatility and general applicability of our\nfine-tuned models that incorporate these strategies. Core code is available at\nhttps://github.com/nlpdata/strategy/.</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "sun2018open", "citations": "400", "year": "2018", "title":"Open Domain Question Answering Using Early Fusion Of Knowledge Bases And Text", "abstract": "<p>Open Domain Question Answering (QA) is evolving from complex pipelined\nsystems to end-to-end deep neural networks. Specialized neural models have been\ndeveloped for extracting answers from either text alone or Knowledge Bases\n(KBs) alone. In this paper we look at a more practical setting, namely QA over\nthe combination of a KB and entity-linked text, which is appropriate when an\nincomplete KB is available with a large text corpus. Building on recent\nadvances in graph representation learning we propose a novel model, GRAFT-Net,\nfor extracting answers from a question-specific subgraph containing text and KB\nentities and relations. We construct a suite of benchmark tasks for this\nproblem, varying the difficulty of questions, the amount of training data, and\nKB completeness. We show that GRAFT-Net is competitive with the\nstate-of-the-art when tested using either KBs or text alone, and vastly\noutperforms existing methods in the combined setting. Source code is available\nat https://github.com/OceanskySun/GraftNet .</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code","Training Techniques"] },
{"key": "sun2019automatic", "citations": "102", "year": "2020", "title":"Automatic Testing And Improvement Of Machine Translation", "abstract": "<p>This paper presents TransRepair, a fully automatic approach for testing and\nrepairing the consistency of machine translation systems. TransRepair combines\nmutation with metamorphic testing to detect inconsistency bugs (without access\nto human oracles). It then adopts probability-reference or cross-reference to\npost-process the translations, in a grey-box or black-box manner, to repair the\ninconsistencies. Our evaluation on two state-of-the-art translators, Google\nTranslate and Transformer, indicates that TransRepair has a high precision\n(99%) on generating input pairs with consistent translations. With these tests,\nusing automatic consistency metrics and manual assessment, we find that Google\nTranslate and Transformer have approximately 36% and 40% inconsistency bugs.\nBlack-box repair fixes 28% and 19% bugs on average for Google Translate and\nTransformer. Grey-box repair fixes 30% bugs on average for Transformer. Manual\ninspection indicates that the translations repaired by our approach improve\nconsistency in 87% of cases (degrading it in 2%), and that our repairs have\nbetter translation acceptability in 27% of the cases (worse in 8%).</p>\n", "tags": ["Evaluation","Llm For Code","Model Architecture"] },
{"key": "sun2019bert4rec", "citations": "260", "year": "2019", "title":"Bert4rec: Sequential Recommendation With Bidirectional Encoder Representations From Transformer", "abstract": "<p>Modeling users’ dynamic and evolving preferences from their historical\nbehaviors is challenging and crucial for recommendation systems. Previous\nmethods employ sequential neural networks (e.g., Recurrent Neural Network) to\nencode users’ historical interactions from left to right into hidden\nrepresentations for making recommendations. Although these methods achieve\nsatisfactory results, they often assume a rigidly ordered sequence which is not\nalways practical. We argue that such left-to-right unidirectional architectures\nrestrict the power of the historical sequence representations. For this\npurpose, we introduce a Bidirectional Encoder Representations from Transformers\nfor sequential Recommendation (BERT4Rec). However, jointly conditioning on both\nleft and right context in deep bidirectional model would make the training\nbecome trivial since each item can indirectly “see the target item”. To address\nthis problem, we train the bidirectional model using the Cloze task, predicting\nthe masked items in the sequence by jointly conditioning on their left and\nright context. Comparing with predicting the next item at each position in a\nsequence, the Cloze task can produce more samples to train a more powerful\nbidirectional model. Extensive experiments on four benchmark datasets show that\nour model outperforms various state-of-the-art sequential models consistently.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "sun2019dream", "citations": "76", "year": "2019", "title":"DREAM: A Challenge Dataset And Models For Dialogue-based Reading Comprehension", "abstract": "<p>We present DREAM, the first dialogue-based multiple-choice reading\ncomprehension dataset. Collected from English-as-a-foreign-language\nexaminations designed by human experts to evaluate the comprehension level of\nChinese learners of English, our dataset contains 10,197 multiple-choice\nquestions for 6,444 dialogues. In contrast to existing reading comprehension\ndatasets, DREAM is the first to focus on in-depth multi-turn multi-party\ndialogue understanding. DREAM is likely to present significant challenges for\nexisting reading comprehension systems: 84% of answers are non-extractive, 85%\nof questions require reasoning beyond a single sentence, and 34% of questions\nalso involve commonsense knowledge.\n  We apply several popular neural reading comprehension models that primarily\nexploit surface information within the text and find them to, at best, just\nbarely outperform a rule-based approach. We next investigate the effects of\nincorporating dialogue structure and different kinds of general world knowledge\ninto both rule-based and (neural and non-neural) machine learning-based reading\ncomprehension models. Experimental results on the DREAM dataset show the\neffectiveness of dialogue structure and general world knowledge. DREAM will be\navailable at https://dataset.org/dream/.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Has Code"] },
{"key": "sun2019ernie", "citations": "664", "year": "2020", "title":"ERNIE 2.0: A Continual Pre-training Framework For Language Understanding", "abstract": "<p>Recently, pre-trained models have achieved state-of-the-art results in\nvarious language understanding tasks, which indicates that pre-training on\nlarge-scale corpora may play a crucial role in natural language processing.\nCurrent pre-training procedures usually focus on training the model with\nseveral simple tasks to grasp the co-occurrence of words or sentences. However,\nbesides co-occurring, there exists other valuable lexical, syntactic and\nsemantic information in training corpora, such as named entity, semantic\ncloseness and discourse relations. In order to extract to the fullest extent,\nthe lexical, syntactic and semantic information from training corpora, we\npropose a continual pre-training framework named ERNIE 2.0 which builds and\nlearns incrementally pre-training tasks through constant multi-task learning.\nExperimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on\n16 tasks including English tasks on GLUE benchmarks and several common tasks in\nChinese. The source codes and pre-trained models have been released at\nhttps://github.com/PaddlePaddle/ERNIE.</p>\n", "tags": ["AAAI","Has Code","Model Architecture","Training Techniques"] },
{"key": "sun2019how", "citations": "1187", "year": "2019", "title":"How To Fine-tune BERT For Text Classification?", "abstract": "<p>Language model pre-training has proven to be useful in learning universal\nlanguage representations. As a state-of-the-art language model pre-training\nmodel, BERT (Bidirectional Encoder Representations from Transformers) has\nachieved amazing results in many language understanding tasks. In this paper,\nwe conduct exhaustive experiments to investigate different fine-tuning methods\nof BERT on text classification task and provide a general solution for BERT\nfine-tuning. Finally, the proposed solution obtains new state-of-the-art\nresults on eight widely-studied text classification datasets.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "sun2019lamol", "citations": "78", "year": "2019", "title":"LAMOL: Language Modeling For Lifelong Language Learning", "abstract": "<p>Most research on lifelong learning applies to images or games, but not\nlanguage. We present LAMOL, a simple yet effective method for lifelong language\nlearning (LLL) based on language modeling. LAMOL replays pseudo-samples of\nprevious tasks while requiring no extra memory or model capacity. Specifically,\nLAMOL is a language model that simultaneously learns to solve the tasks and\ngenerate training samples. When the model is trained for a new task, it\ngenerates pseudo-samples of previous tasks for training alongside data for the\nnew task. The results show that LAMOL prevents catastrophic forgetting without\nany sign of intransigence and can perform five very different language tasks\nsequentially with only one model. Overall, LAMOL outperforms previous methods\nby a considerable margin and is only 2-3% worse than multitasking, which is\nusually considered the LLL upper bound. The source code is available at\nhttps://github.com/jojotenya/LAMOL.</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "sun2019patient", "citations": "553", "year": "2019", "title":"Patient Knowledge Distillation For BERT Model Compression", "abstract": "<p>Pre-trained language models such as BERT have proven to be highly effective\nfor natural language processing (NLP) tasks. However, the high demand for\ncomputing resources in training such models hinders their application in\npractice. In order to alleviate this resource hunger in large-scale model\ntraining, we propose a Patient Knowledge Distillation approach to compress an\noriginal large model (teacher) into an equally-effective lightweight shallow\nnetwork (student). Different from previous knowledge distillation methods,\nwhich only use the output from the last layer of the teacher network for\ndistillation, our student model patiently learns from multiple intermediate\nlayers of the teacher model for incremental knowledge extraction, following two\nstrategies: (\\(i\\)) PKD-Last: learning from the last \\(k\\) layers; and (\\(ii\\))\nPKD-Skip: learning from every \\(k\\) layers. These two patient distillation\nschemes enable the exploitation of rich information in the teacher’s hidden\nlayers, and encourage the student model to patiently learn from and imitate the\nteacher through a multi-layer distillation process. Empirically, this\ntranslates into improved results on multiple NLP tasks with significant gain in\ntraining efficiency, without sacrificing model accuracy.</p>\n", "tags": ["EMNLP","Efficiency","Model Architecture","Training Techniques"] },
{"key": "sun2019pullnet", "citations": "280", "year": "2019", "title":"Pullnet: Open Domain Question Answering With Iterative Retrieval On Knowledge Bases And Text", "abstract": "<p>We consider open-domain queston answering (QA) where answers are drawn from\neither a corpus, a knowledge base (KB), or a combination of both of these. We\nfocus on a setting in which a corpus is supplemented with a large but\nincomplete KB, and on questions that require non-trivial (e.g., <code class=\"language-plaintext highlighter-rouge\">multi-hop'')\nreasoning. We describe PullNet, an integrated framework for (1) learning what\nto retrieve (from the KB and/or corpus) and (2) reasoning with this\nheterogeneous information to find the best answer. PullNet uses an \\{iterative\\}\nprocess to construct a question-specific subgraph that contains information\nrelevant to the question. In each iteration, a graph convolutional network\n(graph CNN) is used to identify subgraph nodes that should be expanded using\nretrieval (or</code>pull’’) operations on the corpus and/or KB. After the subgraph\nis complete, a similar graph CNN is used to extract the answer from the\nsubgraph. This retrieve-and-reason process allows us to answer multi-hop\nquestions using large KBs and corpora. PullNet is weakly supervised, requiring\nquestion-answer pairs but not gold inference paths. Experimentally PullNet\nimproves over the prior state-of-the art, and in the setting where a corpus is\nused with incomplete KB these improvements are often dramatic. PullNet is also\noften superior to prior systems in a KB-only setting or a text-only setting.</p>\n", "tags": ["Datasets","EMNLP","Tools"] },
{"key": "sun2019treegen", "citations": "146", "year": "2020", "title":"Treegen: A Tree-based Transformer Architecture For Code Generation", "abstract": "<p>A code generation system generates programming language code based on an\ninput natural language description. State-of-the-art approaches rely on neural\nnetworks for code generation. However, these code generators suffer from two\nproblems. One is the long dependency problem, where a code element often\ndepends on another far-away code element. A variable reference, for example,\ndepends on its definition, which may appear quite a few lines before. The other\nproblem is structure modeling, as programs contain rich structural information.\nIn this paper, we propose a novel tree-based neural architecture, TreeGen, for\ncode generation. TreeGen uses the attention mechanism of Transformers to\nalleviate the long-dependency problem, and introduces a novel AST reader\n(encoder) to incorporate grammar rules and AST structures into the network. We\nevaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing\nbenchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art\napproach by 4.5 percentage points on HearthStone, and achieved the best\naccuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%).\nWe also conducted an ablation test to better understand each component of our\nmodel.</p>\n", "tags": ["AAAI","Evaluation","Llm For Code","Model Architecture"] },
{"key": "sun2019videobert", "citations": "1044", "year": "2019", "title":"Videobert: A Joint Model For Video And Language Representation Learning", "abstract": "<p>Self-supervised learning has become increasingly important to leverage the\nabundance of unlabeled data available on platforms like YouTube. Whereas most\nexisting approaches learn low-level representations, we propose a joint\nvisual-linguistic model to learn high-level features without any explicit\nsupervision. In particular, inspired by its recent success in language\nmodeling, we build upon the BERT model to learn bidirectional joint\ndistributions over sequences of visual and linguistic tokens, derived from\nvector quantization of video data and off-the-shelf speech recognition outputs,\nrespectively. We use VideoBERT in numerous tasks, including action\nclassification and video captioning. We show that it can be applied directly to\nopen-vocabulary classification, and confirm that large amounts of training data\nand cross-modal information are critical to performance. Furthermore, we\noutperform the state-of-the-art on video captioning, and quantitative results\nverify that the model learns high-level semantic features.</p>\n", "tags": ["ICCV","Model Architecture","Training Techniques"] },
{"key": "sun2020adv", "citations": "83", "year": "2020", "title":"Adv-bert: BERT Is Not Robust On Misspellings! Generating Nature Adversarial Samples On BERT", "abstract": "<p>There is an increasing amount of literature that claims the brittleness of\ndeep neural networks in dealing with adversarial examples that are created\nmaliciously. It is unclear, however, how the models will perform in realistic\nscenarios where \\textit{natural rather than malicious} adversarial instances\noften exist. This work systematically explores the robustness of BERT, the\nstate-of-the-art Transformer-style model in NLP, in dealing with noisy data,\nparticularly mistakes in typing the keyboard, that occur inadvertently.\nIntensive experiments on sentiment analysis and question answering benchmarks\nindicate that: (i) Typos in various words of a sentence do not influence\nequally. The typos in informative words make severer damages; (ii) Mistype is\nthe most damaging factor, compared with inserting, deleting, etc.; (iii) Humans\nand machines have different focuses on recognizing adversarial attacks.</p>\n", "tags": ["Model Architecture","Security"] },
{"key": "sun2020colake", "citations": "141", "year": "2020", "title":"Colake: Contextualized Language And Knowledge Embedding", "abstract": "<p>With the emerging branch of incorporating factual knowledge into pre-trained\nlanguage models such as BERT, most existing models consider shallow, static,\nand separately pre-trained entity embeddings, which limits the performance\ngains of these models. Few works explore the potential of deep contextualized\nknowledge representation when injecting knowledge. In this paper, we propose\nthe Contextualized Language and Knowledge Embedding (CoLAKE), which jointly\nlearns contextualized representation for both language and knowledge with the\nextended MLM objective. Instead of injecting only entity embeddings, CoLAKE\nextracts the knowledge context of an entity from large-scale knowledge bases.\nTo handle the heterogeneity of knowledge context and language context, we\nintegrate them in a unified data structure, word-knowledge graph (WK graph).\nCoLAKE is pre-trained on large-scale WK graphs with the modified Transformer\nencoder. We conduct experiments on knowledge-driven tasks, knowledge probing\ntasks, and language understanding tasks. Experimental results show that CoLAKE\noutperforms previous counterparts on most of the tasks. Besides, CoLAKE\nachieves surprisingly high performance on our synthetic task called\nword-knowledge graph completion, which shows the superiority of simultaneously\ncontextualizing language and knowledge representation.</p>\n", "tags": ["COLING","Model Architecture"] },
{"key": "sun2020mixup", "citations": "74", "year": "2020", "title":"Mixup-transformer: Dynamic Data Augmentation For NLP Tasks", "abstract": "<p>Mixup is the latest data augmentation technique that linearly interpolates\ninput examples and the corresponding labels. It has shown strong effectiveness\nin image classification by interpolating images at the pixel level. Inspired by\nthis line of research, in this paper, we explore i) how to apply mixup to\nnatural language processing tasks since text data can hardly be mixed in the\nraw format; ii) if mixup is still effective in transformer-based learning\nmodels, e.g., BERT. To achieve the goal, we incorporate mixup to\ntransformer-based pre-trained architecture, named “mixup-transformer”, for a\nwide range of NLP tasks while keeping the whole end-to-end training system. We\nevaluate the proposed framework by running extensive experiments on the GLUE\nbenchmark. Furthermore, we also examine the performance of mixup-transformer in\nlow-resource scenarios by reducing the training data with a certain ratio. Our\nstudies show that mixup is a domain-independent data augmentation technique to\npre-trained language models, resulting in significant performance improvement\nfor transformer-based models.</p>\n", "tags": ["COLING","Datasets","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "sun2021chinesebert", "citations": "150", "year": "2021", "title":"Chinesebert: Chinese Pretraining Enhanced By Glyph And Pinyin Information", "abstract": "<p>Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.</p>\n", "tags": ["Datasets","Has Code","Training Techniques"] },
{"key": "sun2021ernie", "citations": "167", "year": "2021", "title":"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training For Language Understanding And Generation", "abstract": "<p>Pre-trained models have achieved state-of-the-art results in various Natural\nLanguage Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown\nthat scaling up pre-trained language models can improve their generalization\nabilities. Particularly, the GPT-3 model with 175 billion parameters shows its\nstrong task-agnostic zero-shot/few-shot learning capabilities. Despite their\nsuccess, these large-scale models are trained on plain texts without\nintroducing knowledge such as linguistic knowledge and world knowledge. In\naddition, most large-scale models are trained in an auto-regressive way. As a\nresult, this kind of traditional fine-tuning approach demonstrates relatively\nweak performance when solving downstream language understanding tasks. In order\nto solve the above problems, we propose a unified framework named ERNIE 3.0 for\npre-training large-scale knowledge enhanced models. It fuses auto-regressive\nnetwork and auto-encoding network, so that the trained model can be easily\ntailored for both natural language understanding and generation tasks with\nzero-shot learning, few-shot learning or fine-tuning. We trained the model with\n10 billion parameters on a 4TB corpus consisting of plain texts and a\nlarge-scale knowledge graph. Empirical results show that the model outperforms\nthe state-of-the-art models on 54 Chinese NLP tasks, and its English version\nachieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing\nthe human performance by +0.8% (90.6% vs. 89.8%).</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "sun2021lightningdot", "citations": "73", "year": "2021", "title":"Lightningdot: Pre-training Visual-semantic Embeddings For Real-time Image-text Retrieval", "abstract": "<p>Multimodal pre-training has propelled great advancement in\nvision-and-language research. These large-scale pre-trained models, although\nsuccessful, fatefully suffer from slow inference speed due to enormous\ncomputation cost mainly from cross-modal attention in Transformer architecture.\nWhen applied to real-life applications, such latency and computation demand\nseverely deter the practical use of pre-trained models. In this paper, we study\nImage-text retrieval (ITR), the most mature scenario of V+L application, which\nhas been widely studied even prior to the emergence of recent pre-trained\nmodels. We propose a simple yet highly effective approach, LightningDOT that\naccelerates the inference time of ITR by thousands of times, without\nsacrificing accuracy. LightningDOT removes the time-consuming cross-modal\nattention by pre-training on three novel learning objectives, extracting\nfeature indexes offline, and employing instant dot-product matching with\nfurther re-ranking, which significantly speeds up retrieval process. In fact,\nLightningDOT achieves new state of the art across multiple ITR benchmarks such\nas Flickr30k, COCO and Multi30K, outperforming existing pre-trained models that\nconsume 1000x magnitude of computational hours. Code and pre-training\ncheckpoints are available at https://github.com/intersun/LightningDOT.</p>\n", "tags": ["Applications","Has Code","Model Architecture","NAACL","Training Techniques"] },
{"key": "sun2022investigating", "citations": "142", "year": "2022", "title":"Investigating The Benefits Of Free-form Rationales", "abstract": "<p>Free-form rationales aim to aid model interpretability by supplying the\nbackground knowledge that can help understand model decisions. Crowdsourced\nrationales are provided for commonsense QA instances in popular datasets such\nas CoS-E and ECQA, but their utility remains under-investigated. We present\nhuman studies which show that ECQA rationales indeed provide additional\nbackground information to understand a decision, while over 88% of CoS-E\nrationales do not. Inspired by this finding, we ask: can the additional context\nprovided by free-form rationales benefit models, similar to human users? We\ninvestigate the utility of rationales as an additional source of supervision,\nby varying the quantity and quality of rationales during training. After\ncontrolling for instances where rationales leak the correct answer while not\nproviding additional background knowledge, we find that incorporating only 5%\nof rationales during training can boost model performance by 47.22% for CoS-E\nand 57.14% for ECQA during inference. Moreover, we also show that rationale\nquality matters: compared to crowdsourced rationales, T5-generated rationales\nprovide not only weaker supervision to models, but are also not helpful for\nhumans in aiding model interpretability.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "sun2023all", "citations": "71", "year": "2023", "title":"All In One: Multi-task Prompting For Graph Neural Networks", "abstract": "<p>Recently, ‘‘pre-training and fine-tuning’’ has been adopted as a standard\nworkflow for many graph tasks since it can take general graph knowledge to\nrelieve the lack of graph annotations from each application. However, graph\ntasks with node level, edge level, and graph level are far diversified, making\nthe pre-training pretext often incompatible with these multiple tasks. This gap\nmay even cause a ‘‘negative transfer’’ to the specific application, leading to\npoor results. Inspired by the prompt learning in natural language processing\n(NLP), which has presented significant effectiveness in leveraging prior\nknowledge for various NLP tasks, we study the prompting topic for graphs with\nthe motivation of filling the gap between pre-trained models and various graph\ntasks. In this paper, we propose a novel multi-task prompting method for graph\nmodels. Specifically, we first unify the format of graph prompts and language\nprompts with the prompt token, token structure, and inserting pattern. In this\nway, the prompting idea from NLP can be seamlessly introduced to the graph\narea. Then, to further narrow the gap between various graph tasks and\nstate-of-the-art pre-training strategies, we further study the task space of\nvarious graph applications and reformulate downstream problems to the\ngraph-level task. Afterward, we introduce meta-learning to efficiently learn a\nbetter initialization for the multi-task prompt of graphs so that our prompting\nframework can be more reliable and general for different tasks. We conduct\nextensive experiments, results from which demonstrate the superiority of our\nmethod.</p>\n", "tags": ["Applications","Fine-Tuning","KDD","Prompting","Tools","Training Techniques"] },
{"key": "sun2023eva", "citations": "63", "year": "2023", "title":"EVA-CLIP: Improved Training Techniques For CLIP At Scale", "abstract": "<p>Contrastive language-image pre-training, CLIP for short, has gained\nincreasing attention for its potential in various scenarios. In this paper, we\npropose EVA-CLIP, a series of models that significantly improve the efficiency\nand effectiveness of CLIP training. Our approach incorporates new techniques\nfor representation learning, optimization, and augmentation, enabling EVA-CLIP\nto achieve superior performance compared to previous CLIP models with the same\nnumber of parameters but significantly smaller training costs. Notably, our\nlargest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples\nachieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller\nEVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples\nachieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open\naccess and open research, we release the complete suite of EVA-CLIP to the\ncommunity at https://github.com/baaivision/EVA/tree/master/EVA-CLIP.</p>\n", "tags": ["Efficiency","Has Code","Model Architecture","Training Techniques"] },
{"key": "sun2023is", "citations": "88", "year": "2023", "title":"Is Chatgpt Good At Search? Investigating Large Language Models As Re-ranking Agents", "abstract": "<p>Large Language Models (LLMs) have demonstrated remarkable zero-shot\ngeneralization across various language-related tasks, including search engines.\nHowever, existing work utilizes the generative ability of LLMs for Information\nRetrieval (IR) rather than direct passage ranking. The discrepancy between the\npre-training objectives of LLMs and the ranking objective poses another\nchallenge. In this paper, we first investigate generative LLMs such as ChatGPT\nand GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal\nthat properly instructed LLMs can deliver competitive, even superior results to\nstate-of-the-art supervised methods on popular IR benchmarks. Furthermore, to\naddress concerns about data contamination of LLMs, we collect a new test set\ncalled NovelEval, based on the latest knowledge and aiming to verify the\nmodel’s ability to rank unknown knowledge. Finally, to improve efficiency in\nreal-world applications, we delve into the potential for distilling the ranking\ncapabilities of ChatGPT into small specialized models using a permutation\ndistillation scheme. Our evaluation results turn out that a distilled 440M\nmodel outperforms a 3B supervised model on the BEIR benchmark. The code to\nreproduce our results is available at www.github.com/sunnweiwei/RankGPT.</p>\n", "tags": ["Applications","Datasets","EMNLP","Efficiency","Evaluation","Model Architecture","Training Techniques"] },
{"key": "sun2023retentive", "citations": "73", "year": "2023", "title":"Retentive Network: A Successor To Transformer For Large Language Models", "abstract": "<p>In this work, we propose Retentive Network (RetNet) as a foundation\narchitecture for large language models, simultaneously achieving training\nparallelism, low-cost inference, and good performance. We theoretically derive\nthe connection between recurrence and attention. Then we propose the retention\nmechanism for sequence modeling, which supports three computation paradigms,\ni.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel\nrepresentation allows for training parallelism. The recurrent representation\nenables low-cost \\(O(1)\\) inference, which improves decoding throughput, latency,\nand GPU memory without sacrificing performance. The chunkwise recurrent\nrepresentation facilitates efficient long-sequence modeling with linear\ncomplexity, where each chunk is encoded parallelly while recurrently\nsummarizing the chunks. Experimental results on language modeling show that\nRetNet achieves favorable scaling results, parallel training, low-cost\ndeployment, and efficient inference. The intriguing properties make RetNet a\nstrong successor to Transformer for large language models. Code will be\navailable at https://aka.ms/retnet.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "sun2023text", "citations": "86", "year": "2023", "title":"Text Classification Via Large Language Models", "abstract": "<p>Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for \\(k\\)NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM’s generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Prompting"] },
{"key": "sung2021vl", "citations": "185", "year": "2022", "title":"Vl-adapter: Parameter-efficient Transfer Learning For Vision-and-language Tasks", "abstract": "<p>Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&amp;L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&amp;L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&amp;L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&amp;L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.</p>\n", "tags": ["CVPR","Datasets","Efficiency","Evaluation","Fine-Tuning","Has Code","Prompting","Training Techniques"] },
{"key": "sung2022lst", "citations": "64", "year": "2022", "title":"LST: Ladder Side-tuning For Parameter And Memory Efficient Transfer Learning", "abstract": "<p>Fine-tuning large pre-trained models on downstream tasks has been adopted in\na variety of domains recently. However, it is costly to update the entire\nparameter set of large pre-trained models. Although recently proposed\nparameter-efficient transfer learning (PETL) techniques allow updating a small\nsubset of parameters (e.g. only using 2% of parameters) inside a pre-trained\nbackbone network for a new task, they only reduce the training memory\nrequirement by up to 30%. This is because the gradient computation for the\ntrainable parameters still requires backpropagation through the large\npre-trained backbone model. To address this, we propose Ladder Side-Tuning\n(LST), a new PETL technique that can reduce training memory requirements by\nmore substantial amounts. Unlike existing parameter-efficient methods that\ninsert additional parameters inside backbone networks, we train a ladder side\nnetwork, a small and separate network that takes intermediate activations as\ninput via shortcut connections (called ladders) from backbone networks and\nmakes predictions. LST has significantly lower memory requirements than\nprevious methods, because it does not require backpropagation through the\nbackbone network, but instead only through the side network and ladder\nconnections. We evaluate our method with various models (T5 and CLIP-T5) on\nboth NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST\nsaves 69% of the memory costs to fine-tune the whole network, while other\nmethods only save 26% of that in similar parameter usages (hence, 2.7x more\nmemory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA\nin a low-memory regime. To further show the advantage of this better memory\nefficiency, we also apply LST to larger T5 models, attaining better GLUE\nperformance than full fine-tuning and other PETL methods. The\naccuracy-efficiency trade-off also holds on VL tasks.</p>\n", "tags": ["Efficiency","Fine-Tuning","Training Techniques"] },
{"key": "surís2023vipergpt", "citations": "99", "year": "2023", "title":"Vipergpt: Visual Inference Via Python Execution For Reasoning", "abstract": "<p>Answering visual queries is a complex task that requires both visual\nprocessing and reasoning. End-to-end models, the dominant approach for this\ntask, do not explicitly differentiate between the two, limiting\ninterpretability and generalization. Learning modular programs presents a\npromising alternative, but has proven challenging due to the difficulty of\nlearning both the programs and modules simultaneously. We introduce ViperGPT, a\nframework that leverages code-generation models to compose vision-and-language\nmodels into subroutines to produce a result for any query. ViperGPT utilizes a\nprovided API to access the available modules, and composes them by generating\nPython code that is later executed. This simple approach requires no further\ntraining, and achieves state-of-the-art results across various complex visual\ntasks.</p>\n", "tags": ["ICCV","Tools","Training Techniques"] },
{"key": "susan2022opt", "citations": "820", "year": "2022", "title":"OPT: Open Pre-trained Transformer Language Models", "abstract": "<p>Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.</p>\n", "tags": ["Few-Shot","Model Architecture"] },
{"key": "susnjak2022chatgpt", "citations": "286", "year": "2022", "title":"Chatgpt: The End Of Online Exam Integrity?", "abstract": "<p>This study evaluated the ability of ChatGPT, a recently developed artificial\nintelligence (AI) agent, to perform high-level cognitive tasks and produce text\nthat is indistinguishable from human-generated text. This capacity raises\nconcerns about the potential use of ChatGPT as a tool for academic misconduct\nin online exams. The study found that ChatGPT is capable of exhibiting critical\nthinking skills and generating highly realistic text with minimal input, making\nit a potential threat to the integrity of online exams, particularly in\ntertiary education settings where such exams are becoming more prevalent.\nReturning to invigilated and oral exams could form part of the solution, while\nusing advanced proctoring techniques and AI-text output detectors may be\neffective in addressing this issue, they are not likely to be foolproof\nsolutions. Further research is needed to fully understand the implications of\nlarge language models like ChatGPT and to devise strategies for combating the\nrisk of cheating using these tools. It is crucial for educators and\ninstitutions to be aware of the possibility of ChatGPT being used for cheating\nand to investigate measures to address it in order to maintain the fairness and\nvalidity of online exams for all students.</p>\n", "tags": ["Agentic","Ethics & Fairness","Tools"] },
{"key": "suzgun2022challenging", "citations": "115", "year": "2023", "title":"Challenging Big-bench Tasks And Whether Chain-of-thought Can Solve Them", "abstract": "<p>BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that\nfocuses on tasks believed to be beyond the capabilities of current language\nmodels. Language models have already made good progress on this benchmark, with\nthe best model in the BIG-Bench paper outperforming average reported\nhuman-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But\non what tasks do language models fall short of average human-rater performance,\nand are those tasks actually unsolvable by current language models?\n  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we\ncall BIG-Bench Hard (BBH). These are the task for which prior language model\nevaluations did not outperform the average human-rater. We find that applying\nchain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the\naverage human-rater performance on 10 of the 23 tasks, and Codex\n(code-davinci-002) to surpass the average human-rater performance on 17 of the\n23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot\nprompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,\n2022), substantially underestimates the best performance and capabilities of\nlanguage models, which is better captured via CoT prompting. As further\nanalysis, we explore the interaction between CoT and model scale on BBH,\nfinding that CoT enables emergent task performance on several BBH tasks with\notherwise flat scaling curves.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Evaluation","Few-Shot","In Context Learning","Prompting"] },
{"key": "suárez2020monolingual", "citations": "154", "year": "2020", "title":"A Monolingual Approach To Contextualized Word Embeddings For Mid-resource Languages", "abstract": "<p>We use the multilingual OSCAR corpus, extracted from Common Crawl via\nlanguage classification, filtering and cleaning, to train monolingual\ncontextualized word embeddings (ELMo) for five mid-resource languages. We then\ncompare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for\nthese languages on the part-of-speech tagging and parsing tasks. We show that,\ndespite the noise in the Common-Crawl-based OSCAR data, embeddings trained on\nOSCAR perform much better than monolingual embeddings trained on Wikipedia.\nThey actually equal or improve the current state of the art in tagging and\nparsing for all five languages. In particular, they also improve over\nmultilingual Wikipedia-based contextual embeddings (multilingual BERT), which\nalmost always constitutes the previous state of the art, thereby showing that\nthe benefit of a larger, more diverse corpus surpasses the cross-lingual\nbenefit of multilingual embedding architectures.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "svyatkovskiy2019pythia", "citations": "135", "year": "2019", "title":"Pythia: Ai-assisted Code Completion System", "abstract": "<p>In this paper, we propose a novel end-to-end approach for AI-assisted code\ncompletion called Pythia. It generates ranked lists of method and API\nrecommendations which can be used by software developers at edit time. The\nsystem is currently deployed as part of Intellicode extension in Visual Studio\nCode IDE. Pythia exploits state-of-the-art large-scale deep learning models\ntrained on code contexts extracted from abstract syntax trees. It is designed\nto work at a high throughput predicting the best matching code completions on\nthe order of 100 \\(ms\\).\n  We describe the architecture of the system, perform comparisons to\nfrequency-based approach and invocation-based Markov Chain language model, and\ndiscuss challenges serving Pythia models on lightweight client devices.\n  The offline evaluation results obtained on 2700 Python open source software\nGitHub repositories show a top-5 accuracy of 92%, surpassing the baseline\nmodels by 20% averaged over classes, for both intra and cross-project\nsettings.</p>\n", "tags": ["Evaluation","Has Code","KDD","Llm For Code","Model Architecture","Tools"] },
{"key": "tachibana2017efficiently", "citations": "273", "year": "2018", "title":"Efficiently Trainable Text-to-speech System Based On Deep Convolutional Networks With Guided Attention", "abstract": "<p>This paper describes a novel text-to-speech (TTS) technique based on deep\nconvolutional neural networks (CNN), without use of any recurrent units.\nRecurrent neural networks (RNN) have become a standard technique to model\nsequential data recently, and this technique has been used in some cutting-edge\nneural TTS techniques. However, training RNN components often requires a very\npowerful computer, or a very long time, typically several days or weeks. Recent\nother studies, on the other hand, have shown that CNN-based sequence synthesis\ncan be much faster than RNN-based techniques, because of high\nparallelizability. The objective of this paper is to show that an alternative\nneural TTS based only on CNN alleviate these economic costs of training. In our\nexperiment, the proposed Deep Convolutional TTS was sufficiently trained\novernight (15 hours), using an ordinary gaming PC equipped with two GPUs, while\nthe quality of the synthesized speech was almost acceptable.</p>\n", "tags": ["ICASSP","Model Architecture","Training Techniques"] },
{"key": "taigman2017voiceloop", "citations": "84", "year": "2018", "title":"Voiceloop: Voice Fitting And Synthesis Via A Phonological Loop", "abstract": "<p>We present a new neural text to speech (TTS) method that is able to transform\ntext to speech in voices that are sampled in the wild. Unlike other systems,\nour solution is able to deal with unconstrained voice samples and without\nrequiring aligned phonemes or linguistic features. The network architecture is\nsimpler than those in the existing literature and is based on a novel shifting\nbuffer working memory. The same buffer is used for estimating the attention,\ncomputing the output audio, and for updating the buffer itself. The input\nsentence is encoded using a context-free lookup table that contains one entry\nper character or phoneme. The speakers are similarly represented by a short\nvector that can also be fitted to new identities, even with only a few samples.\nVariability in the generated speech is achieved by priming the buffer prior to\ngenerating the audio. Experimental results on several datasets demonstrate\nconvincing capabilities, making TTS accessible to a wider range of\napplications. In order to promote reproducibility, we release our source code\nand models.</p>\n", "tags": ["Applications","Datasets","Model Architecture"] },
{"key": "takanobu2019guided", "citations": "70", "year": "2019", "title":"Guided Dialog Policy Learning: Reward Estimation For Multi-domain Task-oriented Dialog", "abstract": "<p>Dialog policy decides what and how a task-oriented dialog system will\nrespond, and plays a vital role in delivering effective conversations. Many\nstudies apply Reinforcement Learning to learn a dialog policy with the reward\nfunction which requires elaborate design and pre-specified user goals. With the\ngrowing needs to handle complex goals across multiple domains, such manually\ndesigned reward functions are not affordable to deal with the complexity of\nreal-world tasks. To this end, we propose Guided Dialog Policy Learning, a\nnovel algorithm based on Adversarial Inverse Reinforcement Learning for joint\nreward estimation and policy optimization in multi-domain task-oriented dialog.\nThe proposed approach estimates the reward signal and infers the user goal in\nthe dialog sessions. The reward estimator evaluates the state-action pairs so\nthat it can guide the dialog policy at each dialog turn. Extensive experiments\non a multi-domain dialog dataset show that the dialog policy guided by the\nlearned reward function achieves remarkably higher task success than\nstate-of-the-art baselines.</p>\n", "tags": ["Datasets","EMNLP","Reinforcement Learning"] },
{"key": "takase2019positional", "citations": "88", "year": "2019", "title":"Positional Encoding To Control Output Sequence Length", "abstract": "<p>Neural encoder-decoder models have been successful in natural language\ngeneration tasks. However, real applications of abstractive summarization must\nconsider additional constraint that a generated summary should not exceed a\ndesired length. In this paper, we propose a simple but effective extension of a\nsinusoidal positional encoding (Vaswani et al., 2017) to enable neural\nencoder-decoder model to preserves the length constraint. Unlike in previous\nstudies where that learn embeddings representing each length, the proposed\nmethod can generate a text of any length even if the target length is not\npresent in training data. The experimental results show that the proposed\nmethod can not only control the generation length but also improve the ROUGE\nscores.</p>\n", "tags": ["Applications","Training Techniques"] },
{"key": "talmor2018web", "citations": "431", "year": "2018", "title":"The Web As A Knowledge-base For Answering Complex Questions", "abstract": "<p>Answering complex questions is a time-consuming activity for humans that\nrequires reasoning and integration of information. Recent work on reading\ncomprehension made headway in answering simple questions, but tackling complex\nquestions is still an ongoing research challenge. Conversely, semantic parsers\nhave been successful at handling compositionality, but only when the\ninformation resides in a target knowledge-base. In this paper, we present a\nnovel framework for answering broad and complex questions, assuming answering\nsimple questions is possible using a search engine and a reading comprehension\nmodel. We propose to decompose complex questions into a sequence of simple\nquestions, and compute the final answer from the sequence of answers. To\nillustrate the viability of our approach, we create a new dataset of complex\nquestions, ComplexWebQuestions, and present a model that decomposes questions\nand interacts with the web to compute an answer. We empirically demonstrate\nthat question decomposition improves performance from 20.8 precision@1 to 27.5\nprecision@1 on this new dataset.</p>\n", "tags": ["Datasets","NAACL","Tools"] },
{"key": "talmor2019multiqa", "citations": "184", "year": "2019", "title":"Multiqa: An Empirical Investigation Of Generalization And Transfer In Reading Comprehension", "abstract": "<p>A large number of reading comprehension (RC) datasets has been created\nrecently, but little analysis has been done on whether they generalize to one\nanother, and the extent to which existing datasets can be leveraged for\nimproving performance on new ones. In this paper, we conduct such an\ninvestigation over ten RC datasets, training on one or more source RC datasets,\nand evaluating generalization, as well as transfer to a target RC dataset. We\nanalyze the factors that contribute to generalization, and show that training\non a source RC dataset and transferring to a target dataset substantially\nimproves performance, even in the presence of powerful contextual\nrepresentations from BERT (Devlin et al., 2019). We also find that training on\nmultiple source RC datasets leads to robust generalization and transfer, and\ncan reduce the cost of example collection for a new RC dataset. Following our\nanalysis, we propose MultiQA, a BERT-based model, trained on multiple RC\ndatasets, which leads to state-of-the-art performance on five RC datasets. We\nshare our infrastructure for the benefit of the research community.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "talmor2019olmpics", "citations": "299", "year": "2020", "title":"Olmpics -- On What Language Model Pre-training Captures", "abstract": "<p>Recent success of pre-trained language models (LMs) has spurred widespread\ninterest in the language capabilities that they possess. However, efforts to\nunderstand whether LM representations are useful for symbolic reasoning tasks\nhave been limited and scattered. In this work, we propose eight reasoning\ntasks, which conceptually require operations such as comparison, conjunction,\nand composition. A fundamental challenge is to understand whether the\nperformance of a LM on a task should be attributed to the pre-trained\nrepresentations or to the process of fine-tuning on the task data. To address\nthis, we propose an evaluation protocol that includes both zero-shot evaluation\n(no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to\nthe learning curve of multiple controls, which paints a rich picture of the LM\ncapabilities. Our main findings are that: (a) different LMs exhibit\nqualitatively different reasoning abilities, e.g., RoBERTa succeeds in\nreasoning tasks where BERT fails completely; (b) LMs do not reason in an\nabstract manner and are context-dependent, e.g., while RoBERTa can compare\nages, it can do so only when the ages are in the typical range of human ages;\n(c) On half of our reasoning tasks all models fail completely. Our findings and\ninfrastructure can help future work on designing new datasets, models and\nobjective functions for pre-training.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","TACL","Training Techniques"] },
{"key": "tam2021improving", "citations": "91", "year": "2021", "title":"Improving And Simplifying Pattern Exploiting Training", "abstract": "<p>Recently, pre-trained language models (LMs) have achieved strong performance\nwhen fine-tuned on difficult benchmarks like SuperGLUE. However, performance\ncan suffer when there are very few labeled examples available for fine-tuning.\nPattern Exploiting Training (PET) is a recent approach that leverages patterns\nfor few-shot learning. However, PET uses task-specific unlabeled data. In this\npaper, we focus on few-shot learning without any unlabeled data and introduce\nADAPET, which modifies PET’s objective to provide denser supervision during\nfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any\ntask-specific unlabeled data. Our code can be found at\nhttps://github.com/rrmenon10/ADAPET.</p>\n", "tags": ["EMNLP","Few-Shot","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "tambe2020edgebert", "citations": "71", "year": "2021", "title":"Edgebert: Sentence-level Energy Optimizations For Latency-aware Multi-task NLP Inference", "abstract": "<p>Transformer-based language models such as BERT provide significant accuracy\nimprovement for a multitude of natural language processing (NLP) tasks.\nHowever, their hefty computational and memory demands make them challenging to\ndeploy to resource-constrained edge platforms with strict latency requirements.\nWe present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware\nenergy optimization for multi-task NLP. EdgeBERT employs entropy-based early\nexit predication in order to perform dynamic voltage-frequency scaling (DVFS),\nat a sentence granularity, for minimal energy consumption while adhering to a\nprescribed target latency. Computation and memory footprint overheads are\nfurther alleviated by employing a calibrated combination of adaptive attention\nspan, selective network pruning, and floating-point quantization. Furthermore,\nin order to maximize the synergistic benefits of these algorithms in always-on\nand intermediate edge computing settings, we specialize a 12nm scalable\nhardware accelerator system, integrating a fast-switching low-dropout voltage\nregulator (LDO), an all-digital phase-locked loop (ADPLL), as well as,\nhigh-density embedded non-volatile memories (eNVMs) wherein the sparse\nfloating-point bit encodings of the shared multi-task parameters are carefully\nstored. Altogether, latency-aware multi-task NLP inference acceleration on the\nEdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy\ncompared to the conventional inference without early stopping, the\nlatency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson\nTegra X2 mobile GPU, respectively.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "tambwekar2018controllable", "citations": "81", "year": "2019", "title":"Controllable Neural Story Plot Generation Via Reward Shaping", "abstract": "<p>Language-modeling–based approaches to story plot generation attempt to\nconstruct a plot by sampling from a language model (LM) to predict the next\ncharacter, word, or sentence to add to the story. LM techniques lack the\nability to receive guidance from the user to achieve a specific goal, resulting\nin stories that don’t have a clear sense of progression and lack coherence. We\npresent a reward-shaping technique that analyzes a story corpus and produces\nintermediate rewards that are backpropagated into a pre-trained LM in order to\nguide the model towards a given goal. Automated evaluations show our technique\ncan create a model that generates story plots which consistently achieve a\nspecified goal. Human-subject studies show that the generated stories have more\nplausible event ordering than baseline plot generation techniques.</p>\n", "tags": ["Datasets","IJCAI","Reinforcement Learning"] },
{"key": "tamchyna2017modeling", "citations": "60", "year": "2017", "title":"Modeling Target-side Inflection In Neural Machine Translation", "abstract": "<p>NMT systems have problems with large vocabulary sizes. Byte-pair encoding\n(BPE) is a popular approach to solving this problem, but while BPE allows the\nsystem to generate any target-side word, it does not enable effective\ngeneralization over the rich vocabulary in morphologically rich languages with\nstrong inflectional phenomena. We introduce a simple approach to overcome this\nproblem by training a system to produce the lemma of a word and its\nmorphologically rich POS tag, which is then followed by a deterministic\ngeneration step. We apply this strategy for English-Czech and English-German\ntranslation scenarios, obtaining improvements in both settings. We furthermore\nshow that the improvement is not due to only adding explicit morphological\ninformation.</p>\n", "tags": ["Training Techniques"] },
{"key": "tan2016improved", "citations": "648", "year": "2016", "title":"Improved Recurrent Neural Networks For Session-based Recommendations", "abstract": "<p>Recurrent neural networks (RNNs) were recently proposed for the session-based\nrecommendation task. The models showed promising improvements over traditional\nrecommendation approaches. In this work, we further study RNN-based models for\nsession-based recommendations. We propose the application of two techniques to\nimprove model performance, namely, data augmentation, and a method to account\nfor shifts in the input data distribution. We also empirically study the use of\ngeneralised distillation, and a novel alternative model that directly predicts\nitem embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate\nrelative improvements of 12.8% and 14.8% over previously reported results on\nthe Recall@20 and Mean Reciprocal Rank@20 metrics respectively.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "tan2017s", "citations": "72", "year": "2017", "title":"S-net: From Answer Extraction To Answer Generation For Machine Reading Comprehension", "abstract": "<p>In this paper, we present a novel approach to machine reading comprehension\nfor the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a\nquestion with exact text spans in a passage, the MS-MARCO dataset defines the\ntask as answering a question from multiple passages and the words in the answer\nare not necessary in the passages. We therefore develop an\nextraction-then-synthesis framework to synthesize answers from extraction\nresults. Specifically, the answer extraction model is first employed to predict\nthe most important sub-spans from the passage as evidence, and the answer\nsynthesis model takes the evidence as additional features along with the\nquestion and passage to further elaborate the final answers. We build the\nanswer extraction model with state-of-the-art neural networks for single\npassage reading comprehension, and propose an additional task of passage\nranking to help answer extraction in multiple passages. The answer synthesis\nmodel is based on the sequence-to-sequence neural networks with extracted\nevidences as features. Experiments show that our extraction-then-synthesis\nmethod outperforms state-of-the-art methods.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "tan2018text2scene", "citations": "94", "year": "2019", "title":"Text2scene: Generating Compositional Scenes From Textual Descriptions", "abstract": "<p>In this paper, we propose Text2Scene, a model that generates various forms of\ncompositional scene representations from natural language descriptions. Unlike\nrecent works, our method does NOT use Generative Adversarial Networks (GANs).\nText2Scene instead learns to sequentially generate objects and their attributes\n(location, size, appearance, etc) at every time step by attending to different\nparts of the input text and the current status of the generated scene. We show\nthat under minor modifications, the proposed framework can handle the\ngeneration of different forms of scene representations, including cartoon-like\nscenes, object layouts corresponding to real images, and synthetic images. Our\nmethod is not only competitive when compared with state-of-the-art GAN-based\nmethods using automatic metrics and superior based on human judgments but also\nhas the advantage of producing interpretable results.</p>\n", "tags": ["CVPR","Evaluation","Tools"] },
{"key": "tan2019learning", "citations": "272", "year": "2019", "title":"Learning To Navigate Unseen Environments: Back Translation With Environmental Dropout", "abstract": "<p>A grand goal in AI is to build a robot that can accurately navigate based on\nnatural language instructions, which requires the agent to perceive the scene,\nunderstand and ground language, and act in the real-world environment. One key\nchallenge here is to learn to navigate in new environments that are unseen\nduring training. Most of the existing approaches perform dramatically worse in\nunseen environments as compared to seen ones. In this paper, we present a\ngeneralizable navigational agent. Our agent is trained in two stages. The first\nstage is training via mixed imitation and reinforcement learning, combining the\nbenefits from both off-policy and on-policy optimization. The second stage is\nfine-tuning via newly-introduced ‘unseen’ triplets (environment, path,\ninstruction). To generate these unseen triplets, we propose a simple but\neffective ‘environmental dropout’ method to mimic unseen environments, which\novercomes the problem of limited seen environment variability. Next, we apply\nsemi-supervised learning (via back-translation) on these dropped-out\nenvironments to generate new paths and instructions. Empirically, we show that\nour agent is substantially better at generalizability when fine-tuned with\nthese triplets, outperforming the state-of-art approaches by a large margin on\nthe private unseen test set of the Room-to-Room task, and achieving the top\nrank on the leaderboard.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Reinforcement Learning","Training Techniques"] },
{"key": "tan2019lxmert", "citations": "1994", "year": "2019", "title":"LXMERT: Learning Cross-modality Encoder Representations From Transformers", "abstract": "<p>Vision-and-language reasoning requires an understanding of visual concepts,\nlanguage semantics, and, most importantly, the alignment and relationships\nbetween these two modalities. We thus propose the LXMERT (Learning\nCross-Modality Encoder Representations from Transformers) framework to learn\nthese vision-and-language connections. In LXMERT, we build a large-scale\nTransformer model that consists of three encoders: an object relationship\nencoder, a language encoder, and a cross-modality encoder. Next, to endow our\nmodel with the capability of connecting vision and language semantics, we\npre-train the model with large amounts of image-and-sentence pairs, via five\ndiverse representative pre-training tasks: masked language modeling, masked\nobject prediction (feature regression and label classification), cross-modality\nmatching, and image question answering. These tasks help in learning both\nintra-modality and cross-modality relationships. After fine-tuning from our\npre-trained parameters, our model achieves the state-of-the-art results on two\nvisual question answering datasets (i.e., VQA and GQA). We also show the\ngeneralizability of our pre-trained cross-modality model by adapting it to a\nchallenging visual-reasoning task, NLVR2, and improve the previous best result\nby 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies\nto prove that both our novel model components and pre-training strategies\nsignificantly contribute to our strong results; and also present several\nattention visualizations for the different encoders. Code and pre-trained\nmodels publicly available at: https://github.com/airsplay/lxmert</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "tan2019multilingual", "citations": "146", "year": "2019", "title":"Multilingual Neural Machine Translation With Knowledge Distillation", "abstract": "<p>Multilingual machine translation, which translates multiple languages with a\nsingle model, has attracted much attention due to its efficiency of offline\ntraining and online serving. However, traditional multilingual translation\nusually yields inferior accuracy compared with the counterpart using individual\nmodels for each language pair, due to language diversity and model capacity\nlimitations. In this paper, we propose a distillation-based approach to boost\nthe accuracy of multilingual machine translation. Specifically, individual\nmodels are first trained and regarded as teachers, and then the multilingual\nmodel is trained to fit the training data and match the outputs of individual\nmodels simultaneously through knowledge distillation. Experiments on IWSLT, WMT\nand Ted talk translation datasets demonstrate the effectiveness of our method.\nParticularly, we show that one model is enough to handle multiple languages (up\nto 44 languages in our experiment), with comparable or even better accuracy\nthan individual models.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "tan2020progressive", "citations": "61", "year": "2021", "title":"Progressive Generation Of Long Text With Pretrained Language Models", "abstract": "<p>Large-scale language models (LMs) pretrained on massive corpora of text, such\nas GPT-2, are powerful open-domain text generators. However, as our systematic\nexamination reveals, it is still challenging for such models to generate\ncoherent long passages of text (e.g., 1000 tokens), especially when the models\nare fine-tuned to the target domain on a small corpus. Previous\nplanning-then-generation methods also fall short of producing such long text in\nvarious domains. To overcome the limitations, we propose a simple but effective\nmethod of generating text in a progressive manner, inspired by generating\nimages from low to high resolution. Our method first produces domain-specific\ncontent keywords and then progressively refines them into complete passages in\nmultiple stages. The simple design allows our approach to take advantage of\npretrained LMs at each stage and effectively adapt to any target domain given\nonly a small set of examples. We conduct a comprehensive empirical study with a\nbroad set of evaluation metrics, and show that our approach significantly\nimproves upon the fine-tuned large LMs and various planning-then-generation\nmethods in terms of quality and sample efficiency. Human evaluation also\nvalidates that our model generations are more coherent.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","NAACL"] },
{"key": "tanaka2018atts2s", "citations": "105", "year": "2019", "title":"Atts2s-vc: Sequence-to-sequence Voice Conversion With Attention And Context Preservation Mechanisms", "abstract": "<p>This paper describes a method based on a sequence-to-sequence learning\n(Seq2Seq) with attention and context preservation mechanism for voice\nconversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving\nsequence modeling such as speech synthesis and recognition, machine\ntranslation, and image captioning. In contrast to current VC techniques, our\nmethod 1) stabilizes and accelerates the training procedure by considering\nguided attention and proposed context preservation losses, 2) allows not only\nspectral envelopes but also fundamental frequency contours and durations of\nspeech to be converted, 3) requires no context information such as phoneme\nlabels, and 4) requires no time-aligned source and target speech data in\nadvance. In our experiment, the proposed VC framework can be trained in only\none day, using only one GPU of an NVIDIA Tesla K80, while the quality of the\nsynthesized speech is higher than that of speech converted by Gaussian mixture\nmodel-based VC and is comparable to that of speech generated by recurrent\nneural network-based text-to-speech synthesis, which can be regarded as an\nupper limit on VC performance.</p>\n", "tags": ["ICASSP","Model Architecture","Tools","Training Techniques"] },
{"key": "tandon2018reasoning", "citations": "91", "year": "2018", "title":"Reasoning About Actions And State Changes By Injecting Commonsense Knowledge", "abstract": "<p>Comprehending procedural text, e.g., a paragraph describing photosynthesis,\nrequires modeling actions and the state changes they produce, so that questions\nabout entities at different timepoints can be answered. Although several recent\nsystems have shown impressive progress in this task, their predictions can be\nglobally inconsistent or highly improbable. In this paper, we show how the\npredicted effects of actions in the context of a paragraph can be improved in\ntwo ways: (1) by incorporating global, commonsense constraints (e.g., a\nnon-existent entity cannot be destroyed), and (2) by biasing reading with\npreferences from large-scale corpora (e.g., trees rarely move). Unlike earlier\nmethods, we treat the problem as a neural structured prediction task, allowing\nhard and soft constraints to steer the model away from unlikely predictions. We\nshow that the new model significantly outperforms earlier systems on a\nbenchmark dataset for procedural text comprehension (+8% relative gain), and\nthat it also avoids some of the nonsensical predictions that earlier systems\nmake.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "tang2016context", "citations": "73", "year": "2016", "title":"Context-aware Natural Language Generation With Recurrent Neural Networks", "abstract": "<p>This paper studied generating natural languages at particular contexts or\nsituations. We proposed two novel approaches which encode the contexts into a\ncontinuous semantic representation and then decode the semantic representation\ninto text sequences with recurrent neural networks. During decoding, the\ncontext information are attended through a gating mechanism, addressing the\nproblem of long-range dependency caused by lengthy sequences. We evaluate the\neffectiveness of the proposed approaches on user review data, in which rich\ncontexts are available and two informative contexts, sentiments and products,\nare selected for evaluation. Experiments show that the fake reviews generated\nby our approaches are very natural. Results of fake review detection with human\njudges show that more than 50% of the fake reviews are misclassified as the\nreal reviews, and more than 90% are misclassified by existing state-of-the-art\nfake review detection algorithm.</p>\n", "tags": ["Evaluation"] },
{"key": "tang2017question", "citations": "176", "year": "2017", "title":"Question Answering And Question Generation As Dual Tasks", "abstract": "<p>We study the problem of joint question answering (QA) and question generation\n(QG) in this paper.\n  Our intuition is that QA and QG have intrinsic connections and these two\ntasks could improve each other.\n  On one side, the QA model judges whether the generated question of a QG model\nis relevant to the answer.\n  On the other side, the QG model provides the probability of generating a\nquestion given the answer, which is a useful evidence that in turn facilitates\nQA.\n  In this paper we regard QA and QG as dual tasks.\n  We propose a training framework that trains the models of QA and QG\nsimultaneously, and explicitly leverages their probabilistic correlation to\nguide the training process of both models.\n  We implement a QG model based on sequence-to-sequence learning, and a QA\nmodel based on recurrent neural network.\n  As all the components of the QA and QG models are differentiable, all the\nparameters involved in these two models could be conventionally learned with\nback propagation.\n  We conduct experiments on three datasets. Empirical results show that our\ntraining framework improves both QA and QG tasks.\n  The improved QA model performs comparably with strong baseline approaches on\nall three datasets.</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "tang2018adversarial", "citations": "170", "year": "2019", "title":"Adversarial Training Towards Robust Multimedia Recommender System", "abstract": "<p>With the prevalence of multimedia content on the Web, developing recommender\nsolutions that can effectively leverage the rich signal in multimedia data is\nin urgent need. Owing to the success of deep neural networks in representation\nlearning, recent advance on multimedia recommendation has largely focused on\nexploring deep learning methods to improve the recommendation accuracy. To\ndate, however, there has been little effort to investigate the robustness of\nmultimedia representation and its impact on the performance of multimedia\nrecommendation.\n  In this paper, we shed light on the robustness of multimedia recommender\nsystem. Using the state-of-the-art recommendation framework and deep image\nfeatures, we demonstrate that the overall system is not robust, such that a\nsmall (but purposeful) perturbation on the input image will severely decrease\nthe recommendation accuracy. This implies the possible weakness of multimedia\nrecommender system in predicting user preference, and more importantly, the\npotential of improvement by enhancing its robustness. To this end, we propose a\nnovel solution named Adversarial Multimedia Recommendation (AMR), which can\nlead to a more robust multimedia recommender model by using adversarial\nlearning. The idea is to train the model to defend an adversary, which adds\nperturbations to the target image with the purpose of decreasing the model’s\naccuracy. We conduct experiments on two representative multimedia\nrecommendation tasks, namely, image recommendation and visually-aware product\nrecommendation. Extensive results verify the positive effect of adversarial\nlearning and demonstrate the effectiveness of our AMR method. Source codes are\navailable in https://github.com/duxy-me/AMR.</p>\n", "tags": ["Has Code","Security","Tools","Training Techniques"] },
{"key": "tang2018why", "citations": "292", "year": "2018", "title":"Why Self-attention? A Targeted Evaluation Of Neural Machine Translation Architectures", "abstract": "<p>Recently, non-recurrent architectures (convolutional, self-attentional) have\noutperformed RNNs in neural machine translation. CNNs and self-attentional\nnetworks can connect distant words via shorter network paths than RNNs, and it\nhas been speculated that this improves their ability to model long-range\ndependencies. However, this theoretical argument has not been tested\nempirically, nor have alternative explanations for their strong performance\nbeen explored in-depth. We hypothesize that the strong performance of CNNs and\nself-attentional networks could also be due to their ability to extract\nsemantic features from the source text, and we evaluate RNNs, CNNs and\nself-attention networks on two tasks: subject-verb agreement (where capturing\nlong-range dependencies is required) and word sense disambiguation (where\nsemantic feature extraction is required). Our experimental results show that:\n1) self-attentional networks and CNNs do not outperform RNNs in modeling\nsubject-verb agreement over long distances; 2) self-attentional networks\nperform distinctly better than RNNs and CNNs on word sense disambiguation.</p>\n", "tags": ["EMNLP","Evaluation","Model Architecture"] },
{"key": "tang2019distilling", "citations": "358", "year": "2019", "title":"Distilling Task-specific Knowledge From BERT Into Simple Neural Networks", "abstract": "<p>In the natural language processing literature, neural networks are becoming\nincreasingly deeper and complex. The recent poster child of this trend is the\ndeep language representation model, which includes BERT, ELMo, and GPT. These\ndevelopments have led to the conviction that previous-generation, shallower\nneural networks for language understanding are obsolete. In this paper,\nhowever, we demonstrate that rudimentary, lightweight neural networks can still\nbe made competitive without architecture changes, external training data, or\nadditional input features. We propose to distill knowledge from BERT, a\nstate-of-the-art language representation model, into a single-layer BiLSTM, as\nwell as its siamese counterpart for sentence-pair tasks. Across multiple\ndatasets in paraphrasing, natural language inference, and sentiment\nclassification, we achieve comparable results with ELMo, while using roughly\n100 times fewer parameters and 15 times less inference time.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "tang2019target", "citations": "112", "year": "2019", "title":"Target-guided Open-domain Conversation", "abstract": "<p>Many real-world open-domain conversation applications have specific goals to\nachieve during open-ended chats, such as recommendation, psychotherapy,\neducation, etc. We study the problem of imposing conversational goals on\nopen-domain chat agents. In particular, we want a conversational system to chat\nnaturally with human and proactively guide the conversation to a designated\ntarget subject. The problem is challenging as no public data is available for\nlearning such a target-guided strategy. We propose a structured approach that\nintroduces coarse-grained keywords to control the intended content of system\nresponses. We then attain smooth conversation transition through turn-level\nsupervised learning, and drive the conversation towards the target with\ndiscourse-level constraints. We further derive a keyword-augmented conversation\ndataset for the study. Quantitative and human evaluations show our system can\nproduce meaningful and effective conversations, significantly improving over\nother approaches.</p>\n", "tags": ["Applications","Datasets"] },
{"key": "tang2020multilingual", "citations": "174", "year": "2020", "title":"Multilingual Translation With Extensible Multilingual Pretraining And Finetuning", "abstract": "<p>Recent work demonstrates the potential of multilingual pretraining of\ncreating one model that can be used for various tasks in different languages.\nPrevious work in multilingual pretraining has demonstrated that machine\ntranslation systems can be created by finetuning on bitext. In this work, we\nshow that multilingual translation models can be created through multilingual\nfinetuning. Instead of finetuning on one direction, a pretrained model is\nfinetuned on many directions at the same time. Compared to multilingual models\ntrained from scratch, starting from pretrained models incorporates the benefits\nof large quantities of unlabeled monolingual data, which is particularly\nimportant for low resource languages where bitext is not available. We\ndemonstrate that pretrained models can be extended to incorporate additional\nlanguages without loss of performance. We double the number of languages in\nmBART to support multilingual machine translation models of 50 languages.\nFinally, we create the ML50 benchmark, covering low, mid, and high resource\nlanguages, to facilitate reproducible research by standardizing training and\nevaluation data. On ML50, we demonstrate that multilingual finetuning improves\non average 1 BLEU over the strongest baselines (being either multilingual from\nscratch or bilingual finetuning) while improving 9.3 BLEU on average over\nbilingual baselines from scratch.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "tang2021clip4caption", "citations": "93", "year": "2021", "title":"Clip4caption: CLIP For Video Caption", "abstract": "<p>Video captioning is a challenging task since it requires generating sentences\ndescribing various diverse and complex videos. Existing video captioning models\nlack adequate visual representation due to the neglect of the existence of gaps\nbetween videos and texts. To bridge this gap, in this paper, we propose a\nCLIP4Caption framework that improves video captioning based on a CLIP-enhanced\nvideo-text matching network (VTM). This framework is taking full advantage of\nthe information from both vision and language and enforcing the model to learn\nstrongly text-correlated video features for text generation. Besides, unlike\nmost existing models using LSTM or GRU as the sentence decoder, we adopt a\nTransformer structured decoder network to effectively learn the long-range\nvisual and language dependency. Additionally, we introduce a novel ensemble\nstrategy for captioning tasks. Experimental results demonstrate the\neffectiveness of our method on two datasets: 1) on MSR-VTT dataset, our method\nachieved a new state-of-the-art result with a significant gain of up to 10% in\nCIDEr; 2) on the private test data, our method ranking 2nd place in the ACM MM\nmultimedia grand challenge 2021: Pre-training for Video Understanding\nChallenge. It is noted that our model is only trained on the MSR-VTT dataset.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "tang2021improving", "citations": "66", "year": "2021", "title":"Improving Speech Translation By Understanding And Learning From The Auxiliary Text Translation Task", "abstract": "<p>Pretraining and multitask learning are widely used to improve the speech to\ntext translation performance. In this study, we are interested in training a\nspeech to text translation model along with an auxiliary text to text\ntranslation task. We conduct a detailed analysis to understand the impact of\nthe auxiliary task on the primary task within the multitask learning framework.\nOur analysis confirms that multitask learning tends to generate similar decoder\nrepresentations from different modalities and preserve more information from\nthe pretrained text translation modules. We observe minimal negative transfer\neffect between the two tasks and sharing more parameters is helpful to transfer\nknowledge from the text task to the speech task. The analysis also reveals that\nthe modality representation difference at the top decoder layers is still not\nnegligible, and those layers are critical for the translation quality. Inspired\nby these findings, we propose three methods to improve translation quality.\nFirst, a parameter sharing and initialization strategy is proposed to enhance\ninformation sharing between the tasks. Second, a novel attention-based\nregularization is proposed for the encoders and pulls the representations from\ndifferent modalities closer. Third, an online knowledge distillation is\nproposed to enhance the knowledge transfer from the text to the speech task.\nOur experiments show that the proposed approach improves translation\nperformance by more than 2 BLEU over a strong baseline and achieves\nstate-of-the-art results on the \\textsc{MuST-C} English-German, English-French\nand English-Spanish language pairs.</p>\n", "tags": ["Efficiency","Model Architecture","Tools","Training Techniques"] },
{"key": "tang2023does", "citations": "71", "year": "2023", "title":"Does Synthetic Data Generation Of Llms Help Clinical Text Mining?", "abstract": "<p>Recent advancements in large language models (LLMs) have led to the\ndevelopment of highly potent models like OpenAI’s ChatGPT. These models have\nexhibited exceptional performance in a variety of tasks, such as question\nanswering, essay composition, and code generation. However, their effectiveness\nin the healthcare sector remains uncertain. In this study, we seek to\ninvestigate the potential of ChatGPT to aid in clinical text mining by\nexamining its ability to extract structured information from unstructured\nhealthcare texts, with a focus on biological named entity recognition and\nrelation extraction. However, our preliminary results indicate that employing\nChatGPT directly for these tasks resulted in poor performance and raised\nprivacy concerns associated with uploading patients’ information to the ChatGPT\nAPI. To overcome these limitations, we propose a new training paradigm that\ninvolves generating a vast quantity of high-quality synthetic data with labels\nutilizing ChatGPT and fine-tuning a local model for the downstream task. Our\nmethod has resulted in significant improvements in the performance of\ndownstream tasks, improving the F1-score from 23.37% to 63.99% for the named\nentity recognition task and from 75.86% to 83.59% for the relation extraction\ntask. Furthermore, generating data using ChatGPT can significantly reduce the\ntime and effort required for data collection and labeling, as well as mitigate\ndata privacy concerns. In summary, the proposed framework presents a promising\nsolution to enhance the applicability of LLM models to clinical text mining.</p>\n", "tags": ["Datasets","Fine-Tuning","Llm For Code","Privacy","Tools","Training Techniques"] },
{"key": "tankelevitch2023metacognitive", "citations": "69", "year": "2024", "title":"The Metacognitive Demands And Opportunities Of Generative AI", "abstract": "<p>Generative AI (GenAI) systems offer unprecedented opportunities for\ntransforming professional and personal work, yet present challenges around\nprompting, evaluating and relying on outputs, and optimizing workflows. We\nargue that metacognition\\(\\unicode{x2013}\\)the psychological ability to monitor\nand control one’s thoughts and behavior\\(\\unicode{x2013}\\)offers a valuable lens\nto understand and design for these usability challenges. Drawing on research in\npsychology and cognitive science, and recent GenAI user studies, we illustrate\nhow GenAI systems impose metacognitive demands on users, requiring a high\ndegree of metacognitive monitoring and control. We propose these demands could\nbe addressed by integrating metacognitive support strategies into GenAI\nsystems, and by designing GenAI systems to reduce their metacognitive demand by\ntargeting explainability and customizability. Metacognition offers a coherent\nframework for understanding the usability challenges posed by GenAI, and\nprovides novel research and design directions to advance human-AI interaction.</p>\n", "tags": ["Prompting","Tools"] },
{"key": "tanti2017where", "citations": "114", "year": "2018", "title":"Where To Put The Image In An Image Caption Generator", "abstract": "<p>When a recurrent neural network language model is used for caption\ngeneration, the image information can be fed to the neural network either by\ndirectly incorporating it in the RNN – conditioning the language model by\n<code class=\"language-plaintext highlighter-rouge\">injecting' image features -- or in a layer following the RNN -- conditioning\nthe language model by </code>merging’ image features. While both options are attested\nin the literature, there is as yet no systematic comparison between the two. In\nthis paper we empirically show that it is not especially detrimental to\nperformance whether one architecture is used or another. The merge architecture\ndoes have practical advantages, as conditioning by merging allows the RNN’s\nhidden state vector to shrink in size by up to four times. Our results suggest\nthat the visual and linguistic modalities for caption generation need not be\njointly encoded by the RNN as that yields large, memory-intensive models with\nfew tangible advantages in performance; rather, the multimodal integration\nshould be delayed to a subsequent stage.</p>\n", "tags": ["Model Architecture"] },
{"key": "tao2020df", "citations": "200", "year": "2022", "title":"DF-GAN: A Simple And Effective Baseline For Text-to-image Synthesis", "abstract": "<p>Synthesizing high-quality realistic images from text descriptions is a\nchallenging task. Existing text-to-image Generative Adversarial Networks\ngenerally employ a stacked architecture as the backbone yet still remain three\nflaws. First, the stacked architecture introduces the entanglements between\ngenerators of different image scales. Second, existing studies prefer to apply\nand fix extra networks in adversarial learning for text-image semantic\nconsistency, which limits the supervision capability of these networks. Third,\nthe cross-modal attention-based text-image fusion that widely adopted by\nprevious works is limited on several special image scales because of the\ncomputational cost. To these ends, we propose a simpler but more effective Deep\nFusion Generative Adversarial Networks (DF-GAN). To be specific, we propose:\n(i) a novel one-stage text-to-image backbone that directly synthesizes\nhigh-resolution images without entanglements between different generators, (ii)\na novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty\nand One-Way Output, which enhances the text-image semantic consistency without\nintroducing extra networks, (iii) a novel deep text-image fusion block, which\ndeepens the fusion process to make a full fusion between text and visual\nfeatures. Compared with current state-of-the-art methods, our proposed DF-GAN\nis simpler but more efficient to synthesize realistic and text-matching images\nand achieves better performance on widely used datasets.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "tao2023galip", "citations": "89", "year": "2023", "title":"GALIP: Generative Adversarial Clips For Text-to-image Synthesis", "abstract": "<p>Synthesizing high-fidelity complex images from text is challenging. Based on\nlarge pretraining, the autoregressive and diffusion models can synthesize\nphoto-realistic images. Although these large models have shown notable\nprogress, there remain three flaws. 1) These models require tremendous training\ndata and parameters to achieve good performance. 2) The multi-step generation\ndesign slows the image synthesis process heavily. 3) The synthesized visual\nfeatures are difficult to control and require delicately designed prompts. To\nenable high-quality, efficient, fast, and controllable text-to-image synthesis,\nwe propose Generative Adversarial CLIPs, namely GALIP. GALIP leverages the\npowerful pretrained CLIP model both in the discriminator and generator.\nSpecifically, we propose a CLIP-based discriminator. The complex scene\nunderstanding ability of CLIP enables the discriminator to accurately assess\nthe image quality. Furthermore, we propose a CLIP-empowered generator that\ninduces the visual concepts from CLIP through bridge features and prompts. The\nCLIP-integrated generator and discriminator boost training efficiency, and as a\nresult, our model only requires about 3% training data and 6% learnable\nparameters, achieving comparable results to large pretrained autoregressive and\ndiffusion models. Moreover, our model achieves 120 times faster synthesis speed\nand inherits the smooth latent space from GAN. The extensive experimental\nresults demonstrate the excellent performance of our GALIP. Code is available\nat https://github.com/tobran/GALIP.</p>\n", "tags": ["CVPR","Has Code","Training Techniques"] },
{"key": "tavakoli2017paying", "citations": "78", "year": "2017", "title":"Paying Attention To Descriptions Generated By Image Captioning Models", "abstract": "<p>To bridge the gap between humans and machines in image understanding and\ndescribing, we need further insight into how people describe a perceived scene.\nIn this paper, we study the agreement between bottom-up saliency-based visual\nattention and object referrals in scene description constructs. We investigate\nthe properties of human-written descriptions and machine-generated ones. We\nthen propose a saliency-boosted image captioning model in order to investigate\nbenefits from low-level cues in language models. We learn that (1) humans\nmention more salient objects earlier than less salient ones in their\ndescriptions, (2) the better a captioning model performs, the better attention\nagreement it has with human descriptions, (3) the proposed saliency-boosted\nmodel, compared to its baseline form, does not improve significantly on the MS\nCOCO database, indicating explicit bottom-up boosting does not help when the\ntask is well learnt and tuned on a data, (4) a better generalization is,\nhowever, observed for the saliency-boosted model on unseen data.</p>\n", "tags": ["ICCV","Model Architecture"] },
{"key": "tay2017latent", "citations": "285", "year": "2018", "title":"Latent Relational Metric Learning Via Memory-based Attention For Collaborative Ranking", "abstract": "<p>This paper proposes a new neural architecture for collaborative ranking with\nimplicit feedback. Our model, LRML (\\textit{Latent Relational Metric Learning})\nis a novel metric learning approach for recommendation. More specifically,\ninstead of simple push-pull mechanisms between user and item pairs, we propose\nto learn latent relations that describe each user item interaction. This helps\nto alleviate the potential geometric inflexibility of existing metric learing\napproaches. This enables not only better performance but also a greater extent\nof modeling capability, allowing our model to scale to a larger number of\ninteractions. In order to do so, we employ a augmented memory module and learn\nto attend over these memory blocks to construct latent relations. The\nmemory-based attention module is controlled by the user-item interaction,\nmaking the learned relation vector specific to each user-item pair. Hence, this\ncan be interpreted as learning an exclusive and optimal relational translation\nfor each user-item interaction. The proposed architecture demonstrates the\nstate-of-the-art performance across multiple recommendation benchmarks. LRML\noutperforms other metric learning models by \\(6%-7.5%\\) in terms of Hits@10 and\nnDCG@10 on large datasets such as Netflix and MovieLens20M. Moreover,\nqualitative studies also demonstrate evidence that our proposed model is able\nto infer and encode explicit sentiment, temporal and attribute information\ndespite being only trained on implicit feedback. As such, this ascertains the\nability of LRML to uncover hidden relational structure within implicit\ndatasets.</p>\n", "tags": ["Datasets","Memory & Context","Model Architecture","Reinforcement Learning"] },
{"key": "tay2017learning", "citations": "119", "year": "2017", "title":"Learning To Rank Question Answer Pairs With Holographic Dual LSTM Architecture", "abstract": "<p>We describe a new deep learning architecture for learning to rank question\nanswer pairs. Our approach extends the long short-term memory (LSTM) network\nwith holographic composition to model the relationship between question and\nanswer representations. As opposed to the neural tensor layer that has been\nadopted recently, the holographic composition provides the benefits of scalable\nand rich representational learning approach without incurring huge parameter\ncosts. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified\narchitecture for both deep sentence modeling and semantic matching.\nEssentially, our model is trained end-to-end whereby the parameters of the LSTM\nare optimized in a way that best explains the correlation between question and\nanswer representations. In addition, our proposed deep learning architecture\nrequires no extensive feature engineering. Via extensive experiments, we show\nthat HD-LSTM outperforms many other neural architectures on two popular\nbenchmark QA datasets. Empirical studies confirm the effectiveness of\nholographic composition over the neural tensor layer.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","SIGIR"] },
{"key": "tay2017skipflow", "citations": "105", "year": "2018", "title":"Skipflow: Incorporating Neural Coherence Features For End-to-end Automatic Text Scoring", "abstract": "<p>Deep learning has demonstrated tremendous potential for Automatic Text\nScoring (ATS) tasks. In this paper, we describe a new neural architecture that\nenhances vanilla neural network models with auxiliary neural coherence\nfeatures. Our new method proposes a new \\textsc{SkipFlow} mechanism that models\nrelationships between snapshots of the hidden representations of a long\nshort-term memory (LSTM) network as it reads. Subsequently, the semantic\nrelationships between multiple snapshots are used as auxiliary features for\nprediction. This has two main benefits. Firstly, essays are typically long\nsequences and therefore the memorization capability of the LSTM network may be\ninsufficient. Implicit access to multiple snapshots can alleviate this problem\nby acting as a protection against vanishing gradients. The parameters of the\n\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,\nmodeling relationships between multiple positions allows our model to learn\nfeatures that represent and approximate textual coherence. In our model, we\ncall this \\textit{neural coherence} features. Overall, we present a unified\ndeep learning architecture that generates neural coherence features as it reads\nin an end-to-end fashion. Our approach demonstrates state-of-the-art\nperformance on the benchmark ASAP dataset, outperforming not only feature\nengineering baselines but also other deep learning models.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture"] },
{"key": "tay2019simple", "citations": "91", "year": "2019", "title":"Simple And Effective Curriculum Pointer-generator Networks For Reading Comprehension Over Long Narratives", "abstract": "<p>This paper tackles the problem of reading comprehension over long narratives\nwhere documents easily span over thousands of tokens. We propose a curriculum\nlearning (CL) based Pointer-Generator framework for reading/sampling over large\ndocuments, enabling diverse training of the neural model based on the notion of\nalternating contextual difficulty. This can be interpreted as a form of domain\nrandomization and/or generative pretraining during training. To this end, the\nusage of the Pointer-Generator softens the requirement of having the answer\nwithin the context, enabling us to construct diverse training samples for\nlearning. Additionally, we propose a new Introspective Alignment Layer (IAL),\nwhich reasons over decomposed alignments using block-based self-attention. We\nevaluate our proposed method on the NarrativeQA reading comprehension\nbenchmark, achieving state-of-the-art performance, improving existing baselines\nby \\(51%\\) relative improvement on BLEU-4 and \\(17%\\) relative improvement on\nRouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and\nCL components.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "tay2020long", "citations": "180", "year": "2020", "title":"Long Range Arena: A Benchmark For Efficient Transformers", "abstract": "<p>Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from \\(1K\\) to \\(16K\\) tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Tools"] },
{"key": "tay2020synthesizer", "citations": "192", "year": "2020", "title":"Synthesizer: Rethinking Self-attention In Transformer Models", "abstract": "<p>The dot product self-attention is known to be central and indispensable to\nstate-of-the-art Transformer models. But is it really required? This paper\ninvestigates the true importance and contribution of the dot product-based\nself-attention mechanism on the performance of Transformer models. Via\nextensive experiments, we find that (1) random alignment matrices surprisingly\nperform quite competitively and (2) learning attention weights from token-token\n(query-key) interactions is useful but not that important after all. To this\nend, we propose \\textsc{Synthesizer}, a model that learns synthetic attention\nweights without token-token interactions. In our experiments, we first show\nthat simple Synthesizers achieve highly competitive performance when compared\nagainst vanilla Transformer models across a range of tasks, including machine\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\nWhen composed with dot product attention, we find that Synthesizers\nconsistently outperform Transformers. Moreover, we conduct additional\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\nRandom Synthesizer is not only \\(60%\\) faster but also improves perplexity by a\nrelative \\(3.5%\\). Finally, we show that simple factorized Synthesizers can\noutperform Linformers on encoding only tasks.</p>\n", "tags": ["Model Architecture"] },
{"key": "tay2021charformer", "citations": "70", "year": "2021", "title":"Charformer: Fast Character Transformers Via Gradient-based Subword Tokenization", "abstract": "<p>State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "tay2022ul2", "citations": "82", "year": "2022", "title":"UL2: Unifying Language Learning Paradigms", "abstract": "<p>Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives – two concepts that are commonly conflated. Next, we\npresent a generalized &amp; unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 &amp; GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B &amp; Flan-UL2 20B.</p>\n", "tags": ["Datasets","Evaluation Frameworks","Fine-Tuning","In Context Learning","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "teney2016graph", "citations": "461", "year": "2017", "title":"Graph-structured Representations For Visual Question Answering", "abstract": "<p>This paper proposes to improve visual question answering (VQA) with\nstructured representations of both scene contents and questions. A key\nchallenge in VQA is to require joint reasoning over the visual and text\ndomains. The predominant CNN/LSTM-based approach to VQA is limited by\nmonolithic vector representations that largely ignore structure in the scene\nand in the form of the question. CNN feature vectors cannot effectively capture\nsituations as simple as multiple object instances, and LSTMs process questions\nas series of words, which does not reflect the true complexity of language\nstructure. We instead propose to build graphs over the scene objects and over\nthe question words, and we describe a deep neural network that exploits the\nstructure in these representations. This shows significant benefit over the\nsequential processing of LSTMs. The overall efficacy of our approach is\ndemonstrated by significant improvements over the state-of-the-art, from 71.2%\nto 74.4% in accuracy on the “abstract scenes” multiple-choice benchmark, and\nfrom 34.7% to 39.1% in accuracy over pairs of “balanced” scenes, i.e. images\nwith fine-grained differences and opposite yes/no answers to a same question.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture"] },
{"key": "teney2016zero", "citations": "67", "year": "2016", "title":"Zero-shot Visual Question Answering", "abstract": "<p>Part of the appeal of Visual Question Answering (VQA) is its promise to\nanswer new questions about previously unseen images. Most current methods\ndemand training questions that illustrate every possible concept, and will\ntherefore never achieve this capability, since the volume of required training\ndata would be prohibitive. Answering general questions about images requires\nmethods capable of Zero-Shot VQA, that is, methods able to answer questions\nbeyond the scope of the training questions. We propose a new evaluation\nprotocol for VQA methods which measures their ability to perform Zero-Shot VQA,\nand in doing so highlights significant practical deficiencies of current\napproaches, some of which are masked by the biases in current datasets. We\npropose and evaluate several strategies for achieving Zero-Shot VQA, including\nmethods based on pretrained word embeddings, object classifiers with semantic\nembeddings, and test-time retrieval of example images. Our extensive\nexperiments are intended to serve as baselines for Zero-Shot VQA, and they also\nachieve state-of-the-art performance in the standard VQA evaluation setting.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "teney2017tips", "citations": "407", "year": "2018", "title":"Tips And Tricks For Visual Question Answering: Learnings From The 2017 Challenge", "abstract": "<p>This paper presents a state-of-the-art model for visual question answering\n(VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of\nsignificant importance for research in artificial intelligence, given its\nmultimodal nature, clear evaluation protocol, and potential real-world\napplications. The performance of deep neural networks for VQA is very dependent\non choices of architectures and hyperparameters. To help further research in\nthe area, we describe in detail our high-performing, though relatively simple\nmodel. Through a massive exploration of architectures and hyperparameters\nrepresenting more than 3,000 GPU-hours, we identified tips and tricks that lead\nto its success, namely: sigmoid outputs, soft training targets, image features\nfrom bottom-up attention, gated tanh activations, output embeddings initialized\nusing GloVe and Google Images, large mini-batches, and smart shuffling of\ntraining data. We provide a detailed analysis of their impact on performance to\nassist others in making an appropriate selection.</p>\n", "tags": ["Applications","CVPR","Evaluation","Model Architecture","Training Techniques"] },
{"key": "tenney2019bert", "citations": "1258", "year": "2019", "title":"BERT Rediscovers The Classical NLP Pipeline", "abstract": "<p>Pre-trained text encoders have rapidly advanced the state of the art on many\nNLP tasks. We focus on one such model, BERT, and aim to quantify where\nlinguistic information is captured within the network. We find that the model\nrepresents the steps of the traditional NLP pipeline in an interpretable and\nlocalizable way, and that the regions responsible for each step appear in the\nexpected sequence: POS tagging, parsing, NER, semantic roles, then coreference.\nQualitative analysis reveals that the model can and often does adjust this\npipeline dynamically, revising lower-level decisions on the basis of\ndisambiguating information from higher-level representations.</p>\n", "tags": ["Model Architecture"] },
{"key": "tenney2019what", "citations": "439", "year": "2019", "title":"What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Representations", "abstract": "<p>Contextualized representation models such as ELMo (Peters et al., 2018a) and\nBERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a\ndiverse array of downstream NLP tasks. Building on recent token-level probing\nwork, we introduce a novel edge probing task design and construct a broad suite\nof sub-sentence tasks derived from the traditional structured NLP pipeline. We\nprobe word-level contextual representations from four recent models and\ninvestigate how they encode sentence structure across a range of syntactic,\nsemantic, local, and long-range phenomena. We find that existing models trained\non language modeling and translation produce strong representations for\nsyntactic phenomena, but only offer comparably small improvements on semantic\ntasks over a non-contextual baseline.</p>\n", "tags": ["In Context Learning","Model Architecture"] },
{"key": "tetko2020state", "citations": "332", "year": "2020", "title":"State-of-the-art Augmented NLP Transformer Models For Direct And Single-step Retrosynthesis", "abstract": "<p>We investigated the effect of different training scenarios on predicting the\n(retro)synthesis of chemical compounds using a text-like representation of\nchemical reactions (SMILES) and Natural Language Processing neural network\nTransformer architecture. We showed that data augmentation, which is a powerful\nmethod used in image processing, eliminated the effect of data memorization by\nneural networks, and improved their performance for the prediction of new\nsequences. This effect was observed when augmentation was used simultaneously\nfor input and the target data simultaneously. The top-5 accuracy was 84.8% for\nthe prediction of the largest fragment (thus identifying principal\ntransformation for classical retro-synthesis) for the USPTO-50k test dataset\nand was achieved by a combination of SMILES augmentation and a beam search\nalgorithm. The same approach provided significantly better results for the\nprediction of direct reactions from the single-step USPTO-MIT test set. Our\nmodel achieved 90.6% top-1 and 96.1% top-5 accuracy for its challenging mixed\nset and 97% top-5 accuracy for the USPTO-MIT separated set. It also\nsignificantly improved results for USPTO-full set single-step retrosynthesis\nfor both top-1 and top-10 accuracies. The appearance frequency of the most\nabundantly generated SMILES was well correlated with the prediction outcome and\ncan be used as a measure of the quality of reaction prediction.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "tevet2022motionclip", "citations": "155", "year": "2022", "title":"Motionclip: Exposing Human Motion Generation To CLIP Space", "abstract": "<p>We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent\nembedding that is disentangled, well behaved, and supports highly semantic\ntextual descriptions. MotionCLIP gains its unique power by aligning its latent\nspace with that of the Contrastive Language-Image Pre-training (CLIP) model.\nAligning the human motion manifold to CLIP space implicitly infuses the\nextremely rich semantic knowledge of CLIP into the manifold. In particular, it\nhelps continuity by placing semantically similar motions close to one another,\nand disentanglement, which is inherited from the CLIP-space structure.\nMotionCLIP comprises a transformer-based motion auto-encoder, trained to\nreconstruct motion while being aligned to its text label’s position in\nCLIP-space. We further leverage CLIP’s unique visual understanding and inject\nan even stronger signal through aligning motion to rendered frames in a\nself-supervised manner. We show that although CLIP has never seen the motion\ndomain, MotionCLIP offers unprecedented text-to-motion abilities, allowing\nout-of-domain actions, disentangled editing, and abstract language\nspecification. For example, the text prompt “couch” is decoded into a sitting\ndown motion, due to lingual similarity, and the prompt “Spiderman” results in a\nweb-swinging-like solution that is far from seen during training. In addition,\nwe show how the introduced latent space can be leveraged for motion\ninterpolation, editing and recognition.</p>\n", "tags": ["Model Architecture","Prompting","Training Techniques"] },
{"key": "tewel2021zerocap", "citations": "90", "year": "2022", "title":"Zerocap: Zero-shot Image-to-text Generation For Visual-semantic Arithmetic", "abstract": "<p>Recent text-to-image matching models apply contrastive learning to large\ncorpora of uncurated pairs of images and sentences. While such models can\nprovide a powerful score for matching and subsequent zero-shot tasks, they are\nnot capable of generating caption given an image. In this work, we repurpose\nsuch models to generate a descriptive text given an image at inference time,\nwithout any further training or tuning steps. This is done by combining the\nvisual-semantic model with a large language model, benefiting from the\nknowledge in both web-scale models. The resulting captions are much less\nrestrictive than those obtained by supervised captioning methods. Moreover, as\na zero-shot learning method, it is extremely flexible and we demonstrate its\nability to perform image arithmetic in which the inputs can be either images or\ntext, and the output is a sentence. This enables novel high-level vision\ncapabilities such as comparing two images or solving visual analogy tests. Our\ncode is available at: https://github.com/YoadTew/zero-shot-image-to-text.</p>\n", "tags": ["CVPR","Has Code","Training Techniques"] },
{"key": "tewel2023key", "citations": "66", "year": "2023", "title":"Key-locked Rank One Editing For Text-to-image Personalization", "abstract": "<p>Text-to-image models (T2I) offer a new level of flexibility by allowing users\nto guide the creative process through natural language. However, personalizing\nthese models to align with user-provided visual concepts remains a challenging\nproblem. The task of T2I personalization poses multiple hard challenges, such\nas maintaining high visual fidelity while allowing creative control, combining\nmultiple personalized concepts in a single image, and keeping a small model\nsize. We present Perfusion, a T2I personalization method that addresses these\nchallenges using dynamic rank-1 updates to the underlying T2I model. Perfusion\navoids overfitting by introducing a new mechanism that “locks” new concepts’\ncross-attention Keys to their superordinate category. Additionally, we develop\na gated rank-1 approach that enables us to control the influence of a learned\nconcept during inference time and to combine multiple concepts. This allows\nruntime-efficient balancing of visual-fidelity and textual-alignment with a\nsingle 100KB trained model, which is five orders of magnitude smaller than the\ncurrent state of the art. Moreover, it can span different operating points\nacross the Pareto front without additional training. Finally, we show that\nPerfusion outperforms strong baselines in both qualitative and quantitative\nterms. Importantly, key-locking leads to novel results compared to traditional\napproaches, allowing to portray personalized object interactions in\nunprecedented ways, even in one-shot settings.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "thakur2021beir", "citations": "96", "year": "2021", "title":"BEIR: A Heterogenous Benchmark For Zero-shot Evaluation Of Information Retrieval Models", "abstract": "<p>Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Retrieval Systems","Tools"] },
{"key": "thapa2022transformer", "citations": "62", "year": "2022", "title":"Transformer-based Language Models For Software Vulnerability Detection", "abstract": "<p>The large transformer-based language models demonstrate excellent performance\nin natural language processing. By considering the transferability of the\nknowledge gained by these models in one domain to other related domains, and\nthe closeness of natural languages to high-level programming languages, such as\nC/C++, this work studies how to leverage (large) transformer-based language\nmodels in detecting software vulnerabilities and how good are these models for\nvulnerability detection tasks. In this regard, firstly, a systematic (cohesive)\nframework that details source code translation, model preparation, and\ninference is presented. Then, an empirical analysis is performed with software\nvulnerability datasets with C/C++ source codes having multiple vulnerabilities\ncorresponding to the library function call, pointer usage, array usage, and\narithmetic expression. Our empirical results demonstrate the good performance\nof the language models in vulnerability detection. Moreover, these language\nmodels have better performance metrics, such as F1-score, than the contemporary\nmodels, namely bidirectional long short-term memory and bidirectional gated\nrecurrent unit. Experimenting with the language models is always challenging\ndue to the requirement of computing resources, platforms, libraries, and\ndependencies. Thus, this paper also analyses the popular platforms to\nefficiently fine-tune these models and present recommendations while choosing\nthe platforms.</p>\n", "tags": ["Datasets","Evaluation","Llm For Code","Model Architecture","Security","Tools"] },
{"key": "thawani2021representing", "citations": "70", "year": "2021", "title":"Representing Numbers In NLP: A Survey And A Vision", "abstract": "<p>NLP systems rarely give special consideration to numbers found in text. This\nstarkly contrasts with the consensus in neuroscience that, in the brain,\nnumbers are represented differently from words. We arrange recent NLP work on\nnumeracy into a comprehensive taxonomy of tasks and methods. We break down the\nsubjective notion of numeracy into 7 subtasks, arranged along two dimensions:\ngranularity (exact vs approximate) and units (abstract vs grounded). We analyze\nthe myriad representational choices made by 18 previously published number\nencoders and decoders. We synthesize best practices for representing numbers in\ntext and articulate a vision for holistic numeracy in NLP, comprised of design\ntrade-offs and a unified evaluation.</p>\n", "tags": ["Evaluation","NAACL","Survey Paper"] },
{"key": "thomason2019improving", "citations": "61", "year": "2019", "title":"Improving Grounded Natural Language Understanding Through Human-robot Dialog", "abstract": "<p>Natural language understanding for robotics can require substantial domain-\nand platform-specific engineering. For example, for mobile robots to\npick-and-place objects in an environment to satisfy human commands, we can\nspecify the language humans use to issue such commands, and connect concept\nwords like red can to physical object properties. One way to alleviate this\nengineering for a new domain is to enable robots in human environments to adapt\ndynamically—continually learning new language constructions and perceptual\nconcepts. In this work, we present an end-to-end pipeline for translating\nnatural language commands to discrete robot actions, and use clarification\ndialogs to jointly improve language parsing and concept grounding. We train and\nevaluate this agent in a virtual setting on Amazon Mechanical Turk, and we\ntransfer the learned agent to a physical robot platform to demonstrate it in\nthe real world.</p>\n", "tags": ["Agentic","Dialogue & Multi Turn","ICRA","Tools"] },
{"key": "thomason2019vision", "citations": "127", "year": "2019", "title":"Vision-and-dialog Navigation", "abstract": "<p>Robots navigating in human environments should use language to ask for\nassistance and be able to understand human responses. To study this challenge,\nwe introduce Cooperative Vision-and-Dialog Navigation, a dataset of over 2k\nembodied, human-human dialogs situated in simulated, photorealistic home\nenvironments. The Navigator asks questions to their partner, the Oracle, who\nhas privileged access to the best next steps the Navigator should take\naccording to a shortest path planner. To train agents that search an\nenvironment for a goal location, we define the Navigation from Dialog History\ntask. An agent, given a target object and a dialog history between humans\ncooperating to find that object, must infer navigation actions towards the goal\nin unexplored environments. We establish an initial, multi-modal\nsequence-to-sequence model and demonstrate that looking farther back in the\ndialog history improves performance. Sourcecode and a live interface demo can\nbe found at https://cvdn.dev/</p>\n", "tags": ["Agentic","Datasets"] },
{"key": "thompson2020automatic", "citations": "128", "year": "2020", "title":"Automatic Machine Translation Evaluation In Many Languages Via Zero-shot Paraphrasing", "abstract": "<p>We frame the task of machine translation evaluation as one of scoring machine\ntranslation output with a sequence-to-sequence paraphraser, conditioned on a\nhuman reference. We propose training the paraphraser as a multilingual NMT\nsystem, treating paraphrasing as a zero-shot translation task (e.g., Czech to\nCzech). This results in the paraphraser’s output mode being centered around a\ncopy of the input sequence, which represents the best case scenario where the\nMT system output matches a human reference. Our method is simple and intuitive,\nand does not require human judgements for training. Our single model (trained\nin 39 languages) outperforms or statistically ties with all prior metrics on\nthe WMT 2019 segment-level shared metrics task in all languages (excluding\nGujarati where the model had no training data). We also explore using our model\nfor the task of quality estimation as a metric–conditioning on the source\ninstead of the reference–and find that it significantly outperforms every\nsubmission to the WMT 2019 shared task on quality estimation in every language\npair.</p>\n", "tags": ["EMNLP","Evaluation","Training Techniques"] },
{"key": "thrush2022winoground", "citations": "119", "year": "2022", "title":"Winoground: Probing Vision And Language Models For Visio-linguistic Compositionality", "abstract": "<p>We present a novel task and dataset for evaluating the ability of vision and\nlanguage models to conduct visio-linguistic compositional reasoning, which we\ncall Winoground. Given two images and two captions, the goal is to match them\ncorrectly - but crucially, both captions contain a completely identical set of\nwords, only in a different order. The dataset was carefully hand-curated by\nexpert annotators and is labeled with a rich set of fine-grained tags to assist\nin analyzing model performance. We probe a diverse range of state-of-the-art\nvision and language models and find that, surprisingly, none of them do much\nbetter than chance. Evidently, these models are not as skilled at\nvisio-linguistic compositional reasoning as we might have hoped. We perform an\nextensive analysis to obtain insights into how future work might try to\nmitigate these models’ shortcomings. We aim for Winoground to serve as a useful\nevaluation set for advancing the state of the art and driving further progress\nin the field. The dataset is available at\nhttps://huggingface.co/datasets/facebook/winoground.</p>\n", "tags": ["CVPR","Datasets","Evaluation"] },
{"key": "tian2023is", "citations": "92", "year": "2023", "title":"Is Chatgpt The Ultimate Programming Assistant -- How Far Is It?", "abstract": "<p>Recently, the ChatGPT LLM has received great attention: it can be used as a\nbot for discussing source code, prompting it to suggest changes, provide\ndescriptions or even generate code. Typical demonstrations generally focus on\nexisting benchmarks, which may have been used in model training (i.e., data\nleakage). To assess the feasibility of using an LLM as a useful assistant bot\nfor programmers, we must assess its realistic capabilities on unseen problems\nas well as its capabilities on various tasks. In this paper, we present an\nempirical study of ChatGPT’s potential as a fully automated programming\nassistant, focusing on the tasks of code generation, program repair, and code\nsummariziation. The study investigates ChatGPT’s performance on common\nprogramming problems and compares it with state-of-the-art approaches on two\nbenchmarks. Among several findings, our study shows that ChatGPT is effective\nin dealing with common programming problems. However, our experiments also\nreveal limitations in terms of its attention span: detailed descriptions will\nconstrain the focus of ChatGPT and prevent it from leveraging its vast\nknowledge to solve the actual problem. Surprisingly, we have identified the\nability of ChatGPT to reason the original intention of the code. We expect\nfuture work to build on this insight for dealing with the open question of the\noracle problem. Our findings contribute interesting insights to the development\nof LLMs for programming assistance, notably by demonstrating the importance of\nprompt engineering, and providing a better understanding of ChatGPT’s practical\napplications for software engineering.</p>\n", "tags": ["Applications","Llm For Code","Model Architecture","Prompting","Training Techniques"] },
{"key": "tian2023opportunities", "citations": "198", "year": "2023", "title":"Opportunities And Challenges For Chatgpt And Large Language Models In Biomedicine And Health", "abstract": "<p>ChatGPT has drawn considerable attention from both the general public and\ndomain experts with its remarkable text generation capabilities. This has\nsubsequently led to the emergence of diverse applications in the field of\nbiomedicine and health. In this work, we examine the diverse applications of\nlarge language models (LLMs), such as ChatGPT, in biomedicine and health.\nSpecifically we explore the areas of biomedical information retrieval, question\nanswering, medical text summarization, information extraction, and medical\neducation, and investigate whether LLMs possess the transformative power to\nrevolutionize these tasks or whether the distinct complexities of biomedical\ndomain presents unique challenges. Following an extensive literature survey, we\nfind that significant advances have been made in the field of text generation\ntasks, surpassing the previous state-of-the-art methods. For other\napplications, the advances have been modest. Overall, LLMs have not yet\nrevolutionized biomedicine, but recent rapid progress indicates that such\nmethods hold great potential to provide valuable means for accelerating\ndiscovery and improving health. We also find that the use of LLMs, like\nChatGPT, in the fields of biomedicine and health entails various risks and\nchallenges, including fabricated information in its generated responses, as\nwell as legal and privacy concerns associated with sensitive patient data. We\nbelieve this survey can provide a comprehensive and timely overview to\nbiomedical researchers and healthcare practitioners on the opportunities and\nchallenges associated with using ChatGPT and other LLMs for transforming\nbiomedicine and health.</p>\n", "tags": ["Applications","Model Architecture","Survey Paper"] },
{"key": "tiedemann2017neural", "citations": "247", "year": "2017", "title":"Neural Machine Translation With Extended Context", "abstract": "<p>We investigate the use of extended context in attention-based neural machine\ntranslation. We base our experiments on translated movie subtitles and discuss\nthe effect of increasing the segments beyond single translation units. We study\nthe use of extended source language context as well as bilingual context\nextensions. The models learn to distinguish between information from different\nsegments and are surprisingly robust with respect to translation quality. In\nthis pilot study, we observe interesting cross-sentential attention patterns\nthat improve textual coherence in translation at least in some selected cases.</p>\n", "tags": ["Memory & Context","Model Architecture"] },
{"key": "tiedemann2020tatoeba", "citations": "75", "year": "2020", "title":"The Tatoeba Translation Challenge -- Realistic Data Sets For Low Resource And Multilingual MT", "abstract": "<p>This paper describes the development of a new benchmark for machine\ntranslation that provides training and test data for thousands of language\npairs covering over 500 languages and tools for creating state-of-the-art\ntranslation models from that collection. The main goal is to trigger the\ndevelopment of open translation tools and models with a much broader coverage\nof the World’s languages. Using the package it is possible to work on realistic\nlow-resource scenarios avoiding artificially reduced setups that are common\nwhen demonstrating zero-shot or few-shot learning. For the first time, this\npackage provides a comprehensive collection of diverse data sets in hundreds of\nlanguages with systematic language and script annotation and data splits to\nextend the narrow coverage of existing benchmarks. Together with the data\nrelease, we also provide a growing number of pre-trained baseline models for\nindividual language pairs and selected language groups.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Training Techniques"] },
{"key": "tinghao2016visual", "citations": "198", "year": "2016", "title":"Visual Storytelling", "abstract": "<p>We introduce the first dataset for sequential vision-to-language, and explore\nhow this data may be used for the task of visual storytelling. The first\nrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211\nsequences, aligned to both descriptive (caption) and story language. We\nestablish several strong baselines for the storytelling task, and motivate an\nautomatic metric to benchmark progress. Modelling concrete description as well\nas figurative and social language, as provided in this dataset and the\nstorytelling task, has the potential to move artificial intelligence from basic\nunderstandings of typical visual scenes towards more and more human-like\nunderstanding of grounded event structure and subjective expression.</p>\n", "tags": ["Datasets","Evaluation","NAACL"] },
{"key": "tinn2021fine", "citations": "99", "year": "2023", "title":"Fine-tuning Large Neural Language Models For Biomedical Natural Language Processing", "abstract": "<p>Motivation: A perennial challenge for biomedical researchers and clinical\npractitioners is to stay abreast with the rapid growth of publications and\nmedical notes. Natural language processing (NLP) has emerged as a promising\ndirection for taming information overload. In particular, large neural language\nmodels facilitate transfer learning by pretraining on unlabeled text, as\nexemplified by the successes of BERT models in various NLP applications.\nHowever, fine-tuning such models for an end task remains challenging,\nespecially with small labeled datasets, which are common in biomedical NLP.\n  Results: We conduct a systematic study on fine-tuning stability in biomedical\nNLP. We show that finetuning performance may be sensitive to pretraining\nsettings, especially in low-resource domains. Large models have potential to\nattain better performance, but increasing model size also exacerbates\nfinetuning instability. We thus conduct a comprehensive exploration of\ntechniques for addressing fine-tuning instability. We show that these\ntechniques can substantially improve fine-tuning performance for lowresource\nbiomedical NLP applications. Specifically, freezing lower layers is helpful for\nstandard BERT-BASE models, while layerwise decay is more effective for\nBERT-LARGE and ELECTRA models. For low-resource text similarity tasks such as\nBIOSSES, reinitializing the top layer is the optimal strategy. Overall,\ndomainspecific vocabulary and pretraining facilitate more robust models for\nfine-tuning. Based on these findings, we establish new state of the art on a\nwide range of biomedical NLP applications.\n  Availability and implementation: To facilitate progress in biomedical NLP, we\nrelease our state-of-the-art pretrained and fine-tuned models:\nhttps://aka.ms/BLURB.</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "tjandra2017compressing", "citations": "87", "year": "2017", "title":"Compressing Recurrent Neural Network With Tensor Train", "abstract": "<p>Recurrent Neural Network (RNN) are a popular choice for modeling temporal and\nsequential tasks and achieve many state-of-the-art performance on various\ncomplex problems. However, most of the state-of-the-art RNNs have millions of\nparameters and require many computational resources for training and predicting\nnew data. This paper proposes an alternative RNN model to reduce the number of\nparameters significantly by representing the weight parameters based on Tensor\nTrain (TT) format. In this paper, we implement the TT-format representation for\nseveral RNN architectures such as simple RNN and Gated Recurrent Unit (GRU). We\ncompare and evaluate our proposed RNN model with uncompressed RNN model on\nsequence classification and sequence prediction tasks. Our proposed RNNs with\nTT-format are able to preserve the performance while reducing the number of RNN\nparameters significantly up to 40 times smaller.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "toledo2019automatic", "citations": "69", "year": "2019", "title":"Automatic Argument Quality Assessment -- New Datasets And Methods", "abstract": "<p>We explore the task of automatic assessment of argument quality. To that end,\nwe actively collected 6.3k arguments, more than a factor of five compared to\npreviously examined data. Each argument was explicitly and carefully annotated\nfor its quality. In addition, 14k pairs of arguments were annotated\nindependently, identifying the higher quality argument in each pair. In spite\nof the inherent subjective nature of the task, both annotation schemes led to\nsurprisingly consistent results. We release the labeled datasets to the\ncommunity. Furthermore, we suggest neural methods based on a recently released\nlanguage model, for argument ranking as well as for argument-pair\nclassification. In the former task, our results are comparable to\nstate-of-the-art; in the latter task our results significantly outperform\nearlier methods.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "tomar2017neural", "citations": "65", "year": "2017", "title":"Neural Paraphrase Identification Of Questions With Noisy Pretraining", "abstract": "<p>We present a solution to the problem of paraphrase identification of\nquestions. We focus on a recent dataset of question pairs annotated with binary\nparaphrase labels and show that a variant of the decomposable attention model\n(Parikh et al., 2016) results in accurate performance on this task, while being\nfar simpler than many competing neural architectures. Furthermore, when the\nmodel is pretrained on a noisy dataset of automatically collected question\nparaphrases, it obtains the best reported performance on the dataset.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "tonekaboni2019what", "citations": "164", "year": "2019", "title":"What Clinicians Want: Contextualizing Explainable Machine Learning For Clinical End Use", "abstract": "<p>Translating machine learning (ML) models effectively to clinical practice\nrequires establishing clinicians’ trust. Explainability, or the ability of an\nML model to justify its outcomes and assist clinicians in rationalizing the\nmodel prediction, has been generally understood to be critical to establishing\ntrust. However, the field suffers from the lack of concrete definitions for\nusable explanations in different settings. To identify specific aspects of\nexplainability that may catalyze building trust in ML models, we surveyed\nclinicians from two distinct acute care specialties (Intenstive Care Unit and\nEmergency Department). We use their feedback to characterize when\nexplainability helps to improve clinicians’ trust in ML models. We further\nidentify the classes of explanations that clinicians identified as most\nrelevant and crucial for effective translation to clinical practice. Finally,\nwe discern concrete metrics for rigorous evaluation of clinical explainability\nmethods. By integrating perceptions of explainability between clinicians and ML\nresearchers we hope to facilitate the endorsement and broader adoption and\nsustained use of ML systems in healthcare.</p>\n", "tags": ["Evaluation"] },
{"key": "toneva2018empirical", "citations": "198", "year": "2018", "title":"An Empirical Study Of Example Forgetting During Deep Neural Network Learning", "abstract": "<p>Inspired by the phenomenon of catastrophic forgetting, we investigate the\nlearning dynamics of neural networks as they train on single classification\ntasks. Our goal is to understand whether a related phenomenon occurs when data\ndoes not undergo a clear distributional shift. We define a `forgetting event’\nto have occurred when an individual training example transitions from being\nclassified correctly to incorrectly over the course of learning. Across several\nbenchmark data sets, we find that: (i) certain examples are forgotten with high\nfrequency, and some not at all; (ii) a data set’s (un)forgettable examples\ngeneralize across neural architectures; and (iii) based on forgetting dynamics,\na significant fraction of examples can be omitted from the training data set\nwhile still maintaining state-of-the-art generalization performance.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "toneva2019interpreting", "citations": "82", "year": "2019", "title":"Interpreting And Improving Natural-language Processing (in Machines) With Natural Language-processing (in The Brain)", "abstract": "<p>Neural networks models for NLP are typically implemented without the explicit\nencoding of language rules and yet they are able to break one performance\nrecord after another. This has generated a lot of research interest in\ninterpreting the representations learned by these networks. We propose here a\nnovel interpretation approach that relies on the only processing system we have\nthat does understand language: the human brain. We use brain imaging recordings\nof subjects reading complex natural text to interpret word and sequence\nembeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We\nstudy how their representations differ across layer depth, context length, and\nattention type. Our results reveal differences in the context-related\nrepresentations across these models. Further, in the transformer models, we\nfind an interaction between layer depth and context length, and between layer\ndepth and attention type. We finally hypothesize that altering BERT to better\nalign with brain recordings would enable it to also better understand language.\nProbing the altered BERT using syntactic NLP tasks reveals that the model with\nincreased brain-alignment outperforms the original model. Cognitive\nneuroscientists have already begun using NLP networks to study the brain, and\nthis work closes the loop to allow the interaction between NLP and cognitive\nneuroscience to be a true cross-pollination.</p>\n", "tags": ["Memory & Context","Model Architecture"] },
{"key": "torabi2016learning", "citations": "79", "year": "2016", "title":"Learning Language-visual Embedding For Movie Understanding With Natural-language", "abstract": "<p>Learning a joint language-visual embedding has a number of very appealing\nproperties and can result in variety of practical application, including\nnatural language image/video annotation and search. In this work, we study\nthree different joint language-visual neural network model architectures. We\nevaluate our models on large scale LSMDC16 movie dataset for two tasks: 1)\nStandard Ranking for video annotation and retrieval 2) Our proposed movie\nmultiple-choice test. This test facilitate automatic evaluation of\nvisual-language models for natural language video annotation based on human\nactivities. In addition to original Audio Description (AD) captions, provided\nas part of LSMDC16, we collected and will make available a) manually generated\nre-phrasings of those captions obtained using Amazon MTurk b) automatically\ngenerated human activity elements in “Predicate + Object” (PO) phrases based on\n“Knowlywood”, an activity knowledge mining model. Our best model archives\nRecall@10 of 19.2% on annotation and 18.9% on video retrieval tasks for subset\nof 1000 samples. For multiple-choice test, our best model achieve accuracy\n58.11% over whole LSMDC16 public test-set.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "toral2017multifaceted", "citations": "115", "year": "2017", "title":"A Multifaceted Evaluation Of Neural Versus Phrase-based Machine Translation For 9 Language Directions", "abstract": "<p>We aim to shed light on the strengths and weaknesses of the newly introduced\nneural machine translation paradigm. To that end, we conduct a multifaceted\nevaluation in which we compare outputs produced by state-of-the-art neural\nmachine translation and phrase-based machine translation systems for 9 language\ndirections across a number of dimensions. Specifically, we measure the\nsimilarity of the outputs, their fluency and amount of reordering, the effect\nof sentence length and performance across different error categories. We find\nout that translations produced by neural machine translation systems are\nconsiderably different, more fluent and more accurate in terms of word order\ncompared to those produced by phrase-based systems. Neural machine translation\nsystems are also more accurate at producing inflected forms, but they perform\npoorly when translating very long sentences.</p>\n", "tags": ["Evaluation","NAACL"] },
{"key": "toral2018attaining", "citations": "203", "year": "2018", "title":"Attaining The Unattainable? Reassessing Claims Of Human Parity In Neural Machine Translation", "abstract": "<p>We reassess a recent study (Hassan et al., 2018) that claimed that machine\ntranslation (MT) has reached human parity for the translation of news from\nChinese into English, using pairwise ranking and considering three variables\nthat were not taken into account in that previous study: the language in which\nthe source side of the test set was originally written, the translation\nproficiency of the evaluators, and the provision of inter-sentential context.\nIf we consider only original source text (i.e. not translated from another\nlanguage, or translationese), then we find evidence showing that human parity\nhas not been achieved. We compare the judgments of professional translators\nagainst those of non-experts and discover that those of the experts result in\nhigher inter-annotator agreement and better discrimination between human and\nmachine translations. In addition, we analyse the human translations of the\ntest set and identify important translation issues. Finally, based on these\nfindings, we provide a set of recommendations for future human evaluations of\nMT.</p>\n", "tags": ["Evaluation"] },
{"key": "toral2018what", "citations": "74", "year": "2018", "title":"What Level Of Quality Can Neural Machine Translation Attain On Literary Text?", "abstract": "<p>Given the rise of a new approach to MT, Neural MT (NMT), and its promising\nperformance on different text types, we assess the translation quality it can\nattain on what is perceived to be the greatest challenge for MT: literary text.\nSpecifically, we target novels, arguably the most popular type of literary\ntext. We build a literary-adapted NMT system for the English-to-Catalan\ntranslation direction and evaluate it against a system pertaining to the\nprevious dominant paradigm in MT: statistical phrase-based MT (PBSMT). To this\nend, for the first time we train MT systems, both NMT and PBSMT, on large\namounts of literary text (over 100 million words) and evaluate them on a set of\ntwelve widely known novels spanning from the the 1920s to the present day.\nAccording to the BLEU automatic evaluation metric, NMT is significantly better\nthan PBSMT (p &lt; 0.01) on all the novels considered. Overall, NMT results in a\n11% relative improvement (3 points absolute) over PBSMT. A complementary human\nevaluation on three of the books shows that between 17% and 34% of the\ntranslations, depending on the book, produced by NMT (versus 8% and 20% with\nPBSMT) are perceived by native speakers of the target language to be of\nequivalent quality to translations produced by a professional human translator.</p>\n", "tags": ["Applications","Evaluation"] },
{"key": "touvron2023llama", "citations": "2811", "year": "2023", "title":"Llama 2: Open Foundation And Fine-tuned Chat Models", "abstract": "<p>In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.</p>\n", "tags": ["Applications","Fine-Tuning","Training Techniques"] },
{"key": "tran2016recurrent", "citations": "78", "year": "2016", "title":"Recurrent Memory Networks For Language Modeling", "abstract": "<p>Recurrent Neural Networks (RNN) have obtained excellent result in many\nnatural language processing (NLP) tasks. However, understanding and\ninterpreting the source of this success remains a challenge. In this paper, we\npropose Recurrent Memory Network (RMN), a novel RNN architecture, that not only\namplifies the power of RNN but also facilitates our understanding of its\ninternal functioning and allows us to discover underlying patterns in data. We\ndemonstrate the power of RMN on language modeling and sentence completion\ntasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)\nnetwork on three large German, Italian, and English dataset. Additionally we\nperform in-depth analysis of various linguistic dimensions that RMN captures.\nOn Sentence Completion Challenge, for which it is essential to capture sentence\ncoherence, our RMN obtains 69.2% accuracy, surpassing the previous\nstate-of-the-art by a large margin.</p>\n", "tags": ["Datasets","Model Architecture","NAACL"] },
{"key": "tran2018regularizing", "citations": "63", "year": "2018", "title":"Regularizing Matrix Factorization With User And Item Embeddings For Recommendation", "abstract": "<p>Following recent successes in exploiting both latent factor and word\nembedding models in recommendation, we propose a novel Regularized\nMulti-Embedding (RME) based recommendation model that simultaneously\nencapsulates the following ideas via decomposition: (1) which items a user\nlikes, (2) which two users co-like the same items, (3) which two items users\noften co-liked, and (4) which two items users often co-disliked. In\nexperimental validation, the RME outperforms competing state-of-the-art models\nin both explicit and implicit feedback datasets, significantly improving\nRecall@5 by 5.9~7.0%, NDCG@20 by 4.3~5.6%, and MAP@10 by 7.9~8.9%. In addition,\nunder the cold-start scenario for users with the lowest number of interactions,\nagainst the competing models, the RME outperforms NDCG@5 by 20.2% and 29.4% in\nMovieLens-10M and MovieLens-20M datasets, respectively. Our datasets and source\ncode are available at: https://github.com/thanhdtran/RME.git.</p>\n", "tags": ["CIKM","Has Code"] },
{"key": "tran2020transform", "citations": "75", "year": "2020", "title":"Transform And Tell: Entity-aware News Image Captioning", "abstract": "<p>We propose an end-to-end model which generates captions for images embedded\nin news articles. News images present two key challenges: they rely on\nreal-world knowledge, especially about named entities; and they typically have\nlinguistically rich captions that include uncommon words. We address the first\nchallenge by associating words in the caption with faces and objects in the\nimage, via a multi-modal, multi-head attention mechanism. We tackle the second\nchallenge with a state-of-the-art transformer language model that uses\nbyte-pair-encoding to generate captions as a sequence of word parts. On the\nGoodNews dataset, our model outperforms the previous state of the art by a\nfactor of four in CIDEr score (13 to 54). This performance gain comes from a\nunique combination of language models, word representation, image embeddings,\nface embeddings, object embeddings, and improvements in neural network design.\nWe also introduce the NYTimes800k dataset which is 70% larger than GoodNews,\nhas higher article quality, and includes the locations of images within\narticles as an additional contextual cue.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "trinh2018simple", "citations": "341", "year": "2018", "title":"A Simple Method For Commonsense Reasoning", "abstract": "<p>Commonsense reasoning is a long-standing challenge for deep learning. For\nexample, it is difficult to use neural networks to tackle the Winograd Schema\ndataset (Levesque et al., 2011). In this paper, we present a simple method for\ncommonsense reasoning with neural networks, using unsupervised learning. Key to\nour method is the use of language models, trained on a massive amount of\nunlabled data, to score multiple choice questions posed by commonsense\nreasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges,\nour models outperform previous state-of-the-art methods by a large margin,\nwithout using expensive annotated knowledge bases or hand-engineered features.\nWe train an array of large RNN language models that operate at word or\ncharacter level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a\ncustomized corpus for this task and show that diversity of training data plays\nan important role in test performance. Further analysis also shows that our\nsystem successfully discovers important features of the context that decide the\ncorrect answer, indicating a good grasp of commonsense knowledge.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "trischler2016natural", "citations": "68", "year": "2016", "title":"Natural Language Comprehension With The Epireader", "abstract": "<p>We present the EpiReader, a novel model for machine comprehension of text.\nMachine comprehension of unstructured, real-world text is a major research goal\nfor natural language processing. Current tests of machine comprehension pose\nquestions whose answers can be inferred from some supporting text, and evaluate\na model’s response to the questions. The EpiReader is an end-to-end neural\nmodel comprising two components: the first component proposes a small set of\ncandidate answers after comparing a question to its supporting text, and the\nsecond component formulates hypotheses using the proposed candidates and the\nquestion, then reranks the hypotheses based on their estimated concordance with\nthe supporting text. We present experiments demonstrating that the EpiReader\nsets a new state-of-the-art on the CNN and Children’s Book Test machine\ncomprehension benchmarks, outperforming previous neural models by a significant\nmargin.</p>\n", "tags": ["EMNLP"] },
{"key": "trischler2016newsqa", "citations": "785", "year": "2017", "title":"Newsqa: A Machine Comprehension Dataset", "abstract": "<p>We present NewsQA, a challenging machine comprehension dataset of over\n100,000 human-generated question-answer pairs. Crowdworkers supply questions\nand answers based on a set of over 10,000 news articles from CNN, with answers\nconsisting of spans of text from the corresponding articles. We collect this\ndataset through a four-stage process designed to solicit exploratory questions\nthat require reasoning. A thorough analysis confirms that NewsQA demands\nabilities beyond simple word matching and recognizing textual entailment. We\nmeasure human performance on the dataset and compare it to several strong\nneural models. The performance gap between humans and machines (0.198 in F1)\nindicates that significant progress can be made on NewsQA through future\nresearch. The dataset is freely available at\nhttps://datasets.maluuba.com/NewsQA.</p>\n", "tags": ["Datasets"] },
{"key": "trivedi2021musique", "citations": "62", "year": "2022", "title":"Musique: Multihop Questions Via Single-hop Question Composition", "abstract": "<p>Multihop reasoning remains an elusive goal as existing multihop benchmarks\nare known to be largely solvable via shortcuts. Can we create a question\nanswering (QA) dataset that, by construction, <em>requires</em> proper multihop\nreasoning? To this end, we introduce a bottom-up approach that systematically\nselects composable pairs of single-hop questions that are connected, i.e.,\nwhere one reasoning step critically relies on information from another. This\nbottom-up methodology lets us explore a vast space of questions and add\nstringent filters as well as other mechanisms targeting connected reasoning. It\nprovides fine-grained control over the construction process and the properties\nof the resulting \\(k\\)-hop questions. We use this methodology to create\nMuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop questions. Relative to\nexisting datasets, MuSiQue-Ans is more difficult overall (3x increase in\nhuman-machine gap), and harder to cheat via disconnected reasoning (e.g., a\nsingle-hop model has a 30 point drop in F1). We further add unanswerable\ncontrast questions to produce a more stringent dataset, MuSiQue-Full. We hope\nour datasets will help the NLP community develop models that perform genuine\nmultihop reasoning.</p>\n", "tags": ["Datasets","TACL"] },
{"key": "trivedi2022interleaving", "citations": "77", "year": "2023", "title":"Interleaving Retrieval With Chain-of-thought Reasoning For Knowledge-intensive Multi-step Questions", "abstract": "<p>Prompting-based large language models (LLMs) are surprisingly powerful at\ngenerating natural language reasoning steps or Chains-of-Thoughts (CoT) for\nmulti-step question answering (QA). They struggle, however, when the necessary\nknowledge is either unavailable to the LLM or not up-to-date within its\nparameters. While using the question to retrieve relevant text from an external\nknowledge source helps LLMs, we observe that this one-step retrieve-and-read\napproach is insufficient for multi-step QA. Here, \\textit{what to retrieve}\ndepends on \\textit{what has already been derived}, which in turn may depend on\n\\textit{what was previously retrieved}. To address this, we propose IRCoT, a\nnew approach for multi-step QA that interleaves retrieval with steps\n(sentences) in a CoT, guiding the retrieval with CoT and in turn using\nretrieved results to improve CoT. Using IRCoT with GPT3 substantially improves\nretrieval (up to 21 points) as well as downstream QA (up to 15 points) on four\ndatasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar\nsubstantial gains in out-of-distribution (OOD) settings as well as with much\nsmaller models such as Flan-T5-large without additional training. IRCoT reduces\nmodel hallucination, resulting in factually more accurate CoT reasoning. Code,\ndata, and prompts are available at https://github.com/stonybrooknlp/ircot</p>\n", "tags": ["Datasets","Has Code","Prompting","Training Techniques"] },
{"key": "trott2022do", "citations": "61", "year": "2023", "title":"Do Large Language Models Know What Humans Know?", "abstract": "<p>Humans can attribute beliefs to others. However, it is unknown to what extent\nthis ability results from an innate biological endowment or from experience\naccrued through child development, particularly exposure to language describing\nothers’ mental states. We test the viability of the language exposure\nhypothesis by assessing whether models exposed to large quantities of human\nlanguage display sensitivity to the implied knowledge states of characters in\nwritten passages. In pre-registered analyses, we present a linguistic version\nof the False Belief Task to both human participants and a Large Language Model,\nGPT-3. Both are sensitive to others’ beliefs, but while the language model\nsignificantly exceeds chance behavior, it does not perform as well as the\nhumans, nor does it explain the full extent of their behavior – despite being\nexposed to more language than a human would in a lifetime. This suggests that\nwhile statistical learning from language exposure may in part explain how\nhumans develop the ability to reason about the mental states of others, other\nmechanisms are also responsible.</p>\n", "tags": ["Model Architecture"] },
{"key": "tsai2018learning", "citations": "191", "year": "2018", "title":"Learning Factorized Multimodal Representations", "abstract": "<p>Learning multimodal representations is a fundamentally complex research\nproblem due to the presence of multiple heterogeneous sources of information.\nAlthough the presence of multiple modalities provides additional valuable\ninformation, there are two key challenges to address when learning from\nmultimodal data: 1) models must learn the complex intra-modal and cross-modal\ninteractions for prediction and 2) models must be robust to unexpected missing\nor noisy modalities during testing. In this paper, we propose to optimize for a\njoint generative-discriminative objective across multimodal data and labels. We\nintroduce a model that factorizes representations into two sets of independent\nfactors: multimodal discriminative and modality-specific generative factors.\nMultimodal discriminative factors are shared across all modalities and contain\njoint multimodal features required for discriminative tasks such as sentiment\nprediction. Modality-specific generative factors are unique for each modality\nand contain the information required for generating data. Experimental results\nshow that our model is able to learn meaningful multimodal representations that\nachieve state-of-the-art or competitive performance on six multimodal datasets.\nOur model demonstrates flexible generative capabilities by conditioning on\nindependent factors and can reconstruct missing modalities without\nsignificantly impacting performance. Lastly, we interpret our factorized\nrepresentations to understand the interactions that influence multimodal\nlearning.</p>\n", "tags": ["Datasets"] },
{"key": "tsai2019multimodal", "citations": "1063", "year": "2019", "title":"Multimodal Transformer For Unaligned Multimodal Language Sequences", "abstract": "<p>Human language is often multimodal, which comprehends a mixture of natural\nlanguage, facial gestures, and acoustic behaviors. However, two major\nchallenges in modeling such multimodal human language time-series data exist:\n1) inherent data non-alignment due to variable sampling rates for the sequences\nfrom each modality; and 2) long-range dependencies between elements across\nmodalities. In this paper, we introduce the Multimodal Transformer (MulT) to\ngenerically address the above issues in an end-to-end manner without explicitly\naligning the data. At the heart of our model is the directional pairwise\ncrossmodal attention, which attends to interactions between multimodal\nsequences across distinct time steps and latently adapt streams from one\nmodality to another. Comprehensive experiments on both aligned and non-aligned\nmultimodal time-series show that our model outperforms state-of-the-art methods\nby a large margin. In addition, empirical analysis suggests that correlated\ncrossmodal signals are able to be captured by the proposed crossmodal attention\nmechanism in MulT.</p>\n", "tags": ["Model Architecture"] },
{"key": "tsimpoukelli2021multimodal", "citations": "248", "year": "2021", "title":"Multimodal Few-shot Learning With Frozen Language Models", "abstract": "<p>When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.</p>\n", "tags": ["Few-Shot"] },
{"key": "tu2016context", "citations": "113", "year": "2017", "title":"Context Gates For Neural Machine Translation", "abstract": "<p>In neural machine translation (NMT), generation of a target word depends on\nboth source and target contexts. We find that source contexts have a direct\nimpact on the adequacy of a translation while target contexts affect the\nfluency. Intuitively, generation of a content word should rely more on the\nsource context and generation of a functional word should rely more on the\ntarget context. Due to the lack of effective control over the influence from\nsource and target contexts, conventional NMT tends to yield fluent but\ninadequate translations. To address this problem, we propose context gates\nwhich dynamically control the ratios at which source and target contexts\ncontribute to the generation of target words. In this way, we can enhance both\nthe adequacy and fluency of NMT with more careful control of the information\nflow from contexts. Experiments show that our approach significantly improves\nupon a standard attention-based NMT system by +2.3 BLEU points.</p>\n", "tags": ["Model Architecture","TACL"] },
{"key": "tu2016neural", "citations": "188", "year": "2017", "title":"Neural Machine Translation With Reconstruction", "abstract": "<p>Although end-to-end Neural Machine Translation (NMT) has achieved remarkable\nprogress in the past two years, it suffers from a major drawback: translations\ngenerated by NMT systems often lack of adequacy. It has been widely observed\nthat NMT tends to repeatedly translate some source words while mistakenly\nignoring other words. To alleviate this problem, we propose a novel\nencoder-decoder-reconstructor framework for NMT. The reconstructor,\nincorporated into the NMT model, manages to reconstruct the input source\nsentence from the hidden layer of the output target sentence, to ensure that\nthe information in the source side is transformed to the target side as much as\npossible. Experiments show that the proposed framework significantly improves\nthe adequacy of NMT output and achieves superior translation result over\nstate-of-the-art NMT and statistical MT systems.</p>\n", "tags": ["AAAI","Tools"] },
{"key": "tu2019multi", "citations": "147", "year": "2019", "title":"Multi-hop Reading Comprehension Across Multiple Documents By Reasoning Over Heterogeneous Graphs", "abstract": "<p>Multi-hop reading comprehension (RC) across documents poses new challenge\nover single-document RC because it requires reasoning over multiple documents\nto reach the final answer. In this paper, we propose a new model to tackle the\nmulti-hop RC problem. We introduce a heterogeneous graph with different types\nof nodes and edges, which is named as Heterogeneous Document-Entity (HDE)\ngraph. The advantage of HDE graph is that it contains different granularity\nlevels of information including candidates, documents and entities in specific\ndocument contexts. Our proposed model can do reasoning over the HDE graph with\nnodes representation initialized with co-attention and self-attention based\ncontext encoders. We employ Graph Neural Networks (GNN) based message passing\nalgorithms to accumulate evidences on the proposed HDE graph. Evaluated on the\nblind test set of the Qangaroo WikiHop data set, our HDE graph based single\nmodel delivers competitive result, and the ensemble model achieves the\nstate-of-the-art performance.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "tu2020empirical", "citations": "116", "year": "2020", "title":"An Empirical Study On Robustness To Spurious Correlations Using Pre-trained Language Models", "abstract": "<p>Recent work has shown that pre-trained language models such as BERT improve\nrobustness to spurious correlations in the dataset. Intrigued by these results,\nwe find that the key to their success is generalization from a small amount of\ncounterexamples where the spurious correlations do not hold. When such minority\nexamples are scarce, pre-trained models perform as poorly as models trained\nfrom scratch. In the case of extreme minority, we propose to use multi-task\nlearning (MTL) to improve generalization. Our experiments on natural language\ninference and paraphrase identification show that MTL with the right auxiliary\ntasks significantly improves performance on challenging examples without\nhurting the in-distribution performance. Further, we show that the gain from\nMTL mainly comes from improved generalization from the minority examples. Our\nresults highlight the importance of data diversity for overcoming spurious\ncorrelations.</p>\n", "tags": ["Datasets","Model Architecture","Security","TACL"] },
{"key": "tu2023towards", "citations": "177", "year": "2024", "title":"Towards Generalist Biomedical AI", "abstract": "<p>Medicine is inherently multimodal, with rich data modalities spanning text,\nimaging, genomics, and more. Generalist biomedical artificial intelligence (AI)\nsystems that flexibly encode, integrate, and interpret this data at scale can\npotentially enable impactful applications ranging from scientific discovery to\ncare delivery. To enable the development of these models, we first curate\nMultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses\n14 diverse tasks such as medical question answering, mammography and\ndermatology image interpretation, radiology report generation and\nsummarization, and genomic variant calling. We then introduce Med-PaLM\nMultimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI\nsystem. Med-PaLM M is a large multimodal generative model that flexibly encodes\nand interprets biomedical data including clinical language, imaging, and\ngenomics with the same set of model weights. Med-PaLM M reaches performance\ncompetitive with or exceeding the state of the art on all MultiMedBench tasks,\noften surpassing specialist models by a wide margin. We also report examples of\nzero-shot generalization to novel medical concepts and tasks, positive transfer\nlearning across tasks, and emergent zero-shot medical reasoning. To further\nprobe the capabilities and limitations of Med-PaLM M, we conduct a radiologist\nevaluation of model-generated (and human) chest X-ray reports and observe\nencouraging performance across model scales. In a side-by-side ranking on 246\nretrospective chest X-rays, clinicians express a pairwise preference for\nMed-PaLM M reports over those produced by radiologists in up to 40.50% of\ncases, suggesting potential clinical utility. While considerable work is needed\nto validate these models in real-world use cases, our results represent a\nmilestone towards the development of generalist biomedical AI systems.</p>\n", "tags": ["Applications","Datasets","Evaluation"] },
{"key": "tu2024towards", "citations": "77", "year": "2024", "title":"Towards Conversational Diagnostic AI", "abstract": "<p>At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians’ expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n  AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE’s performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.</p>\n", "tags": ["Tools"] },
{"key": "tuan2019dykgchat", "citations": "75", "year": "2019", "title":"Dykgchat: Benchmarking Dialogue Generation Grounding On Dynamic Knowledge Graphs", "abstract": "<p>Data-driven, knowledge-grounded neural conversation models are capable of\ngenerating more informative responses. However, these models have not yet\ndemonstrated that they can zero-shot adapt to updated, unseen knowledge graphs.\nThis paper proposes a new task about how to apply dynamic knowledge graphs in\nneural conversation model and presents a novel TV series conversation corpus\n(DyKgChat) for the task. Our new task and corpus aids in understanding the\ninfluence of dynamic knowledge graphs on responses generation. Also, we propose\na preliminary model that selects an output from two networks at each time step:\na sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in\norder to support dynamic knowledge graphs. To benchmark this new task and\nevaluate the capability of adaptation, we introduce several evaluation metrics\nand the experiments show that our proposed approach outperforms previous\nknowledge-grounded conversation models. The proposed corpus and model can\nmotivate the future research directions.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "tufano2019learning", "citations": "201", "year": "2019", "title":"On Learning Meaningful Code Changes Via Neural Machine Translation", "abstract": "<p>Recent years have seen the rise of Deep Learning (DL) techniques applied to\nsource code. Researchers have exploited DL to automate several development and\nmaintenance tasks, such as writing commit messages, generating comments and\ndetecting vulnerabilities among others. One of the long lasting dreams of\napplying DL to source code is the possibility to automate non-trivial coding\nactivities. While some steps in this direction have been taken (e.g., learning\nhow to fix bugs), there is still a glaring lack of empirical evidence on the\ntypes of code changes that can be learned and automatically applied by DL. Our\ngoal is to make this first important step by quantitatively and qualitatively\ninvestigating the ability of a Neural Machine Translation (NMT) model to learn\nhow to automatically apply code changes implemented by developers during pull\nrequests. We train and experiment with the NMT model on a set of 236k pairs of\ncode components before and after the implementation of the changes provided in\nthe pull requests. We show that, when applied in a narrow enough context (i.e.,\nsmall/medium-sized pairs of methods before/after the pull request changes), NMT\ncan automatically replicate the changes implemented by developers during pull\nrequests in up to 36% of the cases. Moreover, our qualitative analysis shows\nthat the model is capable of learning and replicating a wide variety of\nmeaningful code changes, especially refactorings and bug-fixing activities. Our\nresults pave the way for novel research in the area of DL on code, such as the\nautomatic learning and applications of refactoring.</p>\n", "tags": ["Applications","Llm For Code"] },
{"key": "tufano2020generating", "citations": "60", "year": "2022", "title":"Generating Accurate Assert Statements For Unit Test Cases Using Pretrained Transformers", "abstract": "<p>Unit testing represents the foundational basis of the software testing\npyramid, beneath integration and end-to-end testing. Automated software testing\nresearchers have proposed a variety of techniques to assist developers in this\ntime-consuming task. In this paper we present an approach to support developers\nin writing unit test cases by generating accurate and useful assert statements.\nOur approach is based on a state-of-the-art transformer model initially\npretrained on an English textual corpus. This semantically rich model is then\ntrained in a semi-supervised fashion on a large corpus of source code. Finally,\nwe finetune this model on the task of generating assert statements for unit\ntests. The resulting model is able to generate accurate assert statements for a\ngiven method under test. In our empirical evaluation, the model was able to\npredict the exact assert statements written by developers in 62% of the cases\nin the first attempt. The results show 80% relative improvement for top-1\naccuracy over the previous RNN-based approach in the literature. We also show\nthe substantial impact of the pretraining process on the performances of our\nmodel, as well as comparing it with assert auto-completion task. Finally, we\ndemonstrate how our approach can be used to augment EvoSuite test cases, with\nadditional asserts leading to improved test coverage.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "tufano2022using", "citations": "98", "year": "2022", "title":"Using Pre-trained Models To Boost Code Review Automation", "abstract": "<p>Code review is a practice widely adopted in open source and industrial\nprojects. Given the non-negligible cost of such a process, researchers started\ninvestigating the possibility of automating specific code review tasks. We\nrecently proposed Deep Learning (DL) models targeting the automation of two\ntasks: the first model takes as input a code submitted for review and\nimplements in it changes likely to be recommended by a reviewer; the second\ntakes as input the submitted code and a reviewer comment posted in natural\nlanguage and automatically implements the change required by the reviewer.\nWhile the preliminary results we achieved are encouraging, both models had been\ntested in rather simple code review scenarios, substantially simplifying the\ntargeted problem. This was also due to the choices we made when designing both\nthe technique and the experiments. In this paper, we build on top of that work\nby demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5)\nmodel can outperform previous DL models for automating code review tasks. Also,\nwe conducted our experiments on a larger and more realistic (and challenging)\ndataset of code review activities.</p>\n", "tags": ["Datasets","Llm For Code","Model Architecture"] },
{"key": "tumanyan2022plug", "citations": "277", "year": "2023", "title":"Plug-and-play Diffusion Features For Text-driven Image-to-image Translation", "abstract": "<p>Large-scale text-to-image generative models have been a revolutionary\nbreakthrough in the evolution of generative AI, allowing us to synthesize\ndiverse images that convey highly complex visual concepts. However, a pivotal\nchallenge in leveraging such models for real-world content creation tasks is\nproviding users with control over the generated content. In this paper, we\npresent a new framework that takes text-to-image synthesis to the realm of\nimage-to-image translation – given a guidance image and a target text prompt,\nour method harnesses the power of a pre-trained text-to-image diffusion model\nto generate a new image that complies with the target text, while preserving\nthe semantic layout of the source image. Specifically, we observe and\nempirically demonstrate that fine-grained control over the generated structure\ncan be achieved by manipulating spatial features and their self-attention\ninside the model. This results in a simple and effective approach, where\nfeatures extracted from the guidance image are directly injected into the\ngeneration process of the target image, requiring no training or fine-tuning\nand applicable for both real or generated guidance images. We demonstrate\nhigh-quality results on versatile text-guided image translation tasks,\nincluding translating sketches, rough drawings and animations into realistic\nimages, changing of the class and appearance of objects in a given image, and\nmodifications of global qualities such as lighting and color.</p>\n", "tags": ["CVPR","Fine-Tuning","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "tunstall2022efficient", "citations": "78", "year": "2022", "title":"Efficient Few-shot Learning Without Prompts", "abstract": "<p>Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and\npattern exploiting training (PET), have achieved impressive results in\nlabel-scarce settings. However, they are difficult to employ since they are\nsubject to high variability from manually crafted prompts, and typically\nrequire billion-parameter language models to achieve high accuracy. To address\nthese shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an\nefficient and prompt-free framework for few-shot fine-tuning of Sentence\nTransformers (ST). SetFit works by first fine-tuning a pretrained ST on a small\nnumber of text pairs, in a contrastive Siamese manner. The resulting model is\nthen used to generate rich text embeddings, which are used to train a\nclassification head. This simple framework requires no prompts or verbalizers,\nand achieves high accuracy with orders of magnitude less parameters than\nexisting techniques. Our experiments show that SetFit obtains comparable\nresults with PEFT and PET techniques, while being an order of magnitude faster\nto train. We also show that SetFit can be applied in multilingual settings by\nsimply switching the ST body. Our code is available at\nhttps://github.com/huggingface/setfit and our datasets at\nhttps://huggingface.co/setfit .</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "turc2019well", "citations": "438", "year": "2019", "title":"Well-read Students Learn Better: On The Importance Of Pre-training Compact Models", "abstract": "<p>Recent developments in natural language representations have been accompanied\nby large and expensive models that leverage vast amounts of general-domain text\nthrough self-supervised pre-training. Due to the cost of applying such models\nto down-stream tasks, several model compression techniques on pre-trained\nlanguage representations have been proposed (Sun et al., 2019; Sanh, 2019).\nHowever, surprisingly, the simple baseline of just pre-training and fine-tuning\ncompact models has been overlooked. In this paper, we first show that\npre-training remains important in the context of smaller architectures, and\nfine-tuning pre-trained compact models can be competitive to more elaborate\nmethods proposed in concurrent work. Starting with pre-trained compact models,\nwe then explore transferring task knowledge from large fine-tuned models\nthrough standard knowledge distillation. The resulting simple, yet effective\nand general algorithm, Pre-trained Distillation, brings further improvements.\nThrough extensive experiments, we more generally explore the interaction\nbetween pre-training and distillation under two variables that have been\nunder-studied: model size and properties of unlabeled task data. One surprising\nobservation is that they have a compound effect even when sequentially applied\non the same data. To accelerate future research, we will make our 24\npre-trained miniature BERT models publicly available.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "urbanek2019learning", "citations": "143", "year": "2019", "title":"Learning To Speak And Act In A Fantasy Text Adventure Game", "abstract": "<p>We introduce a large scale crowdsourced text adventure game as a research\nplatform for studying grounded dialogue. In it, agents can perceive, emote, and\nact whilst conducting dialogue with other agents. Models and humans can both\nact as characters within the game. We describe the results of training\nstate-of-the-art generative and retrieval models in this setting. We show that\nin addition to using past dialogue, these models are able to effectively use\nthe state of the underlying world to condition their predictions. In\nparticular, we show that grounding on the details of the local environment,\nincluding location descriptions, and the objects (and their affordances) and\ncharacters (and their previous actions) present within it allows better\npredictions of agent behavior and dialogue. We analyze the ingredients\nnecessary for successful grounding in this setting, and how each of these\nfactors relate to agents that can talk and act successfully.</p>\n", "tags": ["Agentic","EMNLP","Tools","Training Techniques"] },
{"key": "utama2020mind", "citations": "96", "year": "2020", "title":"Mind The Trade-off: Debiasing NLU Models Without Degrading The In-distribution Performance", "abstract": "<p>Models for natural language understanding (NLU) tasks often rely on the\nidiosyncratic biases of the dataset, which make them brittle against test cases\noutside the training distribution. Recently, several proposed debiasing methods\nare shown to be very effective in improving out-of-distribution performance.\nHowever, their improvements come at the expense of performance drop when models\nare evaluated on the in-distribution data, which contain examples with higher\ndiversity. This seemingly inevitable trade-off may not tell us much about the\nchanges in the reasoning and understanding capabilities of the resulting models\non broader types of examples beyond the small subset represented in the\nout-of-distribution data. In this paper, we address this trade-off by\nintroducing a novel debiasing method, called confidence regularization, which\ndiscourage models from exploiting biases while enabling them to receive enough\nincentive to learn from all the training examples. We evaluate our method on\nthree NLU tasks and show that, in contrast to its predecessors, it improves the\nperformance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset)\nwhile maintaining the original in-distribution accuracy.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "utama2020towards", "citations": "103", "year": "2020", "title":"Towards Debiasing NLU Models From Unknown Biases", "abstract": "<p>NLU models often exploit biases to achieve high dataset-specific performance\nwithout properly learning the intended task. Recently proposed debiasing\nmethods are shown to be effective in mitigating this tendency. However, these\nmethods rely on a major assumption that the types of bias should be known\na-priori, which limits their application to many NLU tasks and datasets. In\nthis work, we present the first step to bridge this gap by introducing a\nself-debiasing framework that prevents models from mainly utilizing biases\nwithout knowing them in advance. The proposed framework is general and\ncomplementary to the existing debiasing methods. We show that it allows these\nexisting methods to retain the improvement on the challenge datasets (i.e.,\nsets of examples designed to expose models’ reliance on biases) without\nspecifically targeting certain biases. Furthermore, the evaluation suggests\nthat applying the framework results in improved overall robustness.</p>\n", "tags": ["Datasets","EMNLP","Ethics & Fairness","Evaluation","Tools"] },
{"key": "vaibhav2019improving", "citations": "69", "year": "2019", "title":"Improving Robustness Of Machine Translation With Synthetic Noise", "abstract": "<p>Modern Machine Translation (MT) systems perform consistently well on clean,\nin-domain text. However most human generated text, particularly in the realm of\nsocial media, is full of typos, slang, dialect, idiolect and other noise which\ncan have a disastrous impact on the accuracy of output translation. In this\npaper we leverage the Machine Translation of Noisy Text (MTNT) dataset to\nenhance the robustness of MT systems by emulating naturally occurring noise in\notherwise clean data. Synthesizing noise in this manner we are ultimately able\nto make a vanilla MT system resilient to naturally occurring noise and\npartially mitigate loss in accuracy resulting therefrom.</p>\n", "tags": ["Datasets","Security"] },
{"key": "vakulenko2020question", "citations": "129", "year": "2021", "title":"Question Rewriting For Conversational Question Answering", "abstract": "<p>Conversational question answering (QA) requires the ability to correctly\ninterpret a question in the context of previous conversation turns. We address\nthe conversational QA task by decomposing it into question rewriting and\nquestion answering subtasks. The question rewriting (QR) subtask is\nspecifically designed to reformulate ambiguous questions, which depend on the\nconversational context, into unambiguous questions that can be correctly\ninterpreted outside of the conversational context. We introduce a\nconversational QA architecture that sets the new state of the art on the TREC\nCAsT 2019 passage retrieval dataset. Moreover, we show that the same QR model\nimproves QA performance on the QuAC dataset with respect to answer span\nextraction, which is the next step in QA after passage retrieval. Our\nevaluation results indicate that the QR model we proposed achieves near\nhuman-level performance on both datasets and the gap in performance on the\nend-to-end conversational QA task is attributed mostly to the errors in QA.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "valin2018lpcnet", "citations": "414", "year": "2019", "title":"Lpcnet: Improving Neural Speech Synthesis Through Linear Prediction", "abstract": "<p>Neural speech synthesis models have recently demonstrated the ability to\nsynthesize high quality speech for text-to-speech and compression applications.\nThese new models often require powerful GPUs to achieve real-time operation, so\nbeing able to reduce their complexity would open the way for many new\napplications. We propose LPCNet, a WaveRNN variant that combines linear\nprediction with recurrent neural networks to significantly improve the\nefficiency of speech synthesis. We demonstrate that LPCNet can achieve\nsignificantly higher quality than WaveRNN for the same network size and that\nhigh quality LPCNet speech synthesis is achievable with a complexity under 3\nGFLOPS. This makes it easier to deploy neural synthesis applications on\nlower-power devices, such as embedded systems and mobile phones.</p>\n", "tags": ["Applications","ICASSP"] },
{"key": "valle2020flowtron", "citations": "79", "year": "2020", "title":"Flowtron: An Autoregressive Flow-based Generative Network For Text-to-speech Synthesis", "abstract": "<p>In this paper we propose Flowtron: an autoregressive flow-based generative\nnetwork for text-to-speech synthesis with control over speech variation and\nstyle transfer. Flowtron borrows insights from IAF and revamps Tacotron in\norder to provide high-quality and expressive mel-spectrogram synthesis.\nFlowtron is optimized by maximizing the likelihood of the training data, which\nmakes training simple and stable. Flowtron learns an invertible mapping of data\nto a latent space that can be manipulated to control many aspects of speech\nsynthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores\n(MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech\nquality. In addition, we provide results on control of speech variation,\ninterpolation between samples and style transfer between speakers seen and\nunseen during training. Code and pre-trained models will be made publicly\navailable at https://github.com/NVIDIA/flowtron</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "vanaken2019how", "citations": "61", "year": "2019", "title":"How Does BERT Answer Questions? A Layer-wise Analysis Of Transformer Representations", "abstract": "<p>Bidirectional Encoder Representations from Transformers (BERT) reach\nstate-of-the-art results in a variety of Natural Language Processing tasks.\nHowever, understanding of their internal functioning is still insufficient and\nunsatisfactory. In order to better understand BERT and other Transformer-based\nmodels, we present a layer-wise analysis of BERT’s hidden states. Unlike\nprevious research, which mainly focuses on explaining Transformer models by\ntheir attention weights, we argue that hidden states contain equally valuable\ninformation. Specifically, our analysis focuses on models fine-tuned on the\ntask of Question Answering (QA) as an example of a complex downstream task. We\ninspect how QA models transform token vectors in order to find the correct\nanswer. To this end, we apply a set of general and QA-specific probing tasks\nthat reveal the information stored in each representation layer. Our\nqualitative analysis of hidden state visualizations provides additional\ninsights into BERT’s reasoning process. Our results show that the\ntransformations within BERT go through phases that are related to traditional\npipeline tasks. The system can therefore implicitly incorporate task-specific\ninformation into its token representations. Furthermore, our analysis reveals\nthat fine-tuning has little impact on the models’ semantic abilities and that\nprediction errors can be recognized in the vector representations of even early\nlayers.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "vanbrummelen2020teaching", "citations": "77", "year": "2021", "title":"Teaching Tech To Talk: K-12 Conversational Artificial Intelligence Literacy Curriculum And Development Tools", "abstract": "<p>With children talking to smart-speakers, smart-phones and even\nsmart-microwaves daily, it is increasingly important to educate students on how\nthese agents work-from underlying mechanisms to societal implications.\nResearchers are developing tools and curriculum to teach K-12 students broadly\nabout artificial intelligence (AI); however, few studies have evaluated these\ntools with respect to AI-specific learning outcomes, and even fewer have\naddressed student learning about AI-based conversational agents. We evaluate\nour Conversational Agent Interface for MIT App Inventor and workshop curriculum\nwith respect to eight AI competencies from the literature. Furthermore, we\nanalyze teacher (n=9) and student (n=47) feedback from workshops with the\ninterface and recommend that future work leverages design considerations from\nthe literature to optimize engagement, collaborates with teachers, and\naddresses a range of student abilities through pacing and opportunities for\nextension. We found students struggled most with the concepts of AI ethics and\nlearning, and recommend emphasizing these topics when teaching.\n  The appendix, including a demo video, can be found here:\nhttps://gist.github.com/jessvb/1cd959e32415a6ad4389761c49b54bbf</p>\n", "tags": ["AAAI","Agentic","Ethics & Fairness","Has Code","Tools"] },
{"key": "vanderwees2017dynamic", "citations": "137", "year": "2017", "title":"Dynamic Data Selection For Neural Machine Translation", "abstract": "<p>Intelligent selection of training data has proven a successful technique to\nsimultaneously increase training efficiency and translation performance for\nphrase-based machine translation (PBMT). With the recent increase in popularity\nof neural machine translation (NMT), we explore in this paper to what extent\nand how NMT can also benefit from data selection. While state-of-the-art data\nselection (Axelrod et al., 2011) consistently performs well for PBMT, we show\nthat gains are substantially lower for NMT. Next, we introduce dynamic data\nselection for NMT, a method in which we vary the selected subset of training\ndata between different training epochs. Our experiments show that the best\nresults are achieved when applying a technique we call gradual fine-tuning,\nwith improvements up to +2.6 BLEU over the original data selection approach and\nup to +3.1 BLEU over a general baseline.</p>\n", "tags": ["EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "vanhulst2020rel", "citations": "90", "year": "2020", "title":"REL: An Entity Linker Standing On The Shoulders Of Giants", "abstract": "<p>Entity linking is a standard component in modern retrieval system that is\noften performed by third-party toolkits. Despite the plethora of open source\noptions, it is difficult to find a single system that has a modular\narchitecture where certain components may be replaced, does not depend on\nexternal sources, can easily be updated to newer Wikipedia versions, and, most\nimportant of all, has state-of-the-art performance. The REL system presented in\nthis paper aims to fill that gap. Building on state-of-the-art neural\ncomponents from natural language processing research, it is provided as a\nPython package as well as a web API. We also report on an experimental\ncomparison against both well-established systems and the current\nstate-of-the-art on standard entity linking benchmarks.</p>\n", "tags": ["Model Architecture","Retrieval Systems","SIGIR","Tools"] },
{"key": "vanmassenhove2019getting", "citations": "180", "year": "2018", "title":"Getting Gender Right In Neural Machine Translation", "abstract": "<p>Speakers of different languages must attend to and encode strikingly\ndifferent aspects of the world in order to use their language correctly (Sapir,\n1921; Slobin, 1996). One such difference is related to the way gender is\nexpressed in a language. Saying “I am happy” in English, does not encode any\nadditional knowledge of the speaker that uttered the sentence. However, many\nother languages do have grammatical gender systems and so such knowledge would\nbe encoded. In order to correctly translate such a sentence into, say, French,\nthe inherent gender information needs to be retained/recovered. The same\nsentence would become either “Je suis heureux”, for a male speaker or “Je suis\nheureuse” for a female one. Apart from morphological agreement, demographic\nfactors (gender, age, etc.) also influence our use of language in terms of word\nchoices or even on the level of syntactic constructions (Tannen, 1991;\nPennebaker et al., 2003). We integrate gender information into NMT systems. Our\ncontribution is two-fold: (1) the compilation of large datasets with speaker\ninformation for 20 language pairs, and (2) a simple set of experiments that\nincorporate gender information into NMT for multiple language pairs. Our\nexperiments show that adding a gender feature to an NMT system significantly\nimproves the translation quality for some language pairs.</p>\n", "tags": ["Datasets","EMNLP","Ethics & Fairness"] },
{"key": "vanmassenhove2021machine", "citations": "61", "year": "2021", "title":"Machine Translationese: Effects Of Algorithmic Bias On Linguistic Complexity In Machine Translation", "abstract": "<p>Recent studies in the field of Machine Translation (MT) and Natural Language\nProcessing (NLP) have shown that existing models amplify biases observed in the\ntraining data. The amplification of biases in language technology has mainly\nbeen examined with respect to specific phenomena, such as gender bias. In this\nwork, we go beyond the study of gender in MT and investigate how bias\namplification might affect language in a broader sense. We hypothesize that the\n‘algorithmic bias’, i.e. an exacerbation of frequently observed patterns in\ncombination with a loss of less frequent ones, not only exacerbates societal\nbiases present in current datasets but could also lead to an artificially\nimpoverished language: ‘machine translationese’. We assess the linguistic\nrichness (on a lexical and morphological level) of translations created by\ndifferent data-driven MT paradigms - phrase-based statistical (PB-SMT) and\nneural MT (NMT). Our experiments show that there is a loss of lexical and\nmorphological richness in the translations produced by all investigated MT\nparadigms for two language pairs (EN&lt;=&gt;FR and EN&lt;=&gt;ES).</p>\n", "tags": ["Datasets","EACL","Ethics & Fairness","Training Techniques"] },
{"key": "vannguyen2021trankit", "citations": "89", "year": "2021", "title":"Trankit: A Light-weight Transformer-based Toolkit For Multilingual Natural Language Processing", "abstract": "<p>We introduce Trankit, a light-weight Transformer-based Toolkit for\nmultilingual Natural Language Processing (NLP). It provides a trainable\npipeline for fundamental NLP tasks over 100 languages, and 90 pretrained\npipelines for 56 languages. Built on a state-of-the-art pretrained language\nmodel, Trankit significantly outperforms prior multilingual NLP pipelines over\nsentence segmentation, part-of-speech tagging, morphological feature tagging,\nand dependency parsing while maintaining competitive performance for\ntokenization, multi-word token expansion, and lemmatization over 90 Universal\nDependencies treebanks. Despite the use of a large pretrained transformer, our\ntoolkit is still efficient in memory usage and speed. This is achieved by our\nnovel plug-and-play mechanism with Adapters where a multilingual pretrained\ntransformer is shared across pipelines for different languages. Our toolkit\nalong with pretrained models and code are publicly available at:\nhttps://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also\navailable at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video\nfor Trankit at: https://youtu.be/q0KGP3zGjGc.</p>\n", "tags": ["EACL","Has Code","Model Architecture","NAACL"] },
{"key": "vannoord2017neural", "citations": "85", "year": "2017", "title":"Neural Semantic Parsing By Character-based Translation: Experiments With Abstract Meaning Representations", "abstract": "<p>We evaluate the character-level translation method for neural semantic\nparsing on a large corpus of sentences annotated with Abstract Meaning\nRepresentations (AMRs). Using a sequence-to-sequence model, and some trivial\npreprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1\n(F-score on AMR-triples). We examine five different approaches to improve this\nbaseline result: (i) reordering AMR branches to match the word order of the\ninput sentence increases performance to 58.3; (ii) adding part-of-speech tags\n(automatically produced) to the input shows improvement as well (57.2); (iii)\nSo does the introduction of super characters (conflating frequent sequences of\ncharacters to a single character), reaching 57.4; (iv) optimizing the training\nprocess by using pre-training and averaging a set of models increases\nperformance to 58.7; (v) adding silver-standard training data obtained by an\noff-the-shelf parser yields the biggest improvement, resulting in an F-score of\n64.0. Combining all five techniques leads to an F-score of 71.0 on holdout\ndata, which is state-of-the-art in AMR parsing. This is remarkable because of\nthe relative simplicity of the approach.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "vanveen2023adapted", "citations": "229", "year": "2024", "title":"Adapted Large Language Models Can Outperform Medical Experts In Clinical Text Summarization", "abstract": "<p>Analyzing vast textual data and summarizing key information from electronic\nhealth records imposes a substantial burden on how clinicians allocate their\ntime. Although large language models (LLMs) have shown promise in natural\nlanguage processing (NLP), their effectiveness on a diverse range of clinical\nsummarization tasks remains unproven. In this study, we apply adaptation\nmethods to eight LLMs, spanning four distinct clinical summarization tasks:\nradiology reports, patient questions, progress notes, and doctor-patient\ndialogue. Quantitative assessments with syntactic, semantic, and conceptual NLP\nmetrics reveal trade-offs between models and adaptation methods. A clinical\nreader study with ten physicians evaluates summary completeness, correctness,\nand conciseness; in a majority of cases, summaries from our best adapted LLMs\nare either equivalent (45%) or superior (36%) compared to summaries from\nmedical experts. The ensuing safety analysis highlights challenges faced by\nboth LLMs and medical experts, as we connect errors to potential medical harm\nand categorize types of fabricated information. Our research provides evidence\nof LLMs outperforming medical experts in clinical text summarization across\nmultiple tasks. This suggests that integrating LLMs into clinical workflows\ncould alleviate documentation burden, allowing clinicians to focus more on\npatient care.</p>\n", "tags": ["Evaluation"] },
{"key": "vasconcelos2022explanations", "citations": "141", "year": "2023", "title":"Explanations Can Reduce Overreliance On AI Systems During Decision-making", "abstract": "<p>Prior work has identified a resilient phenomenon that threatens the\nperformance of human-AI decision-making teams: overreliance, when people agree\nwith an AI, even when it is incorrect. Surprisingly, overreliance does not\nreduce when the AI produces explanations for its predictions, compared to only\nproviding predictions. Some have argued that overreliance results from\ncognitive biases or uncalibrated trust, attributing overreliance to an\ninevitability of human cognition. By contrast, our paper argues that people\nstrategically choose whether or not to engage with an AI explanation,\ndemonstrating empirically that there are scenarios where AI explanations reduce\noverreliance. To achieve this, we formalize this strategic choice in a\ncost-benefit framework, where the costs and benefits of engaging with the task\nare weighed against the costs and benefits of relying on the AI. We manipulate\nthe costs and benefits in a maze task, where participants collaborate with a\nsimulated AI to find the exit of a maze. Through 5 studies (N = 731), we find\nthat costs such as task difficulty (Study 1), explanation difficulty (Study 2,\n3), and benefits such as monetary compensation (Study 4) affect overreliance.\nFinally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify\nthe utility of different explanations, providing further support for our\nframework. Our results suggest that some of the null effects found in\nliterature could be due in part to the explanation not sufficiently reducing\nthe costs of verifying the AI’s prediction.</p>\n", "tags": ["Tools"] },
{"key": "vasconcelos2023enhancing", "citations": "61", "year": "2023", "title":"Enhancing STEM Learning With Chatgpt And Bing Chat As Objects To Think With: A Case Study", "abstract": "<p>This study investigates the potential of ChatGPT and Bing Chat, advanced\nconversational AIs, as “objects-to-think-with,” resources that foster\nreflective and critical thinking, and concept comprehension in enhancing STEM\neducation, using a constructionist theoretical framework. A single-case study\nmethodology was used to analyse extensive interaction logs between students and\nboth AI systems in simulated STEM learning experiences. The results highlight\nthe ability of ChatGPT and Bing Chat to help learners develop reflective and\ncritical thinking, creativity, problem-solving skills, and concept\ncomprehension. However, integrating AIs with collaborative learning and other\neducational activities is crucial, as is addressing potential limitations like\nconcerns about AI information accuracy and reliability of the AIs’ information\nand diminished human interaction. The study concludes that ChatGPT and Bing\nChat as objects-to-think-with offer promising avenues to revolutionise STEM\neducation through a constructionist lens, fostering engagement in inclusive and\naccessible learning environments.</p>\n", "tags": ["Tools"] },
{"key": "vashishth2019attention", "citations": "98", "year": "2019", "title":"Attention Interpretability Across NLP Tasks", "abstract": "<p>The attention layer in a neural network model provides insights into the\nmodel’s reasoning behind its prediction, which are usually criticized for being\nopaque. Recently, seemingly contradictory viewpoints have emerged about the\ninterpretability of attention weights (Jain &amp; Wallace, 2019; Vig &amp; Belinkov,\n2019). Amid such confusion arises the need to understand attention mechanism\nmore systematically. In this work, we attempt to fill this gap by giving a\ncomprehensive explanation which justifies both kinds of observations (i.e.,\nwhen is attention interpretable and when it is not). Through a series of\nexperiments on diverse NLP tasks, we validate our observations and reinforce\nour claim of interpretability of attention through manual evaluation.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "vasquez2019melnet", "citations": "118", "year": "2019", "title":"Melnet: A Generative Model For Audio In The Frequency Domain", "abstract": "<p>Capturing high-level structure in audio waveforms is challenging because a\nsingle second of audio spans tens of thousands of timesteps. While long-range\ndependencies are difficult to model directly in the time domain, we show that\nthey can be more tractably modelled in two-dimensional time-frequency\nrepresentations such as spectrograms. By leveraging this representational\nadvantage, in conjunction with a highly expressive probabilistic model and a\nmultiscale generation procedure, we design a model capable of generating\nhigh-fidelity audio samples which capture structure at timescales that\ntime-domain models have yet to achieve. We apply our model to a variety of\naudio generation tasks, including unconditional speech generation, music\ngeneration, and text-to-speech synthesis—showing improvements over previous\napproaches in both density estimates and human judgments.</p>\n", "tags": [] },
{"key": "vaswani2018tensor2tensor", "citations": "332", "year": "2018", "title":"Tensor2tensor For Neural Machine Translation", "abstract": "<p>Tensor2Tensor is a library for deep learning models that is well-suited for\nneural machine translation and includes the reference implementation of the\nstate-of-the-art Transformer model.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "vemprala2023chatgpt", "citations": "222", "year": "2024", "title":"Chatgpt For Robotics: Design Principles And Model Abilities", "abstract": "<p>This paper presents an experimental study regarding the use of OpenAI’s\nChatGPT for robotics applications. We outline a strategy that combines design\nprinciples for prompt engineering and the creation of a high-level function\nlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,\nand form factors. We focus our evaluations on the effectiveness of different\nprompt engineering techniques and dialog strategies towards the execution of\nvarious types of robotics tasks. We explore ChatGPT’s ability to use free-form\ndialog, parse XML tags, and to synthesize code, in addition to the use of\ntask-specific prompting functions and closed-loop reasoning through dialogues.\nOur study encompasses a range of tasks within the robotics domain, from basic\nlogical, geometrical, and mathematical reasoning all the way to complex domains\nsuch as aerial navigation, manipulation, and embodied agents. We show that\nChatGPT can be effective at solving several of such tasks, while allowing users\nto interact with it primarily via natural language instructions. In addition to\nthese studies, we introduce an open-sourced research tool called PromptCraft,\nwhich contains a platform where researchers can collaboratively upload and vote\non examples of good prompting schemes for robotics applications, as well as a\nsample robotics simulator with ChatGPT integration, making it easier for users\nto get started with using ChatGPT for robotics.</p>\n", "tags": ["Applications","Prompting","Tools"] },
{"key": "venugopalan2016improving", "citations": "116", "year": "2016", "title":"Improving Lstm-based Video Description With Linguistic Knowledge Mined From Text", "abstract": "<p>This paper investigates how linguistic knowledge mined from large text\ncorpora can aid the generation of natural language descriptions of videos.\nSpecifically, we integrate both a neural language model and distributional\nsemantics trained on large text corpora into a recent LSTM-based architecture\nfor video description. We evaluate our approach on a collection of Youtube\nvideos as well as two large movie description datasets showing significant\nimprovements in grammaticality while modestly improving descriptive quality.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "vig2019analyzing", "citations": "284", "year": "2019", "title":"Analyzing The Structure Of Attention In A Transformer Language Model", "abstract": "<p>The Transformer is a fully attention-based alternative to recurrent networks\nthat has achieved state-of-the-art results across a range of NLP tasks. In this\npaper, we analyze the structure of attention in a Transformer language model,\nthe GPT-2 small pretrained model. We visualize attention for individual\ninstances and analyze the interaction between attention and syntax over a large\ncorpus. We find that attention targets different parts of speech at different\nlayer depths within the model, and that attention aligns with dependency\nrelations most strongly in the middle layers. We also find that the deepest\nlayers of the model capture the most distant relationships. Finally, we extract\nexemplar sentences that reveal highly specific patterns targeted by particular\nattention heads.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "vig2019multiscale", "citations": "453", "year": "2019", "title":"A Multiscale Visualization Of Attention In The Transformer Model", "abstract": "<p>The Transformer is a sequence model that forgoes traditional recurrent\narchitectures in favor of a fully attention-based approach. Besides improving\nperformance, an advantage of using attention is that it can also help to\ninterpret a model by showing how the model assigns weight to different input\nelements. However, the multi-layer, multi-head attention mechanism in the\nTransformer model can be difficult to decipher. To make the model more\naccessible, we introduce an open-source tool that visualizes attention at\nmultiple scales, each of which provides a unique perspective on the attention\nmechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three\nexample use cases: detecting model bias, locating relevant attention heads, and\nlinking neurons to model behavior.</p>\n", "tags": ["Applications","Model Architecture"] },
{"key": "vig2020causal", "citations": "77", "year": "2020", "title":"Causal Mediation Analysis For Interpreting Neural NLP: The Case Of Gender Bias", "abstract": "<p>Common methods for interpreting neural models in natural language processing\ntypically examine either their structure or their behavior, but not both. We\npropose a methodology grounded in the theory of causal mediation analysis for\ninterpreting which parts of a model are causally implicated in its behavior. It\nenables us to analyze the mechanisms by which information flows from input to\noutput through various model components, known as mediators. We apply this\nmethodology to analyze gender bias in pre-trained Transformer language models.\nWe study the role of individual neurons and attention heads in mediating gender\nbias across three datasets designed to gauge a model’s sensitivity to gender\nbias. Our mediation analysis reveals that gender bias effects are (i) sparse,\nconcentrated in a small part of the network; (ii) synergistic, amplified or\nrepressed by different components; and (iii) decomposable into effects flowing\ndirectly from the input and indirectly through the mediators.</p>\n", "tags": ["Datasets","Ethics & Fairness","Model Architecture"] },
{"key": "vijayakumar2016diverse", "citations": "380", "year": "2016", "title":"Diverse Beam Search: Decoding Diverse Solutions From Neural Sequence Models", "abstract": "<p>Neural sequence models are widely used to model time-series data. Equally\nubiquitous is the usage of beam search (BS) as an approximate inference\nalgorithm to decode output sequences from these models. BS explores the search\nspace in a greedy left-right fashion retaining only the top-B candidates -\nresulting in sequences that differ only slightly from each other. Producing\nlists of nearly identical sequences is not only computationally wasteful but\nalso typically fails to capture the inherent ambiguity of complex AI tasks. To\novercome this problem, we propose Diverse Beam Search (DBS), an alternative to\nBS that decodes a list of diverse outputs by optimizing for a\ndiversity-augmented objective. We observe that our method finds better top-1\nsolutions by controlling for the exploration and exploitation of the search\nspace - implying that DBS is a better search algorithm. Moreover, these gains\nare achieved with minimal computational or memory over- head as compared to\nbeam search. To demonstrate the broad applicability of our method, we present\nresults on image captioning, machine translation and visual question generation\nusing both standard quantitative metrics and qualitative human studies.\nFurther, we study the role of diversity for image-grounded language generation\ntasks as the complexity of the image changes. We observe that our method\nconsistently outperforms BS and previously proposed techniques for diverse\ndecoding from neural sequence models.</p>\n", "tags": ["Evaluation"] },
{"key": "villegas2022phenaki", "citations": "71", "year": "2022", "title":"Phenaki: Variable Length Video Generation From Open Domain Textual Description", "abstract": "<p>We present Phenaki, a model capable of realistic video synthesis, given a\nsequence of textual prompts. Generating videos from text is particularly\nchallenging due to the computational cost, limited quantities of high quality\ntext-video data and variable length of videos. To address these issues, we\nintroduce a new model for learning video representation which compresses the\nvideo to a small representation of discrete tokens. This tokenizer uses causal\nattention in time, which allows it to work with variable-length videos. To\ngenerate video tokens from text we are using a bidirectional masked transformer\nconditioned on pre-computed text tokens. The generated video tokens are\nsubsequently de-tokenized to create the actual video. To address data issues,\nwe demonstrate how joint training on a large corpus of image-text pairs as well\nas a smaller number of video-text examples can result in generalization beyond\nwhat is available in the video datasets. Compared to the previous video\ngeneration methods, Phenaki can generate arbitrary long videos conditioned on a\nsequence of prompts (i.e. time variable text or a story) in open domain. To the\nbest of our knowledge, this is the first time a paper studies generating videos\nfrom time variable prompts. In addition, compared to the per-frame baselines,\nthe proposed video encoder-decoder computes fewer tokens per video but results\nin better spatio-temporal consistency.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "vinyals2016show", "citations": "904", "year": "2016", "title":"Show And Tell: Lessons Learned From The 2015 MSCOCO Image Captioning Challenge", "abstract": "<p>Automatically describing the content of an image is a fundamental problem in\nartificial intelligence that connects computer vision and natural language\nprocessing. In this paper, we present a generative model based on a deep\nrecurrent architecture that combines recent advances in computer vision and\nmachine translation and that can be used to generate natural sentences\ndescribing an image. The model is trained to maximize the likelihood of the\ntarget description sentence given the training image. Experiments on several\ndatasets show the accuracy of the model and the fluency of the language it\nlearns solely from image descriptions. Our model is often quite accurate, which\nwe verify both qualitatively and quantitatively. Finally, given the recent\nsurge of interest in this task, a competition was organized in 2015 using the\nnewly released COCO dataset. We describe and analyze the various improvements\nwe applied to our own baseline and show the resulting performance in the\ncompetition, which we won ex-aequo with a team from Microsoft Research, and\nprovide an open source implementation in TensorFlow.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "voita2018context", "citations": "314", "year": "2018", "title":"Context-aware Neural Machine Translation Learns Anaphora Resolution", "abstract": "<p>Standard machine translation systems process sentences in isolation and hence\nignore extra-sentential information, even though extended context can both\nprevent mistakes in ambiguous cases and improve translation coherence. We\nintroduce a context-aware neural machine translation model designed in such way\nthat the flow of information from the extended context to the translation model\ncan be controlled and analyzed. We experiment with an English-Russian subtitles\ndataset, and observe that much of what is captured by our model deals with\nimproving pronoun translation. We measure correspondences between induced\nattention distributions and coreference relations and observe that the model\nimplicitly captures anaphora. It is consistent with gains for sentences where\npronouns need to be gendered in translation. Beside improvements in anaphoric\ncases, the model also improves in overall BLEU, both over its context-agnostic\nversion (+0.7) and over simple concatenation of the context and source\nsentences (+0.6).</p>\n", "tags": ["Datasets","Memory & Context","Model Architecture"] },
{"key": "voita2019bottom", "citations": "141", "year": "2019", "title":"The Bottom-up Evolution Of Representations In The Transformer: A Study With Machine Translation And Language Modeling Objectives", "abstract": "<p>We seek to understand how the representations of individual tokens and the\nstructure of the learned feature space evolve between layers in deep neural\nnetworks under different learning objectives. We focus on the Transformers for\nour analysis as they have been shown effective on various tasks, including\nmachine translation (MT), standard left-to-right language models (LM) and\nmasked language modeling (MLM). Previous work used black-box probing tasks to\nshow that the representations learned by the Transformer differ significantly\ndepending on the objective. In this work, we use canonical correlation analysis\nand mutual information estimators to study how information flows across\nTransformer layers and how this process depends on the choice of learning\nobjective. For example, as you go from bottom to top layers, information about\nthe past in left-to-right language models gets vanished and predictions about\nthe future get formed. In contrast, for MLM, representations initially acquire\ninformation about the context around the token, partially forgetting the token\nidentity and producing a more generalized token representation. The token\nidentity then gets recreated at the top MLM layers.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "voita2019context", "citations": "100", "year": "2019", "title":"Context-aware Monolingual Repair For Neural Machine Translation", "abstract": "<p>Modern sentence-level NMT systems often produce plausible translations of\nisolated sentences. However, when put in context, these translations may end up\nbeing inconsistent with each other. We propose a monolingual DocRepair model to\ncorrect inconsistencies between sentence-level translations. DocRepair performs\nautomatic post-editing on a sequence of sentence-level translations, refining\ntranslations of sentences in context of each other. For training, the DocRepair\nmodel requires only monolingual document-level data in the target language. It\nis trained as a monolingual sequence-to-sequence model that maps inconsistent\ngroups of sentences into consistent ones. The consistent groups come from the\noriginal training data; the inconsistent groups are obtained by sampling\nround-trip translations for each isolated sentence. We show that this approach\nsuccessfully imitates inconsistencies we aim to fix: using contrastive\nevaluation, we show large improvements in the translation of several contextual\nphenomena in an English-Russian translation task, as well as improvements in\nthe BLEU score. We also conduct a human evaluation and show a strong preference\nof the annotators to corrected translations over the baseline ones. Moreover,\nwe analyze which discourse phenomena are hard to capture using monolingual data\nonly.</p>\n", "tags": ["EMNLP","Evaluation","Training Techniques"] },
{"key": "vu2020exploring", "citations": "110", "year": "2020", "title":"Exploring And Predicting Transferability Across NLP Tasks", "abstract": "<p>Recent advances in NLP demonstrate the effectiveness of training large-scale\nlanguage models and transferring them to downstream tasks. Can fine-tuning\nthese models on tasks other than language modeling further improve performance?\nIn this paper, we conduct an extensive study of the transferability between 33\nNLP tasks across three broad classes of problems (text classification, question\nanswering, and sequence labeling). Our results show that transfer learning is\nmore beneficial than previously thought, especially when target task data is\nscarce, and can improve performance even when the source task is small or\ndiffers substantially from the target task (e.g., part-of-speech tagging\ntransfers well to the DROP QA dataset). We also develop task embeddings that\ncan be used to predict the most transferable source tasks for a given target\ntask, and we validate their effectiveness in experiments controlled for source\nand target data size. Overall, our experiments reveal that factors such as\nsource data size, task and domain similarity, and task complexity all play a\nrole in determining transferability.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Training Techniques"] },
{"key": "vu2021spot", "citations": "141", "year": "2022", "title":"Spot: Better Frozen Model Adaptation Through Soft Prompt Transfer", "abstract": "<p>There has been growing interest in parameter-efficient methods to apply\npre-trained language models to downstream tasks. Building on the Prompt Tuning\napproach of Lester et al. (2021), which learns task-specific soft prompts to\ncondition a frozen pre-trained model to perform different tasks, we propose a\nnovel prompt-based transfer learning approach called SPoT: Soft Prompt\nTransfer. SPoT first learns a prompt on one or more source tasks and then uses\nit to initialize the prompt for a target task. We show that SPoT significantly\nboosts the performance of Prompt Tuning across many tasks. More remarkably,\nacross all model sizes, SPoT matches or outperforms standard Model Tuning\n(which fine-tunes all model parameters) on the SuperGLUE benchmark, while using\nup to 27,000x fewer task-specific parameters. To understand where SPoT is most\neffective, we conduct a large-scale study on task transferability with 26 NLP\ntasks in 160 combinations, and demonstrate that many tasks can benefit each\nother via prompt transfer. Finally, we propose an efficient retrieval approach\nthat interprets task prompts as task embeddings to identify similar tasks and\npredict the most transferable source tasks for a novel target task.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Prompting"] },
{"key": "vylomova2020sigmorphon", "citations": "73", "year": "2020", "title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection", "abstract": "<p>A broad goal in natural language processing (NLP) is to develop a system that\nhas the capacity to process any natural language. Most systems, however, are\ndeveloped using data from just one language such as English. The SIGMORPHON\n2020 shared task on morphological reinflection aims to investigate systems’\nability to generalize across typologically distinct languages, many of which\nare low resource. Systems were developed using data from 45 languages and just\n5 language families, fine-tuned with data from an additional 45 languages and\n10 language families (13 in total), and evaluated on all 90 languages. A total\nof 22 systems (19 neural) from 10 teams were submitted to the task. All four\nwinning systems were neural (two monolingual transformers and two massively\nmultilingual RNN-based models with gated attention). Most teams demonstrate\nutility of data hallucination and augmentation, ensembles, and multilingual\ntraining for low-resource languages. Non-neural learners and manually designed\ngrammars showed competitive and even superior performance on some languages\n(such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited\ndata. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were\nrelatively easy for most systems and achieved over 90% mean accuracy while\nothers were more challenging.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "wadhwa2023revisiting", "citations": "93", "year": "2023", "title":"Revisiting Relation Extraction In The Era Of Large Language Models", "abstract": "<p>Relation extraction (RE) is the core NLP task of inferring semantic\nrelationships between entities from text. Standard supervised RE techniques\nentail training modules to tag tokens comprising entity spans and then predict\nthe relationship between them. Recent work has instead treated the problem as a\n<em>sequence-to-sequence</em> task, linearizing relations between entities as\ntarget strings to be generated conditioned on the input. Here we push the\nlimits of this approach, using larger language models (GPT-3 and Flan-T5 large)\nthan considered in prior work and evaluating their performance on standard RE\ntasks under varying levels of supervision. We address issues inherent to\nevaluating generative approaches to RE by doing human evaluations, in lieu of\nrelying on exact matching. Under this refined evaluation, we find that: (1)\nFew-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly\nequivalent to existing fully supervised models; (2) Flan-T5 is not as capable\nin the few-shot setting, but supervising and fine-tuning it with\nChain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA\nresults. We release this model as a new baseline for RE tasks.</p>\n", "tags": ["Evaluation","Few-Shot","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "wagner2022dawn", "citations": "191", "year": "2023", "title":"Dawn Of The Transformer Era In Speech Emotion Recognition: Closing The Valence Gap", "abstract": "<p>Recent advances in transformer-based architectures which are pre-trained in\nself-supervised manner have shown great promise in several machine learning\ntasks. In the audio domain, such architectures have also been successfully\nutilised in the field of speech emotion recognition (SER). However, existing\nworks have not evaluated the influence of model size and pre-training data on\ndownstream performance, and have shown limited attention to generalisation,\nrobustness, fairness, and efficiency. The present contribution conducts a\nthorough analysis of these aspects on several pre-trained variants of wav2vec\n2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and\nvalence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test\ncross-corpus generalisation. To the best of our knowledge, we obtain the top\nperformance for valence prediction without use of explicit linguistic\ninformation, with a concordance correlation coefficient (CCC) of .638 on\nMSP-Podcast. Furthermore, our investigations reveal that transformer-based\narchitectures are more robust to small perturbations compared to a CNN-based\nbaseline and fair with respect to biological sex groups, but not towards\nindividual speakers. Finally, we are the first to show that their extraordinary\nsuccess on valence is based on implicit linguistic information learnt during\nfine-tuning of the transformer layers, which explains why they perform on-par\nwith recent multimodal approaches that explicitly utilise textual information.\nOur findings collectively paint the following picture: transformer-based\narchitectures constitute the new state-of-the-art in SER, but further advances\nare needed to mitigate remaining robustness and individual speaker issues. To\nmake our findings reproducible, we release the best performing model to the\ncommunity.</p>\n", "tags": ["Datasets","Ethics & Fairness","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wallace2018trick", "citations": "127", "year": "2019", "title":"Trick Me If You Can: Human-in-the-loop Generation Of Adversarial Examples For Question Answering", "abstract": "<p>Adversarial evaluation stress tests a model’s understanding of natural\nlanguage. While past approaches expose superficial patterns, the resulting\nadversarial examples are limited in complexity and diversity. We propose\nhuman-in-the-loop adversarial generation, where human authors are guided to\nbreak models. We aid the authors with interpretations of model predictions\nthrough an interactive user interface. We apply this generation framework to a\nquestion answering task called Quizbowl, where trivia enthusiasts craft\nadversarial questions. The resulting questions are validated via live\nhuman–computer matches: although the questions appear ordinary to humans, they\nsystematically stump neural and information retrieval models. The adversarial\nquestions cover diverse phenomena from multi-hop reasoning to entity type\ndistractors, exposing open challenges in robust question answering.</p>\n", "tags": ["Evaluation","Security","TACL","Tools"] },
{"key": "wallace2019allennlp", "citations": "129", "year": "2019", "title":"Allennlp Interpret: A Framework For Explaining Predictions Of NLP Models", "abstract": "<p>Neural NLP models are increasingly accurate but are imperfect and\nopaque—they break in counterintuitive ways and leave end users puzzled at\ntheir behavior. Model interpretation methods ameliorate this opacity by\nproviding explanations for specific model predictions. Unfortunately, existing\ninterpretation codebases make it difficult to apply these methods to new models\nand tasks, which hinders adoption for practitioners and burdens\ninterpretability researchers. We introduce AllenNLP Interpret, a flexible\nframework for interpreting NLP models. The toolkit provides interpretation\nprimitives (e.g., input gradients) for any AllenNLP model and task, a suite of\nbuilt-in interpretation methods, and a library of front-end visualization\ncomponents. We demonstrate the toolkit’s flexibility and utility by\nimplementing live demos for five interpretation methods (e.g., saliency maps\nand adversarial attacks) on a variety of models and tasks (e.g., masked\nlanguage modeling using BERT and reading comprehension using BiDAF). These\ndemos, alongside our code and tutorials, are available at\nhttps://allennlp.org/interpret .</p>\n", "tags": ["EMNLP","Tools"] },
{"key": "wallace2019do", "citations": "226", "year": "2019", "title":"Do NLP Models Know Numbers? Probing Numeracy In Embeddings", "abstract": "<p>The ability to understand and work with numbers (numeracy) is critical for\nmany complex reasoning tasks. Currently, most NLP models treat numbers in text\nin the same way as other tokens—they embed them as distributed vectors. Is\nthis enough to capture numeracy? We begin by investigating the numerical\nreasoning capabilities of a state-of-the-art question answering model on the\nDROP dataset. We find this model excels on questions that require numerical\nreasoning, i.e., it already captures numeracy. To understand how this\ncapability emerges, we probe token embedding methods (e.g., BERT, GloVe) on\nsynthetic list maximum, number decoding, and addition tasks. A surprising\ndegree of numeracy is naturally present in standard embeddings. For example,\nGloVe and word2vec accurately encode magnitude for numbers up to 1,000.\nFurthermore, character-level embeddings are even more precise—ELMo captures\nnumeracy the best for all pre-trained methods—but BERT, which uses sub-word\nunits, is less exact.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "wallace2019universal", "citations": "593", "year": "2019", "title":"Universal Adversarial Triggers For Attacking And Analyzing NLP", "abstract": "<p>Adversarial examples highlight model vulnerabilities and are useful for\nevaluation and interpretation. We define universal adversarial triggers:\ninput-agnostic sequences of tokens that trigger a model to produce a specific\nprediction when concatenated to any input from a dataset. We propose a\ngradient-guided search over tokens which finds short trigger sequences (e.g.,\none word for classification and four words for language modeling) that\nsuccessfully trigger the target prediction. For example, triggers cause SNLI\nentailment accuracy to drop from 89.94% to 0.55%, 72% of “why” questions in\nSQuAD to be answered “to kill american people”, and the GPT-2 language model to\nspew racist output even when conditioned on non-racial contexts. Furthermore,\nalthough the triggers are optimized using white-box access to a specific model,\nthey transfer to other models for all tasks we consider. Finally, since\ntriggers are input-agnostic, they provide an analysis of global model behavior.\nFor instance, they confirm that SNLI models exploit dataset biases and help to\ndiagnose heuristics learned by reading comprehension models.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture","Security"] },
{"key": "wallace2020concealed", "citations": "80", "year": "2021", "title":"Concealed Data Poisoning Attacks On NLP Models", "abstract": "<p>Adversarial attacks alter NLP model predictions by perturbing test-time\ninputs. However, it is much less understood whether, and how, predictions can\nbe manipulated with small, concealed changes to the training data. In this\nwork, we develop a new data poisoning attack that allows an adversary to\ncontrol model predictions whenever a desired trigger phrase is present in the\ninput. For instance, we insert 50 poison examples into a sentiment model’s\ntraining set that causes the model to frequently predict Positive whenever the\ninput contains “James Bond”. Crucially, we craft these poison examples using a\ngradient-based procedure so that they do not mention the trigger phrase. We\nalso apply our poison attack to language modeling (“Apple iPhone” triggers\nnegative generations) and machine translation (“iced coffee” mistranslated as\n“hot coffee”). We conclude by proposing three defenses that can mitigate our\nattack at some cost in prediction accuracy or extra human annotation.</p>\n", "tags": ["NAACL","Security","Training Techniques"] },
{"key": "wan2018improving", "citations": "370", "year": "2018", "title":"Improving Automatic Source Code Summarization Via Deep Reinforcement Learning", "abstract": "<p>Code summarization provides a high level natural language description of the\nfunction performed by code, as it can benefit the software maintenance, code\ncategorization and retrieval. To the best of our knowledge, most\nstate-of-the-art approaches follow an encoder-decoder framework which encodes\nthe code into a hidden space and then decode it into natural language space,\nsuffering from two major drawbacks: a) Their encoders only consider the\nsequential content of code, ignoring the tree structure which is also critical\nfor the task of code summarization, b) Their decoders are typically trained to\npredict the next word by maximizing the likelihood of next ground-truth word\nwith previous ground-truth word given. However, it is expected to generate the\nentire sequence from scratch at test time. This discrepancy can cause an\n\\textit{exposure bias} issue, making the learnt decoder suboptimal. In this\npaper, we incorporate an abstract syntax tree structure as well as sequential\ncontent of code snippets into a deep reinforcement learning framework (i.e.,\nactor-critic network). The actor network provides the confidence of predicting\nthe next word according to current state. On the other hand, the critic network\nevaluates the reward value of all possible extensions of the current state and\ncan provide global guidance for explorations. We employ an advantage reward\ncomposed of BLEU metric to train both networks. Comprehensive experiments on a\nreal-world dataset show the effectiveness of our proposed model when compared\nwith some state-of-the-art methods.</p>\n", "tags": ["Datasets","Llm For Code","Reinforcement Learning","Tools"] },
{"key": "wan2023gpt", "citations": "83", "year": "2023", "title":"GPT-RE: In-context Learning For Relation Extraction Using Large Language Models", "abstract": "<p>In spite of the potential for ground-breaking achievements offered by large\nlanguage models (LLMs) (e.g., GPT-3), they still lag significantly behind\nfully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).\nThis is due to the two major shortcomings of LLMs in RE: (1) low relevance\nregarding entity and relation in retrieved demonstrations for in-context\nlearning; and (2) the strong inclination to wrongly classify NULL examples into\nother pre-defined labels.\n  In this paper, we propose GPT-RE to bridge the gap between LLMs and\nfully-supervised baselines. GPT-RE successfully addresses the aforementioned\nissues by (1) incorporating task-specific entity representations in\ndemonstration retrieval; and (2) enriching the demonstrations with gold\nlabel-induced reasoning logic. We evaluate GPT-RE on four widely-used RE\ndatasets, and observe that GPT-RE achieves improvements over not only existing\nGPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE\nachieves SOTA performances on the Semeval and SciERC datasets, and competitive\nperformances on the TACRED and ACE05 datasets.</p>\n", "tags": ["Datasets","EMNLP","In Context Learning","Model Architecture"] },
{"key": "wang2016fvqa", "citations": "440", "year": "2017", "title":"FVQA: Fact-based Visual Question Answering", "abstract": "<p>Visual Question Answering (VQA) has attracted a lot of attention in both\nComputer Vision and Natural Language Processing communities, not least because\nit offers insight into the relationships between two important sources of\ninformation. Current datasets, and the models built upon them, have focused on\nquestions which are answerable by direct analysis of the question and image\nalone. The set of such questions that require no external information to answer\nis interesting, but very limited. It excludes questions which require common\nsense, or basic factual knowledge to answer, for example. Here we introduce\nFVQA, a VQA dataset which requires, and supports, much deeper reasoning. FVQA\nonly contains questions which require external information to answer.\n  We thus extend a conventional visual question answering dataset, which\ncontains image-question-answerg triplets, through additional\nimage-question-answer-supporting fact tuples. The supporting fact is\nrepresented as a structural triplet, such as &lt;Cat,CapableOf,ClimbingTrees&gt;.\n  We evaluate several baseline models on the FVQA dataset, and describe a novel\nmodel which is capable of reasoning about an image on the basis of supporting\nfacts.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "wang2016image", "citations": "251", "year": "2016", "title":"Image Captioning With Deep Bidirectional Lstms", "abstract": "<p>This work presents an end-to-end trainable deep bidirectional LSTM\n(Long-Short Term Memory) model for image captioning. Our model builds on a deep\nconvolutional neural network (CNN) and two separate LSTM networks. It is\ncapable of learning long term visual-language interactions by making use of\nhistory and future context information at high level semantic space. Two novel\ndeep bidirectional variant models, in which we increase the depth of\nnonlinearity transition in different way, are proposed to learn hierarchical\nvisual-language embeddings. Data augmentation techniques such as multi-crop,\nmulti-scale and vertical mirror are proposed to prevent overfitting in training\ndeep models. We visualize the evolution of bidirectional LSTM internal states\nover time and qualitatively analyze how our models “translate” image to\nsentence. Our proposed models are evaluated on caption generation and\nimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,\nFlickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models\nachieve highly competitive performance to the state-of-the-art results on\ncaption generation even without integrating additional mechanism (e.g. object\ndetection, attention model etc.) and significantly outperform recent methods on\nretrieval task.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "wang2016learning", "citations": "143", "year": "2016", "title":"Learning Sentence Representation With Guidance Of Human Attention", "abstract": "<p>Recently, much progress has been made in learning general-purpose sentence\nrepresentations that can be used across domains. However, most of the existing\nmodels typically treat each word in a sentence equally. In contrast, extensive\nstudies have proven that human read sentences efficiently by making a sequence\nof fixation and saccades. This motivates us to improve sentence representations\nby assigning different weights to the vectors of the component words, which can\nbe treated as an attention mechanism on single sentences. To that end, we\npropose two novel attention models, in which the attention weights are derived\nusing significant predictors of human reading time, i.e., Surprisal, POS tags\nand CCG supertags. The extensive experiments demonstrate that the proposed\nmethods significantly improve upon the state-of-the-art sentence representation\nmodels.</p>\n", "tags": ["Model Architecture"] },
{"key": "wang2016machine", "citations": "418", "year": "2016", "title":"Machine Comprehension Using Match-lstm And Answer Pointer", "abstract": "<p>Machine comprehension of text is an important problem in natural language\nprocessing. A recently released dataset, the Stanford Question Answering\nDataset (SQuAD), offers a large number of real questions and their answers\ncreated by humans through crowdsourcing. SQuAD provides a challenging testbed\nfor evaluating machine comprehension algorithms, partly because compared with\nprevious datasets, in SQuAD the answers do not come from a small set of\ncandidate answers and they have variable lengths. We propose an end-to-end\nneural architecture for the task. The architecture is based on match-LSTM, a\nmodel we proposed previously for textual entailment, and Pointer Net, a\nsequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the\noutput tokens to be from the input sequences. We propose two ways of using\nPointer Net for our task. Our experiments show that both of our two models\nsubstantially outperform the best results obtained by Rajpurkar et al.(2016)\nusing logistic regression and manually crafted features.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "wang2016memory", "citations": "63", "year": "2016", "title":"Memory-enhanced Decoder For Neural Machine Translation", "abstract": "<p>We propose to enhance the RNN decoder in a neural machine translator (NMT)\nwith external memory, as a natural but powerful extension to the state in the\ndecoding RNN. This memory-enhanced RNN decoder is called \\textsc{MemDec}. At\neach time during decoding, \\textsc{MemDec} will read from this memory and write\nto this memory once, both with content-based addressing. Unlike the unbounded\nmemory in previous work\\cite{RNNsearch} to store the representation of source\nsentence, the memory in \\textsc{MemDec} is a matrix with pre-determined size\ndesigned to better capture the information important for the decoding process\nat each time step. Our empirical study on Chinese-English translation shows\nthat it can improve by \\(4.8\\) BLEU upon Groundhog and \\(5.3\\) BLEU upon on Moses,\nyielding the best performance achieved with the same training set.</p>\n", "tags": ["EMNLP","Memory & Context","Training Techniques"] },
{"key": "wang2016multi", "citations": "119", "year": "2016", "title":"Multi-perspective Context Matching For Machine Comprehension", "abstract": "<p>Previous machine comprehension (MC) datasets are either too small to train\nend-to-end deep learning models, or not difficult enough to evaluate the\nability of current MC techniques. The newly released SQuAD dataset alleviates\nthese limitations, and gives us a chance to develop more realistic MC models.\nBased on this dataset, we propose a Multi-Perspective Context Matching (MPCM)\nmodel, which is an end-to-end system that directly predicts the answer\nbeginning and ending points in a passage. Our model first adjusts each\nword-embedding vector in the passage by multiplying a relevancy weight computed\nagainst the question. Then, we encode the question and weighted passage by\nusing bi-directional LSTMs. For each point in the passage, our model matches\nthe context of this point against the encoded question from multiple\nperspectives and produces a matching vector. Given those matched vectors, we\nemploy another bi-directional LSTM to aggregate all the information and predict\nthe beginning and ending points. Experimental result on the test set of SQuAD\nshows that our model achieves a competitive result on the leaderboard.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "wang2016multimodal", "citations": "168", "year": "2018", "title":"Multimodal Memory Modelling For Video Captioning", "abstract": "<p>Video captioning which automatically translates video clips into natural\nlanguage sentences is a very important task in computer vision. By virtue of\nrecent deep learning technologies, e.g., convolutional neural networks (CNNs)\nand recurrent neural networks (RNNs), video captioning has made great progress.\nHowever, learning an effective mapping from visual sequence space to language\nspace is still a challenging problem. In this paper, we propose a Multimodal\nMemory Model (M3) to describe videos, which builds a visual and textual shared\nmemory to model the long-term visual-textual dependency and further guide\nglobal visual attention on described targets. Specifically, the proposed M3\nattaches an external memory to store and retrieve both visual and textual\ncontents by interacting with video and sentence with multiple read and write\noperations. First, text representation in the Long Short-Term Memory (LSTM)\nbased text decoder is written into the memory, and the memory contents will be\nread out to guide an attention to select related visual targets. Then, the\nselected visual information is written into the memory, which will be further\nread out to the text decoder. To evaluate the proposed model, we perform\nexperiments on two publicly benchmark datasets: MSVD and MSR-VTT. The\nexperimental results demonstrate that our method outperforms the\nstate-of-theart methods in terms of BLEU and METEOR.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Memory & Context","Model Architecture"] },
{"key": "wang2016neural", "citations": "98", "year": "2017", "title":"Neural Machine Translation Advised By Statistical Machine Translation", "abstract": "<p>Neural Machine Translation (NMT) is a new approach to machine translation\nthat has made great progress in recent years. However, recent studies show that\nNMT generally produces fluent but inadequate translations (Tu et al. 2016b; Tu\net al. 2016a; He et al. 2016; Tu et al. 2017). This is in contrast to\nconventional Statistical Machine Translation (SMT), which usually yields\nadequate but non-fluent translations. It is natural, therefore, to leverage the\nadvantages of both models for better translations, and in this work we propose\nto incorporate SMT model into NMT framework. More specifically, at each\ndecoding step, SMT offers additional recommendations of generated words based\non the decoding information from NMT (e.g., the generated partial translation\nand attention history). Then we employ an auxiliary classifier to score the SMT\nrecommendations and a gating function to combine the SMT recommendations with\nNMT generations, both of which are jointly trained within the NMT architecture\nin an end-to-end manner. Experimental results on Chinese-English translation\nshow that the proposed approach achieves significant and consistent\nimprovements over state-of-the-art NMT and SMT systems on multiple NIST test\nsets.</p>\n", "tags": ["AAAI","Model Architecture","Tools"] },
{"key": "wang2017exploiting", "citations": "201", "year": "2017", "title":"Exploiting Cross-sentence Context For Neural Machine Translation", "abstract": "<p>In translation, considering the document as a whole can help to resolve\nambiguities and inconsistencies. In this paper, we propose a cross-sentence\ncontext-aware approach and investigate the influence of historical contextual\ninformation on the performance of neural machine translation (NMT). First, this\nhistory is summarized in a hierarchical way. We then integrate the historical\nrepresentation into NMT in two strategies: 1) a warm-start of encoder and\ndecoder states, and 2) an auxiliary context source for updating decoder states.\nExperimental results on a large Chinese-English translation task show that our\napproach significantly improves upon a strong attention-based NMT system by up\nto +2.1 BLEU points.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "wang2017joint", "citations": "87", "year": "2017", "title":"A Joint Model For Question Answering And Question Generation", "abstract": "<p>We propose a generative machine comprehension model that learns jointly to\nask and answer questions based on documents. The proposed model uses a\nsequence-to-sequence framework that encodes the document and generates a\nquestion (answer) given an answer (question). Significant improvement in model\nperformance is observed empirically on the SQuAD corpus, confirming our\nhypothesis that the model benefits from jointly learning to perform both tasks.\nWe believe the joint model’s novelty offers a new perspective on machine\ncomprehension beyond architectural engineering, and serves as a first step\ntowards autonomous information seeking.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "wang2017naturalizing", "citations": "68", "year": "2017", "title":"Naturalizing A Programming Language Via Interactive Learning", "abstract": "<p>Our goal is to create a convenient natural language interface for performing\nwell-specified but complex actions such as analyzing data, manipulating text,\nand querying databases. However, existing natural language interfaces for such\ntasks are quite primitive compared to the power one wields with a programming\nlanguage. To bridge this gap, we start with a core programming language and\nallow users to “naturalize” the core language incrementally by defining\nalternative, more natural syntax and increasingly complex concepts in terms of\ncompositions of simpler ones. In a voxel world, we show that a community of\nusers can simultaneously teach a common system a diverse language and use it to\nbuild hundreds of complex voxel structures. Over the course of three days,\nthese users went from using only the core language to using the naturalized\nlanguage in 85.9% of the last 10K utterances.</p>\n", "tags": [] },
{"key": "wang2017r", "citations": "92", "year": "2017", "title":"R\\(^3\\): Reinforced Reader-ranker For Open-domain Question Answering", "abstract": "<p>In recent years researchers have achieved considerable success applying\nneural network methods to question answering (QA). These approaches have\nachieved state of the art results in simplified closed-domain settings such as\nthe SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected\npassage, from which the answer to a given question may be extracted. More\nrecently, researchers have begun to tackle open-domain QA, in which the model\nis given a question and access to a large corpus (e.g., wikipedia) instead of a\npre-selected passage (Chen et al., 2017a). This setting is more complex as it\nrequires large-scale search for relevant passages by an information retrieval\ncomponent, combined with a reading comprehension model that “reads” the\npassages to generate an answer to the question. Performance in this setting\nlags considerably behind closed-domain performance. In this paper, we present a\nnovel open-domain QA system called Reinforced Ranker-Reader \\((R^3)\\), based on\ntwo algorithmic innovations. First, we propose a new pipeline for open-domain\nQA with a Ranker component, which learns to rank retrieved passages in terms of\nlikelihood of generating the ground-truth answer to a given question. Second,\nwe propose a novel method that jointly trains the Ranker along with an\nanswer-generation Reader model, based on reinforcement learning. We report\nextensive experimental results showing that our method significantly improves\non the state of the art for multiple open-domain QA datasets.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning"] },
{"key": "wang2017skeleton", "citations": "99", "year": "2017", "title":"Skeleton Key: Image Captioning By Skeleton-attribute Decomposition", "abstract": "<p>Recently, there has been a lot of interest in automatically generating\ndescriptions for an image. Most existing language-model based approaches for\nthis task learn to generate an image description word by word in its original\nword order. However, for humans, it is more natural to locate the objects and\ntheir relationships first, and then elaborate on each object, describing\nnotable attributes. We present a coarse-to-fine method that decomposes the\noriginal image description into a skeleton sentence and its attributes, and\ngenerates the skeleton sentence and attribute phrases separately. By this\ndecomposition, our method can generate more accurate and novel descriptions\nthan the previous state-of-the-art. Experimental results on the MS-COCO and a\nlarger scale Stock3M datasets show that our algorithm yields consistent\nimprovements across different evaluation metrics, especially on the SPICE\nmetric, which has much higher correlation with human ratings than the\nconventional metrics. Furthermore, our algorithm can generate descriptions with\nvaried length, benefiting from the separate control of the skeleton and\nattributes. This enables image description generation that better accommodates\nuser preferences.</p>\n", "tags": ["CVPR","Datasets","Evaluation"] },
{"key": "wang2017steering", "citations": "71", "year": "2017", "title":"Steering Output Style And Topic In Neural Response Generation", "abstract": "<p>We propose simple and flexible training and decoding methods for influencing\noutput style and topic in neural encoder-decoder based language generation.\nThis capability is desirable in a variety of applications, including\nconversational systems, where successful agents need to produce language in a\nspecific style and generate responses steered by a human puppeteer or external\nknowledge. We decompose the neural generation process into empirically easier\nsub-problems: a faithfulness model and a decoding method based on\nselective-sampling. We also describe training and sampling algorithms that bias\nthe generation process with a specific language style restriction, or a topic\nrestriction. Human evaluation results show that our proposed methods are able\nto restrict style and topic without degrading output quality in conversational\ntasks.</p>\n", "tags": ["Applications","EMNLP","Evaluation","Training Techniques"] },
{"key": "wang2017tacotron", "citations": "1519", "year": "2017", "title":"Tacotron: Towards End-to-end Speech Synthesis", "abstract": "<p>A text-to-speech synthesis system typically consists of multiple stages, such\nas a text analysis frontend, an acoustic model and an audio synthesis module.\nBuilding these components often requires extensive domain expertise and may\ncontain brittle design choices. In this paper, we present Tacotron, an\nend-to-end generative text-to-speech model that synthesizes speech directly\nfrom characters. Given &lt;text, audio&gt; pairs, the model can be trained completely\nfrom scratch with random initialization. We present several key techniques to\nmake the sequence-to-sequence framework perform well for this challenging task.\nTacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,\noutperforming a production parametric system in terms of naturalness. In\naddition, since Tacotron generates speech at the frame level, it’s\nsubstantially faster than sample-level autoregressive methods.</p>\n", "tags": ["INTERSPEECH","Tools"] },
{"key": "wang2017video", "citations": "269", "year": "2018", "title":"Video Captioning Via Hierarchical Reinforcement Learning", "abstract": "<p>Video captioning is the task of automatically generating a textual\ndescription of the actions in a video. Although previous work (e.g.\nsequence-to-sequence model) has shown promising results in abstracting a coarse\ndescription of a short video, it is still very challenging to caption a video\ncontaining multiple fine-grained actions with a detailed description. This\npaper aims to address the challenge by proposing a novel hierarchical\nreinforcement learning framework for video captioning, where a high-level\nManager module learns to design sub-goals and a low-level Worker module\nrecognizes the primitive actions to fulfill the sub-goal. With this\ncompositional framework to reinforce video captioning at different levels, our\napproach significantly outperforms all the baseline methods on a newly\nintroduced large-scale dataset for fine-grained video captioning. Furthermore,\nour non-ensemble model has already achieved the state-of-the-art results on the\nwidely-used MSR-VTT dataset.</p>\n", "tags": ["CVPR","Datasets","Reinforcement Learning","Tools"] },
{"key": "wang2018can", "citations": "101", "year": "2019", "title":"Can You Tell Me How To Get Past Sesame Street? Sentence-level Pretraining Beyond Language Modeling", "abstract": "<p>Natural language understanding has recently seen a surge of progress with the\nuse of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et\nal., 2019) which are pretrained on variants of language modeling. We conduct\nthe first large-scale systematic study of candidate pretraining tasks,\ncomparing 19 different tasks both as alternatives and complements to language\nmodeling. Our primary results support the use language modeling, especially\nwhen combined with pretraining on additional labeled-data tasks. However, our\nresults are mixed across pretraining tasks and show some concerning trends: In\nELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong\nand results vary strikingly across target tasks. In addition, fine-tuning BERT\non an intermediate task often negatively impacts downstream transfer. In a more\npositive trend, we see modest gains from multitask training, suggesting the\ndevelopment of more sophisticated multitask and transfer learning techniques as\nan avenue for further research.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wang2018cnn", "citations": "65", "year": "2018", "title":"CNN+CNN: Convolutional Decoders For Image Captioning", "abstract": "<p>Image captioning is a challenging task that combines the field of computer\nvision and natural language processing. A variety of approaches have been\nproposed to achieve the goal of automatically describing an image, and\nrecurrent neural network (RNN) or long-short term memory (LSTM) based models\ndominate this field. However, RNNs or LSTMs cannot be calculated in parallel\nand ignore the underlying hierarchical structure of a sentence. In this paper,\nwe propose a framework that only employs convolutional neural networks (CNNs)\nto generate captions. Owing to parallel computing, our basic model is around 3\ntimes faster than NIC (an LSTM-based model) during training time, while also\nproviding better results. We conduct extensive experiments on MSCOCO and\ninvestigate the influence of the model width and depth. Compared with\nLSTM-based models that apply similar attention mechanisms, our proposed models\nachieves comparable scores of BLEU-1,2,3,4 and METEOR, and higher scores of\nCIDEr. We also test our model on the paragraph annotation dataset, and get\nhigher CIDEr score compared with hierarchical LSTMs</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "wang2018co", "citations": "105", "year": "2018", "title":"A Co-matching Model For Multi-choice Reading Comprehension", "abstract": "<p>Multi-choice reading comprehension is a challenging task, which involves the\nmatching between a passage and a question-answer pair. This paper proposes a\nnew co-matching approach to this problem, which jointly models whether a\npassage can match both a question and a candidate answer. Experimental results\non the RACE dataset demonstrate that our approach achieves state-of-the-art\nperformance.</p>\n", "tags": ["Datasets"] },
{"key": "wang2018explainable", "citations": "111", "year": "2018", "title":"Explainable Recommendation Via Multi-task Learning In Opinionated Text Data", "abstract": "<p>Explaining automatically generated recommendations allows users to make more\ninformed and accurate decisions about which results to utilize, and therefore\nimproves their satisfaction. In this work, we develop a multi-task learning\nsolution for explainable recommendation. Two companion learning tasks of user\npreference modeling for recommendation} and \\textit{opinionated content\nmodeling for explanation are integrated via a joint tensor factorization. As a\nresult, the algorithm predicts not only a user’s preference over a list of\nitems, i.e., recommendation, but also how the user would appreciate a\nparticular item at the feature level, i.e., opinionated textual explanation.\nExtensive experiments on two large collections of Amazon and Yelp reviews\nconfirmed the effectiveness of our solution in both recommendation and\nexplanation tasks, compared with several existing recommendation algorithms.\nAnd our extensive user study clearly demonstrates the practical value of the\nexplainable recommendations generated by our algorithm.</p>\n", "tags": ["SIGIR"] },
{"key": "wang2018glue", "citations": "3629", "year": "2018", "title":"GLUE: A Multi-task Benchmark And Analysis Platform For Natural Language Understanding", "abstract": "<p>For natural language understanding (NLU) technology to be maximally useful,\nboth practically and as a scientific object of study, it must be general: it\nmust be able to process language in a way that is not exclusively tailored to\nany one specific task or dataset. In pursuit of this objective, we introduce\nthe General Language Understanding Evaluation benchmark (GLUE), a tool for\nevaluating and analyzing the performance of models across a diverse range of\nexisting NLU tasks. GLUE is model-agnostic, but it incentivizes sharing\nknowledge across tasks because certain tasks have very limited training data.\nWe further provide a hand-crafted diagnostic test suite that enables detailed\nlinguistic analysis of NLU models. We evaluate baselines based on current\nmethods for multi-task and transfer learning and find that they do not\nimmediately give substantial improvements over the aggregate performance of\ntraining a separate model per task, indicating room for improvement in\ndeveloping general and robust NLU systems.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Fine-Tuning","Tools"] },
{"key": "wang2018learning", "citations": "66", "year": "2018", "title":"Learning To Ask Questions In Open-domain Conversational Systems With Typed Decoders", "abstract": "<p>Asking good questions in large-scale, open-domain conversational systems is\nquite significant yet rather untouched. This task, substantially different from\ntraditional question generation, requires to question not only with various\npatterns but also on diverse and relevant topics. We observe that a good\nquestion is a natural composition of {\\it interrogatives}, {\\it topic words},\nand {\\it ordinary words}. Interrogatives lexicalize the pattern of questioning,\ntopic words address the key information for topic transition in dialogue, and\nordinary words play syntactical and grammatical roles in making a natural\nsentence. We devise two typed decoders (\\textit{soft typed decoder} and\n\\textit{hard typed decoder}) in which a type distribution over the three types\nis estimated and used to modulate the final generation distribution. Extensive\nexperiments show that the typed decoders outperform state-of-the-art baselines\nand can generate more meaningful questions.</p>\n", "tags": [] },
{"key": "wang2018look", "citations": "194", "year": "2018", "title":"Look Before You Leap: Bridging Model-free And Model-based Reinforcement Learning For Planned-ahead Vision-and-language Navigation", "abstract": "<p>Existing research studies on vision and language grounding for robot\nnavigation focus on improving model-free deep reinforcement learning (DRL)\nmodels in synthetic environments. However, model-free DRL models do not\nconsider the dynamics in the real-world environments, and they often fail to\ngeneralize to new scenes. In this paper, we take a radical approach to bridge\nthe gap between synthetic studies and real-world practices—We propose a\nnovel, planned-ahead hybrid reinforcement learning model that combines\nmodel-free and model-based reinforcement learning to solve a real-world\nvision-language navigation task. Our look-ahead module tightly integrates a\nlook-ahead policy model with an environment model that predicts the next state\nand the reward. Experimental results suggest that our proposed method\nsignificantly outperforms the baselines and achieves the best on the real-world\nRoom-to-Room dataset. Moreover, our scalable method is more generalizable when\ntransferring to unseen environments.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning"] },
{"key": "wang2018multi", "citations": "105", "year": "2018", "title":"Multi-granularity Hierarchical Attention Fusion Networks For Reading Comprehension And Question Answering", "abstract": "<p>This paper describes a novel hierarchical attention network for reading\ncomprehension style question answering, which aims to answer questions for a\ngiven narrative paragraph. In the proposed method, attention and fusion are\nconducted horizontally and vertically across layers at different levels of\ngranularity between question and paragraph. Specifically, it first encode the\nquestion and paragraph with fine-grained language embeddings, to better capture\nthe respective representations at semantic level. Then it proposes a\nmulti-granularity fusion approach to fully fuse information from both global\nand attended representations. Finally, it introduces a hierarchical attention\nnetwork to focuses on the answer span progressively with multi-level\nsoftalignment. Extensive experiments on the large-scale SQuAD and TriviaQA\ndatasets validate the effectiveness of the proposed method. At the time of\nwriting the paper (Jan. 12th 2018), our model achieves the first position on\nthe SQuAD leaderboard for both single and ensemble models. We also achieves\nstate-of-the-art results on TriviaQA, AddSent and AddOne-Sent datasets.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "wang2018no", "citations": "178", "year": "2018", "title":"No Metrics Are Perfect: Adversarial Reward Learning For Visual Storytelling", "abstract": "<p>Though impressive results have been achieved in visual captioning, the task\nof generating abstract stories from photo streams is still a little-tapped\nproblem. Different from captions, stories have more expressive language styles\nand contain many imaginary concepts that do not appear in the images. Thus it\nposes challenges to behavioral cloning algorithms. Furthermore, due to the\nlimitations of automatic metrics on evaluating story quality, reinforcement\nlearning methods with hand-crafted rewards also face difficulties in gaining an\noverall performance boost. Therefore, we propose an Adversarial REward Learning\n(AREL) framework to learn an implicit reward function from human\ndemonstrations, and then optimize policy search with the learned reward\nfunction. Though automatic eval- uation indicates slight performance boost over\nstate-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation\nshows that our approach achieves significant improvement in generating more\nhuman-like stories than SOTA systems.</p>\n", "tags": ["Evaluation","Reinforcement Learning","Tools"] },
{"key": "wang2018robust", "citations": "103", "year": "2018", "title":"Robust Text-to-sql Generation With Execution-guided Decoding", "abstract": "<p>We consider the problem of neural semantic parsing, which translates natural\nlanguage questions into executable SQL queries. We introduce a new mechanism,\nexecution guidance, to leverage the semantics of SQL. It detects and excludes\nfaulty programs during the decoding procedure by conditioning on the execution\nof partially generated program. The mechanism can be used with any\nautoregressive generative model, which we demonstrate on four state-of-the-art\nrecurrent or template-based semantic parsing models. We demonstrate that\nexecution guidance universally improves model performance on various\ntext-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS,\nand GeoQuery. As a result, we achieve new state-of-the-art execution accuracy\nof 83.8% on WikiSQL.</p>\n", "tags": ["Datasets","Llm For Code","NAACL"] },
{"key": "wang2018semi", "citations": "80", "year": "2018", "title":"Semi-autoregressive Neural Machine Translation", "abstract": "<p>Existing approaches to neural machine translation are typically\nautoregressive models. While these models attain state-of-the-art translation\nquality, they are suffering from low parallelizability and thus slow at\ndecoding long sequences. In this paper, we propose a novel model for fast\nsequence generation — the semi-autoregressive Transformer (SAT). The SAT\nkeeps the autoregressive property in global but relieves in local and thus is\nable to produce multiple successive words in parallel at each time step.\nExperiments conducted on English-German and Chinese-English translation tasks\nshow that the SAT achieves a good balance between translation quality and\ndecoding speed. On WMT’14 English-German translation, the SAT achieves\n5.58\\(\\times\\) speedup while maintains 88% translation quality, significantly\nbetter than the previous non-autoregressive methods. When produces two words at\neach time step, the SAT is almost lossless (only 1% degeneration in BLEU\nscore).</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "wang2018switchout", "citations": "203", "year": "2018", "title":"Switchout: An Efficient Data Augmentation Algorithm For Neural Machine Translation", "abstract": "<p>In this work, we examine methods for data augmentation for text-based tasks\nsuch as neural machine translation (NMT). We formulate the design of a data\naugmentation policy with desirable properties as an optimization problem, and\nderive a generic analytic solution. This solution not only subsumes some\nexisting augmentation schemes, but also leads to an extremely simple data\naugmentation strategy for NMT: randomly replacing words in both the source\nsentence and the target sentence with other random words from their\ncorresponding vocabularies. We name this method SwitchOut. Experiments on three\ntranslation datasets of different scales show that SwitchOut yields consistent\nimprovements of about 0.5 BLEU, achieving better or comparable performances to\nstrong alternatives such as word dropout (Sennrich et al., 2016a). Code to\nimplement this method is included in the appendix.</p>\n", "tags": ["Datasets","EMNLP","Efficiency"] },
{"key": "wang2018task", "citations": "67", "year": "2019", "title":"A Task In A Suit And A Tie: Paraphrase Generation With Semantic Augmentation", "abstract": "<p>Paraphrasing is rooted in semantics. We show the effectiveness of\ntransformers (Vaswani et al. 2017) for paraphrase generation and further\nimprovements by incorporating PropBank labels via a multi-encoder. Evaluating\non MSCOCO and WikiAnswers, we find that transformers are fast and effective,\nand that semantic augmentation for both transformers and LSTMs leads to sizable\n2-3 point gains in BLEU, METEOR and TER. More importantly, we find surprisingly\nlarge gains on human evaluations compared to previous models. Nevertheless,\nmanual inspection of generated paraphrases reveals ample room for improvement:\neven our best model produces human-acceptable paraphrases for only 28% of\ncaptions from the CHIA dataset (Sharma et al. 2018), and it fails spectacularly\non sentences from Wikipedia. Overall, these results point to the potential for\nincorporating semantics in the task while highlighting the need for stronger\nevaluation.</p>\n", "tags": ["AAAI","Datasets","Evaluation"] },
{"key": "wang2018yuanfudao", "citations": "83", "year": "2018", "title":"Yuanfudao At Semeval-2018 Task 11: Three-way Attention And Relational Knowledge For Commonsense Machine Comprehension", "abstract": "<p>This paper describes our system for SemEval-2018 Task 11: Machine\nComprehension using Commonsense Knowledge. We use Three-way Attentive Networks\n(TriAN) to model interactions between the passage, question and answers. To\nincorporate commonsense knowledge, we augment the input with relation embedding\nfrom the graph of general knowledge ConceptNet (Speer et al., 2017). As a\nresult, our system achieves state-of-the-art performance with 83.95% accuracy\non the official test data. Code is publicly available at\nhttps://github.com/intfloat/commonsense-rc</p>\n", "tags": ["Evaluation","Has Code","Model Architecture"] },
{"key": "wang2019adversarial", "citations": "60", "year": "2019", "title":"Adversarial Domain Adaptation For Machine Reading Comprehension", "abstract": "<p>In this paper, we focus on unsupervised domain adaptation for Machine Reading\nComprehension (MRC), where the source domain has a large amount of labeled\ndata, while only unlabeled passages are available in the target domain. To this\nend, we propose an Adversarial Domain Adaptation framework (AdaMRC), where\n(\\(i\\)) pseudo questions are first generated for unlabeled passages in the target\ndomain, and then (\\(ii\\)) a domain classifier is incorporated into an MRC model\nto predict which domain a given passage-question pair comes from. The\nclassifier and the passage-question encoder are jointly trained using\nadversarial learning to enforce domain-invariant representation learning.\nComprehensive evaluations demonstrate that our approach (\\(i\\)) is generalizable\nto different MRC models and datasets, (\\(ii\\)) can be combined with pre-trained\nlarge-scale language models (such as ELMo and BERT), and (\\(iii\\)) can be\nextended to semi-supervised learning.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "wang2019bridging", "citations": "82", "year": "2020", "title":"Bridging The Gap Between Pre-training And Fine-tuning For End-to-end Speech Translation", "abstract": "<p>End-to-end speech translation, a hot topic in recent years, aims to translate\na segment of audio into a specific language with an end-to-end model.\nConventional approaches employ multi-task learning and pre-training methods for\nthis task, but they suffer from the huge gap between pre-training and\nfine-tuning. To address these issues, we propose a Tandem Connectionist\nEncoding Network (TCEN) which bridges the gap by reusing all subnets in\nfine-tuning, keeping the roles of subnets consistent, and pre-training the\nattention module. Furthermore, we propose two simple but effective methods to\nguarantee the speech encoder outputs and the MT encoder inputs are consistent\nin terms of semantic representation and sequence length. Experimental results\nshow that our model outperforms baselines 2.2 BLEU on a large benchmark\ndataset.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wang2019does", "citations": "112", "year": "2019", "title":"Does It Make Sense? And Why? A Pilot Study For Sense Making And Explanation", "abstract": "<p>Introducing common sense to natural language understanding systems has\nreceived increasing research attention. It remains a fundamental question on\nhow to evaluate whether a system has a sense making capability. Existing\nbenchmarks measures commonsense knowledge indirectly and without explanation.\nIn this paper, we release a benchmark to directly test whether a system can\ndifferentiate natural language statements that make sense from those that do\nnot make sense. In addition, a system is asked to identify the most crucial\nreason why a statement does not make sense. We evaluate models trained over\nlarge-scale language modeling tasks as well as human performance, showing that\nthere are different challenges for system sense making.</p>\n", "tags": ["Evaluation"] },
{"key": "wang2019espresso", "citations": "68", "year": "2019", "title":"Espresso: A Fast End-to-end Neural Speech Recognition Toolkit", "abstract": "<p>We present Espresso, an open-source, modular, extensible end-to-end neural\nautomatic speech recognition (ASR) toolkit based on the deep learning library\nPyTorch and the popular neural machine translation toolkit fairseq. Espresso\nsupports distributed training across GPUs and computing nodes, and features\nvarious decoding approaches commonly employed in ASR, including look-ahead\nword-based language model fusion, for which a fast, parallelized decoder is\nimplemented. Espresso achieves state-of-the-art ASR performance on the WSJ,\nLibriSpeech, and Switchboard data sets among other end-to-end systems without\ndata augmentation, and is 4–11x faster for decoding than similar systems (e.g.\nESPnet).</p>\n", "tags": ["ASRU","Tools","Training Techniques"] },
{"key": "wang2019fine", "citations": "115", "year": "2019", "title":"Fine-tune Bert For Docred With Two-step Process", "abstract": "<p>Modelling relations between multiple entities has attracted increasing\nattention recently, and a new dataset called DocRED has been collected in order\nto accelerate the research on the document-level relation extraction. Current\nbaselines for this task uses BiLSTM to encode the whole document and are\ntrained from scratch. We argue that such simple baselines are not strong enough\nto model to complex interaction between entities. In this paper, we further\napply a pre-trained language model (BERT) to provide a stronger baseline for\nthis task. We also find that solving this task in phases can further improve\nthe performance. The first step is to predict whether or not two entities have\na relation, the second step is to predict the specific relation.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture"] },
{"key": "wang2019kepler", "citations": "506", "year": "2021", "title":"KEPLER: A Unified Model For Knowledge Embedding And Pre-trained Language Representation", "abstract": "<p>Pre-trained language representation models (PLMs) cannot well capture factual\nknowledge from text. In contrast, knowledge embedding (KE) methods can\neffectively represent the relational facts in knowledge graphs (KGs) with\ninformative entity embeddings, but conventional KE models cannot take full\nadvantage of the abundant textual information. In this paper, we propose a\nunified model for Knowledge Embedding and Pre-trained LanguagE Representation\n(KEPLER), which can not only better integrate factual knowledge into PLMs but\nalso produce effective text-enhanced KE with the strong PLMs. In KEPLER, we\nencode textual entity descriptions with a PLM as their embeddings, and then\njointly optimize the KE and language modeling objectives. Experimental results\nshow that KEPLER achieves state-of-the-art performances on various NLP tasks,\nand also works remarkably well as an inductive KE model on KG link prediction.\nFurthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a\nlarge-scale KG dataset with aligned entity descriptions, and benchmark\nstate-of-the-art KE methods on it. It shall serve as a new KE benchmark and\nfacilitate the research on large KG, inductive KE, and KG with text. The source\ncode can be obtained from https://github.com/THU-KEG/KEPLER.</p>\n", "tags": ["Datasets","Evaluation","Has Code","TACL","Training Techniques"] },
{"key": "wang2019language", "citations": "66", "year": "2019", "title":"Language Models With Transformers", "abstract": "<p>The Transformer architecture is superior to RNN-based models in computational\nefficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer\nmodels on various NLP tasks using pre-trained language models on large-scale\ncorpora. Surprisingly, these Transformer architectures are suboptimal for\nlanguage model itself. Neither self-attention nor the positional encoding in\nthe Transformer is able to efficiently incorporate the word-level sequential\ncontext crucial to language modeling.\n  In this paper, we explore effective Transformer architectures for language\nmodel, including adding additional LSTM layers to better capture the sequential\ncontext while still keeping the computation efficient. We propose Coordinate\nArchitecture Search (CAS) to find an effective architecture through iterative\nrefinement of the model. Experimental results on the PTB, WikiText-2, and\nWikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all\nproblems, i.e. on average an improvement of 12.0 perplexity units compared to\nstate-of-the-art LSTMs. The source code is publicly available.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "wang2019learning", "citations": "566", "year": "2019", "title":"Learning From Explanations With Neural Execution Tree", "abstract": "<p>While deep neural networks have achieved impressive performance on a range of\nNLP tasks, these data-hungry models heavily rely on labeled data, which\nrestricts their applications in scenarios where data annotation is expensive.\nNatural language (NL) explanations have been demonstrated very useful\nadditional supervision, which can provide sufficient domain knowledge for\ngenerating more labeled data over new instances, while the annotation time only\ndoubles. However, directly applying them for augmenting model learning\nencounters two challenges: (1) NL explanations are unstructured and inherently\ncompositional, which asks for a modularized model to represent their semantics,\n(2) NL explanations often have large numbers of linguistic variants, resulting\nin low recall and limited generalization ability. In this paper, we propose a\nnovel Neural Execution Tree (NExT) framework to augment training data for text\nclassification using NL explanations. After transforming NL explanations into\nexecutable logical forms by semantic parsing, NExT generalizes different types\nof actions specified by the logical forms for labeling data instances, which\nsubstantially increases the coverage of each NL explanation. Experiments on two\nNLP tasks (relation extraction and sentiment analysis) demonstrate its\nsuperiority over baseline methods. Its extension to multi-hop question\nanswering achieves performance gain with light annotation effort.</p>\n", "tags": ["Applications","Datasets","Tools","Training Techniques"] },
{"key": "wang2019r", "citations": "88", "year": "2019", "title":"R-transformer: Recurrent Neural Network Enhanced Transformer", "abstract": "<p>Recurrent Neural Networks have long been the dominating choice for sequence\nmodeling. However, it severely suffers from two issues: impotent in capturing\nvery long-term dependencies and unable to parallelize the sequential\ncomputation procedure. Therefore, many non-recurrent sequence models that are\nbuilt on convolution and attention operations have been proposed recently.\nNotably, models with multi-head attention such as Transformer have demonstrated\nextreme effectiveness in capturing long-term dependencies in a variety of\nsequence modeling tasks. Despite their success, however, these models lack\nnecessary components to model local structures in sequences and heavily rely on\nposition embeddings that have limited effects and require a considerable amount\nof design efforts. In this paper, we propose the R-Transformer which enjoys the\nadvantages of both RNNs and the multi-head attention mechanism while avoids\ntheir respective drawbacks. The proposed model can effectively capture both\nlocal structures and global long-term dependencies in sequences without any use\nof position embeddings. We evaluate R-Transformer through extensive experiments\nwith data from a wide range of domains and the empirical results show that\nR-Transformer outperforms the state-of-the-art methods by a large margin in\nmost of the tasks. We have made the code publicly available at\nhttps://github.com/DSE-MSU/R-transformer.</p>\n", "tags": ["Has Code","Model Architecture","Time Series"] },
{"key": "wang2019sentence", "citations": "104", "year": "2019", "title":"Sentence Embedding Alignment For Lifelong Relation Extraction", "abstract": "<p>Conventional approaches to relation extraction usually require a fixed set of\npre-defined relations. Such requirement is hard to meet in many real\napplications, especially when new data and relations are emerging incessantly\nand it is computationally expensive to store all data and re-train the whole\nmodel every time new data and relations come in. We formulate such a\nchallenging problem as lifelong relation extraction and investigate\nmemory-efficient incremental learning methods without catastrophically\nforgetting knowledge learned from previous tasks. We first investigate a\nmodified version of the stochastic gradient methods with a replay memory, which\nsurprisingly outperforms recent state-of-the-art lifelong learning methods. We\nfurther propose to improve this approach to alleviate the forgetting problem by\nanchoring the sentence embedding space. Specifically, we utilize an explicit\nalignment model to mitigate the sentence embedding distortion of the learned\nmodel when training on new data and new relations. Experiment results on\nmultiple benchmarks show that our proposed method significantly outperforms the\nstate-of-the-art lifelong learning approaches.</p>\n", "tags": ["Applications","Training Techniques"] },
{"key": "wang2019structbert", "citations": "138", "year": "2019", "title":"Structbert: Incorporating Language Structures Into Pre-training For Deep Language Understanding", "abstract": "<p>Recently, the pre-trained language model, BERT (and its robustly optimized\nversion RoBERTa), has attracted a lot of attention in natural language\nunderstanding (NLU), and achieved state-of-the-art accuracy in various NLU\ntasks, such as sentiment classification, natural language inference, semantic\ntextual similarity and question answering. Inspired by the linearization\nexploration work of Elman [8], we extend BERT to a new model, StructBERT, by\nincorporating language structures into pre-training. Specifically, we pre-train\nStructBERT with two auxiliary tasks to make the most of the sequential order of\nwords and sentences, which leverage language structures at the word and\nsentence levels, respectively. As a result, the new model is adapted to\ndifferent levels of language understanding required by downstream tasks. The\nStructBERT with structural pre-training gives surprisingly good empirical\nresults on a variety of downstream tasks, including pushing the\nstate-of-the-art on the GLUE benchmark to 89.0 (outperforming all published\nmodels), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on\nSNLI to 91.7.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "wang2019structured", "citations": "92", "year": "2020", "title":"Structured Pruning Of Large Language Models", "abstract": "<p>Large language models have recently achieved state of the art performance\nacross a wide variety of natural language tasks. Meanwhile, the size of these\nmodels and their latency have significantly increased, which makes their usage\ncostly, and raises an interesting question: do language models need to be\nlarge? We study this question through the lens of model compression. We present\na generic, structured pruning approach by parameterizing each weight matrix\nusing its low-rank factorization, and adaptively removing rank-1 components\nduring training. On language modeling tasks, our structured approach\noutperforms other unstructured and block-structured pruning baselines at\nvarious compression levels, while achieving significant speedups during both\ntraining and inference. We also demonstrate that our method can be applied to\npruning adaptive word embeddings in large language models, and to pruning the\nBERT model on several downstream fine-tuning classification benchmarks.</p>\n", "tags": ["EMNLP","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wang2019superglue", "citations": "921", "year": "2019", "title":"Superglue: A Stickier Benchmark For General-purpose Language Understanding Systems", "abstract": "<p>In the last year, new models and methods for pretraining and transfer\nlearning have driven striking performance improvements across a range of\nlanguage understanding tasks. The GLUE benchmark, introduced a little over one\nyear ago, offers a single-number metric that summarizes progress on a diverse\nset of such tasks, but performance on the benchmark has recently surpassed the\nlevel of non-expert humans, suggesting limited headroom for further research.\nIn this paper we present SuperGLUE, a new benchmark styled after GLUE with a\nnew set of more difficult language understanding tasks, a software toolkit, and\na public leaderboard. SuperGLUE is available at super.gluebenchmark.com.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "wang2019topic", "citations": "62", "year": "2019", "title":"Topic-aware Neural Keyphrase Generation For Social Media Language", "abstract": "<p>A huge volume of user-generated content is daily produced on social media. To\nfacilitate automatic language understanding, we study keyphrase prediction,\ndistilling salient information from massive posts. While most existing methods\nextract words from source posts to form keyphrases, we propose a\nsequence-to-sequence (seq2seq) based neural keyphrase generation framework,\nenabling absent keyphrases to be created. Moreover, our model, being\ntopic-aware, allows joint modeling of corpus-level latent topic\nrepresentations, which helps alleviate the data sparsity that widely exhibited\nin social media language. Experiments on three datasets collected from English\nand Chinese social media platforms show that our model significantly\noutperforms both extraction and generation models that do not exploit latent\ntopics. Further discussions show that our model learns meaningful topics, which\ninterprets its superiority in social media keyphrase generation.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "wang2019translating", "citations": "71", "year": "2020", "title":"Translating Math Formula Images To Latex Sequences Using Deep Neural Networks With Sequence-level Training", "abstract": "<p>In this paper we propose a deep neural network model with an encoder-decoder\narchitecture that translates images of math formulas into their LaTeX markup\nsequences. The encoder is a convolutional neural network (CNN) that transforms\nimages into a group of feature maps. To better capture the spatial\nrelationships of math symbols, the feature maps are augmented with 2D\npositional encoding before being unfolded into a vector. The decoder is a\nstacked bidirectional long short-term memory (LSTM) model integrated with the\nsoft attention mechanism, which works as a language model to translate the\nencoder output into a sequence of LaTeX tokens. The neural network is trained\nin two steps. The first step is token-level training using the\nMaximum-Likelihood Estimation (MLE) as the objective function. At completion of\nthe token-level training, the sequence-level training objective function is\nemployed to optimize the overall model based on the policy gradient algorithm\nfrom reinforcement learning. Our design also overcomes the exposure bias\nproblem by closing the feedback loop in the decoder during sequence-level\ntraining, i.e., feeding in the predicted token instead of the ground truth\ntoken at every time step. The model is trained and evaluated on the\nIM2LATEX-100K dataset and shows state-of-the-art performance on both\nsequence-based and image-based evaluation metrics.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "wang2019tree", "citations": "123", "year": "2019", "title":"Tree Transformer: Integrating Tree Structures Into Self-attention", "abstract": "<p>Pre-training Transformer from large-scale raw texts and fine-tuning on the\ndesired task have achieved state-of-the-art results on diverse NLP tasks.\nHowever, it is unclear what the learned attention captures. The attention\ncomputed by attention heads seems not to match human intuitions about\nhierarchical structures. This paper proposes Tree Transformer, which adds an\nextra constraint to attention heads of the bidirectional Transformer encoder in\norder to encourage the attention heads to follow tree structures. The tree\nstructures can be automatically induced from raw texts by our proposed\n“Constituent Attention” module, which is simply implemented by self-attention\nbetween two adjacent words. With the same training procedure identical to BERT,\nthe experiments demonstrate the effectiveness of Tree Transformer in terms of\ninducing tree structures, better language modeling, and further learning more\nexplainable attention scores.</p>\n", "tags": ["EMNLP","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wang2020asking", "citations": "290", "year": "2020", "title":"Asking And Answering Questions To Evaluate The Factual Consistency Of Summaries", "abstract": "<p>Practical applications of abstractive summarization models are limited by\nfrequent factual inconsistencies with respect to their input. Existing\nautomatic evaluation metrics for summarization are largely insensitive to such\nerrors. We propose an automatic evaluation protocol called QAGS (pronounced\n“kags”) that is designed to identify factual inconsistencies in a generated\nsummary. QAGS is based on the intuition that if we ask questions about a\nsummary and its source, we will receive similar answers if the summary is\nfactually consistent with the source. To evaluate QAGS, we collect human\njudgments of factual consistency on model-generated summaries for the\nCNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018)\nsummarization datasets. QAGS has substantially higher correlations with these\njudgments than other automatic evaluation metrics. Also, QAGS offers a natural\nform of interpretability: The answers and questions generated while computing\nQAGS indicate which tokens of a summary are inconsistent and why. We believe\nQAGS is a promising tool in automatically generating usable and factually\nconsistent text.</p>\n", "tags": ["Applications","Datasets","Evaluation"] },
{"key": "wang2020automated", "citations": "121", "year": "2021", "title":"Automated Concatenation Of Embeddings For Structured Prediction", "abstract": "<p>Pretrained contextualized embeddings are powerful word representations for\nstructured prediction tasks. Recent work found that better word representations\ncan be obtained by concatenating different types of embeddings. However, the\nselection of embeddings to form the best concatenated representation usually\nvaries depending on the task and the collection of candidate embeddings, and\nthe ever-increasing number of embedding types makes it a more difficult\nproblem. In this paper, we propose Automated Concatenation of Embeddings (ACE)\nto automate the process of finding better concatenations of embeddings for\nstructured prediction tasks, based on a formulation inspired by recent progress\non neural architecture search. Specifically, a controller alternately samples a\nconcatenation of embeddings, according to its current belief of the\neffectiveness of individual embedding types in consideration for a task, and\nupdates the belief based on a reward. We follow strategies in reinforcement\nlearning to optimize the parameters of the controller and compute the reward\nbased on the accuracy of a task model, which is fed with the sampled\nconcatenation as input and trained on a task dataset. Empirical results on 6\ntasks and 21 datasets show that our approach outperforms strong baselines and\nachieves state-of-the-art performance with fine-tuned embeddings in all the\nevaluations.</p>\n", "tags": ["Datasets","Model Architecture","Reinforcement Learning"] },
{"key": "wang2020balancing", "citations": "77", "year": "2020", "title":"Balancing Training For Multilingual Neural Machine Translation", "abstract": "<p>When training multilingual machine translation (MT) models that can translate\nto/from multiple languages, we are faced with imbalanced training sets: some\nlanguages have much more training data than others. Standard practice is to\nup-sample less resourced languages to increase representation, and the degree\nof up-sampling has a large effect on the overall performance. In this paper, we\npropose a method that instead automatically learns how to weight training data\nthrough a data scorer that is optimized to maximize performance on all test\nlanguages. Experiments on two sets of languages under both one-to-many and\nmany-to-one MT settings show our method not only consistently outperforms\nheuristic baselines in terms of average performance, but also offers flexible\ncontrol over the performance of which languages are optimized.</p>\n", "tags": ["Training Techniques"] },
{"key": "wang2020curriculum", "citations": "92", "year": "2020", "title":"Curriculum Pre-training For End-to-end Speech Translation", "abstract": "<p>End-to-end speech translation poses a heavy burden on the encoder, because it\nhas to transcribe, understand, and learn cross-lingual semantics\nsimultaneously. To obtain a powerful encoder, traditional methods pre-train it\non ASR data to capture speech features. However, we argue that pre-training the\nencoder only through simple speech recognition is not enough and high-level\nlinguistic knowledge should be considered. Inspired by this, we propose a\ncurriculum pre-training method that includes an elementary course for\ntranscription learning and two advanced courses for understanding the utterance\nand mapping words in two languages. The difficulty of these courses is\ngradually increasing. Experiments show that our curriculum pre-training method\nleads to significant improvements on En-De and En-Fr speech translation\nbenchmarks.</p>\n", "tags": ["Training Techniques"] },
{"key": "wang2020fairseq", "citations": "95", "year": "2020", "title":"Fairseq S2T: Fast Speech-to-text Modeling With Fairseq", "abstract": "<p>We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)\nmodeling tasks such as end-to-end speech recognition and speech-to-text\ntranslation. It follows fairseq’s careful design for scalability and\nextensibility. We provide end-to-end workflows from data pre-processing, model\ntraining to offline (online) inference. We implement state-of-the-art\nRNN-based, Transformer-based as well as Conformer-based models and open-source\ndetailed training recipes. Fairseq’s machine translation models and language\nmodels can be seamlessly integrated into S2T workflows for multi-task learning\nor transfer learning. Fairseq S2T documentation and examples are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.</p>\n", "tags": ["Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "wang2020hat", "citations": "193", "year": "2020", "title":"HAT: Hardware-aware Transformers For Efficient Natural Language Processing", "abstract": "<p>Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but\nthey are difficult to be deployed on hardware due to the intensive computation.\nTo enable low-latency inference on resource-constrained hardware platforms, we\npropose to design Hardware-Aware Transformers (HAT) with neural architecture\nsearch. We first construct a large design space with \\(\\textit{arbitrary\nencoder-decoder attention}\\) and \\(\\textit{heterogeneous layers}\\). Then we train\na \\(\\textit{SuperTransformer}\\) that covers all candidates in the design space,\nand efficiently produces many \\(\\textit{SubTransformers}\\) with weight sharing.\nFinally, we perform an evolutionary search with a hardware latency constraint\nto find a specialized \\(\\textit{SubTransformer}\\) dedicated to run fast on the\ntarget hardware. Extensive experiments on four machine translation tasks\ndemonstrate that HAT can discover efficient models for different hardware (CPU,\nGPU, IoT device). When running WMT’14 translation task on Raspberry Pi-4, HAT\ncan achieve \\(\\textbf{3}\\times\\) speedup, \\(\\textbf{3.7}\\times\\) smaller size over\nbaseline Transformer; \\(\\textbf{2.7}\\times\\) speedup, \\(\\textbf{3.6}\\times\\)\nsmaller size over Evolved Transformer with \\(\\textbf{12,041}\\times\\) less search\ncost and no performance loss. HAT code is\nhttps://github.com/mit-han-lab/hardware-aware-transformers.git</p>\n", "tags": ["Efficiency","Has Code","Model Architecture"] },
{"key": "wang2020inference", "citations": "63", "year": "2020", "title":"On The Inference Calibration Of Neural Machine Translation", "abstract": "<p>Confidence calibration, which aims to make model predictions equal to the\ntrue correctness measures, is important for neural machine translation (NMT)\nbecause it is able to offer useful indicators of translation errors in the\ngenerated output. While prior studies have shown that NMT models trained with\nlabel smoothing are well-calibrated on the ground-truth training data, we find\nthat miscalibration still remains a severe challenge for NMT during inference\ndue to the discrepancy between training and inference. By carefully designing\nexperiments on three language pairs, our work provides in-depth analyses of the\ncorrelation between calibration and translation performance as well as\nlinguistic properties of miscalibration and reports a number of interesting\nfindings that might help humans better analyze, understand and improve NMT\nmodels. Based on these observations, we further propose a new graduated label\nsmoothing method that can improve both inference calibration and translation\nperformance.</p>\n", "tags": ["Training Techniques"] },
{"key": "wang2020k", "citations": "341", "year": "2021", "title":"K-adapter: Infusing Knowledge Into Pre-trained Models With Adapters", "abstract": "<p>We study the problem of injecting knowledge into large pre-trained models\nlike BERT and RoBERTa. Existing methods typically update the original\nparameters of pre-trained models when injecting knowledge. However, when\nmultiple kinds of knowledge are injected, the historically injected knowledge\nwould be flushed away. To address this, we propose K-Adapter, a framework that\nretains the original parameters of the pre-trained model fixed and supports the\ndevelopment of versatile knowledge-infused model. Taking RoBERTa as the\nbackbone model, K-Adapter has a neural adapter for each kind of infused\nknowledge, like a plug-in connected to RoBERTa. There is no information flow\nbetween different adapters, thus multiple adapters can be efficiently trained\nin a distributed way. As a case study, we inject two kinds of knowledge in this\nwork, including (1) factual knowledge obtained from automatically aligned\ntext-triplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained\nvia dependency parsing. Results on three knowledge-driven tasks, including\nrelation classification, entity typing, and question answering, demonstrate\nthat each adapter improves the performance and the combination of both adapters\nbrings further improvements. Further analysis indicates that K-Adapter captures\nversatile knowledge than RoBERTa.</p>\n", "tags": ["Model Architecture","Tools"] },
{"key": "wang2020language", "citations": "80", "year": "2020", "title":"Language-mediated, Object-centric Representation Learning", "abstract": "<p>We present Language-mediated, Object-centric Representation Learning (LORL),\na paradigm for learning disentangled, object-centric scene representations from\nvision and language. LORL builds upon recent advances in unsupervised object\ndiscovery and segmentation, notably MONet and Slot Attention. While these\nalgorithms learn an object-centric representation just by reconstructing the\ninput image, LORL enables them to further learn to associate the learned\nrepresentations to concepts, i.e., words for object categories, properties, and\nspatial relationships, from language input. These object-centric concepts\nderived from language facilitate the learning of object-centric\nrepresentations. LORL can be integrated with various unsupervised object\ndiscovery algorithms that are language-agnostic. Experiments show that the\nintegration of LORL consistently improves the performance of unsupervised\nobject discovery methods on two datasets via the help of language. We also show\nthat concepts learned by LORL, in conjunction with object discovery methods,\naid downstream tasks such as referring expression comprehension.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "wang2020large", "citations": "95", "year": "2020", "title":"A Large-scale Chinese Short-text Conversation Dataset", "abstract": "<p>The advancements of neural dialogue generation models show promising results\non modeling short-text conversations. However, training such models usually\nneeds a large-scale high-quality dialogue corpus, which is hard to access. In\nthis paper, we present a large-scale cleaned Chinese conversation dataset,\nLCCC, which contains a base version (6.8million dialogues) and a large version\n(12.0 million dialogues). The quality of our dataset is ensured by a rigorous\ndata cleaning pipeline, which is built based on a set of rules and a classifier\nthat is trained on manually annotated 110K dialogue pairs. We also release\npre-training dialogue models which are trained on LCCC-base and LCCC-large\nrespectively. The cleaned dataset and the pre-training models will facilitate\nthe research of short-text conversation modeling. All the models and datasets\nare available at https://github.com/thu-coai/CDial-GPT.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn"] },
{"key": "wang2020linformer", "citations": "856", "year": "2020", "title":"Linformer: Self-attention With Linear Complexity", "abstract": "<p>Large transformer models have shown extraordinary success in achieving\nstate-of-the-art results in many natural language processing applications.\nHowever, training and deploying these models can be prohibitively costly for\nlong sequences, as the standard self-attention mechanism of the Transformer\nuses \\(O(n^2)\\) time and space with respect to sequence length. In this paper, we\ndemonstrate that the self-attention mechanism can be approximated by a low-rank\nmatrix. We further exploit this finding to propose a new self-attention\nmechanism, which reduces the overall self-attention complexity from \\(O(n^2)\\) to\n\\(O(n)\\) in both time and space. The resulting linear transformer, the\n\\textit{Linformer}, performs on par with standard Transformer models, while\nbeing much more memory- and time-efficient.</p>\n", "tags": ["Applications","Model Architecture","Training Techniques"] },
{"key": "wang2020minilm", "citations": "551", "year": "2020", "title":"Minilm: Deep Self-attention Distillation For Task-agnostic Compression Of Pre-trained Transformers", "abstract": "<p>Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its\nvariants) have achieved remarkable success in varieties of NLP tasks. However,\nthese models usually consist of hundreds of millions of parameters which brings\nchallenges for fine-tuning and online serving in real-life applications due to\nlatency and capacity constraints. In this work, we present a simple and\neffective approach to compress large Transformer (Vaswani et al., 2017) based\npre-trained models, termed as deep self-attention distillation. The small model\n(student) is trained by deeply mimicking the self-attention module, which plays\na vital role in Transformer networks, of the large model (teacher).\nSpecifically, we propose distilling the self-attention module of the last\nTransformer layer of the teacher, which is effective and flexible for the\nstudent. Furthermore, we introduce the scaled dot-product between values in the\nself-attention module as the new deep self-attention knowledge, in addition to\nthe attention distributions (i.e., the scaled dot-product of queries and keys)\nthat have been used in existing works. Moreover, we show that introducing a\nteacher assistant (Mirzadeh et al., 2019) also helps the distillation of large\npre-trained Transformer models. Experimental results demonstrate that our\nmonolingual model outperforms state-of-the-art baselines in different parameter\nsize of student models. In particular, it retains more than 99% accuracy on\nSQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer\nparameters and computations of the teacher model. We also obtain competitive\nresults in applying deep self-attention distillation to multilingual\npre-trained models.</p>\n", "tags": ["Applications","Datasets","Efficiency","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wang2020minilmv2", "citations": "184", "year": "2021", "title":"Minilmv2: Multi-head Self-attention Relation Distillation For Compressing Pretrained Transformers", "abstract": "<p>We generalize deep self-attention distillation in MiniLM (Wang et al., 2020)\nby only using self-attention relation distillation for task-agnostic\ncompression of pretrained Transformers. In particular, we define multi-head\nself-attention relations as scaled dot-product between the pairs of query, key,\nand value vectors within each self-attention module. Then we employ the above\nrelational knowledge to train the student model. Besides its simplicity and\nunified principle, more favorably, there is no restriction in terms of the\nnumber of student’s attention heads, while most previous work has to guarantee\nthe same head number between teacher and student. Moreover, the fine-grained\nself-attention relations tend to fully exploit the interaction knowledge\nlearned by Transformer. In addition, we thoroughly examine the layer selection\nstrategy for teacher models, rather than just relying on the last layer as in\nMiniLM. We conduct extensive experiments on compressing both monolingual and\nmultilingual pretrained models. Experimental results demonstrate that our\nmodels distilled from base-size and large-size teachers (BERT, RoBERTa and\nXLM-R) outperform the state-of-the-art.</p>\n", "tags": ["Efficiency","Model Architecture"] },
{"key": "wang2020spatten", "citations": "216", "year": "2021", "title":"Spatten: Efficient Sparse Attention Architecture With Cascade Token And Head Pruning", "abstract": "<p>The attention mechanism is becoming increasingly popular in Natural Language\nProcessing (NLP) applications, showing superior performance than convolutional\nand recurrent architectures. However, attention becomes the compution\nbottleneck because of its quadratic computational complexity to input length,\ncomplicated data movement and low arithmetic intensity. Moreover, existing NN\naccelerators mainly focus on optimizing convolutional or recurrent models, and\ncannot efficiently support attention. In this paper, we present SpAtten, an\nefficient algorithm-architecture co-design that leverages token sparsity, head\nsparsity, and quantization opportunities to reduce the attention computation\nand memory access. Inspired by the high redundancy of human languages, we\npropose the novel cascade token pruning to prune away unimportant tokens in the\nsentence. We also propose cascade head pruning to remove unessential heads.\nCascade pruning is fundamentally different from weight pruning since there is\nno trainable weight in the attention mechanism, and the pruned tokens and heads\nare selected on the fly. To efficiently support them on hardware, we design a\nnovel top-k engine to rank token and head importance scores with high\nthroughput. Furthermore, we propose progressive quantization that first fetches\nMSBs only and performs the computation; if the confidence is low, it fetches\nLSBs and recomputes the attention outputs, trading computation for memory\nreduction.\n  Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces\nDRAM access by 10.0x with no accuracy loss, and achieves 1.6x, 3.0x, 162x, 347x\nspeedup, and 1,4x, 3.2x, 1193x, 4059x energy savings over A3 accelerator,\nMNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.</p>\n", "tags": ["Applications","Efficiency","Model Architecture"] },
{"key": "wang2020transmodality", "citations": "115", "year": "2020", "title":"Transmodality: An End2end Fusion Method With Transformer For Multimodal Sentiment Analysis", "abstract": "<p>Multimodal sentiment analysis is an important research area that predicts\nspeaker’s sentiment tendency through features extracted from textual, visual\nand acoustic modalities. The central challenge is the fusion method of the\nmultimodal information. A variety of fusion methods have been proposed, but few\nof them adopt end-to-end translation models to mine the subtle correlation\nbetween modalities. Enlightened by recent success of Transformer in the area of\nmachine translation, we propose a new fusion method, TransModality, to address\nthe task of multimodal sentiment analysis. We assume that translation between\nmodalities contributes to a better joint representation of speaker’s utterance.\nWith Transformer, the learned features embody the information both from the\nsource modality and the target modality. We validate our model on multiple\nmultimodal datasets: CMU-MOSI, MELD, IEMOCAP. The experiments show that our\nproposed method achieves the state-of-the-art performance.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "wang2020vd", "citations": "63", "year": "2020", "title":"VD-BERT: A Unified Vision And Dialog Transformer With BERT", "abstract": "<p>Visual dialog is a challenging vision-language task, where a dialog agent\nneeds to answer a series of questions through reasoning on the image content\nand dialog history. Prior work has mostly focused on various attention\nmechanisms to model such intricate interactions. By contrast, in this work, we\npropose VD-BERT, a simple yet effective framework of unified vision-dialog\nTransformer that leverages the pretrained BERT language models for Visual\nDialog tasks. The model is unified in that (1) it captures all the interactions\nbetween the image and the multi-turn dialog using a single-stream Transformer\nencoder, and (2) it supports both answer ranking and answer generation\nseamlessly through the same architecture. More crucially, we adapt BERT for the\neffective fusion of vision and dialog contents via visually grounded training.\nWithout the need of pretraining on external vision-language data, our model\nyields new state of the art, achieving the top position in both single-model\nand ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog\nleaderboard. Our code and pretrained models are released at\nhttps://github.com/salesforce/VD-BERT.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP","Evaluation","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "wang2020what", "citations": "68", "year": "2020", "title":"What Do Position Embeddings Learn? An Empirical Study Of Pre-trained Language Model Positional Encoding", "abstract": "<p>In recent years, pre-trained Transformers have dominated the majority of NLP\nbenchmark tasks. Many variants of pre-trained Transformers have kept breaking\nout, and most focus on designing different pre-training objectives or variants\nof self-attention. Embedding the position information in the self-attention\nmechanism is also an indispensable factor in Transformers however is often\ndiscussed at will. Therefore, this paper carries out an empirical study on\nposition embeddings of mainstream pre-trained Transformers, which mainly\nfocuses on two questions: 1) Do position embeddings really learn the meaning of\npositions? 2) How do these different learned position embeddings affect\nTransformers for NLP tasks? This paper focuses on providing a new insight of\npre-trained position embeddings through feature-level analysis and empirical\nexperiments on most of iconic NLP tasks. It is believed that our experimental\nresults can guide the future work to choose the suitable positional encoding\nfunction for specific tasks given the application property.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture","Training Techniques"] },
{"key": "wang2021clip", "citations": "231", "year": "2022", "title":"Clip-nerf: Text-and-image Driven Manipulation Of Neural Radiance Fields", "abstract": "<p>We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural\nradiance fields (NeRF). By leveraging the joint language-image embedding space\nof the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose\na unified framework that allows manipulating NeRF in a user-friendly way, using\neither a short text prompt or an exemplar image. Specifically, to combine the\nnovel view synthesis capability of NeRF and the controllable manipulation\nability of latent representations from generative models, we introduce a\ndisentangled conditional NeRF architecture that allows individual control over\nboth shape and appearance. This is achieved by performing the shape\nconditioning via applying a learned deformation field to the positional\nencoding and deferring color conditioning to the volumetric rendering stage. To\nbridge this disentangled latent representation to the CLIP embedding, we design\ntwo code mappers that take a CLIP embedding as input and update the latent\ncodes to reflect the targeted editing. The mappers are trained with a\nCLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we\npropose an inverse optimization method that accurately projects an input image\nto the latent codes for manipulation to enable editing on real images. We\nevaluate our approach by extensive experiments on a variety of text prompts and\nexemplar images and also provide an intuitive interface for interactive\nediting. Our implementation is available at\nhttps://cassiepython.github.io/clipnerf/</p>\n", "tags": ["CVPR","Has Code","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "wang2021codet5", "citations": "720", "year": "2021", "title":"Codet5: Identifier-aware Unified Pre-trained Encoder-decoder Models For Code Understanding And Generation", "abstract": "<p>Pre-trained models for Natural Languages (NL) like BERT and GPT have been\nrecently shown to transfer well to Programming Languages (PL) and largely\nbenefit a broad set of code-related tasks. Despite their success, most current\nmethods either rely on an encoder-only (or decoder-only) pre-training that is\nsuboptimal for generation (resp. understanding) tasks or process the code\nsnippet in the same way as NL, neglecting the special characteristics of PL\nsuch as token types. We present CodeT5, a unified pre-trained encoder-decoder\nTransformer model that better leverages the code semantics conveyed from the\ndeveloper-assigned identifiers. Our model employs a unified framework to\nseamlessly support both code understanding and generation tasks and allows for\nmulti-task learning. Besides, we propose a novel identifier-aware pre-training\ntask that enables the model to distinguish which code tokens are identifiers\nand to recover them when they are masked. Furthermore, we propose to exploit\nthe user-written code comments with a bimodal dual generation task for better\nNL-PL alignment. Comprehensive experiments show that CodeT5 significantly\noutperforms prior methods on understanding tasks such as code defect detection\nand clone detection, and generation tasks across various directions including\nPL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better\ncapture semantic information from code. Our code and pre-trained models are\nreleased at https: //github.com/salesforce/CodeT5 .</p>\n", "tags": ["EMNLP","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "wang2021cris", "citations": "206", "year": "2022", "title":"CRIS: Clip-driven Referring Image Segmentation", "abstract": "<p>Referring image segmentation aims to segment a referent via a natural\nlinguistic expression.Due to the distinct data properties between text and\nimage, it is challenging for a network to well align text and pixel-level\nfeatures. Existing approaches use pretrained models to facilitate learning, yet\nseparately transfer the language/vision knowledge from pretrained models,\nignoring the multi-modal corresponding information. Inspired by the recent\nadvance in Contrastive Language-Image Pretraining (CLIP), in this paper, we\npropose an end-to-end CLIP-Driven Referring Image Segmentation framework\n(CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to\nvision-language decoding and contrastive learning for achieving the\ntext-to-pixel alignment. More specifically, we design a vision-language decoder\nto propagate fine-grained semantic information from textual representations to\neach pixel-level activation, which promotes consistency between the two\nmodalities. In addition, we present text-to-pixel contrastive learning to\nexplicitly enforce the text feature similar to the related pixel-level features\nand dissimilar to the irrelevances. The experimental results on three benchmark\ndatasets demonstrate that our proposed framework significantly outperforms the\nstate-of-the-art performance without any post-processing. The code will be\nreleased.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Tools"] },
{"key": "wang2021documentation", "citations": "60", "year": "2022", "title":"Documentation Matters: Human-centered AI System To Assist Data Science Code Documentation In Computational Notebooks", "abstract": "<p>Computational notebooks allow data scientists to express their ideas through\na combination of code and documentation. However, data scientists often pay\nattention only to the code, and neglect creating or updating their\ndocumentation during quick iterations. Inspired by human documentation\npractices learned from 80 highly-voted Kaggle notebooks, we design and\nimplement Themisto, an automated documentation generation system to explore how\nhuman-centered AI systems can support human data scientists in the machine\nlearning code documentation scenario. Themisto facilitates the creation of\ndocumentation via three approaches: a deep-learning-based approach to generate\ndocumentation for source code, a query-based approach to retrieve online API\ndocumentation for source code, and a user prompt approach to nudge users to\nwrite documentation. We evaluated Themisto in a within-subjects experiment with\n24 data science practitioners, and found that automated documentation\ngeneration techniques reduced the time for writing documentation, reminded\nparticipants to document code they would have ignored, and improved\nparticipants’ satisfaction with their computational notebook.</p>\n", "tags": ["Llm For Code","Model Architecture","Prompting","Tools"] },
{"key": "wang2021entailment", "citations": "105", "year": "2021", "title":"Entailment As Few-shot Learner", "abstract": "<p>Large pre-trained language models (LMs) have demonstrated remarkable ability\nas few-shot learners. However, their success hinges largely on scaling model\nparameters to a degree that makes it challenging to train and serve. In this\npaper, we propose a new approach, named as EFL, that can turn small LMs into\nbetter few-shot learners. The key idea of this approach is to reformulate\npotential NLP task into an entailment one, and then fine-tune the model with as\nlittle as 8 examples. We further demonstrate our proposed method can be: (i)\nnaturally combined with an unsupervised contrastive learning-based data\naugmentation method; (ii) easily extended to multilingual few-shot learning. A\nsystematic evaluation on 18 standard NLP tasks demonstrates that this approach\nimproves the various existing SOTA few-shot learning methods by 12%, and\nyields competitive few-shot performance with 500 times larger models, such as\nGPT-3.</p>\n", "tags": ["Evaluation","Few-Shot","Model Architecture"] },
{"key": "wang2021measure", "citations": "66", "year": "2022", "title":"Measure And Improve Robustness In NLP Models: A Survey", "abstract": "<p>As NLP models achieved state-of-the-art performances over benchmarks and\ngained wide applications, it has been increasingly important to ensure the safe\ndeployment of these models in the real world, e.g., making sure the models are\nrobust against unseen or challenging scenarios. Despite robustness being an\nincreasingly studied topic, it has been separately explored in applications\nlike vision and NLP, with various definitions, evaluation and mitigation\nstrategies in multiple lines of research. In this paper, we aim to provide a\nunifying survey of how to define, measure and improve robustness in NLP. We\nfirst connect multiple definitions of robustness, then unify various lines of\nwork on identifying robustness failures and evaluating models’ robustness.\nCorrespondingly, we present mitigation strategies that are data-driven,\nmodel-driven, and inductive-prior-based, with a more systematic view of how to\neffectively improve robustness in NLP models. Finally, we conclude by outlining\nopen challenges and future directions to motivate further research in this\narea.</p>\n", "tags": ["Applications","Evaluation","NAACL","Security","Survey Paper"] },
{"key": "wang2021screen2words", "citations": "70", "year": "2021", "title":"Screen2words: Automatic Mobile UI Summarization With Multimodal Learning", "abstract": "<p>Mobile User Interface Summarization generates succinct language descriptions\nof mobile screens for conveying important contents and functionalities of the\nscreen, which can be useful for many language-based application scenarios. We\npresent Screen2Words, a novel screen summarization approach that automatically\nencapsulates essential information of a UI screen into a coherent language\nphrase. Summarizing mobile screens requires a holistic understanding of the\nmulti-modal data of mobile UIs, including text, image, structures as well as UI\nsemantics, motivating our multi-modal learning approach. We collected and\nanalyzed a large-scale screen summarization dataset annotated by human workers.\nOur dataset contains more than 112k language summarization across \\(\\sim\\)22k\nunique UI screens. We then experimented with a set of deep models with\ndifferent configurations. Our evaluation of these models with both automatic\naccuracy metrics and human rating shows that our approach can generate\nhigh-quality summaries for mobile screens. We demonstrate potential use cases\nof Screen2Words and open-source our dataset and model to lay the foundations\nfor further bridging language and user interfaces.</p>\n", "tags": ["Applications","Datasets","Evaluation"] },
{"key": "wang2021session", "citations": "78", "year": "2021", "title":"Session-based Recommendation With Hypergraph Attention Networks", "abstract": "<p>Session-based recommender systems aim to improve recommendations in\nshort-term sessions that can be found across many platforms. A critical\nchallenge is to accurately model user intent with only limited evidence in\nthese short sessions. For example, is a flower bouquet being viewed meant as\npart of a wedding purchase or for home decoration? Such different perspectives\ngreatly impact what should be recommended next. Hence, this paper proposes a\nnovel session-based recommendation system empowered by hypergraph attention\nnetworks. Three unique properties of the proposed approach are: (i) it\nconstructs a hypergraph for each session to model the item correlations defined\nby various contextual windows in the session simultaneously, to uncover item\nmeanings; (ii) it is equipped with hypergraph attention layers to generate item\nembeddings by flexibly aggregating the contextual information from correlated\nitems in the session; and (iii) it aggregates the dynamic item representations\nfor each session to infer the general purpose and current need, which is\ndecoded to infer the next interesting item in the session. Through experiments\non three benchmark datasets, we find the proposed model is effective in\ngenerating informative dynamic item embeddings and providing more accurate\nrecommendations compared to the state-of-the-art.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "wang2021simvlm", "citations": "309", "year": "2021", "title":"Simvlm: Simple Visual Language Model Pretraining With Weak Supervision", "abstract": "<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "wang2021structured", "citations": "71", "year": "2021", "title":"Structured Reordering For Modeling Latent Alignments In Sequence Transduction", "abstract": "<p>Despite success in many domains, neural models struggle in settings where\ntrain and test examples are drawn from different distributions. In particular,\nin contrast to humans, conventional sequence-to-sequence (seq2seq) models fail\nto generalize systematically, i.e., interpret sentences representing novel\ncombinations of concepts (e.g., text segments) seen in training. Traditional\ngrammar formalisms excel in such settings by implicitly encoding alignments\nbetween input and output segments, but are hard to scale and maintain. Instead\nof engineering a grammar, we directly model segment-to-segment alignments as\ndiscrete structured latent variables within a neural seq2seq model. To\nefficiently explore the large space of alignments, we introduce a reorder-first\nalign-later framework whose central component is a neural reordering module\nproducing {\\it separable} permutations. We present an efficient dynamic\nprogramming algorithm performing exact marginal inference of separable\npermutations, and, thus, enabling end-to-end differentiable training of our\nmodel. The resulting seq2seq model exhibits better systematic generalization\nthan standard models on synthetic problems and NLP tasks (i.e., semantic\nparsing and machine translation).</p>\n", "tags": ["CVPR","Tools","Training Techniques"] },
{"key": "wang2021want", "citations": "99", "year": "2021", "title":"Want To Reduce Labeling Cost? GPT-3 Can Help", "abstract": "<p>Data annotation is a time-consuming and labor-intensive process for many NLP\ntasks. Although there exist various methods to produce pseudo data labels, they\nare often task-specific and require a decent amount of labeled data to start\nwith. Recently, the immense language model GPT-3 with 175 billion parameters\nhas achieved tremendous improvement across many few-shot learning tasks. In\nthis paper, we explore ways to leverage GPT-3 as a low-cost data labeler to\ntrain other models. We find that, to make the downstream model achieve the same\nperformance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use\nlabels from GPT-3 than using labels from humans. Furthermore, we propose a\nnovel framework of combining pseudo labels from GPT-3 with human labels, which\nleads to even better performance with limited labeling budget. These results\npresent a cost-effective data labeling methodology that is generalizable to\nmany practical applications.</p>\n", "tags": ["Applications","Datasets","EMNLP","Few-Shot","Model Architecture","Tools"] },
{"key": "wang2022all", "citations": "92", "year": "2023", "title":"All In One: Exploring Unified Video-language Pre-training", "abstract": "<p>Mainstream Video-Language Pre-training models \\cite{actbert,clipbert,violet}\nconsist of three parts, a video encoder, a text encoder, and a video-text\nfusion Transformer. They pursue better performance via utilizing heavier\nunimodal encoders or multimodal fusion Transformers, resulting in increased\nparameters with lower efficiency in downstream tasks. In this work, we for the\nfirst time introduce an end-to-end video-language model, namely\n\\textit{all-in-one Transformer}, that embeds raw video and textual signals into\njoint representations using a unified backbone architecture. We argue that the\nunique temporal information of video data turns out to be a key barrier\nhindering the design of a modality-agnostic Transformer. To overcome the\nchallenge, we introduce a novel and effective token rolling operation to encode\ntemporal representations from video clips in a non-parametric manner. The\ncareful design enables the representation learning of both video-text\nmultimodal inputs and unimodal inputs using a unified backbone model. Our\npre-trained all-in-one Transformer is transferred to various downstream\nvideo-text tasks after fine-tuning, including text-video retrieval,\nvideo-question answering, multiple choice and visual commonsense reasoning.\nState-of-the-art performances with the minimal model FLOPs on nine datasets\ndemonstrate the superiority of our method compared to the competitive\ncounterparts. The code and pretrained model have been released in\nhttps://github.com/showlab/all-in-one.</p>\n", "tags": ["CVPR","Datasets","Efficiency","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "wang2022diffusiondb", "citations": "88", "year": "2023", "title":"Diffusiondb: A Large-scale Prompt Gallery Dataset For Text-to-image Generative Models", "abstract": "<p>With recent advancements in diffusion models, users can generate high-quality\nimages by writing text prompts in natural language. However, generating images\nwith desired details requires proper prompts, and it is often unclear how a\nmodel reacts to different prompts or what the best prompts are. To help\nresearchers tackle these critical challenges, we introduce DiffusionDB, the\nfirst large-scale text-to-image prompt dataset totaling 6.5TB, containing 14\nmillion images generated by Stable Diffusion, 1.8 million unique prompts, and\nhyperparameters specified by real users. We analyze the syntactic and semantic\ncharacteristics of prompts. We pinpoint specific hyperparameter values and\nprompt styles that can lead to model errors and present evidence of potentially\nharmful model usage, such as the generation of misinformation. The\nunprecedented scale and diversity of this human-actuated dataset provide\nexciting research opportunities in understanding the interplay between prompts\nand generative models, detecting deepfakes, and designing human-AI interaction\ntools to help users more easily use these models. DiffusionDB is publicly\navailable at: https://poloclub.github.io/diffusiondb.</p>\n", "tags": ["Datasets","Has Code","Prompting","Tools"] },
{"key": "wang2022dualprompt", "citations": "186", "year": "2022", "title":"Dualprompt: Complementary Prompting For Rehearsal-free Continual Learning", "abstract": "<p>Continual learning aims to enable a single model to learn a sequence of tasks\nwithout catastrophic forgetting. Top-performing methods usually require a\nrehearsal buffer to store past pristine examples for experience replay, which,\nhowever, limits their practical value due to privacy and memory constraints. In\nthis work, we present a simple yet effective framework, DualPrompt, which\nlearns a tiny set of parameters, called prompts, to properly instruct a\npre-trained model to learn tasks arriving sequentially without buffering past\nexamples. DualPrompt presents a novel approach to attach complementary prompts\nto the pre-trained backbone, and then formulates the objective as learning\ntask-invariant and task-specific “instructions”. With extensive experimental\nvalidation, DualPrompt consistently sets state-of-the-art performance under the\nchallenging class-incremental setting. In particular, DualPrompt outperforms\nrecent advanced continual learning methods with relatively large buffer sizes.\nWe also introduce a more challenging benchmark, Split ImageNet-R, to help\ngeneralize rehearsal-free continual learning research. Source code is available\nat https://github.com/google-research/l2p.</p>\n", "tags": ["Evaluation","Has Code","Prompting","Tools"] },
{"key": "wang2022end", "citations": "106", "year": "2022", "title":"End-to-end Transformer Based Model For Image Captioning", "abstract": "<p>CNN-LSTM based architectures have played an important role in image\ncaptioning, but limited by the training efficiency and expression ability,\nresearchers began to explore the CNN-Transformer based models and achieved\ngreat success. Meanwhile, almost all recent works adopt Faster R-CNN as the\nbackbone encoder to extract region-level features from given images. However,\nFaster R-CNN needs a pre-training on an additional dataset, which divides the\nimage captioning task into two stages and limits its potential applications. In\nthis paper, we build a pure Transformer-based model, which integrates image\ncaptioning into one stage and realizes end-to-end training. Firstly, we adopt\nSwinTransformer to replace Faster R-CNN as the backbone encoder to extract\ngrid-level features from given images; Then, referring to Transformer, we build\na refining encoder and a decoder. The refining encoder refines the grid\nfeatures by capturing the intra-relationship between them, and the decoder\ndecodes the refined features into captions word by word. Furthermore, in order\nto increase the interaction between multi-modal (vision and language) features\nto enhance the modeling capability, we calculate the mean pooling of grid\nfeatures as the global feature, then introduce it into refining encoder to\nrefine with grid features together, and add a pre-fusion process of refined\nglobal feature and generated words in decoder. To validate the effectiveness of\nour proposed model, we conduct experiments on MSCOCO dataset. The experimental\nresults compared to existing published works demonstrate that our model\nachieves new state-of-the-art performances of 138.2% (single model) and 141.0%\n(ensemble of 4 models) CIDEr scores on `Karpathy’ offline test split and 136.0%\n(c5) and 138.3% (c40) CIDEr scores on the official online test server. Trained\nmodels and source code will be released.</p>\n", "tags": ["AAAI","Applications","Datasets","Model Architecture","Training Techniques"] },
{"key": "wang2022git", "citations": "172", "year": "2022", "title":"GIT: A Generative Image-to-text Transformer For Vision And Language", "abstract": "<p>In this paper, we design and train a Generative Image-to-text Transformer,\nGIT, to unify vision-language tasks such as image/video captioning and question\nanswering. While generative models provide a consistent network architecture\nbetween pre-training and fine-tuning, existing work typically contains complex\nstructures (uni/multi-modal encoder/decoder) and depends on external modules\nsuch as object detectors/taggers and optical character recognition (OCR). In\nGIT, we simplify the architecture as one image encoder and one text decoder\nunder a single language modeling task. We also scale up the pre-training data\nand the model size to boost the model performance. Without bells and whistles,\nour GIT establishes new state of the arts on 12 challenging benchmarks with a\nlarge margin. For instance, our model surpasses the human performance for the\nfirst time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a\nnew scheme of generation-based image classification and scene text recognition,\nachieving decent performance on standard benchmarks. Codes are released at\nhttps://github.com/microsoft/GenerativeImage2Text.</p>\n", "tags": ["Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "wang2022image", "citations": "140", "year": "2022", "title":"Image As A Foreign Language: Beit Pretraining For All Vision And Vision-language Tasks", "abstract": "<p>A big convergence of language, vision, and multimodal pretraining is\nemerging. In this work, we introduce a general-purpose multimodal foundation\nmodel BEiT-3, which achieves state-of-the-art transfer performance on both\nvision and vision-language tasks. Specifically, we advance the big convergence\nfrom three aspects: backbone architecture, pretraining task, and model scaling\nup. We introduce Multiway Transformers for general-purpose modeling, where the\nmodular architecture enables both deep fusion and modality-specific encoding.\nBased on the shared backbone, we perform masked “language” modeling on images\n(Imglish), texts (English), and image-text pairs (“parallel sentences”) in a\nunified manner. Experimental results show that BEiT-3 obtains state-of-the-art\nperformance on object detection (COCO), semantic segmentation (ADE20K), image\nclassification (ImageNet), visual reasoning (NLVR2), visual question answering\n(VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).</p>\n", "tags": ["Model Architecture"] },
{"key": "wang2022images", "citations": "89", "year": "2023", "title":"Images Speak In Images: A Generalist Painter For In-context Visual Learning", "abstract": "<p>In-context learning, as a new paradigm in NLP, allows the model to rapidly\nadapt to various tasks with only a handful of prompts and examples. But in\ncomputer vision, the difficulties for in-context learning lie in that tasks\nvary significantly in the output representations, thus it is unclear how to\ndefine the general-purpose task prompts that the vision model can understand\nand transfer to out-of-domain tasks. In this work, we present Painter, a\ngeneralist model which addresses these obstacles with an “image”-centric\nsolution, that is, to redefine the output of core vision tasks as images, and\nspecify task prompts as also images. With this idea, our training process is\nextremely simple, which performs standard masked image modeling on the stitch\nof input and output image pairs. This makes the model capable of performing\ntasks conditioned on visible image patches. Thus, during inference, we can\nadopt a pair of input and output images from the same task as the input\ncondition, to indicate which task to perform. Without bells and whistles, our\ngeneralist Painter can achieve competitive performance compared to\nwell-established task-specific models, on seven representative vision tasks\nranging from high-level visual understanding to low-level image processing. In\naddition, Painter significantly outperforms recent generalist models on several\nchallenging tasks.</p>\n", "tags": ["CVPR","In Context Learning","Training Techniques"] },
{"key": "wang2022lilt", "citations": "92", "year": "2022", "title":"Lilt: A Simple Yet Effective Language-independent Layout Transformer For Structured Document Understanding", "abstract": "<p>Structured document understanding has attracted considerable attention and\nmade significant progress recently, owing to its crucial role in intelligent\ndocument processing. However, most existing related models can only deal with\nthe document data of specific language(s) (typically English) included in the\npre-training collection, which is extremely limited. To address this issue, we\npropose a simple yet effective Language-independent Layout Transformer (LiLT)\nfor structured document understanding. LiLT can be pre-trained on the\nstructured documents of a single language and then directly fine-tuned on other\nlanguages with the corresponding off-the-shelf monolingual/multilingual\npre-trained textual models. Experimental results on eight languages have shown\nthat LiLT can achieve competitive or even superior performance on diverse\nwidely-used downstream benchmarks, which enables language-independent benefit\nfrom the pre-training of document layout structure. Code and model are publicly\navailable at https://github.com/jpWang/LiLT.</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "wang2022multimodal", "citations": "108", "year": "2022", "title":"Multimodal Adaptive Distillation For Leveraging Unimodal Encoders For Vision-language Tasks", "abstract": "<p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with\ncarefully curated vision-language datasets. While these datasets reach an order\nof 10 million samples, the labor cost is prohibitive to scale further.\nConversely, unimodal encoders are pretrained with simpler annotations that are\nless cost-prohibitive, achieving scales of hundreds of millions to billions. As\na result, unimodal encoders have achieved state-of-art (SOTA) on many\ndownstream tasks. However, challenges remain when applying to VL tasks. The\npretraining data is not optimal for cross-modal architectures and requires\nheavy computational resources. In addition, unimodal architectures lack\ncross-modal interactions that have demonstrated significant benefits for VL\ntasks. Therefore, how to best leverage pretrained unimodal encoders for VL\ntasks is still an area of active research. In this work, we propose a method to\nleverage unimodal vision and text encoders for VL tasks that augment existing\nVL approaches while conserving computational complexity. Specifically, we\npropose Multimodal Adaptive Distillation (MAD), which adaptively distills\nuseful knowledge from pretrained encoders to cross-modal VL encoders. Second,\nto better capture nuanced impacts on VL task performance, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata constraints and conditions of domain shift. Experiments demonstrate that\nMAD leads to consistent gains in the low-shot, domain-shifted, and\nfully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA\nperformance on VCR compared to other single models pretrained with image-text\ndata. Finally, MAD outperforms concurrent works utilizing pretrained vision\nencoder from CLIP. Code will be made available.</p>\n", "tags": ["CVPR","Datasets","Efficiency","Evaluation"] },
{"key": "wang2022no", "citations": "87", "year": "2022", "title":"No More Fine-tuning? An Experimental Evaluation Of Prompt Tuning In Code Intelligence", "abstract": "<p>Pre-trained models have been shown effective in many code intelligence tasks.\nThese models are pre-trained on large-scale unlabeled corpus and then\nfine-tuned in downstream tasks. However, as the inputs to pre-training and\ndownstream tasks are in different forms, it is hard to fully explore the\nknowledge of pre-trained models. Besides, the performance of fine-tuning\nstrongly relies on the amount of downstream data, while in practice, the\nscenarios with scarce data are common. Recent studies in the natural language\nprocessing (NLP) field show that prompt tuning, a new paradigm for tuning,\nalleviates the above issues and achieves promising results in various NLP\ntasks. In prompt tuning, the prompts inserted during tuning provide\ntask-specific knowledge, which is especially beneficial for tasks with\nrelatively scarce data. In this paper, we empirically evaluate the usage and\neffect of prompt tuning in code intelligence tasks. We conduct prompt tuning on\npopular pre-trained models CodeBERT and CodeT5 and experiment with three code\nintelligence tasks including defect prediction, code summarization, and code\ntranslation. Our experimental results show that prompt tuning consistently\noutperforms fine-tuning in all three tasks. In addition, prompt tuning shows\ngreat potential in low-resource scenarios, e.g., improving the BLEU scores of\nfine-tuning by more than 26% on average for code summarization. Our results\nsuggest that instead of fine-tuning, we could adapt prompt tuning for code\nintelligence tasks to achieve better performance, especially when lacking\ntask-specific data.</p>\n", "tags": ["Evaluation","Fine-Tuning","Llm For Code","Prompting","Training Techniques"] },
{"key": "wang2022self", "citations": "476", "year": "2022", "title":"Self-instruct: Aligning Language Models With Self-generated Instructions", "abstract": "<p>Large “instruction-tuned” language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Tools"] },
{"key": "wang2022super", "citations": "173", "year": "2022", "title":"Super-naturalinstructions: Generalization Via Declarative Instructions On 1600+ NLP Tasks", "abstract": "<p>How well can NLP models generalize to a variety of unseen tasks when provided\nwith task instructions? To address this question, we first introduce\nSuper-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their\nexpert-written instructions. Our collection covers 76 distinct task types,\nincluding but not limited to classification, extraction, infilling, sequence\ntagging, text rewriting, and text composition. This large and diverse\ncollection of tasks enables rigorous benchmarking of cross-task generalization\nunder instructions – training models to follow instructions on a subset of\ntasks and evaluating them on the remaining unseen ones. Furthermore, we build\nTk-Instruct, a transformer model trained to follow a variety of in-context\ninstructions (plain language task definitions or k-shot examples). Our\nexperiments show that Tk-Instruct outperforms existing instruction-following\nmodels such as InstructGPT by over 9% on our benchmark despite being an order\nof magnitude smaller. We further analyze generalization as a function of\nvarious scaling parameters, such as the number of observed tasks, the number of\ninstances per task, and model sizes. We hope our dataset and model facilitate\nfuture progress towards more general-purpose NLP models.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Instruction Following","Model Architecture","Training Techniques"] },
{"key": "wang2022text", "citations": "74", "year": "2022", "title":"Text Embeddings By Weakly-supervised Contrastive Pre-training", "abstract": "<p>This paper presents E5, a family of state-of-the-art text embeddings that\ntransfer well to a wide range of tasks. The model is trained in a contrastive\nmanner with weak supervision signals from our curated large-scale text pair\ndataset (called CCPairs). E5 can be readily used as a general-purpose embedding\nmodel for any tasks requiring a single-vector representation of texts such as\nretrieval, clustering, and classification, achieving strong performance in both\nzero-shot and fine-tuned settings. We conduct extensive evaluations on 56\ndatasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the\nfirst model that outperforms the strong BM25 baseline on the BEIR retrieval\nbenchmark without using any labeled data. When fine-tuned, E5 obtains the best\nresults on the MTEB benchmark, beating existing embedding models with 40x more\nparameters.</p>\n", "tags": ["Datasets","Evaluation","Training Techniques"] },
{"key": "wang2022training", "citations": "66", "year": "2022", "title":"Training Data Is More Valuable Than You Think: A Simple And Effective Method By Retrieving From Training Data", "abstract": "<p>Retrieval-based methods have been shown to be effective in NLP tasks via\nintroducing external knowledge. However, the indexing and retrieving of\nlarge-scale corpora bring considerable computational cost. Surprisingly, we\nfound that REtrieving from the traINing datA (REINA) only can lead to\nsignificant gains on multiple NLG and NLU tasks. We retrieve the labeled\ntraining instances most similar to the input text and then concatenate them\nwith the input to feed into the model to generate the output. Experimental\nresults show that this simple method can achieve significantly better\nperformance on a variety of NLU and NLG tasks, including summarization, machine\ntranslation, language modeling, and question answering tasks. For instance, our\nproposed method achieved state-of-the-art results on XSum, BigPatent, and\nCommonsenseQA. Our code is released, https://github.com/microsoft/REINA .</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "wang2023can", "citations": "138", "year": "2023", "title":"Can Prompt Learning Benefit Radiology Report Generation?", "abstract": "<p>Radiology report generation aims to automatically provide clinically\nmeaningful descriptions of radiology images such as MRI and X-ray. Although\ngreat success has been achieved in natural scene image captioning tasks,\nradiology report generation remains challenging and requires prior medical\nknowledge. In this paper, we propose PromptRRG, a method that utilizes prompt\nlearning to activate a pretrained model and incorporate prior knowledge. Since\nprompt learning for radiology report generation has not been explored before,\nwe begin with investigating prompt designs and categorise them based on varying\nlevels of knowledge: common, domain-specific and disease-enriched prompts.\nAdditionally, we propose an automatic prompt learning mechanism to alleviate\nthe burden of manual prompt engineering. This is the first work to\nsystematically examine the effectiveness of prompt learning for radiology\nreport generation. Experimental results on the largest radiology report\ngeneration benchmark, MIMIC-CXR, demonstrate that our proposed method achieves\nstate-of-the-art performance. Code will be available upon the acceptance.</p>\n", "tags": ["Datasets","Evaluation","Prompting","SIGIR"] },
{"key": "wang2023codet5", "citations": "146", "year": "2023", "title":"Codet5+: Open Code Large Language Models For Code Understanding And Generation", "abstract": "<p>Large language models (LLMs) pretrained on vast source code have achieved\nprominent progress in code intelligence. However, existing code LLMs have two\nmain limitations in terms of architecture and pretraining tasks. First, they\noften adopt a specific architecture (encoder-only or decoder-only) or rely on a\nunified encoder-decoder network for different downstream tasks. The former\nparadigm is limited by inflexibility in applications while in the latter, the\nmodel is treated as a single system for all tasks, leading to suboptimal\nperformance on a subset of tasks. Secondly, they often employ a limited set of\npretraining objectives which might not be relevant to some downstream tasks and\nhence result in substantial performance degrade. To address these limitations,\nwe propose ``CodeT5+’’, a family of encoder-decoder LLMs for code in which\ncomponent modules can be flexibly combined to suit a wide range of downstream\ncode tasks. Such flexibility is enabled by our proposed mixture of pretraining\nobjectives to mitigate the pretrain-finetune discrepancy. These objectives\ncover span denoising, contrastive learning, text-code matching, and causal LM\npretraining tasks, on both unimodal and bimodal multilingual code corpora.\nFurthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs\nwithout training from scratch to efficiently scale up our models, and explore\ninstruction-tuning to align with natural language instructions. We extensively\nevaluate CodeT5+ on over 20 code-related benchmarks in different settings,\nincluding zero-shot, finetuning, and instruction-tuning. We observe\nstate-of-the-art (SoTA) model performance on various code-related tasks, such\nas code generation and completion, math programming, and text-to-code retrieval\ntasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA\nresults on HumanEval code generation task against other open code LLMs.</p>\n", "tags": ["Applications","EMNLP","Fine-Tuning","Llm For Code","Model Architecture","Training Techniques"] },
{"key": "wang2023document", "citations": "80", "year": "2023", "title":"Document-level Machine Translation With Large Language Models", "abstract": "<p>Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs’ ability on discourse modeling. The\nstudy focuses on three aspects: 1) Effects of Context-Aware Prompts, where we\ninvestigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of ChatGPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and shed\nlight on impacts of training techniques on discourse modeling. By evaluating on\na number of benchmarks, we surprisingly find that LLMs have demonstrated\nsuperior performance and show potential to become a new paradigm for\ndocument-level translation: 1) leveraging their powerful long-text modeling\ncapabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of\nhuman evaluation; 2) GPT-4 demonstrates a stronger ability for probing\nlinguistic knowledge than GPT-3.5. This work highlights the challenges and\nopportunities of LLMs for MT, which we hope can inspire the future design and\nevaluation of LLMs.We release our data and annotations at\nhttps://github.com/longyuewangdcu/Document-MT-LLM.</p>\n", "tags": ["EMNLP","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "wang2023gpt", "citations": "101", "year": "2023", "title":"GPT-NER: Named Entity Recognition Via Large Language Models", "abstract": "<p>Despite the fact that large-scale Language Models (LLM) have achieved SOTA\nperformances on a variety of NLP tasks, its performance on NER is still\nsignificantly below supervised baselines. This is due to the gap between the\ntwo tasks the NER and LLMs: the former is a sequence labeling task in nature\nwhile the latter is a text-generation model.\n  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the\ngap by transforming the sequence labeling task to a generation task that can be\neasily adapted by LLMs e.g., the task of finding location entities in the input\ntext “Columbus is a city” is transformed to generate the text sequence\n“@@Columbus## is a city”, where special tokens @@## marks the entity to\nextract. To efficiently address the “hallucination” issue of LLMs, where LLMs\nhave a strong inclination to over-confidently label NULL inputs as entities, we\npropose a self-verification strategy by prompting LLMs to ask itself whether\nthe extracted entities belong to a labeled entity tag.\n  We conduct experiments on five widely adopted NER datasets, and GPT-NER\nachieves comparable performances to fully supervised baselines, which is the\nfirst time as far as we are concerned. More importantly, we find that GPT-NER\nexhibits a greater ability in the low-resource and few-shot setups, when the\namount of training data is extremely scarce, GPT-NER performs significantly\nbetter than supervised models. This demonstrates the capabilities of GPT-NER in\nreal-world NER applications where the number of labeled examples is limited.</p>\n", "tags": ["Applications","Datasets","Few-Shot","Model Architecture","Prompting","Training Techniques"] },
{"key": "wang2023huatuo", "citations": "71", "year": "2023", "title":"Huatuo: Tuning Llama Model With Chinese Medical Knowledge", "abstract": "<p>Large Language Models (LLMs), such as the LLaMA model, have demonstrated\ntheir effectiveness in various general-domain natural language processing (NLP)\ntasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\ntasks due to the need for medical expertise in the responses. In response to\nthis challenge, we propose HuaTuo, a LLaMA-based model that has been\nsupervised-fine-tuned with generated QA (Question-Answer) instances. The\nexperimental results demonstrate that HuaTuo generates responses that possess\nmore reliable medical knowledge. Our proposed HuaTuo model is accessible at\nhttps://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.</p>\n", "tags": ["Applications","Fine-Tuning","Has Code"] },
{"key": "wang2023is", "citations": "138", "year": "2023", "title":"Is GPT Powerful Enough To Analyze The Emotions Of Memes?", "abstract": "<p>Large Language Models (LLMs), representing a significant achievement in\nartificial intelligence (AI) research, have demonstrated their ability in a\nmultitude of tasks. This project aims to explore the capabilities of GPT-3.5, a\nleading example of LLMs, in processing the sentiment analysis of Internet\nmemes. Memes, which include both verbal and visual aspects, act as a powerful\nyet complex tool for expressing ideas and sentiments, demanding an\nunderstanding of societal norms and cultural contexts. Notably, the detection\nand moderation of hateful memes pose a significant challenge due to their\nimplicit offensive nature. This project investigates GPT’s proficiency in such\nsubjective tasks, revealing its strengths and potential limitations. The tasks\ninclude the classification of meme sentiment, determination of humor type, and\ndetection of implicit hate in memes. The performance evaluation, using datasets\nfrom SemEval-2020 Task 8 and Facebook hateful memes, offers a comparative\nunderstanding of GPT responses against human annotations. Despite GPT’s\nremarkable progress, our findings underscore the challenges faced by these\nmodels in handling subjective tasks, which are rooted in their inherent\nlimitations including contextual understanding, interpretation of implicit\nmeanings, and data biases. This research contributes to the broader discourse\non the applicability of AI in handling complex, context-dependent tasks, and\noffers valuable insights for future advancements.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "wang2023plan", "citations": "94", "year": "2023", "title":"Plan-and-solve Prompting: Improving Zero-shot Chain-of-thought Reasoning By Large Language Models", "abstract": "<p>Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n“Let’s think step by step” as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.</p>\n", "tags": ["Datasets","Few-Shot","Has Code","Model Architecture","Prompting"] },
{"key": "wang2023query2doc", "citations": "70", "year": "2023", "title":"Query2doc: Query Expansion With Large Language Models", "abstract": "<p>This paper introduces a simple yet effective query expansion approach,\ndenoted as query2doc, to improve both sparse and dense retrieval systems. The\nproposed method first generates pseudo-documents by few-shot prompting large\nlanguage models (LLMs), and then expands the query with generated\npseudo-documents. LLMs are trained on web-scale text corpora and are adept at\nknowledge memorization. The pseudo-documents from LLMs often contain highly\nrelevant information that can aid in query disambiguation and guide the\nretrievers. Experimental results demonstrate that query2doc boosts the\nperformance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and\nTREC DL, without any model fine-tuning. Furthermore, our method also benefits\nstate-of-the-art dense retrievers in terms of both in-domain and out-of-domain\nresults.</p>\n", "tags": ["Datasets","EMNLP","Few-Shot","Fine-Tuning","Prompting","Retrieval Systems"] },
{"key": "wang2023reprompt", "citations": "67", "year": "2023", "title":"Reprompt: Automatic Prompt Editing To Refine Ai-generative Art Towards Precise Expressions", "abstract": "<p>Generative AI models have shown impressive ability to produce images with\ntext prompts, which could benefit creativity in visual art creation and\nself-expression. However, it is unclear how precisely the generated images\nexpress contexts and emotions from the input texts. We explored the emotional\nexpressiveness of AI-generated images and developed RePrompt, an automatic\nmethod to refine text prompts toward precise expression of the generated\nimages. Inspired by crowdsourced editing strategies, we curated intuitive text\nfeatures, such as the number and concreteness of nouns, and trained a proxy\nmodel to analyze the feature effects on the AI-generated image. With model\nexplanations of the proxy model, we curated a rubric to adjust text prompts to\noptimize image generation for precise emotion expression. We conducted\nsimulation and user studies, which showed that RePrompt significantly improves\nthe emotional expressiveness of AI-generated images, especially for negative\nemotions.</p>\n", "tags": ["Prompting"] },
{"key": "wang2023robustness", "citations": "73", "year": "2023", "title":"On The Robustness Of Chatgpt: An Adversarial And Out-of-distribution Perspective", "abstract": "<p>ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.</p>\n", "tags": ["Applications","Datasets","Ethics & Fairness","Evaluation","Model Architecture","Security"] },
{"key": "wang2023visionllm", "citations": "78", "year": "2023", "title":"Visionllm: Large Language Model Is Also An Open-ended Decoder For Vision-centric Tasks", "abstract": "<p>Large language models (LLMs) have notably accelerated progress towards\nartificial general intelligence (AGI), with their impressive zero-shot capacity\nfor user-tailored tasks, endowing them with immense potential across a range of\napplications. However, in the field of computer vision, despite the\navailability of numerous powerful vision foundation models (VFMs), they are\nstill restricted to tasks in a pre-defined form, struggling to match the\nopen-ended task capabilities of LLMs. In this work, we present an LLM-based\nframework for vision-centric tasks, termed VisionLLM. This framework provides a\nunified perspective for vision and language tasks by treating images as a\nforeign language and aligning vision-centric tasks with language tasks that can\nbe flexibly defined and managed using language instructions. An LLM-based\ndecoder can then make appropriate predictions based on these instructions for\nopen-ended tasks. Extensive experiments show that the proposed VisionLLM can\nachieve different levels of task customization through language instructions,\nfrom fine-grained object-level to coarse-grained task-level customization, all\nwith good results. It’s noteworthy that, with a generalist LLM-based framework,\nour model can achieve over 60% mAP on COCO, on par with detection-specific\nmodels. We hope this model can set a new baseline for generalist vision and\nlanguage models. The demo shall be released based on\nhttps://github.com/OpenGVLab/InternGPT. The code shall be released at\nhttps://github.com/OpenGVLab/VisionLLM.</p>\n", "tags": ["Applications","Has Code","Tools"] },
{"key": "warstadt2022what", "citations": "70", "year": "2022", "title":"What Artificial Neural Networks Can Tell Us About Human Language Acquisition", "abstract": "<p>Rapid progress in machine learning for natural language processing has the\npotential to transform debates about how humans learn language. However, the\nlearning environments and biases of current artificial learners and humans\ndiverge in ways that weaken the impact of the evidence obtained from learning\nsimulations. For example, today’s most effective neural language models are\ntrained on roughly one thousand times the amount of linguistic data available\nto a typical child. To increase the relevance of learnability results from\ncomputational models, we need to train model learners without significant\nadvantages over humans. If an appropriate model successfully acquires some\ntarget linguistic knowledge, it can provide a proof of concept that the target\nis learnable in a hypothesized human learning scenario. Plausible model\nlearners will enable us to carry out experimental manipulations to make causal\ninferences about variables in the learning environment, and to rigorously test\npoverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in\nhumans on the basis of speculations about learnability. Comparable experiments\nwill never be possible with human subjects due to practical and ethical\nconsiderations, making model learners an indispensable resource. So far,\nattempts to deprive current models of unfair advantages obtain sub-human\nresults for key grammatical behaviors such as acceptability judgments. But\nbefore we can justifiably conclude that language learning requires more prior\ndomain-specific knowledge than current models possess, we must first explore\nnon-linguistic inputs in the form of multimodal stimuli and multi-agent\ninteraction as ways to make our learners more efficient at learning from\nlimited linguistic input.</p>\n", "tags": ["Agentic"] },
{"key": "way2018quality", "citations": "84", "year": "2018", "title":"Quality Expectations Of Machine Translation", "abstract": "<p>Machine Translation (MT) is being deployed for a range of use-cases by\nmillions of people on a daily basis. There should, therefore, be no doubt as to\nthe utility of MT. However, not everyone is convinced that MT can be useful,\nespecially as a productivity enhancer for human translators. In this chapter, I\naddress this issue, describing how MT is currently deployed, how its output is\nevaluated and how this could be enhanced, especially as MT quality itself\nimproves. Central to these issues is the acceptance that there is no longer a\nsingle ‘gold standard’ measure of quality, such that the situation in which MT\nis deployed needs to be borne in mind, especially with respect to the expected\n‘shelf-life’ of the translation itself.</p>\n", "tags": ["Applications"] },
{"key": "wayne2023survey", "citations": "1099", "year": "2023", "title":"A Survey Of Large Language Models", "abstract": "<p>Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.</p>\n", "tags": ["Evaluation","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "webb2022emergent", "citations": "240", "year": "2023", "title":"Emergent Analogical Reasoning In Large Language Models", "abstract": "<p>The recent advent of large language models has reinvigorated debate over\nwhether human cognitive capacities might emerge in such generic models given\nsufficient training data. Of particular interest is the ability of these models\nto reason about novel problems zero-shot, without any direct training. In human\ncognition, this capacity is closely tied to an ability to reason by analogy.\nHere, we performed a direct comparison between human reasoners and a large\nlanguage model (the text-davinci-003 variant of GPT-3) on a range of analogical\ntasks, including a non-visual matrix reasoning task based on the rule structure\nof Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a\nsurprisingly strong capacity for abstract pattern induction, matching or even\nsurpassing human capabilities in most settings; preliminary tests of GPT-4\nindicated even better performance. Our results indicate that large language\nmodels such as GPT-3 have acquired an emergent ability to find zero-shot\nsolutions to a broad range of analogy problems.</p>\n", "tags": ["Emergent Abilities","Model Architecture","Training Techniques"] },
{"key": "weber2017imagination", "citations": "182", "year": "2017", "title":"Imagination-augmented Agents For Deep Reinforcement Learning", "abstract": "<p>We introduce Imagination-Augmented Agents (I2As), a novel architecture for\ndeep reinforcement learning combining model-free and model-based aspects. In\ncontrast to most existing model-based reinforcement learning and planning\nmethods, which prescribe how a model should be used to arrive at a policy, I2As\nlearn to interpret predictions from a learned environment model to construct\nimplicit plans in arbitrary ways, by using the predictions as additional\ncontext in deep policy networks. I2As show improved data efficiency,\nperformance, and robustness to model misspecification compared to several\nbaselines.</p>\n", "tags": ["Agentic","Efficiency","Model Architecture","Reinforcement Learning","Security"] },
{"key": "weber2019nlprolog", "citations": "76", "year": "2019", "title":"Nlprolog: Reasoning With Weak Unification For Question Answering In Natural Language", "abstract": "<p>Rule-based models are attractive for various tasks because they inherently\nlead to interpretable and explainable decisions and can easily incorporate\nprior knowledge. However, such systems are difficult to apply to problems\ninvolving natural language, due to its linguistic variability. In contrast,\nneural models can cope very well with ambiguity by learning distributed\nrepresentations of words and their composition from data, but lead to models\nthat are difficult to interpret. In this paper, we describe a model combining\nneural networks with logic programming in a novel manner for solving multi-hop\nreasoning tasks over natural language. Specifically, we propose to use a Prolog\nprover which we extend to utilize a similarity function over pretrained\nsentence encoders. We fine-tune the representations for the similarity function\nvia backpropagation. This leads to a system that can apply rule-based reasoning\nto natural language, and induce domain-specific rules from training data. We\nevaluate the proposed system on two different question answering tasks, showing\nthat it outperforms two baselines – BIDAF (Seo et al., 2016a) and FAST QA\n(Weissenborn et al., 2017b) on a subset of the WikiHop corpus and achieves\ncompetitive results on the MedHop data set (Welbl et al., 2017).</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "webersinke2021climatebert", "citations": "103", "year": "2022", "title":"Climatebert: A Pretrained Language Model For Climate-related Text", "abstract": "<p>Over the recent years, large pretrained language models (LM) have\nrevolutionized the field of natural language processing (NLP). However, while\npretraining on general language has been shown to work very well for common\nlanguage, it has been observed that niche language poses problems. In\nparticular, climate-related texts include specific language that common LMs can\nnot represent accurately. We argue that this shortcoming of today’s LMs limits\nthe applicability of modern NLP to the broad field of text processing of\nclimate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based\nlanguage model that is further pretrained on over 2 million paragraphs of\nclimate-related texts, crawled from various sources such as common news,\nresearch articles, and climate reporting of companies. We find that CLIMATEBERT\nleads to a 48% improvement on a masked language model objective which, in turn,\nleads to lowering error rates by 3.57% to 35.71% for various climate-related\ndownstream tasks like text classification, sentiment analysis, and\nfact-checking.</p>\n", "tags": ["Model Architecture"] },
{"key": "weberwulff2023testing", "citations": "165", "year": "2023", "title":"Testing Of Detection Tools For Ai-generated Text", "abstract": "<p>Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AI-generated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AI-generated\ntext. Furthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.</p>\n", "tags": ["Ethics & Fairness","Model Architecture","Tools"] },
{"key": "webson2021do", "citations": "148", "year": "2022", "title":"Do Prompt-based Models Really Understand The Meaning Of Their Prompts?", "abstract": "<p>Recently, a boom of papers has shown extraordinary progress in zero-shot and\nfew-shot learning with various prompt-based models. It is commonly argued that\nprompts help models to learn faster in the same way that humans learn faster\nwhen provided with task instructions expressed in natural language. In this\nstudy, we experiment with over 30 prompt templates manually written for natural\nlanguage inference (NLI). We find that models learn just as fast with many\nprompts that are intentionally irrelevant or even pathologically misleading as\nthey do with instructively “good” prompts. Further, such patterns hold even for\nmodels as large as 175 billion parameters (Brown et al., 2020) as well as the\nrecently proposed instruction-tuned models which are trained on hundreds of\nprompts (Sanh et al., 2022). That is, instruction-tuned models often produce\ngood predictions with irrelevant and misleading prompts even at zero shots. In\nsum, notwithstanding prompt-based models’ impressive improvement, we find\nevidence of serious limitations that question the degree to which such\nimprovement is derived from models understanding task instructions in ways\nanalogous to humans’ use of task instructions.</p>\n", "tags": ["Few-Shot","NAACL","Prompting"] },
{"key": "webster2020measuring", "citations": "107", "year": "2020", "title":"Measuring And Reducing Gendered Correlations In Pre-trained Models", "abstract": "<p>Pre-trained models have revolutionized natural language understanding.\nHowever, researchers have found they can encode artifacts undesired in many\napplications, such as professions correlating with one gender more than\nanother. We explore such gendered correlations as a case study for how to\naddress unintended correlations in pre-trained models. We define metrics and\nreveal that it is possible for models with similar accuracy to encode\ncorrelations at very different rates. We show how measured correlations can be\nreduced with general-purpose techniques, and highlight the trade offs different\nstrategies have. With these results, we make recommendations for training\nrobust models: (1) carefully evaluate unintended correlations, (2) be mindful\nof seemingly innocuous configuration differences, and (3) focus on general\nmitigations.</p>\n", "tags": ["Applications","Ethics & Fairness","Evaluation","Training Techniques"] },
{"key": "wei2019imitation", "citations": "72", "year": "2019", "title":"Imitation Learning For Non-autoregressive Neural Machine Translation", "abstract": "<p>Non-autoregressive translation models (NAT) have achieved impressive\ninference speedup. A potential issue of the existing NAT algorithms, however,\nis that the decoding is conducted in parallel, without directly considering\nprevious context. In this paper, we propose an imitation learning framework for\nnon-autoregressive machine translation, which still enjoys the fast translation\nspeed but gives comparable translation performance compared to its\nauto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and\nWMT16 datasets. Our proposed model achieves a significant speedup over the\nautoregressive models, while keeping the translation quality comparable to the\nautoregressive models. By sampling sentence length in parallel at inference\ntime, we achieve the performance of 31.85 BLEU on WMT16 Ro\\(\\rightarrow\\)En and\n30.68 BLEU on IWSLT16 En\\(\\rightarrow\\)De.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "wei2020learning", "citations": "62", "year": "2021", "title":"On Learning Universal Representations Across Languages", "abstract": "<p>Recent studies have demonstrated the overwhelming advantage of cross-lingual\npre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual\nNLP tasks. However, existing approaches essentially capture the co-occurrence\namong tokens through involving the masked language model (MLM) objective with\ntoken-level cross entropy. In this work, we extend these approaches to learn\nsentence-level representations and show the effectiveness on cross-lingual\nunderstanding and generation. Specifically, we propose a Hierarchical\nContrastive Learning (HiCTL) method to (1) learn universal representations for\nparallel sentences distributed in one or multiple languages and (2) distinguish\nthe semantically-related words from a shared cross-lingual vocabulary for each\nsentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME\nand machine translation. Experimental results show that the HiCTL outperforms\nthe state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME\nbenchmark as well as achieves substantial improvements on both of the\nhigh-resource and low-resource English-to-X translation tasks over strong\nbaselines.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "wei2021pangu", "citations": "76", "year": "2021", "title":"Pangu-\\(α\\): Large-scale Autoregressive Pretrained Chinese Language Models With Auto-parallel Computation", "abstract": "<p>Large-scale Pretrained Language Models (PLMs) have become the new paradigm\nfor Natural Language Processing (NLP). PLMs with hundreds of billions\nparameters such as GPT-3 have demonstrated strong performances on natural\nlanguage understanding and generation with \\textit{few-shot in-context}\nlearning. In this work, we present our practice on training large-scale\nautoregressive language models named PanGu-\\(\\alpha\\), with up to 200 billion\nparameters. PanGu-\\(\\alpha\\) is developed under the MindSpore and trained on a\ncluster of 2048 Ascend 910 AI processors. The training parallelism strategy is\nimplemented based on MindSpore Auto-parallel, which composes five parallelism\ndimensions to scale the training task to 2048 processors efficiently, including\ndata parallelism, op-level model parallelism, pipeline model parallelism,\noptimizer model parallelism and rematerialization. To enhance the\ngeneralization ability of PanGu-\\(\\alpha\\), we collect 1.1TB high-quality Chinese\ndata from a wide range of domains to pretrain the model. We empirically test\nthe generation ability of PanGu-\\(\\alpha\\) in various scenarios including text\nsummarization, question answering, dialogue generation, etc. Moreover, we\ninvestigate the effect of model scales on the few-shot performances across a\nbroad range of Chinese NLP tasks. The experimental results demonstrate the\nsuperior capabilities of PanGu-\\(\\alpha\\) in performing various tasks under\nfew-shot or zero-shot settings.</p>\n", "tags": ["Few-Shot","Model Architecture","Training Techniques"] },
{"key": "wei2023evaluation", "citations": "62", "year": "2024", "title":"Evaluation Of Chatgpt-generated Medical Responses: A Systematic Review And Meta-analysis", "abstract": "<p>Large language models such as ChatGPT are increasingly explored in medical\ndomains. However, the absence of standard guidelines for performance evaluation\nhas led to methodological inconsistencies. This study aims to summarize the\navailable evidence on evaluating ChatGPT’s performance in medicine and provide\ndirection for future research. We searched ten medical literature databases on\nJune 15, 2023, using the keyword “ChatGPT”. A total of 3520 articles were\nidentified, of which 60 were reviewed and summarized in this paper and 17 were\nincluded in the meta-analysis. The analysis showed that ChatGPT displayed an\noverall integrated accuracy of 56% (95% CI: 51%-60%, I2 = 87%) in addressing\nmedical queries. However, the studies varied in question resource,\nquestion-asking process, and evaluation metrics. Moreover, many studies failed\nto report methodological details, including the version of ChatGPT and whether\neach question was used independently or repeatedly. Our findings revealed that\nalthough ChatGPT demonstrated considerable potential for application in\nhealthcare, the heterogeneity of the studies and insufficient reporting may\naffect the reliability of these results. Further well-designed studies with\ncomprehensive and transparent reporting are needed to evaluate ChatGPT’s\nperformance in medicine.</p>\n", "tags": ["Evaluation","Survey Paper"] },
{"key": "wei2023larger", "citations": "86", "year": "2023", "title":"Larger Language Models Do In-context Learning Differently", "abstract": "<p>We study how in-context learning (ICL) in language models is affected by\nsemantic priors versus input-label mappings. We investigate two setups-ICL with\nflipped labels and ICL with semantically-unrelated labels-across various model\nfamilies (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments\non ICL with flipped labels show that overriding semantic priors is an emergent\nability of model scale. While small language models ignore flipped labels\npresented in-context and thus rely primarily on semantic priors from\npretraining, large models can override semantic priors when presented with\nin-context exemplars that contradict priors, despite the stronger semantic\npriors that larger models may hold. We next study semantically-unrelated label\nICL (SUL-ICL), in which labels are semantically unrelated to their inputs\n(e.g., foo/bar instead of negative/positive), thereby forcing language models\nto learn the input-label mappings shown in in-context exemplars in order to\nperform the task. The ability to do SUL-ICL also emerges primarily with scale,\nand large-enough language models can even perform linear classification in a\nSUL-ICL setting. Finally, we evaluate instruction-tuned models and find that\ninstruction tuning strengthens both the use of semantic priors and the capacity\nto learn input-label mappings, but more of the former.</p>\n", "tags": ["In Context Learning","Model Architecture"] },
{"key": "wei2023llmrec", "citations": "68", "year": "2024", "title":"Llmrec: Large Language Models With Graph Augmentation For Recommendation", "abstract": "<p>The problem of data sparsity has long been a challenge in recommendation\nsystems, and previous studies have attempted to address this issue by\nincorporating side information. However, this approach often introduces side\neffects such as noise, availability issues, and low data quality, which in turn\nhinder the accurate modeling of user preferences and adversely impact\nrecommendation performance. In light of the recent advancements in large\nlanguage models (LLMs), which possess extensive knowledge bases and strong\nreasoning capabilities, we propose a novel framework called LLMRec that\nenhances recommender systems by employing three simple yet effective LLM-based\ngraph augmentation strategies. Our approach leverages the rich content\navailable within online platforms (e.g., Netflix, MovieLens) to augment the\ninteraction graph in three ways: (i) reinforcing user-item interaction egde,\n(ii) enhancing the understanding of item node attributes, and (iii) conducting\nuser node profiling, intuitively from the natural language perspective. By\nemploying these strategies, we address the challenges posed by sparse implicit\nfeedback and low-quality side information in recommenders. Besides, to ensure\nthe quality of the augmentation, we develop a denoised data robustification\nmechanism that includes techniques of noisy implicit feedback pruning and\nMAE-based feature enhancement that help refine the augmented data and improve\nits reliability. Furthermore, we provide theoretical analysis to support the\neffectiveness of LLMRec and clarify the benefits of our method in facilitating\nmodel optimization. Experimental results on benchmark datasets demonstrate the\nsuperiority of our LLM-based augmentation approach over state-of-the-art\ntechniques. To ensure reproducibility, we have made our code and augmented data\npublicly available at: https://github.com/HKUDS/LLMRec.git</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Has Code","Tools"] },
{"key": "weiss2020wave", "citations": "67", "year": "2021", "title":"Wave-tacotron: Spectrogram-free End-to-end Text-to-speech Synthesis", "abstract": "<p>We describe a sequence-to-sequence neural network which directly generates\nspeech waveforms from text inputs. The architecture extends the Tacotron model\nby incorporating a normalizing flow into the autoregressive decoder loop.\nOutput waveforms are modeled as a sequence of non-overlapping fixed-length\nblocks, each one containing hundreds of samples. The interdependencies of\nwaveform samples within each block are modeled using the normalizing flow,\nenabling parallel training and synthesis. Longer-term dependencies are handled\nautoregressively by conditioning each flow on preceding blocks.This model can\nbe optimized directly with maximum likelihood, with-out using intermediate,\nhand-designed features nor additional loss terms. Contemporary state-of-the-art\ntext-to-speech (TTS) systems use a cascade of separately learned models: one\n(such as Tacotron) which generates intermediate features (such as spectrograms)\nfrom text, followed by a vocoder (such as WaveRNN) which generates waveform\nsamples from the intermediate features. The proposed system, in contrast, does\nnot use a fixed intermediate representation, and learns all parameters\nend-to-end. Experiments show that the proposed model generates speech with\nquality approaching a state-of-the-art neural TTS system, with significantly\nimproved generation speed.</p>\n", "tags": ["ICASSP","Model Architecture","Training Techniques"] },
{"key": "weissenborn2017dynamic", "citations": "68", "year": "2017", "title":"Dynamic Integration Of Background Knowledge In Neural NLU Systems", "abstract": "<p>Common-sense and background knowledge is required to understand natural\nlanguage, but in most neural natural language understanding (NLU) systems, this\nknowledge must be acquired from training corpora during learning, and then it\nis static at test time. We introduce a new architecture for the dynamic\nintegration of explicit background knowledge in NLU models. A general-purpose\nreading module reads background knowledge in the form of free-text statements\n(together with task-specific text inputs) and yields refined word\nrepresentations to a task-specific NLU architecture that reprocesses the task\ninputs with these representations. Experiments on document question answering\n(DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness\nand flexibility of the approach. Analysis shows that our model learns to\nexploit knowledge in a semantically appropriate way.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "weissenborn2017making", "citations": "185", "year": "2017", "title":"Making Neural QA As Simple As Possible But Not Simpler", "abstract": "<p>Recent development of large-scale question answering (QA) datasets triggered\na substantial amount of research into end-to-end neural architectures for QA.\nIncreasingly complex systems have been conceived without comparison to simpler\nneural baseline systems that would justify their complexity. In this work, we\npropose a simple heuristic that guides the development of neural baseline\nsystems for the extractive QA task. We find that there are two ingredients\nnecessary for building a high-performing neural QA system: first, the awareness\nof question words while processing the context and second, a composition\nfunction that goes beyond simple bag-of-words modeling, such as recurrent\nneural networks. Our results show that FastQA, a system that meets these two\nrequirements, can achieve very competitive performance compared with existing\nmodels. We argue that this surprising finding puts results of previous systems\nand the complexity of recent QA datasets into perspective.</p>\n", "tags": ["Datasets"] },
{"key": "weisz2021perfection", "citations": "93", "year": "2021", "title":"Perfection Not Required? Human-ai Partnerships In Code Translation", "abstract": "<p>Generative models have become adept at producing artifacts such as images,\nvideos, and prose at human-like levels of proficiency. New generative\ntechniques, such as unsupervised neural machine translation (NMT), have\nrecently been applied to the task of generating source code, translating it\nfrom one programming language to another. The artifacts produced in this way\nmay contain imperfections, such as compilation or logical errors. We examine\nthe extent to which software engineers would tolerate such imperfections and\nexplore ways to aid the detection and correction of those errors. Using a\ndesign scenario approach, we interviewed 11 software engineers to understand\ntheir reactions to the use of an NMT model in the context of application\nmodernization, focusing on the task of translating source code from one\nlanguage to another. Our three-stage scenario sparked discussions about the\nutility and desirability of working with an imperfect AI system, how acceptance\nof that system’s outputs would be established, and future opportunities for\ngenerative AI in application modernization. Our study highlights how UI\nfeatures such as confidence highlighting and alternate translations help\nsoftware engineers work with and better understand generative NMT models.</p>\n", "tags": ["Llm For Code"] },
{"key": "welbl2017constructing", "citations": "520", "year": "2018", "title":"Constructing Datasets For Multi-hop Reading Comprehension Across Documents", "abstract": "<p>Most Reading Comprehension methods limit themselves to queries which can be\nanswered using a single sentence, paragraph, or document. Enabling models to\ncombine disjoint pieces of textual evidence would extend the scope of machine\ncomprehension methods, but currently there exist no resources to train and test\nthis capability. We propose a novel task to encourage the development of models\nfor text understanding across multiple documents and to investigate the limits\nof existing methods. In our task, a model learns to seek and combine evidence -\neffectively performing multi-hop (alias multi-step) inference. We devise a\nmethodology to produce datasets for this task, given a collection of\nquery-answer pairs and thematically linked documents. Two datasets from\ndifferent domains are induced, and we identify potential pitfalls and devise\ncircumvention strategies. We evaluate two previously proposed competitive\nmodels and find that one can integrate information across documents. However,\nboth models struggle to select relevant information, as providing documents\nguaranteed to be relevant greatly improves their performance. While the models\noutperform several strong baselines, their best accuracy reaches 42.9% compared\nto human performance at 74.0% - leaving ample room for improvement.</p>\n", "tags": ["Datasets","TACL"] },
{"key": "welbl2021challenges", "citations": "69", "year": "2021", "title":"Challenges In Detoxifying Language Models", "abstract": "<p>Large language models (LM) generate remarkably fluent text and can be\nefficiently adapted across NLP tasks. Measuring and guaranteeing the quality of\ngenerated text in terms of safety is imperative for deploying LMs in the real\nworld; to this end, prior work often relies on automatic evaluation of LM\ntoxicity. We critically discuss this approach, evaluate several toxicity\nmitigation strategies with respect to both automatic and human evaluation, and\nanalyze consequences of toxicity mitigation in terms of model bias and LM\nquality. We demonstrate that while basic intervention strategies can\neffectively optimize previously established automatic metrics on the\nRealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for\nboth texts about, and dialects of, marginalized groups. Additionally, we find\nthat human raters often disagree with high automatic toxicity scores after\nstrong toxicity reduction interventions – highlighting further the nuances\ninvolved in careful evaluation of LM toxicity.</p>\n", "tags": ["EMNLP","Ethics & Fairness","Evaluation"] },
{"key": "weld2018challenge", "citations": "219", "year": "2019", "title":"The Challenge Of Crafting Intelligible Intelligence", "abstract": "<p>Since Artificial Intelligence (AI) software uses techniques like deep\nlookahead search and stochastic optimization of huge neural networks to fit\nmammoth datasets, it often results in complex behavior that is difficult for\npeople to understand. Yet organizations are deploying AI algorithms in many\nmission-critical settings. To trust their behavior, we must make AI\nintelligible, either by using inherently interpretable models or by developing\nnew methods for explaining and controlling otherwise overwhelmingly complex\ndecisions using local approximation, vocabulary alignment, and interactive\nexplanation. This paper argues that intelligibility is essential, surveys\nrecent work on building such systems, and highlights key directions for\nresearch.</p>\n", "tags": ["Datasets"] },
{"key": "welivita2020taxonomy", "citations": "77", "year": "2020", "title":"A Taxonomy Of Empathetic Response Intents In Human Social Conversations", "abstract": "<p>Open-domain conversational agents or chatbots are becoming increasingly\npopular in the natural language processing community. One of the challenges is\nenabling them to converse in an empathetic manner. Current neural response\ngeneration methods rely solely on end-to-end learning from large scale\nconversation data to generate dialogues. This approach can produce socially\nunacceptable responses due to the lack of large-scale quality data used to\ntrain the neural models. However, recent work has shown the promise of\ncombining dialogue act/intent modelling and neural response generation. This\nhybrid method improves the response quality of chatbots and makes them more\ncontrollable and interpretable. A key element in dialog intent modelling is the\ndevelopment of a taxonomy. Inspired by this idea, we have manually labeled 500\nresponse intents using a subset of a sizeable empathetic dialogue dataset (25K\ndialogues). Our goal is to produce a large-scale taxonomy for empathetic\nresponse intents. Furthermore, using lexical and machine learning methods, we\nautomatically analysed both speaker and listener utterances of the entire\ndataset with identified response intents and 32 emotion categories. Finally, we\nuse information visualization methods to summarize emotional dialogue exchange\npatterns and their temporal progression. These results reveal novel and\nimportant empathy patterns in human-human open-domain conversations and can\nserve as heuristics for hybrid approaches.</p>\n", "tags": ["COLING","Datasets"] },
{"key": "welleck2018dialogue", "citations": "252", "year": "2019", "title":"Dialogue Natural Language Inference", "abstract": "<p>Consistency is a long standing issue faced by dialogue models. In this paper,\nwe frame the consistency of dialogue agents as natural language inference (NLI)\nand create a new natural language inference dataset called Dialogue NLI. We\npropose a method which demonstrates that a model trained on Dialogue NLI can be\nused to improve the consistency of a dialogue model, and evaluate the method\nwith human evaluation and with automatic metrics on a suite of evaluation sets\ndesigned to measure a dialogue model’s consistency.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation"] },
{"key": "welleck2019neural", "citations": "255", "year": "2019", "title":"Neural Text Generation With Unlikelihood Training", "abstract": "<p>Neural text generation is a key tool in natural language applications, but it\nis well known there are major problems at its core. In particular, standard\nlikelihood training and decoding leads to dull and repetitive outputs. While\nsome post-hoc fixes have been proposed, in particular top-\\(k\\) and nucleus\nsampling, they do not address the fact that the token-level probabilities\npredicted by the model are poor. In this paper we show that the likelihood\nobjective itself is at fault, resulting in a model that assigns too much\nprobability to sequences containing repeats and frequent words, unlike those\nfrom the human training distribution. We propose a new objective, unlikelihood\ntraining, which forces unlikely generations to be assigned lower probability by\nthe model. We show that both token and sequence level unlikelihood training\ngive less repetitive, less dull text while maintaining perplexity, giving\nsuperior generations using standard greedy or beam search. According to human\nevaluations, our approach with standard beam search also outperforms the\ncurrently popular decoding methods of nucleus sampling or beam blocking, thus\nproviding a strong alternative to existing techniques.</p>\n", "tags": ["Applications","Training Techniques"] },
{"key": "welleck2019non", "citations": "68", "year": "2019", "title":"Non-monotonic Sequential Text Generation", "abstract": "<p>Standard sequential generation methods assume a pre-specified generation\norder, such as text generation methods which generate words from left to right.\nIn this work, we propose a framework for training models of text generation\nthat operate in non-monotonic orders; the model directly learns good orders,\nwithout any additional annotation. Our framework operates by generating a word\nat an arbitrary position, and then recursively generating words to its left and\nthen words to its right, yielding a binary tree. Learning is framed as\nimitation learning, including a coaching method which moves from imitating an\noracle to reinforcing the policy’s own preferences. Experimental results\ndemonstrate that using the proposed method, it is possible to learn policies\nwhich generate text without pre-specifying a generation order, while achieving\ncompetitive performance with conventional left-to-right generation.</p>\n", "tags": ["Tools","Training Techniques"] },
{"key": "weller2019humor", "citations": "105", "year": "2019", "title":"Humor Detection: A Transformer Gets The Last Laugh", "abstract": "<p>Much previous work has been done in attempting to identify humor in text. In\nthis paper we extend that capability by proposing a new task: assessing whether\nor not a joke is humorous. We present a novel way of approaching this problem\nby building a model that learns to identify humorous jokes based on ratings\ngleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using\nthese ratings to determine the level of humor, we then employ a Transformer\narchitecture for its advantages in learning from sentence context. We\ndemonstrate the effectiveness of this approach and show results that are\ncomparable to human performance. We further demonstrate our model’s increased\ncapabilities on humor identification problems, such as the previously created\ndatasets for short jokes and puns. These experiments show that this method\noutperforms all previous work done on these tasks, with an F-measure of 93.1%\nfor the Puns dataset and 98.6% on the Short Jokes dataset.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "weller2020learning", "citations": "61", "year": "2020", "title":"Learning From Task Descriptions", "abstract": "<p>Typically, machine learning systems solve new tasks by training on thousands\nof examples. In contrast, humans can solve new tasks by reading some\ninstructions, with perhaps an example or two. To take a step toward closing\nthis gap, we introduce a framework for developing NLP systems that solve new\ntasks after reading their descriptions, synthesizing prior work in this area.\nWe instantiate this framework with a new English language dataset, ZEST,\nstructured for task-oriented evaluation on unseen tasks. Formulating task\ndescriptions as questions, we ensure each is general enough to apply to many\npossible inputs, thus comprehensively evaluating a model’s ability to solve\neach task. Moreover, the dataset’s structure tests specific types of systematic\ngeneralization. We find that the state-of-the-art T5 model achieves a score of\n12% on ZEST, leaving a significant challenge for NLP researchers.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Tools","Training Techniques"] },
{"key": "wen2016conditional", "citations": "74", "year": "2016", "title":"Conditional Generation And Snapshot Learning In Neural Dialogue Systems", "abstract": "<p>Recently a variety of LSTM-based conditional language models (LM) have been\napplied across a range of language generation tasks. In this work we study\nvarious model architectures and different ways to represent and aggregate the\nsource information in an end-to-end neural dialogue system framework. A method\ncalled snapshot learning is also proposed to facilitate learning from\nsupervised sequential signals by applying a companion cross-entropy objective\nfunction to the conditioning vector. The experimental and analytical results\ndemonstrate firstly that competition occurs between the conditioning vector and\nthe LM, and the differing architectures provide different trade-offs between\nthe two. Secondly, the discriminative power and transparency of the\nconditioning vector is key to providing both model interpretability and better\nperformance. Thirdly, snapshot learning leads to consistent performance\nimprovements independent of which architecture is used.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP","Model Architecture","Tools"] },
{"key": "wen2016multi", "citations": "194", "year": "2016", "title":"Multi-domain Neural Network Language Generation For Spoken Dialogue Systems", "abstract": "<p>Moving from limited-domain natural language generation (NLG) to open domain\nis difficult because the number of semantic input combinations grows\nexponentially with the number of domains. Therefore, it is important to\nleverage existing resources and exploit similarities between domains to\nfacilitate domain adaptation. In this paper, we propose a procedure to train\nmulti-domain, Recurrent Neural Network-based (RNN) language generators via\nmultiple adaptation steps. In this procedure, a model is first trained on\ncounterfeited data synthesised from an out-of-domain dataset, and then fine\ntuned on a small set of in-domain utterances with a discriminative objective\nfunction. Corpus-based evaluation results show that the proposed procedure can\nachieve competitive performance in terms of BLEU score and slot error rate\nwhile significantly reducing the data needed to train generators in new, unseen\ndomains. In subjective testing, human judges confirm that the procedure greatly\nimproves generator performance when only a small amount of data is available in\nthe domain.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","NAACL"] },
{"key": "wen2016network", "citations": "861", "year": "2017", "title":"A Network-based End-to-end Trainable Task-oriented Dialogue System", "abstract": "<p>Teaching machines to accomplish tasks by conversing naturally with humans is\nchallenging. Currently, developing task-oriented dialogue systems requires\ncreating multiple components and typically this involves either a large amount\nof handcrafting, or acquiring costly labelled datasets to solve a statistical\nlearning problem for each component. In this work we introduce a neural\nnetwork-based text-in, text-out end-to-end trainable goal-oriented dialogue\nsystem along with a new way of collecting dialogue data based on a novel\npipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue\nsystems easily and without making too many assumptions about the task at hand.\nThe results show that the model can converse with human subjects naturally\nwhilst helping them to accomplish tasks in a restaurant search domain.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","NAACL","Tools"] },
{"key": "wen2017latent", "citations": "137", "year": "2017", "title":"Latent Intention Dialogue Models", "abstract": "<p>Developing a dialogue agent that is capable of making autonomous decisions\nand communicating by natural language is one of the long-term goals of machine\nlearning research. Traditional approaches either rely on hand-crafting a small\nstate-action set for applying reinforcement learning that is not scalable or\nconstructing deterministic models for learning dialogue sentences that fail to\ncapture natural conversational variability. In this paper, we propose a Latent\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\nlearn underlying dialogue intentions in the framework of neural variational\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\ninterpreted as actions guiding the generation of machine responses, which can\nbe further refined autonomously by reinforcement learning. The experimental\nevaluation of LIDM shows that the model out-performs published benchmarks for\nboth corpus-based and human evaluation, demonstrating the effectiveness of\ndiscrete latent variable models for learning goal-oriented dialogues.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Reinforcement Learning","Tools"] },
{"key": "weng2019acquiring", "citations": "66", "year": "2020", "title":"Acquiring Knowledge From Pre-trained Model To Neural Machine Translation", "abstract": "<p>Pre-training and fine-tuning have achieved great success in the natural\nlanguage process field. The standard paradigm of exploiting them includes two\nsteps: first, pre-training a model, e.g. BERT, with a large scale unlabeled\nmonolingual data. Then, fine-tuning the pre-trained model with labeled data\nfrom downstream tasks. However, in neural machine translation (NMT), we address\nthe problem that the training objective of the bilingual task is far different\nfrom the monolingual pre-trained model. This gap leads that only using\nfine-tuning in NMT can not fully utilize prior language knowledge. In this\npaper, we propose an APT framework for acquiring knowledge from the pre-trained\nmodel to NMT. The proposed approach includes two modules: 1). a dynamic fusion\nmechanism to fuse task-specific features adapted from general knowledge into\nNMT network, 2). a knowledge distillation paradigm to learn language knowledge\ncontinuously during the NMT training process. The proposed approach could\nintegrate suitable knowledge from pre-trained models to improve the NMT.\nExperimental results on WMT English to German, German to English and Chinese to\nEnglish machine translation tasks show that our model outperforms strong\nbaselines and the fine-tuning counterparts.</p>\n", "tags": ["AAAI","Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "west2021symbolic", "citations": "129", "year": "2022", "title":"Symbolic Knowledge Distillation: From General Language Models To Commonsense Models", "abstract": "<p>The common practice for training commonsense models has gone\nfrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs in\norder to train commonsense models. In this work, we investigate an alternative,\nfrom-machine-to-corpus-to-machine: general language models author these\ncommonsense knowledge graphs to train commonsense models. Our study leads to a\nnew framework, Symbolic Knowledge Distillation. As with prior art in Knowledge\nDistillation (Hinton et al., 2015), our approach uses larger models to teach\nsmaller models. A key difference is that we distill knowledge symbolically-as\ntext-in addition to the neural model. We also distill only one aspect-the\ncommonsense of a general language model teacher, allowing the student to be a\ndifferent type, a commonsense model. Altogether, we show that careful prompt\nengineering and a separately trained critic model allow us to selectively\ndistill high-quality causal commonsense from GPT-3, a general language model.\nEmpirical results demonstrate that, for the first time, a human-authored\ncommonsense knowledge graph is surpassed by our automatically distilled variant\nin all three criteria: quantity, quality, and diversity. In addition, it\nresults in a neural commonsense model that surpasses the teacher model’s\ncommonsense capabilities despite its 100x smaller size. We apply this to the\nATOMIC resource, and share our new symbolic knowledge graph and commonsense\nmodels.</p>\n", "tags": ["Datasets","Model Architecture","NAACL","Training Techniques"] },
{"key": "weston2016dialog", "citations": "81", "year": "2016", "title":"Dialog-based Language Learning", "abstract": "<p>A long-term goal of machine learning research is to build an intelligent\ndialog agent. Most research in natural language understanding has focused on\nlearning from fixed training sets of labeled data, with supervision either at\nthe word level (tagging, parsing tasks) or sentence level (question answering,\nmachine translation). This kind of supervision is not realistic of how humans\nlearn, where language is both learned by, and used for, communication. In this\nwork, we study dialog-based language learning, where supervision is given\nnaturally and implicitly in the response of the dialog partner during the\nconversation. We study this setup in two domains: the bAbI dataset of (Weston\net al., 2015) and large-scale question answering from (Dodge et al., 2015). We\nevaluate a set of baseline learning strategies on these tasks, and show that a\nnovel model incorporating predictive lookahead is a promising approach for\nlearning from a teacher’s response. In particular, a surprising result is that\nit can learn to answer questions correctly without any reward-based supervision\nat all.</p>\n", "tags": ["Dialogue & Multi Turn","Reinforcement Learning","Training Techniques"] },
{"key": "weston2018retrieve", "citations": "186", "year": "2018", "title":"Retrieve And Refine: Improved Sequence Generation Models For Dialogue", "abstract": "<p>Sequence generation models for dialogue are known to have several problems:\nthey tend to produce short, generic sentences that are uninformative and\nunengaging. Retrieval models on the other hand can surface interesting\nresponses, but are restricted to the given retrieval set leading to erroneous\nreplies that cannot be tuned to the specific context. In this work we develop a\nmodel that combines the two approaches to avoid both their deficiencies: first\nretrieve a response and then refine it – the final sequence generator treating\nthe retrieval as additional context. We show on the recent CONVAI2 challenge\ntask our approach produces responses superior to both standard retrieval and\ngeneration models in human evaluations.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP"] },
{"key": "wettig2022should", "citations": "108", "year": "2023", "title":"Should You Mask 15% In Masked Language Modeling?", "abstract": "<p>Masked language models (MLMs) conventionally mask 15% of tokens due to the\nbelief that more masking would leave insufficient context to learn good\nrepresentations; this masking rate has been widely used, regardless of model\nsizes or masking strategies. In this work, we revisit this important choice of\nMLM pre-training. We first establish that 15% is not universally optimal, and\nlarger models should adopt a higher masking rate. Specifically, we find that\nmasking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD.\nInterestingly, an extremely high masking rate of 80% can still preserve 95%\nfine-tuning performance and most of the accuracy in linguistic probing,\nchallenging the conventional wisdom about the role of the masking rate. We then\nexamine the interplay between masking rates and masking strategies and find\nthat uniform masking requires a higher masking rate compared to sophisticated\nmasking strategies such as span or PMI masking. Finally, we argue that\nincreasing the masking rate has two distinct effects: it leads to more\ncorruption, which makes the prediction task more difficult; it also enables\nmore predictions, which benefits optimization. Using this framework, we revisit\nBERT’s 80-10-10 corruption strategy. Together, our results contribute to a\nbetter understanding of MLM pre-training.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "whang2019effective", "citations": "70", "year": "2020", "title":"An Effective Domain Adaptive Post-training Method For BERT In Response Selection", "abstract": "<p>We focus on multi-turn response selection in a retrieval-based dialog system.\nIn this paper, we utilize the powerful pre-trained language model\nBi-directional Encoder Representations from Transformer (BERT) for a multi-turn\ndialog system and propose a highly effective post-training method on\ndomain-specific corpus. Although BERT is easily adopted to various NLP tasks\nand outperforms previous baselines of each task, it still has limitations if a\ntask corpus is too focused on a certain domain. Post-training on\ndomain-specific corpus (e.g., Ubuntu Corpus) helps the model to train\ncontextualized representations and words that do not appear in general corpus\n(e.g., English Wikipedia). Experimental results show that our approach achieves\nnew state-of-the-art on two response selection benchmarks (i.e., Ubuntu Corpus\nV1, Advising Corpus) performance improvement by 5.9% and 6% on R@1.</p>\n", "tags": ["Datasets","INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "white2023prompt", "citations": "499", "year": "2023", "title":"A Prompt Pattern Catalog To Enhance Prompt Engineering With Chatgpt", "abstract": "<p>Prompt engineering is an increasingly important skill set needed to converse\neffectively with large language models (LLMs), such as ChatGPT. Prompts are\ninstructions given to an LLM to enforce rules, automate processes, and ensure\nspecific qualities (and quantities) of generated output. Prompts are also a\nform of programming that can customize the outputs and interactions with an\nLLM. This paper describes a catalog of prompt engineering techniques presented\nin pattern form that have been applied to solve common problems when conversing\nwith LLMs. Prompt patterns are a knowledge transfer method analogous to\nsoftware patterns since they provide reusable solutions to common problems\nfaced in a particular context, i.e., output generation and interaction when\nworking with LLMs. This paper provides the following contributions to research\non prompt engineering that apply LLMs to automate software development tasks.\nFirst, it provides a framework for documenting patterns for structuring prompts\nto solve a range of problems so that they can be adapted to different domains.\nSecond, it presents a catalog of patterns that have been applied successfully\nto improve the outputs of LLM conversations. Third, it explains how prompts can\nbe built from multiple patterns and illustrates prompt patterns that benefit\nfrom combination with other prompt patterns.</p>\n", "tags": ["Prompting","Tools"] },
{"key": "wiedemann2020uhh", "citations": "65", "year": "2020", "title":"UHH-LT At Semeval-2020 Task 12: Fine-tuning Of Pre-trained Transformer Networks For Offensive Language Detection", "abstract": "<p>Fine-tuning of pre-trained transformer networks such as BERT yield\nstate-of-the-art results for text classification tasks. Typically, fine-tuning\nis performed on task-specific training datasets in a supervised manner. One can\nalso fine-tune in unsupervised manner beforehand by further pre-training the\nmasked language modeling (MLM) task. Hereby, in-domain data for unsupervised\nMLM resembling the actual classification target dataset allows for domain\nadaptation of the model. In this paper, we compare current pre-trained\ntransformer networks with and without MLM fine-tuning on their performance for\noffensive language detection. Our MLM fine-tuned RoBERTa-based classifier\nofficially ranks 1st in the SemEval 2020 Shared Task~12 for the English\nlanguage. Further experiments with the ALBERT model even surpass this result.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wiegreffe2019attention", "citations": "753", "year": "2019", "title":"Attention Is Not Not Explanation", "abstract": "<p>Attention mechanisms play a central role in NLP systems, especially within\nrecurrent neural network (RNN) models. Recently, there has been increasing\ninterest in whether or not the intermediate representations offered by these\nmodules may be used to explain the reasoning for a model’s prediction, and\nconsequently reach insights regarding the model’s decision-making process. A\nrecent paper claims that `Attention is not Explanation’ (Jain and Wallace,\n2019). We challenge many of the assumptions underlying this work, arguing that\nsuch a claim depends on one’s definition of explanation, and that testing it\nneeds to take into account all elements of the model, using a rigorous\nexperimental design. We propose four alternative tests to determine\nwhen/whether attention can be used as explanation: a simple uniform-weights\nbaseline; a variance calibration based on multiple random seed runs; a\ndiagnostic framework using frozen weights from pretrained models; and an\nend-to-end adversarial attention training protocol. Each allows for meaningful\ninterpretation of attention mechanisms in RNN models. We show that even when\nreliable adversarial distributions can be found, they don’t perform well on the\nsimple diagnostic, indicating that prior work does not disprove the usefulness\nof attention mechanisms for explainability.</p>\n", "tags": ["EMNLP","Model Architecture","Tools","Training Techniques"] },
{"key": "wiegreffe2020measuring", "citations": "88", "year": "2021", "title":"Measuring Association Between Labels And Free-text Rationales", "abstract": "<p>In interpretable NLP, we require faithful rationales that reflect the model’s\ndecision-making process for an explained instance. While prior work focuses on\nextractive rationales (a subset of the input words), we investigate their\nless-studied counterpart: free-text natural language rationales. We demonstrate\nthat pipelines, existing models for faithful extractive rationalization on\ninformation-extraction style tasks, do not extend as reliably to “reasoning”\ntasks requiring free-text rationales. We turn to models that jointly predict\nand rationalize, a class of widely used high-performance models for free-text\nrationalization whose faithfulness is not yet established. We define\nlabel-rationale association as a necessary property for faithfulness: the\ninternal mechanisms of the model producing the label and the rationale must be\nmeaningfully correlated. We propose two measurements to test this property:\nrobustness equivalence and feature importance agreement. We find that\nstate-of-the-art T5-based joint models exhibit both properties for\nrationalizing commonsense question-answering and natural language inference,\nindicating their potential for producing faithful free-text rationales.</p>\n", "tags": ["EMNLP"] },
{"key": "wiegreffe2021reframing", "citations": "69", "year": "2022", "title":"Reframing Human-ai Collaboration For Generating Free-text Explanations", "abstract": "<p>Large language models are increasingly capable of generating fluent-appearing\ntext with relatively little task-specific supervision. But can these models\naccurately explain classification decisions? We consider the task of generating\nfree-text explanations using human-written examples in a few-shot manner. We\nfind that (1) authoring higher quality prompts results in higher quality\ngenerations; and (2) surprisingly, in a head-to-head comparison, crowdworkers\noften prefer explanations generated by GPT-3 to crowdsourced explanations in\nexisting datasets. Our human studies also show, however, that while models\noften produce factual, grammatical, and sufficient explanations, they have room\nto improve along axes such as providing novel information and supporting the\nlabel. We create a pipeline that combines GPT-3 with a supervised filter that\nincorporates binary acceptability judgments from humans in the loop. Despite\nthe intrinsic subjectivity of acceptability judgments, we demonstrate that\nacceptability is partially correlated with various fine-grained attributes of\nexplanations. Our approach is able to consistently filter GPT-3-generated\nexplanations deemed acceptable by humans.</p>\n", "tags": ["Datasets","Few-Shot","Model Architecture","NAACL"] },
{"key": "wieting2017learning", "citations": "93", "year": "2017", "title":"Learning Paraphrastic Sentence Embeddings From Back-translated Bitext", "abstract": "<p>We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings in the setting of Wieting et al. (2016b). We use neural machine\ntranslation to generate sentential paraphrases via back-translation of\nbilingual sentence pairs. We evaluate the paraphrase pairs by their ability to\nserve as training data for learning paraphrastic sentence embeddings. We find\nthat the data quality is stronger than prior work based on bitext and on par\nwith manually-written English paraphrase pairs, with the advantage that our\napproach can scale up to generate large training sets for many languages and\ndomains. We experiment with several language pairs and data sources, and\ndevelop a variety of data filtering techniques. In the process, we explore how\nneural machine translation output differs from human-written sentences, finding\nclear differences in length, the amount of repetition, and the use of rare\nwords.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "wieting2017paranmt", "citations": "346", "year": "2018", "title":"Paranmt-50m: Pushing The Limits Of Paraphrastic Sentence Embeddings With Millions Of Machine Translations", "abstract": "<p>We describe PARANMT-50M, a dataset of more than 50 million English-English\nsentential paraphrase pairs. We generated the pairs automatically by using\nneural machine translation to translate the non-English side of a large\nparallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M\ncan be a valuable resource for paraphrase generation and can provide a rich\nsource of semantic knowledge to improve downstream natural language\nunderstanding tasks. To show its utility, we use ParaNMT-50M to train\nparaphrastic sentence embeddings that outperform all supervised systems on\nevery SemEval semantic textual similarity competition, in addition to showing\nhow it can be used for paraphrase generation.</p>\n", "tags": ["Datasets"] },
{"key": "wieting2017revisiting", "citations": "74", "year": "2017", "title":"Revisiting Recurrent Networks For Paraphrastic Sentence Embeddings", "abstract": "<p>We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings, revisiting the setting of Wieting et al. (2016b). While they found\nLSTM recurrent networks to underperform word averaging, we present several\ndevelopments that together produce the opposite conclusion. These include\ntraining on sentence pairs rather than phrase pairs, averaging states to\nrepresent sequences, and regularizing aggressively. These improve LSTMs in both\ntransfer learning and supervised settings. We also introduce a new recurrent\narchitecture, the Gated Recurrent Averaging Network, that is inspired by\naveraging and LSTMs while outperforming them both. We analyze our learned\nmodels, finding evidence of preferences for particular parts of speech and\ndependency relations.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wilcox2018what", "citations": "166", "year": "2018", "title":"What Do RNN Language Models Learn About Filler-gap Dependencies?", "abstract": "<p>RNN language models have achieved state-of-the-art perplexity results and\nhave proven useful in a suite of NLP tasks, but it is as yet unclear what\nsyntactic generalizations they learn. Here we investigate whether\nstate-of-the-art RNN language models represent long-distance filler-gap\ndependencies and constraints on them. Examining RNN behavior on experimentally\ncontrolled sentences designed to expose filler-gap dependencies, we show that\nRNNs can represent the relationship in multiple syntactic positions and over\nlarge spans of text. Furthermore, we show that RNNs learn a subset of the known\nrestrictions on filler-gap dependencies, known as island constraints: RNNs show\nevidence for wh-islands, adjunct islands, and complex NP islands. These studies\ndemonstrates that state-of-the-art RNN models are able to learn and generalize\nabout empty syntactic positions.</p>\n", "tags": ["EMNLP"] },
{"key": "wilcox2019structural", "citations": "66", "year": "2019", "title":"Structural Supervision Improves Learning Of Non-local Grammatical Dependencies", "abstract": "<p>State-of-the-art LSTM language models trained on large corpora learn\nsequential contingencies in impressive detail and have been shown to acquire a\nnumber of non-local grammatical dependencies with some success. Here we\ninvestigate whether supervision with hierarchical structure enhances learning\nof a range of grammatical dependencies, a question that has previously been\naddressed only for subject-verb agreement. Using controlled experimental\nmethods from psycholinguistics, we compare the performance of word-based LSTM\nmodels versus two models that represent hierarchical structure and deploy it in\nleft-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer et\nal., 2016) and a incrementalized version of the Parsing-as-Language-Modeling\nconfiguration from Chariak et al., (2016). Models are tested on a diverse range\nof configurations for two classes of non-local grammatical dependencies in\nEnglish—Negative Polarity licensing and Filler–Gap Dependencies. Using the\nsame training data across models, we find that structurally-supervised models\noutperform the LSTM, with the RNNG demonstrating best results on both types of\ngrammatical dependencies and even learning many of the Island Constraints on\nthe filler–gap dependency. Structural supervision thus provides data\nefficiency advantages over purely string-based training of neural language\nmodels in acquiring human-like generalizations about non-local grammatical\ndependencies.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "wilcox2020predictive", "citations": "87", "year": "2020", "title":"On The Predictive Power Of Neural Language Models For Human Real-time Comprehension Behavior", "abstract": "<p>Human reading behavior is tuned to the statistics of natural language: the\ntime it takes human subjects to read a word can be predicted from estimates of\nthe word’s probability in context. However, it remains an open question what\ncomputational architecture best characterizes the expectations deployed in real\ntime by humans that determine the behavioral signatures of reading. Here we\ntest over two dozen models, independently manipulating computational\narchitecture and training dataset size, on how well their next-word\nexpectations predict human reading time behavior on naturalistic text corpora.\nWe find that across model architectures and training dataset sizes the\nrelationship between word log-probability and reading time is (near-)linear. We\nnext evaluate how features of these models determine their psychometric\npredictive power, or ability to predict human reading behavior. In general, the\nbetter a model’s next-word expectations, the better its psychometric predictive\npower. However, we find nontrivial differences across model architectures. For\nany given perplexity, deep Transformer models and n-gram models generally show\nsuperior psychometric predictive power over LSTM or structurally supervised\nneural models, especially for eye movement data. Finally, we compare models’\npsychometric predictive power to the depth of their syntactic knowledge, as\nmeasured by a battery of syntactic generalization tests developed using methods\nfrom controlled psycholinguistic experiments. Once perplexity is controlled\nfor, we find no significant relationship between syntactic knowledge and\npredictive power. These results suggest that different approaches may be\nrequired to best model human real-time language comprehension behavior in\nnaturalistic reading versus behavior for controlled linguistic materials\ndesigned for targeted probing of syntactic knowledge.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "williams2016end", "citations": "127", "year": "2016", "title":"End-to-end Lstm-based Dialog Control Optimized With Supervised And Reinforcement Learning", "abstract": "<p>This paper presents a model for end-to-end learning of task-oriented dialog\nsystems. The main component of the model is a recurrent neural network (an\nLSTM), which maps from raw dialog history directly to a distribution over\nsystem actions. The LSTM automatically infers a representation of dialog\nhistory, which relieves the system developer of much of the manual feature\nengineering of dialog state. In addition, the developer can provide software\nthat expresses business rules and provides access to programmatic APIs,\nenabling the LSTM to take actions in the real world on behalf of the user. The\nLSTM can be optimized using supervised learning (SL), where a domain expert\nprovides example dialogs which the LSTM should imitate; or using reinforcement\nlearning (RL), where the system improves by interacting directly with end\nusers. Experiments show that SL and RL are complementary: SL alone can derive a\nreasonable initial policy from a small number of training dialogs; and starting\nRL optimization with a policy trained with SL substantially accelerates the\nlearning rate of RL.</p>\n", "tags": ["Efficiency","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "williams2017broad", "citations": "3361", "year": "2018", "title":"A Broad-coverage Challenge Corpus For Sentence Understanding Through Inference", "abstract": "<p>This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)\ncorpus, a dataset designed for use in the development and evaluation of machine\nlearning models for sentence understanding. In addition to being one of the\nlargest corpora available for the task of NLI, at 433k examples, this corpus\nimproves upon available resources in its coverage: it offers data from ten\ndistinct genres of written and spoken English–making it possible to evaluate\nsystems on nearly the full complexity of the language–and it offers an\nexplicit setting for the evaluation of cross-genre domain adaptation.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","NAACL"] },
{"key": "williams2017hybrid", "citations": "357", "year": "2017", "title":"Hybrid Code Networks: Practical And Efficient End-to-end Dialog Control With Supervised And Reinforcement Learning", "abstract": "<p>End-to-end learning of recurrent neural networks (RNNs) is an attractive\nsolution for dialog systems; however, current techniques are data-intensive and\nrequire thousands of dialogs to learn simple behaviors. We introduce Hybrid\nCode Networks (HCNs), which combine an RNN with domain-specific knowledge\nencoded as software and system action templates. Compared to existing\nend-to-end approaches, HCNs considerably reduce the amount of training data\nrequired, while retaining the key benefit of inferring a latent representation\nof dialog state. In addition, HCNs can be optimized with supervised learning,\nreinforcement learning, or a mixture of both. HCNs attain state-of-the-art\nperformance on the bAbI dialog dataset, and outperform two commercially\ndeployed customer-facing dialog systems.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning","Training Techniques"] },
{"key": "winata2019code", "citations": "92", "year": "2019", "title":"Code-switched Language Models Using Neural Based Synthetic Data From Parallel Sentences", "abstract": "<p>Training code-switched language models is difficult due to lack of data and\ncomplexity in the grammatical structure. Linguistic constraint theories have\nbeen used for decades to generate artificial code-switching sentences to cope\nwith this issue. However, this require external word alignments or constituency\nparsers that create erroneous results on distant languages. We propose a\nsequence-to-sequence model using a copy mechanism to generate code-switching\ndata by leveraging parallel monolingual translations from a limited source of\ncode-switching data. The model learns how to combine words from parallel\nsentences and identifies when to switch one language to the other. Moreover, it\ncaptures code-switching constraints by attending and aligning the words in\ninputs, without requiring any external knowledge. Based on experimental\nresults, the language model trained with the generated sentences achieves\nstate-of-the-art performance and improves end-to-end automatic speech\nrecognition.</p>\n", "tags": ["Training Techniques"] },
{"key": "winata2021language", "citations": "67", "year": "2021", "title":"Language Models Are Few-shot Multilingual Learners", "abstract": "<p>General-purpose language models have demonstrated impressive capabilities,\nperforming on par with state-of-the-art approaches on a range of downstream\nnatural language processing (NLP) tasks and benchmarks when inferring\ninstructions from very few examples. Here, we evaluate the multilingual skills\nof the GPT and T5 models in conducting multi-class classification on\nnon-English languages without any parameter updates. We show that, given a few\nEnglish examples as context, pre-trained language models can predict not only\nEnglish test samples but also non-English ones. Finally, we find the in-context\nfew-shot cross-lingual prediction results of language models are significantly\nbetter than random prediction, and they are competitive compared to the\nexisting state-of-the-art cross-lingual models.</p>\n", "tags": ["Few-Shot","Model Architecture"] },
{"key": "wiseman2016sequence", "citations": "463", "year": "2016", "title":"Sequence-to-sequence Learning As Beam-search Optimization", "abstract": "<p>Sequence-to-Sequence (seq2seq) modeling has rapidly become an important\ngeneral-purpose NLP tool that has proven effective for many text-generation and\nsequence-labeling tasks. Seq2seq builds on deep neural language modeling and\ninherits its remarkable accuracy in estimating local, next-word distributions.\nIn this work, we introduce a model and beam-search training scheme, based on\nthe work of Daume III and Marcu (2005), that extends seq2seq to learn global\nsequence scores. This structured approach avoids classical biases associated\nwith local training and unifies the training loss with the test-time usage,\nwhile preserving the proven model architecture of seq2seq and its efficient\ntraining approach. We show that our system outperforms a highly-optimized\nattention-based seq2seq system and other baselines on three different sequence\nto sequence tasks: word ordering, parsing, and machine translation.</p>\n", "tags": ["EMNLP","Efficiency","Model Architecture","Training Techniques"] },
{"key": "wiseman2017challenges", "citations": "524", "year": "2017", "title":"Challenges In Data-to-document Generation", "abstract": "<p>Recent neural models have shown significant progress on the problem of\ngenerating short descriptive texts conditioned on a small number of database\nrecords. In this work, we suggest a slightly more difficult data-to-text\ngeneration task, and investigate how effective current approaches are on this\ntask. In particular, we introduce a new, large-scale corpus of data records\npaired with descriptive documents, propose a series of extractive evaluation\nmethods for analyzing performance, and obtain baseline results using current\nneural generation methods. Experiments show that these models produce fluent\ntext, but fail to convincingly approximate human-generated documents. Moreover,\neven templated baselines exceed the performance of these neural models on some\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\nimprovements.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "wiseman2018learning", "citations": "214", "year": "2018", "title":"Learning Neural Templates For Text Generation", "abstract": "<p>While neural, encoder-decoder models have had significant empirical success\nin text generation, there remain several unaddressed problems with this style\nof generation. Encoder-decoder models are largely (a) uninterpretable, and (b)\ndifficult to control in terms of their phrasing or content. This work proposes\na neural generation system using a hidden semi-markov model (HSMM) decoder,\nwhich learns latent, discrete templates jointly with learning to generate. We\nshow that this model learns useful templates, and that these templates make\ngeneration both more interpretable and controllable. Furthermore, we show that\nthis approach scales to real data sets and achieves strong performance nearing\nthat of encoder-decoder text generation models.</p>\n", "tags": ["EMNLP"] },
{"key": "witteveen2019paraphrasing", "citations": "78", "year": "2019", "title":"Paraphrasing With Large Language Models", "abstract": "<p>Recently, large language models such as GPT-2 have shown themselves to be\nextremely adept at text generation and have also been able to achieve\nhigh-quality results in many downstream NLP tasks such as text classification,\nsentiment analysis and question answering with the aid of fine-tuning. We\npresent a useful technique for using a large language model to perform the task\nof paraphrasing on a variety of texts and subjects. Our approach is\ndemonstrated to be capable of generating paraphrases not only at a sentence\nlevel but also for longer spans of text such as paragraphs without needing to\nbreak the text into smaller chunks.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wolf2019huggingface", "citations": "3228", "year": "2019", "title":"Huggingface's Transformers: State-of-the-art Natural Language Processing", "abstract": "<p>Recent progress in natural language processing has been driven by advances in\nboth model architecture and model pretraining. Transformer architectures have\nfacilitated building higher-capacity models and pretraining has made it\npossible to effectively utilize this capacity for a wide variety of tasks.\n\\textit{Transformers} is an open-source library with the goal of opening up\nthese advances to the wider machine learning community. The library consists of\ncarefully engineered state-of-the art Transformer architectures under a unified\nAPI. Backing this library is a curated collection of pretrained models made by\nand available for the community. \\textit{Transformers} is designed to be\nextensible by researchers, simple for practitioners, and fast and robust in\nindustrial deployments. The library is available at\nhttps://github.com/huggingface/transformers.</p>\n", "tags": ["Applications","Has Code","Model Architecture","Tools"] },
{"key": "wolf2019transfertransfo", "citations": "289", "year": "2019", "title":"Transfertransfo: A Transfer Learning Approach For Neural Network Based Conversational Agents", "abstract": "<p>We introduce a new approach to generative data-driven dialogue systems (e.g.\nchatbots) called TransferTransfo which is a combination of a Transfer learning\nbased training scheme and a high-capacity Transformer model. Fine-tuning is\nperformed by using a multi-task objective which combines several unsupervised\nprediction tasks. The resulting fine-tuned model shows strong improvements over\nthe current state-of-the-art end-to-end conversational models like memory\naugmented seq2seq and information-retrieval models. On the privately held\nPERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this\napproach obtains a new state-of-the-art, with respective perplexity, Hits@1 and\nF1 metrics of 16.28 (45 % absolute improvement), 80.7 (46 % absolute\nimprovement) and 19.5 (20 % absolute improvement).</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "wong2022seeing", "citations": "70", "year": "2023", "title":"Seeing Like A Toolkit: How Toolkits Envision The Work Of AI Ethics", "abstract": "<p>Numerous toolkits have been developed to support ethical AI development.\nHowever, toolkits, like all tools, encode assumptions in their design about\nwhat work should be done and how. In this paper, we conduct a qualitative\nanalysis of 27 AI ethics toolkits to critically examine how the work of ethics\nis imagined and how it is supported by these toolkits. Specifically, we examine\nthe discourses toolkits rely on when talking about ethical issues, who they\nimagine should do the work of ethics, and how they envision the work practices\ninvolved in addressing ethics. Among the toolkits, we identify a mismatch\nbetween the imagined work of ethics and the support the toolkits provide for\ndoing that work. In particular, we identify a lack of guidance around how to\nnavigate labor, organizational, and institutional power dynamics as they relate\nto performing ethical work. We use these omissions to chart future work for\nresearchers and designers of AI ethics toolkits.</p>\n", "tags": ["Ethics & Fairness","Tools"] },
{"key": "worsham2020multi", "citations": "69", "year": "2020", "title":"Multi-task Learning For Natural Language Processing In The 2020s: Where Are We Going?", "abstract": "<p>Multi-task learning (MTL) significantly pre-dates the deep learning era, and\nit has seen a resurgence in the past few years as researchers have been\napplying MTL to deep learning solutions for natural language tasks. While\nsteady MTL research has always been present, there is a growing interest driven\nby the impressive successes published in the related fields of transfer\nlearning and pre-training, such as BERT, and the release of new challenge\nproblems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place\nmore focus on how weights are shared across networks, evaluate the re-usability\nof network components and identify use cases where MTL can significantly\noutperform single-task solutions. This paper strives to provide a comprehensive\nsurvey of the numerous recent MTL contributions to the field of natural\nlanguage processing and provide a forum to focus efforts on the hardest\nunsolved problems in the next decade. While novel models that improve\nperformance on NLP benchmarks are continually produced, lasting MTL challenges\nremain unsolved which could hold the key to better language understanding,\nknowledge discovery and natural language interfaces.</p>\n", "tags": ["Applications","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "wortsman2022model", "citations": "115", "year": "2022", "title":"Model Soups: Averaging Weights Of Multiple Fine-tuned Models Improves Accuracy Without Increasing Inference Time", "abstract": "<p>The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs – we call\nthe results “model soups.” When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\nImageNet, achieved a new state of the art. Furthermore, we show that the model\nsoup approach extends to multiple image classification and natural language\nprocessing tasks, improves out-of-distribution performance, and improves\nzero-shot performance on new downstream tasks. Finally, we analytically relate\nthe performance similarity of weight-averaging and logit-ensembling to flatness\nof the loss and confidence of the predictions, and validate this relation\nempirically. Code is available at https://github.com/mlfoundations/model-soups.</p>\n", "tags": ["Fine-Tuning","Has Code","Training Techniques"] },
{"key": "wu2016google", "citations": "5821", "year": "2016", "title":"Google's Neural Machine Translation System: Bridging The Gap Between Human And Machine Translation", "abstract": "<p>Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT’s use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google’s\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (“wordpieces”) for both input and output. This\nmethod provides a good balance between the flexibility of “character”-delimited\nmodels and the efficiency of “word”-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT’14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google’s phrase-based production system.</p>\n", "tags": ["Applications","Evaluation","Model Architecture","Training Techniques"] },
{"key": "wu2016image", "citations": "396", "year": "2017", "title":"Image Captioning And Visual Question Answering Based On Attributes And External Knowledge", "abstract": "<p>Much recent progress in Vision-to-Language problems has been achieved through\na combination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs). This approach does not explicitly represent high-level\nsemantic concepts, but rather seeks to progress directly from image features to\ntext. In this paper we first propose a method of incorporating high-level\nconcepts into the successful CNN-RNN approach, and show that it achieves a\nsignificant improvement on the state-of-the-art in both image captioning and\nvisual question answering. We further show that the same mechanism can be used\nto incorporate external knowledge, which is critically important for answering\nhigh level visual questions. Specifically, we design a visual question\nanswering model that combines an internal representation of the content of an\nimage with information extracted from a general knowledge base to answer a\nbroad range of image-based questions. It particularly allows questions to be\nasked about the contents of an image, even when the image itself does not\ncontain a complete answer. Our final model achieves the best reported results\non both image captioning and visual question answering on several benchmark\ndatasets.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "wu2016sequential", "citations": "550", "year": "2017", "title":"Sequential Matching Network: A New Architecture For Multi-turn Response Selection In Retrieval-based Chatbots", "abstract": "<p>We study response selection for multi-turn conversation in retrieval-based\nchatbots. Existing work either concatenates utterances in context or matches a\nresponse with a highly abstract context vector finally, which may lose\nrelationships among utterances or important contextual information. We propose\na sequential matching network (SMN) to address both problems. SMN first matches\na response with each utterance in the context on multiple levels of\ngranularity, and distills important matching information from each pair as a\nvector with convolution and pooling operations. The vectors are then\naccumulated in a chronological order through a recurrent neural network (RNN)\nwhich models relationships among utterances. The final matching score is\ncalculated with the hidden states of the RNN. An empirical study on two public\ndata sets shows that SMN can significantly outperform state-of-the-art methods\nfor response selection in multi-turn conversation.</p>\n", "tags": ["Dialogue & Multi Turn","Model Architecture"] },
{"key": "wu2016visual", "citations": "401", "year": "2017", "title":"Visual Question Answering: A Survey Of Methods And Datasets", "abstract": "<p>Visual Question Answering (VQA) is a challenging task that has received\nincreasing attention from both the computer vision and the natural language\nprocessing communities. Given an image and a question in natural language, it\nrequires reasoning over visual elements of the image and general knowledge to\ninfer the correct answer. In the first part of this survey, we examine the\nstate of the art by comparing modern approaches to the problem. We classify\nmethods by their mechanism to connect the visual and textual modalities. In\nparticular, we examine the common approach of combining convolutional and\nrecurrent neural networks to map images and questions to a common feature\nspace. We also discuss memory-augmented and modular architectures that\ninterface with structured knowledge bases. In the second part of this survey,\nwe review the datasets available for training and evaluating VQA systems. The\nvarious datatsets contain questions at different levels of complexity, which\nrequire different capabilities and types of reasoning. We examine in depth the\nquestion/answer pairs from the Visual Genome project, and evaluate the\nrelevance of the structured annotations of images with scene graphs for VQA.\nFinally, we discuss promising future directions for the field, in particular\nthe connection to structured knowledge bases and the use of natural language\nprocessing models.</p>\n", "tags": ["Datasets","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "wu2017adversarial", "citations": "78", "year": "2017", "title":"Adversarial Neural Machine Translation", "abstract": "<p>In this paper, we study a new learning paradigm for Neural Machine\nTranslation (NMT). Instead of maximizing the likelihood of the human\ntranslation as in previous works, we minimize the distinction between human\ntranslation and the translation given by an NMT model. To achieve this goal,\ninspired by the recent success of generative adversarial networks (GANs), we\nemploy an adversarial training architecture and name it as Adversarial-NMT. In\nAdversarial-NMT, the training of the NMT model is assisted by an adversary,\nwhich is an elaborately designed Convolutional Neural Network (CNN). The goal\nof the adversary is to differentiate the translation result generated by the\nNMT model from that by human. The goal of the NMT model is to produce high\nquality translations so as to cheat the adversary. A policy gradient method is\nleveraged to co-train the NMT model and the adversary. Experimental results on\nEnglish\\(\\rightarrow\\)French and German\\(\\rightarrow\\)English translation tasks\nshow that Adversarial-NMT can achieve significantly better translation quality\nthan several strong baselines.</p>\n", "tags": ["Model Architecture","Reinforcement Learning","Security","Training Techniques"] },
{"key": "wu2017are", "citations": "153", "year": "2018", "title":"Are You Talking To Me? Reasoned Visual Dialog Generation Through Adversarial Learning", "abstract": "<p>The Visual Dialogue task requires an agent to engage in a conversation about\nan image with a human. It represents an extension of the Visual Question\nAnswering task in that the agent needs to answer a question about an image, but\nit needs to do so in light of the previous dialogue that has taken place. The\nkey challenge in Visual Dialogue is thus maintaining a consistent, and natural\ndialogue while continuing to answer questions correctly. We present a novel\napproach that combines Reinforcement Learning and Generative Adversarial\nNetworks (GANs) to generate more human-like responses to questions. The GAN\nhelps overcome the relative paucity of training data, and the tendency of the\ntypical MLE-based approach to generate overly terse answers. Critically, the\nGAN is tightly integrated into the attention mechanism that generates\nhuman-interpretable reasons for each answer. This means that the discriminative\nmodel of the GAN has the task of assessing whether a candidate answer is\ngenerated by a human or not, given the provided reason. This is significant\nbecause it drives the generative model to produce high quality answers that are\nwell supported by the associated reasoning. The method also generates the\nstate-of-the-art results on the primary benchmark.</p>\n", "tags": ["Agentic","CVPR","Datasets","Evaluation","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "wu2017sequential", "citations": "71", "year": "2018", "title":"A Sequential Matching Framework For Multi-turn Response Selection In Retrieval-based Chatbots", "abstract": "<p>We study the problem of response selection for multi-turn conversation in\nretrieval-based chatbots. The task requires matching a response candidate with\na conversation context, whose challenges include how to recognize important\nparts of the context, and how to model the relationships among utterances in\nthe context. Existing matching methods may lose important information in\ncontexts as we can interpret them with a unified framework in which contexts\nare transformed to fixed-length vectors without any interaction with responses\nbefore matching. The analysis motivates us to propose a new matching framework\nthat can sufficiently carry the important information in contexts to matching\nand model the relationships among utterances at the same time. The new\nframework, which we call a sequential matching framework (SMF), lets each\nutterance in a context interacts with a response candidate at the first step\nand transforms the pair to a matching vector. The matching vectors are then\naccumulated following the order of the utterances in the context with a\nrecurrent neural network (RNN) which models the relationships among the\nutterances. The context-response matching is finally calculated with the hidden\nstates of the RNN. Under SMF, we propose a sequential convolutional network and\nsequential attention network and conduct experiments on two public data sets to\ntest their performance. Experimental results show that both models can\nsignificantly outperform the state-of-the-art matching methods. We also show\nthat the models are interpretable with visualizations that provide us insights\non how they capture and leverage the important information in contexts for\nmatching.</p>\n", "tags": ["Dialogue & Multi Turn","Model Architecture","Tools"] },
{"key": "wu2018faithful", "citations": "68", "year": "2019", "title":"Faithful Multimodal Explanation For Visual Question Answering", "abstract": "<p>AI systems’ ability to explain their reasoning is critical to their utility\nand trustworthiness. Deep neural networks have enabled significant progress on\nmany challenging problems such as visual question answering (VQA). However,\nmost of them are opaque black boxes with limited explanatory capability. This\npaper presents a novel approach to developing a high-performing VQA system that\ncan elucidate its answers with integrated textual and visual explanations that\nfaithfully reflect important aspects of its underlying reasoning while\ncapturing the style of comprehensible human explanations. Extensive\nexperimental evaluation demonstrates the advantages of this approach compared\nto competing methods with both automatic evaluation metrics and human\nevaluation metrics.</p>\n", "tags": ["Evaluation"] },
{"key": "wu2018learning", "citations": "133", "year": "2018", "title":"Learning Matching Models With Weak Supervision For Response Selection In Retrieval-based Chatbots", "abstract": "<p>We propose a method that can leverage unlabeled data to learn a matching\nmodel for response selection in retrieval-based chatbots. The method employs a\nsequence-to-sequence architecture (Seq2Seq) model as a weak annotator to judge\nthe matching degree of unlabeled pairs, and then performs learning with both\nthe weak signals and the unlabeled data. Experimental results on two public\ndata sets indicate that matching models get significant improvements when they\nare learned with the proposed method.</p>\n", "tags": ["AAAI","Model Architecture","Retrieval Systems"] },
{"key": "wu2018response", "citations": "118", "year": "2019", "title":"Response Generation By Context-aware Prototype Editing", "abstract": "<p>Open domain response generation has achieved remarkable progress in recent\nyears, but sometimes yields short and uninformative responses. We propose a new\nparadigm for response generation, that is response generation by editing, which\nsignificantly increases the diversity and informativeness of the generation\nresults. Our assumption is that a plausible response can be generated by\nslightly revising an existing response prototype. The prototype is retrieved\nfrom a pre-defined index and provides a good start-point for generation because\nit is grammatical and informative. We design a response editing model, where an\nedit vector is formed by considering differences between a prototype context\nand a current context, and then the edit vector is fed to a decoder to revise\nthe prototype response for the current context. Experiment results on a large\nscale dataset demonstrate that the response editing model outperforms\ngenerative and retrieval-based models on various aspects.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "wu2018study", "citations": "155", "year": "2018", "title":"A Study Of Reinforcement Learning For Neural Machine Translation", "abstract": "<p>Recent studies have shown that reinforcement learning (RL) is an effective\napproach for improving the performance of neural machine translation (NMT)\nsystem. However, due to its instability, successfully RL training is\nchallenging, especially in real-world systems where deep models and large\ndatasets are leveraged. In this paper, taking several large-scale translation\ntasks as testbeds, we conduct a systematic study on how to train better NMT\nmodels using reinforcement learning. We provide a comprehensive comparison of\nseveral important factors (e.g., baseline reward, reward shaping) in RL\ntraining. Furthermore, to fill in the gap that it remains unclear whether RL is\nstill beneficial when monolingual data is used, we propose a new method to\nleverage RL to further boost the performance of NMT systems trained with\nsource/target monolingual data. By integrating all our findings, we obtain\ncompetitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17\nChinese-English translation tasks, especially setting a state-of-the-art\nperformance on WMT17 Chinese-English translation task.</p>\n", "tags": ["Datasets","EMNLP","Reinforcement Learning","Training Techniques"] },
{"key": "wu2019coreference", "citations": "155", "year": "2020", "title":"Coreference Resolution As Query-based Span Prediction", "abstract": "<p>In this paper, we present an accurate and extensible approach for the\ncoreference resolution task. We formulate the problem as a span prediction\ntask, like in machine reading comprehension (MRC): A query is generated for\neach candidate mention using its surrounding context, and a span prediction\nmodule is employed to extract the text spans of the coreferences within the\ndocument using the generated query. This formulation comes with the following\nkey advantages: (1) The span prediction strategy provides the flexibility of\nretrieving mentions left out at the mention proposal stage; (2) In the MRC\nframework, encoding the mention and its context explicitly in a query makes it\npossible to have a deep and thorough examination of cues embedded in the\ncontext of coreferent mentions; and (3) A plethora of existing MRC datasets can\nbe used for data augmentation to improve the model’s generalization capability.\nExperiments demonstrate significant performance boost over previous models,\nwith 87.5 (+2.5) F1 score on the GAP benchmark and 83.1 (+3.5) F1 score on the\nCoNLL-2012 benchmark.</p>\n", "tags": ["Datasets","Evaluation","Tools"] },
{"key": "wu2019global", "citations": "128", "year": "2019", "title":"Global-to-local Memory Pointer Networks For Task-oriented Dialogue", "abstract": "<p>End-to-end task-oriented dialogue is challenging since knowledge bases are\nusually large, dynamic and hard to incorporate into a learning framework. We\npropose the global-to-local memory pointer (GLMP) networks to address this\nissue. In our model, a global memory encoder and a local memory decoder are\nproposed to share external knowledge. The encoder encodes dialogue history,\nmodifies global contextual representation, and generates a global memory\npointer. The decoder first generates a sketch response with unfilled slots.\nNext, it passes the global memory pointer to filter the external knowledge for\nrelevant information, then instantiates the slots via the local memory\npointers. We empirically show that our model can improve copy accuracy and\nmitigate the common out-of-vocabulary problem. As a result, GLMP is able to\nimprove over the previous state-of-the-art models in both simulated bAbI\nDialogue dataset and human-human Stanford Multi-domain Dialogue dataset on\nautomatic and human evaluation.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Memory & Context","Tools"] },
{"key": "wu2019mask", "citations": "99", "year": "2019", "title":"\"mask And Infill\" : Applying Masked Language Model To Sentiment Transfer", "abstract": "<p>This paper focuses on the task of sentiment transfer on non-parallel text,\nwhich modifies sentiment attributes (e.g., positive or negative) of sentences\nwhile preserving their attribute-independent content. Due to the limited\ncapability of RNNbased encoder-decoder structure to capture deep and long-range\ndependencies among words, previous works can hardly generate satisfactory\nsentences from scratch. When humans convert the sentiment attribute of a\nsentence, a simple but effective approach is to only replace the original\nsentimental tokens in the sentence with target sentimental expressions, instead\nof building a new sentence from scratch. Such a process is very similar to the\ntask of Text Infilling or Cloze, which could be handled by a deep bidirectional\nMasked Language Model (e.g. BERT). So we propose a two step approach “Mask and\nInfill”. In the mask step, we separate style from content by masking the\npositions of sentimental tokens. In the infill step, we retrofit MLM to\nAttribute Conditional MLM, to infill the masked positions by predicting words\nor phrases conditioned on the context1 and target sentiment. We evaluate our\nmodel on two review datasets with quantitative, qualitative, and human\nevaluations. Experimental results demonstrate that our models improve\nstate-of-the-art performance.</p>\n", "tags": ["Datasets","IJCAI"] },
{"key": "wu2019pay", "citations": "349", "year": "2019", "title":"Pay Less Attention With Lightweight And Dynamic Convolutions", "abstract": "<p>Self-attention is a useful mechanism to build generative models for language\nand images. It determines the importance of context elements by comparing each\nelement to the current time step. In this paper, we show that a very\nlightweight convolution can perform competitively to the best reported\nself-attention results. Next, we introduce dynamic convolutions which are\nsimpler and more efficient than self-attention. We predict separate convolution\nkernels based solely on the current time-step in order to determine the\nimportance of context elements. The number of operations required by this\napproach scales linearly in the input length, whereas self-attention is\nquadratic. Experiments on large-scale machine translation, language modeling\nand abstractive summarization show that dynamic convolutions improve over\nstrong self-attention models. On the WMT’14 English-German test set dynamic\nconvolutions achieve a new state of the art of 29.7 BLEU.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "wu2019proactive", "citations": "150", "year": "2019", "title":"Proactive Human-machine Conversation With Explicit Conversation Goals", "abstract": "<p>Though great progress has been made for human-machine conversation, current\ndialogue system is still in its infancy: it usually converses passively and\nutters words more as a matter of response, rather than on its own initiatives.\nIn this paper, we take a radical step towards building a human-like\nconversational agent: endowing it with the ability of proactively leading the\nconversation (introducing a new topic or maintaining the current topic). To\nfacilitate the development of such conversation systems, we create a new\ndataset named DuConv where one acts as a conversation leader and the other acts\nas the follower. The leader is provided with a knowledge graph and asked to\nsequentially change the discussion topics, following the given conversation\ngoal, and meanwhile keep the dialogue as natural and engaging as possible.\nDuConv enables a very challenging task as the model needs to both understand\ndialogue and plan over the given knowledge graph. We establish baseline results\non this dataset (about 270K utterances and 30k dialogues) using several\nstate-of-the-art models. Experimental results show that dialogue models that\nplan over the knowledge graph can make full use of related knowledge to\ngenerate more diverse multi-turn conversations. The baseline systems along with\nthe dataset are publicly available</p>\n", "tags": ["Agentic","Datasets","Dialogue & Multi Turn"] },
{"key": "wu2019self", "citations": "93", "year": "2019", "title":"Self-supervised Dialogue Learning", "abstract": "<p>The sequential order of utterances is often meaningful in coherent dialogues,\nand the order changes of utterances could lead to low-quality and incoherent\nconversations. We consider the order information as a crucial supervised signal\nfor dialogue learning, which, however, has been neglected by many previous\ndialogue systems. Therefore, in this paper, we introduce a self-supervised\nlearning task, inconsistent order detection, to explicitly capture the flow of\nconversation in dialogues. Given a sampled utterance pair triple, the task is\nto predict whether it is ordered or misordered. Then we propose a\nsampling-based self-supervised network SSN to perform the prediction with\nsampled triple references from previous dialogue history. Furthermore, we\ndesign a joint learning framework where SSN can guide the dialogue systems\ntowards more coherent and relevant dialogue learning through adversarial\ntraining. We demonstrate that the proposed methods can be applied to both\nopen-domain and task-oriented dialogue scenarios, and achieve the new\nstate-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking\ndatasets.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Tools","Training Techniques"] },
{"key": "wu2019transferable", "citations": "409", "year": "2019", "title":"Transferable Multi-domain State Generator For Task-oriented Dialogue Systems", "abstract": "<p>Over-dependence on domain ontology and lack of knowledge sharing across\ndomains are two practical and yet less studied problems of dialogue state\ntracking. Existing approaches generally fall short in tracking unknown slot\nvalues during inference and often have difficulties in adapting to new domains.\nIn this paper, we propose a Transferable Dialogue State Generator (TRADE) that\ngenerates dialogue states from utterances using a copy mechanism, facilitating\nknowledge transfer when predicting (domain, slot, value) triplets not\nencountered during training. Our model is composed of an utterance encoder, a\nslot gate, and a state generator, which are shared across domains. Empirical\nresults demonstrate that TRADE achieves state-of-the-art joint goal accuracy of\n48.62% for the five domains of MultiWOZ, a human-human dialogue dataset. In\naddition, we show its transferring ability by simulating zero-shot and few-shot\ndialogue state tracking for unseen domains. TRADE achieves 60.58% joint goal\naccuracy in one of the zero-shot domains, and is able to adapt to few-shot\ncases without forgetting already trained domains.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Few-Shot","Training Techniques"] },
{"key": "wu2020applying", "citations": "76", "year": "2021", "title":"Applying The Transformer To Character-level Transduction", "abstract": "<p>The transformer has been shown to outperform recurrent neural network-based\nsequence-to-sequence models in various word-level NLP tasks. Yet for\ncharacter-level transduction tasks, e.g. morphological inflection generation\nand historical text normalization, there are few works that outperform\nrecurrent models using the transformer. In an empirical study, we uncover that,\nin contrast to recurrent sequence-to-sequence models, the batch size plays a\ncrucial role in the performance of the transformer on character-level tasks,\nand we show that with a large enough batch size, the transformer does indeed\noutperform recurrent models. We also introduce a simple technique to handle\nfeature-guided character-level transduction that further improves performance.\nWith these insights, we achieve state-of-the-art performance on morphological\ninflection and historical text normalization. We also show that the transformer\noutperforms a strong baseline on two other character-level transduction tasks:\ngrapheme-to-phoneme conversion and transliteration.</p>\n", "tags": ["EACL","Model Architecture","NAACL"] },
{"key": "wu2020are", "citations": "203", "year": "2020", "title":"Are All Languages Created Equal In Multilingual BERT?", "abstract": "<p>Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly\ngood cross-lingual performance on several NLP tasks, even without explicit\ncross-lingual signals. However, these evaluations have focused on cross-lingual\ntransfer with high-resource languages, covering only a third of the languages\ncovered by mBERT. We explore how mBERT performs on a much wider set of\nlanguages, focusing on the quality of representation for low-resource\nlanguages, measured by within-language performance. We consider three tasks:\nNamed Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency\nParsing (54 languages each). mBERT does better than or comparable to baselines\non high resource languages but does much worse for low resource languages.\nFurthermore, monolingual BERT models for these languages do even worse. Paired\nwith similar languages, the performance gap between monolingual BERT and mBERT\ncan be narrowed. We find that better models for low resource languages require\nmore efficient pretraining techniques or more data.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "wu2020clear", "citations": "229", "year": "2020", "title":"CLEAR: Contrastive Learning For Sentence Representation", "abstract": "<p>Pre-trained language models have proven their unique powers in capturing\nimplicit language features. However, most pre-training approaches focus on the\nword-level training objective, while sentence-level objectives are rarely\nstudied. In this paper, we propose Contrastive LEArning for sentence\nRepresentation (CLEAR), which employs multiple sentence-level augmentation\nstrategies in order to learn a noise-invariant sentence representation. These\naugmentations include word and span deletion, reordering, and substitution.\nFurthermore, we investigate the key reasons that make contrastive learning\neffective through numerous experiments. We observe that different sentence\naugmentations during pre-training lead to different performance improvements on\nvarious downstream tasks. Our approach is shown to outperform multiple existing\nmethods on both SentEval and GLUE benchmarks.</p>\n", "tags": ["Training Techniques"] },
{"key": "wu2020context", "citations": "76", "year": "2021", "title":"Context-guided BERT For Targeted Aspect-based Sentiment Analysis", "abstract": "<p>Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow\nfiner-grained inferences about sentiment to be drawn from the same text,\ndepending on context. For example, a given text can have different targets\n(e.g., neighborhoods) and different aspects (e.g., price or safety), with\ndifferent sentiment associated with each target-aspect pair. In this paper, we\ninvestigate whether adding context to self-attention models improves\nperformance on (T)ABSA. We propose two variants of Context-Guided BERT\n(CG-BERT) that learn to distribute attention under different contexts. We first\nadapt a context-aware Transformer to produce a CG-BERT that uses context-guided\nsoftmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model\nthat learns a compositional attention that supports subtractive attention. We\ntrain both models with pretrained BERT on two (T)ABSA datasets: SentiHood and\nSemEval-2014 (Task 4). Both models achieve new state-of-the-art results with\nour QACG-BERT model having the best performance. Furthermore, we provide\nanalyses of the impact of context in the our proposed models. Our work provides\nmore evidence for the utility of adding context-dependencies to pretrained\nself-attention-based language models for context-based natural language tasks.</p>\n", "tags": ["AAAI","Datasets","Model Architecture"] },
{"key": "wu2020controllable", "citations": "71", "year": "2021", "title":"A Controllable Model Of Grounded Response Generation", "abstract": "<p>Current end-to-end neural conversation models inherently lack the flexibility\nto impose semantic control in the response generation process, often resulting\nin uninteresting responses. Attempts to boost informativeness alone come at the\nexpense of factual accuracy, as attested by pretrained language models’\npropensity to “hallucinate” facts. While this may be mitigated by access to\nbackground knowledge, there is scant guarantee of relevance and informativeness\nin generated responses. We propose a framework that we call controllable\ngrounded response generation (CGRG), in which lexical control phrases are\neither provided by a user or automatically extracted by a control phrase\npredictor from dialogue context and grounding knowledge. Quantitative and\nqualitative results show that, using this framework, a transformer based model\nwith a novel inductive attention mechanism, trained on a conversation-like\nReddit dataset, outperforms strong generation baselines.</p>\n", "tags": ["AAAI","Model Architecture","Tools"] },
{"key": "wu2020lite", "citations": "122", "year": "2020", "title":"Lite Transformer With Long-short Range Attention", "abstract": "<p>Transformer has become ubiquitous in natural language processing (e.g.,\nmachine translation, question answering); however, it requires enormous amount\nof computations to achieve high performance, which makes it not suitable for\nmobile applications that are tightly constrained by the hardware resources and\nbattery. In this paper, we present an efficient mobile NLP architecture, Lite\nTransformer to facilitate deploying mobile NLP applications on edge devices.\nThe key primitive is the Long-Short Range Attention (LSRA), where one group of\nheads specializes in the local context modeling (by convolution) while another\ngroup specializes in the long-distance relationship modeling (by attention).\nSuch specialization brings consistent improvement over the vanilla transformer\non three well-established language tasks: machine translation, abstractive\nsummarization, and language modeling. Under constrained resources (500M/100M\nMACs), Lite Transformer outperforms transformer on WMT’14 English-French by\n1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of\ntransformer base model by 2.5x with 0.3 BLEU score degradation. Combining with\npruning and quantization, we further compressed the model size of Lite\nTransformer by 18.2x. For language modeling, Lite Transformer achieves 1.8\nlower perplexity than the transformer at around 500M MACs. Notably, Lite\nTransformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU\nfor the mobile NLP setting without the costly architecture search that requires\nmore than 250 GPU years. Code has been made available at\nhttps://github.com/mit-han-lab/lite-transformer.</p>\n", "tags": ["Applications","Efficiency","Has Code","Model Architecture"] },
{"key": "wu2020tod", "citations": "190", "year": "2020", "title":"TOD-BERT: Pre-trained Natural Language Understanding For Task-oriented Dialogue", "abstract": "<p>The underlying difference of linguistic patterns between general text and\ntask-oriented dialogue makes existing pre-trained language models less useful\nin practice. In this work, we unify nine human-human and multi-turn\ntask-oriented dialogue datasets for language modeling. To better model dialogue\nbehavior during pre-training, we incorporate user and system tokens into the\nmasked language modeling. We propose a contrastive objective function to\nsimulate the response selection task. Our pre-trained task-oriented dialogue\nBERT (TOD-BERT) outperforms strong baselines like BERT on four downstream\ntask-oriented dialogue applications, including intention recognition, dialogue\nstate tracking, dialogue act prediction, and response selection. We also show\nthat TOD-BERT has a stronger few-shot ability that can mitigate the data\nscarcity problem for task-oriented dialogue.</p>\n", "tags": ["Applications","Datasets","EMNLP","Few-Shot","Model Architecture","Training Techniques"] },
{"key": "wu2021ai", "citations": "259", "year": "2022", "title":"AI Chains: Transparent And Controllable Human-ai Interaction By Chaining Large Language Model Prompts", "abstract": "<p>Although large language models (LLMs) have demonstrated impressive potential\non simple tasks, their breadth of scope, lack of transparency, and insufficient\ncontrollability can make them less effective when assisting humans on more\ncomplex tasks. In response, we introduce the concept of Chaining LLM steps\ntogether, where the output of one step becomes the input for the next, thus\naggregating the gains per step. We first define a set of LLM primitive\noperations useful for Chain construction, then present an interactive system\nwhere users can modify these Chains, along with their intermediate results, in\na modular way. In a 20-person user study, we found that Chaining not only\nimproved the quality of task outcomes, but also significantly enhanced system\ntransparency, controllability, and sense of collaboration. Additionally, we saw\nthat users developed new ways of interacting with LLMs through Chains: they\nleveraged sub-tasks to calibrate model expectations, compared and contrasted\nalternative strategies by observing parallel downstream effects, and debugged\nunexpected model outputs by “unit-testing” sub-components of a Chain. In two\ncase studies, we further explore how LLM Chains may be used in future\napplications</p>\n", "tags": ["Applications","Ethics & Fairness"] },
{"key": "wu2021empowering", "citations": "123", "year": "2021", "title":"Empowering News Recommendation With Pre-trained Language Models", "abstract": "<p>Personalized news recommendation is an essential technique for online news\nservices. News articles usually contain rich textual content, and accurate news\nmodeling is important for personalized news recommendation. Existing news\nrecommendation methods mainly model news texts based on traditional text\nmodeling methods, which is not optimal for mining the deep semantic information\nin news texts. Pre-trained language models (PLMs) are powerful for natural\nlanguage understanding, which has the potential for better news modeling.\nHowever, there is no public report that show PLMs have been applied to news\nrecommendation. In this paper, we report our work on exploiting pre-trained\nlanguage models to empower news recommendation. Offline experimental results on\nboth monolingual and multilingual news recommendation datasets show that\nleveraging PLMs for news modeling can effectively improve the performance of\nnews recommendation. Our PLM-empowered news recommendation models have been\ndeployed to the Microsoft News platform, and achieved significant gains in\nterms of both click and pageview in both English-speaking and global markets.</p>\n", "tags": ["Datasets","SIGIR","Tools"] },
{"key": "wu2021fastformer", "citations": "69", "year": "2021", "title":"Fastformer: Additive Attention Can Be All You Need", "abstract": "<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "wu2021godiva", "citations": "65", "year": "2021", "title":"GODIVA: Generating Open-domain Videos From Natural Descriptions", "abstract": "<p>Generating videos from text is a challenging task due to its high\ncomputational requirements for training and infinite possible answers for\nevaluation. Existing works typically experiment on simple or small datasets,\nwhere the generalization ability is quite limited. In this work, we propose\nGODIVA, an open-domain text-to-video pretrained model that can generate videos\nfrom text in an auto-regressive manner using a three-dimensional sparse\nattention mechanism. We pretrain our model on Howto100M, a large-scale\ntext-video dataset that contains more than 136 million text-video pairs.\nExperiments show that GODIVA not only can be fine-tuned on downstream video\ngeneration tasks, but also has a good zero-shot capability on unseen texts. We\nalso propose a new metric called Relative Matching (RM) to automatically\nevaluate the video generation quality. Several challenges are listed and\ndiscussed as future work.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "wu2021recursively", "citations": "63", "year": "2021", "title":"Recursively Summarizing Books With Human Feedback", "abstract": "<p>A major challenge for scaling machine learning is training models to perform\ntasks that are very difficult or time-consuming for humans to evaluate. We\npresent progress on this problem on the task of abstractive summarization of\nentire fiction novels. Our method combines learning from human feedback with\nrecursive task decomposition: we use models trained on smaller parts of the\ntask to assist humans in giving feedback on the broader task. We collect a\nlarge volume of demonstrations and comparisons from human labelers, and\nfine-tune GPT-3 using behavioral cloning and reward modeling to do\nsummarization recursively. At inference time, the model first summarizes small\nsections of the book and then recursively summarizes these summaries to produce\na summary of the entire book. Our human labelers are able to supervise and\nevaluate the models quickly, despite not having read the entire books\nthemselves. Our resulting model generates sensible summaries of entire books,\neven matching the quality of human-written summaries in a few cases (\\(\\sim5%\\)\nof books). We achieve state-of-the-art results on the recent BookSum dataset\nfor book-length summarization. A zero-shot question-answering model using these\nsummaries achieves state-of-the-art results on the challenging NarrativeQA\nbenchmark for answering questions about books and movie scripts. We release\ndatasets of samples from our model.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "wu2021wav2clip", "citations": "111", "year": "2022", "title":"Wav2clip: Learning Robust Audio Representations From CLIP", "abstract": "<p>We propose Wav2CLIP, a robust audio representation learning method by\ndistilling from Contrastive Language-Image Pre-training (CLIP). We\nsystematically evaluate Wav2CLIP on a variety of audio tasks including\nclassification, retrieval, and generation, and show that Wav2CLIP can\noutperform several publicly available pre-trained audio representation\nalgorithms. Wav2CLIP projects audio into a shared embedding space with images\nand text, which enables multimodal applications such as zero-shot\nclassification, and cross-modal retrieval. Furthermore, Wav2CLIP needs just\n~10% of the data to achieve competitive performance on downstream tasks\ncompared with fully supervised models, and is more efficient to pre-train than\ncompeting methods as it does not require learning a visual model in concert\nwith an auditory model. Finally, we demonstrate image generation from Wav2CLIP\nas qualitative assessment of the shared embedding space. Our code and model\nweights are open sourced and made available for further applications.</p>\n", "tags": ["Applications","ICASSP","Training Techniques"] },
{"key": "wu2022multi", "citations": "73", "year": "2022", "title":"Multi-level Knowledge Distillation For Out-of-distribution Detection In Text", "abstract": "<p>Self-supervised representation learning has proved to be a valuable component\nfor out-of-distribution (OoD) detection with only the texts of in-distribution\n(ID) examples. These approaches either train a language model from scratch or\nfine-tune a pre-trained language model using ID examples, and then take the\nperplexity output by the language model as OoD scores. In this paper, we\nanalyze the complementary characteristics of both OoD detection methods and\npropose a multi-level knowledge distillation approach that integrates their\nstrengths while mitigating their limitations. Specifically, we use a fine-tuned\nmodel as the teacher to teach a randomly initialized student model on the ID\nexamples. Besides the prediction layer distillation, we present a\nsimilarity-based intermediate layer distillation method to thoroughly explore\nthe representation space of the teacher model. In this way, the learned student\ncan better represent the ID data manifold while gaining a stronger ability to\nmap OoD examples outside the ID data manifold with the regularization inherited\nfrom pre-training. Besides, the student model sees only ID examples during\nparameter learning, further promoting more distinguishable features for OoD\ndetection. We conduct extensive experiments over multiple benchmark datasets,\ni.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the\nproposed method yields new state-of-the-art performance. We also explore its\napplication as an AIGC detector to distinguish between answers generated by\nChatGPT and human experts. It is observed that our model exceeds human\nevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Training Techniques"] },
{"key": "wu2022promptchainer", "citations": "119", "year": "2022", "title":"Promptchainer: Chaining Large Language Model Prompts Through Visual Programming", "abstract": "<p>While LLMs can effectively help prototype single ML functionalities, many\nreal-world applications involve complex tasks that cannot be easily handled via\na single run of an LLM. Recent work has found that chaining multiple LLM runs\ntogether (with the output of one step being the input to the next) can help\nusers accomplish these more complex tasks, and in a way that is perceived to be\nmore transparent and controllable. However, it remains unknown what users need\nwhen authoring their own LLM chains – a key step for lowering the barriers for\nnon-AI-experts to prototype AI-infused applications. In this work, we explore\nthe LLM chain authoring process. We conclude from pilot studies find that\nchaining requires careful scaffolding for transforming intermediate node\noutputs, as well as debugging the chain at multiple granularities; to help with\nthese needs, we designed PromptChainer, an interactive interface for visually\nprogramming chains. Through case studies with four people, we show that\nPromptChainer supports building prototypes for a range of applications, and\nconclude with open questions on scaling chains to complex tasks, and supporting\nlow-fi chain prototyping.</p>\n", "tags": ["Applications"] },
{"key": "wu2022tune", "citations": "241", "year": "2023", "title":"Tune-a-video: One-shot Tuning Of Image Diffusion Models For Text-to-video Generation", "abstract": "<p>To replicate the success of text-to-image (T2I) generation, recent works\nemploy large-scale video datasets to train a text-to-video (T2V) generator.\nDespite their promising results, such paradigm is computationally expensive. In\nthis work, we propose a new T2V generation setting\\(\\unicode{x2014}\\)One-Shot\nVideo Tuning, where only one text-video pair is presented. Our model is built\non state-of-the-art T2I diffusion models pre-trained on massive image data. We\nmake two key observations: 1) T2I models can generate still images that\nrepresent verb terms; 2) extending T2I models to generate multiple images\nconcurrently exhibits surprisingly good content consistency. To further learn\ncontinuous motion, we introduce Tune-A-Video, which involves a tailored\nspatio-temporal attention mechanism and an efficient one-shot tuning strategy.\nAt inference, we employ DDIM inversion to provide structure guidance for\nsampling. Extensive qualitative and numerical experiments demonstrate the\nremarkable ability of our method across various applications.</p>\n", "tags": ["Applications","Datasets","ICCV","Model Architecture"] },
{"key": "wu2023ai", "citations": "69", "year": "2023", "title":"Ai-generated Content (AIGC): A Survey", "abstract": "<p>To address the challenges of digital intelligence in the digital economy,\nartificial intelligence-generated content (AIGC) has emerged. AIGC uses\nartificial intelligence to assist or replace manual content generation by\ngenerating content based on user-inputted keywords or requirements. The\ndevelopment of large model algorithms has significantly strengthened the\ncapabilities of AIGC, which makes AIGC products a promising generative tool and\nadds convenience to our lives. As an upstream technology, AIGC has unlimited\npotential to support different downstream applications. It is important to\nanalyze AIGC’s current capabilities and shortcomings to understand how it can\nbe best utilized in future applications. Therefore, this paper provides an\nextensive overview of AIGC, covering its definition, essential conditions,\ncutting-edge capabilities, and advanced features. Moreover, it discusses the\nbenefits of large-scale pre-trained models and the industrial chain of AIGC.\nFurthermore, the article explores the distinctions between auxiliary generation\nand automatic generation within AIGC, providing examples of text generation.\nThe paper also examines the potential integration of AIGC with the Metaverse.\nLastly, the article highlights existing issues and suggests some future\ndirections for application.</p>\n", "tags": ["Applications","Survey Paper"] },
{"key": "wu2023autogen", "citations": "65", "year": "2023", "title":"Autogen: Enabling Next-gen LLM Applications Via Multi-agent Conversation", "abstract": "<p>AutoGen is an open-source framework that allows developers to build LLM\napplications via multiple agents that can converse with each other to\naccomplish tasks. AutoGen agents are customizable, conversable, and can operate\nin various modes that employ combinations of LLMs, human inputs, and tools.\nUsing AutoGen, developers can also flexibly define agent interaction behaviors.\nBoth natural language and computer code can be used to program flexible\nconversation patterns for different applications. AutoGen serves as a generic\ninfrastructure to build diverse applications of various complexities and LLM\ncapacities. Empirical studies demonstrate the effectiveness of the framework in\nmany example applications, with domains ranging from mathematics, coding,\nquestion answering, operations research, online decision-making, entertainment,\netc.</p>\n", "tags": ["Agentic","Applications","Tools"] },
{"key": "wu2023bloomberggpt", "citations": "254", "year": "2023", "title":"Bloomberggpt: A Large Language Model For Finance", "abstract": "<p>The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg’s\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.</p>\n", "tags": ["Applications","Datasets","Evaluation","Training Techniques"] },
{"key": "wu2023multimodal", "citations": "63", "year": "2023", "title":"Multimodal Large Language Models: A Survey", "abstract": "<p>The exploration of multimodal language models integrates multiple data types,\nsuch as images, text, language, audio, and other heterogeneity. While the\nlatest large language models excel in text-based tasks, they often struggle to\nunderstand and process other data types. Multimodal models address this\nlimitation by combining various modalities, enabling a more comprehensive\nunderstanding of diverse data. This paper begins by defining the concept of\nmultimodal and examining the historical development of multimodal algorithms.\nFurthermore, we introduce a range of multimodal products, focusing on the\nefforts of major technology companies. A practical guide is provided, offering\ninsights into the technical aspects of multimodal models. Moreover, we present\na compilation of the latest algorithms and commonly used datasets, providing\nresearchers with valuable resources for experimentation and evaluation. Lastly,\nwe explore the applications of multimodal models and discuss the challenges\nassociated with their development. By addressing these aspects, this paper aims\nto facilitate a deeper understanding of multimodal models and their potential\nin various domains.</p>\n", "tags": ["Applications","Datasets","Evaluation","Survey Paper"] },
{"key": "wu2023next", "citations": "87", "year": "2023", "title":"Next-gpt: Any-to-any Multimodal LLM", "abstract": "<p>While recently Multimodal Large Language Models (MM-LLMs) have made exciting\nstrides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities.\nAs we humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in\narbitrary combinations of text, images, videos, and audio. By leveraging the\nexisting well-trained highly-performing encoders and decoders, NExT-GPT is\ntuned with only a small amount of parameter (1%) of certain projection layers,\nwhich not only benefits low-cost training and also facilitates convenient\nexpansion to more potential modalities. Moreover, we introduce a\nmodality-switching instruction tuning (MosIT) and manually curate a\nhigh-quality dataset for MosIT, based on which NExT-GPT is empowered with\ncomplex cross-modal semantic understanding and content generation. Overall, our\nresearch showcases the promising possibility of building an AI agent capable of\nmodeling universal modalities, paving the way for more human-like AI research\nin the community. Project page: https://next-gpt.github.io/</p>\n", "tags": ["Agentic","Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "wu2023tidybot", "citations": "70", "year": "2023", "title":"Tidybot: Personalized Robot Assistance With Large Language Models", "abstract": "<p>For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people’s preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot"] },
{"key": "wuebker2018compact", "citations": "65", "year": "2018", "title":"Compact Personalized Models For Neural Machine Translation", "abstract": "<p>We propose and compare methods for gradient-based domain adaptation of\nself-attentive neural machine translation models. We demonstrate that a large\nproportion of model parameters can be frozen during adaptation with minimal or\nno reduction in translation quality by encouraging structured sparsity in the\nset of offset tensors during learning via group lasso regularization. We\nevaluate this technique for both batch and incremental adaptation across\nmultiple data sets and language pairs. Our system architecture - combining a\nstate-of-the-art self-attentive model with compact domain adaptation - provides\nhigh quality personalized machine translation that is both space and time\nefficient.</p>\n", "tags": ["EMNLP","Fine-Tuning","Model Architecture"] },
{"key": "xi2023rise", "citations": "180", "year": "2023", "title":"The Rise And Potential Of Large Language Model Based Agents: A Survey", "abstract": "<p>For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.</p>\n", "tags": ["Agentic","Applications","Survey Paper","Tools","Training Techniques"] },
{"key": "xia2016dual", "citations": "639", "year": "2016", "title":"Dual Learning For Machine Translation", "abstract": "<p>While neural machine translation (NMT) is making good progress in the past\ntwo years, tens of millions of bilingual sentence pairs are needed for its\ntraining. However, human labeling is very costly. To tackle this training data\nbottleneck, we develop a dual-learning mechanism, which can enable an NMT\nsystem to automatically learn from unlabeled data through a dual-learning game.\nThis mechanism is inspired by the following observation: any machine\ntranslation task has a dual task, e.g., English-to-French translation (primal)\nversus French-to-English translation (dual); the primal and dual tasks can form\na closed loop, and generate informative feedback signals to train the\ntranslation models, even if without the involvement of a human labeler. In the\ndual-learning mechanism, we use one agent to represent the model for the primal\ntask and the other agent to represent the model for the dual task, then ask\nthem to teach each other through a reinforcement learning process. Based on the\nfeedback signals generated during this process (e.g., the language-model\nlikelihood of the output of a model, and the reconstruction error of the\noriginal sentence after the primal and dual translations), we can iteratively\nupdate the two models until convergence (e.g., using the policy gradient\nmethods). We call the corresponding approach to neural machine translation\n<em>dual-NMT</em>. Experiments show that dual-NMT works very well on\nEnglish\\(\\leftrightarrow\\)French translation; especially, by learning from\nmonolingual data (with 10% bilingual data for warm start), it achieves a\ncomparable accuracy to NMT trained from the full bilingual data for the\nFrench-to-English translation task.</p>\n", "tags": ["NEURIPS","Reinforcement Learning","Training Techniques"] },
{"key": "xia2018zero", "citations": "209", "year": "2018", "title":"Zero-shot User Intent Detection Via Capsule Neural Networks", "abstract": "<p>User intent detection plays a critical role in question-answering and dialog\nsystems. Most previous works treat intent detection as a classification problem\nwhere utterances are labeled with predefined intents. However, it is\nlabor-intensive and time-consuming to label users’ utterances as intents are\ndiversely expressed and novel intents will continually be involved. Instead, we\nstudy the zero-shot intent detection problem, which aims to detect emerging\nuser intents where no labeled utterances are currently available. We propose\ntwo capsule-based architectures: INTENT-CAPSNET that extracts semantic features\nfrom utterances and aggregates them to discriminate existing intents, and\nINTENTCAPSNET-ZSL which gives INTENTCAPSNET the zero-shot learning ability to\ndiscriminate emerging intents via knowledge transfer from existing intents.\nExperiments on two real-world datasets show that our model not only can better\ndiscriminate diversely expressed existing intents, but is also able to\ndiscriminate emerging intents when no labeled utterances are available.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "xia2019generalized", "citations": "110", "year": "2019", "title":"Generalized Data Augmentation For Low-resource Translation", "abstract": "<p>Translation to or from low-resource languages LRLs poses challenges for\nmachine translation in terms of both adequacy and fluency. Data augmentation\nutilizing large amounts of monolingual data is regarded as an effective way to\nalleviate these problems. In this paper, we propose a general framework for\ndata augmentation in low-resource machine translation that not only uses\ntarget-side monolingual data, but also pivots through a related high-resource\nlanguage HRL. Specifically, we experiment with a two-step pivoting method to\nconvert high-resource data to the LRL, making use of available resources to\nbetter approximate the true data distribution of the LRL. First, we inject LRL\nwords into HRL sentences through an induced bilingual dictionary. Second, we\nfurther edit these modified sentences using a modified unsupervised machine\ntranslation framework. Extensive experiments on four low-resource datasets show\nthat under extreme low-resource settings, our data augmentation techniques\nimprove translation quality by up to~1.5 to~8 BLEU points compared to\nsupervised back-translation baselines</p>\n", "tags": ["Datasets","Tools"] },
{"key": "xia2021self", "citations": "149", "year": "2021", "title":"Self-supervised Graph Co-training For Session-based Recommendation", "abstract": "<p>Session-based recommendation targets next-item prediction by exploiting user\nbehaviors within a short time period. Compared with other recommendation\nparadigms, session-based recommendation suffers more from the problem of data\nsparsity due to the very limited short-term interactions. Self-supervised\nlearning, which can discover ground-truth samples from the raw data, holds vast\npotentials to tackle this problem. However, existing self-supervised\nrecommendation models mainly rely on item/segment dropout to augment data,\nwhich are not fit for session-based recommendation because the dropout leads to\nsparser data, creating unserviceable self-supervision signals. In this paper,\nfor informative session-based data augmentation, we combine self-supervised\nlearning with co-training, and then develop a framework to enhance\nsession-based recommendation. Technically, we first exploit the session-based\ngraph to augment two views that exhibit the internal and external\nconnectivities of sessions, and then we build two distinct graph encoders over\nthe two views, which recursively leverage the different connectivity\ninformation to generate ground-truth samples to supervise each other by\ncontrastive learning. In contrast to the dropout strategy, the proposed\nself-supervised graph co-training preserves the complete session information\nand fulfills genuine data augmentation. Extensive experiments on multiple\nbenchmark datasets show that, session-based recommendation can be remarkably\nenhanced under the regime of self-supervised graph co-training, achieving the\nstate-of-the-art performance.</p>\n", "tags": ["CIKM","Datasets","Evaluation","Tools","Training Techniques"] },
{"key": "xia2022structured", "citations": "79", "year": "2022", "title":"Structured Pruning Learns Compact And Accurate Models", "abstract": "<p>The growing size of neural language models has led to increased attention in\nmodel compression. The two predominant approaches are pruning, which gradually\nremoves weights from a pre-trained model, and distillation, which trains a\nsmaller compact model to match a larger one. Pruning methods can significantly\nreduce the model size but hardly achieve large speedups as distillation.\nHowever, distillation methods require large amounts of unlabeled data and are\nexpensive to train. In this work, we propose a task-specific structured pruning\nmethod CoFi (Coarse- and Fine-grained Pruning), which delivers highly\nparallelizable subnetworks and matches the distillation methods in both\naccuracy and latency, without resorting to any unlabeled data. Our key insight\nis to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads\nand hidden units) modules, which controls the pruning decision of each\nparameter with masks of different granularity. We also devise a layerwise\ndistillation strategy to transfer knowledge from unpruned to pruned models\nduring optimization. Our experiments on GLUE and SQuAD datasets show that CoFi\nyields models with over 10x speedups with a small accuracy drop, showing its\neffectiveness and efficiency compared to previous pruning and distillation\napproaches.</p>\n", "tags": ["Datasets","Efficiency","Model Architecture"] },
{"key": "xiao2017weakly", "citations": "118", "year": "2017", "title":"Weakly-supervised Visual Grounding Of Phrases With Linguistic Structures", "abstract": "<p>We propose a weakly-supervised approach that takes image-sentence pairs as\ninput and learns to visually ground (i.e., localize) arbitrary linguistic\nphrases, in the form of spatial attention masks. Specifically, the model is\ntrained with images and their associated image-level captions, without any\nexplicit region-to-phrase correspondence annotations. To this end, we introduce\nan end-to-end model which learns visual groundings of phrases with two types of\ncarefully designed loss functions. In addition to the standard discriminative\nloss, which enforces that attended image regions and phrases are consistently\nencoded, we propose a novel structural loss which makes use of the parse tree\nstructures induced by the sentences. In particular, we ensure complementarity\namong the attention masks that correspond to sibling noun phrases, and\ncompositionality of attention masks among the children and parent phrases, as\ndefined by the sentence parse tree. We validate the effectiveness of our\napproach on the Microsoft COCO and Visual Genome datasets.</p>\n", "tags": ["CVPR","Datasets","Model Architecture"] },
{"key": "xiao2019dynamically", "citations": "181", "year": "2019", "title":"Dynamically Fused Graph Network For Multi-hop Reasoning", "abstract": "<p>Text-based question answering (TBQA) has been studied extensively in recent\nyears. Most existing approaches focus on finding the answer to a question\nwithin a single paragraph. However, many difficult questions require multiple\nsupporting evidence from scattered text among two or more documents. In this\npaper, we propose Dynamically Fused Graph Network(DFGN), a novel method to\nanswer those questions requiring multiple scattered evidence and reasoning over\nthem. Inspired by human’s step-by-step reasoning behavior, DFGN includes a\ndynamic fusion layer that starts from the entities mentioned in the given\nquery, explores along the entity graph dynamically built from the text, and\ngradually finds relevant supporting entities from the given documents. We\nevaluate DFGN on HotpotQA, a public TBQA dataset requiring multi-hop reasoning.\nDFGN achieves competitive results on the public board. Furthermore, our\nanalysis shows DFGN produces interpretable reasoning chains.</p>\n", "tags": ["Datasets"] },
{"key": "xiao2020ernie", "citations": "98", "year": "2020", "title":"Ernie-gram: Pre-training With Explicitly N-gram Masked Language Modeling For Natural Language Understanding", "abstract": "<p>Coarse-grained linguistic information, such as named entities or phrases,\nfacilitates adequately representation learning in pre-training. Previous works\nmainly focus on extending the objective of BERT’s Masked Language Modeling\n(MLM) from masking individual tokens to contiguous sequences of n tokens. We\nargue that such contiguously masking method neglects to model the\nintra-dependencies and inter-relation of coarse-grained linguistic information.\nAs an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method\nto enhance the integration of coarse-grained information into pre-training. In\nERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram\nidentities rather than contiguous sequences of n tokens. Furthermore,\nERNIE-Gram employs a generator model to sample plausible n-gram identities as\noptional n-gram masks and predict them in both coarse-grained and fine-grained\nmanners to enable comprehensive n-gram prediction and relation modeling. We\npre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19\ndownstream tasks. Experimental results show that ERNIE-Gram outperforms\nprevious pre-training models like XLNet and RoBERTa by a large margin, and\nachieves comparable results with state-of-the-art methods. The source codes and\npre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.</p>\n", "tags": ["Has Code","IJCAI","Model Architecture","Training Techniques"] },
{"key": "xiao2021lawformer", "citations": "164", "year": "2021", "title":"Lawformer: A Pre-trained Language Model For Chinese Legal Long Documents", "abstract": "<p>Legal artificial intelligence (LegalAI) aims to benefit legal systems with\nthe technology of artificial intelligence, especially natural language\nprocessing (NLP). Recently, inspired by the success of pre-trained language\nmodels (PLMs) in the generic domain, many LegalAI researchers devote their\neffort to apply PLMs to legal tasks. However, utilizing PLMs to address legal\ntasks is still challenging, as the legal documents usually consist of thousands\nof tokens, which is far longer than the length that mainstream PLMs can\nprocess. In this paper, we release the Longformer-based pre-trained language\nmodel, named as Lawformer, for Chinese legal long documents understanding. We\nevaluate Lawformer on a variety of LegalAI tasks, including judgment\nprediction, similar case retrieval, legal reading comprehension, and legal\nquestion answering. The experimental results demonstrate that our model can\nachieve promising improvement on tasks with long documents as inputs.</p>\n", "tags": [] },
{"key": "xiao2021video", "citations": "63", "year": "2022", "title":"Video As Conditional Graph Hierarchy For Multi-granular Question Answering", "abstract": "<p>Video question answering requires the models to understand and reason about\nboth the complex video and language data to correctly derive the answers.\nExisting efforts have been focused on designing sophisticated cross-modal\ninteractions to fuse the information from two modalities, while encoding the\nvideo and question holistically as frame and word sequences. Despite their\nsuccess, these methods are essentially revolving around the sequential nature\nof video- and question-contents, providing little insight to the problem of\nquestion-answering and lacking interpretability as well. In this work, we argue\nthat while video is presented in frame sequence, the visual elements (e.g.,\nobjects, actions, activities and events) are not sequential but rather\nhierarchical in semantic space. To align with the multi-granular essence of\nlinguistic concepts in language queries, we propose to model video as a\nconditional graph hierarchy which weaves together visual facts of different\ngranularity in a level-wise manner, with the guidance of corresponding textual\ncues. Despite the simplicity, our extensive experiments demonstrate the\nsuperiority of such conditional hierarchical graph architecture, with clear\nperformance improvements over prior methods and also better generalization\nacross different type of questions. Further analyses also demonstrate the\nmodel’s reliability as it shows meaningful visual-textual evidences for the\npredicted answers.</p>\n", "tags": ["AAAI","Model Architecture"] },
{"key": "xiao2023supporting", "citations": "106", "year": "2023", "title":"Supporting Qualitative Analysis With Large Language Models: Combining Codebook With GPT-3 For Deductive Coding", "abstract": "<p>Qualitative analysis of textual contents unpacks rich and valuable\ninformation by assigning labels to the data. However, this process is often\nlabor-intensive, particularly when working with large datasets. While recent\nAI-based tools demonstrate utility, researchers may not have readily available\nAI resources and expertise, let alone be challenged by the limited\ngeneralizability of those task-specific models. In this study, we explored the\nuse of large language models (LLMs) in supporting deductive coding, a major\ncategory of qualitative analysis where researchers use pre-determined codebooks\nto label the data into a fixed set of codes. Instead of training task-specific\nmodels, a pre-trained LLM could be used directly for various tasks without\nfine-tuning through prompt learning. Using a curiosity-driven questions coding\ntask as a case study, we found, by combining GPT-3 with expert-drafted\ncodebooks, our proposed approach achieved fair to substantial agreements with\nexpert-coded results. We lay out challenges and opportunities in using LLMs to\nsupport qualitative coding and beyond.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "xiaolei2022towards", "citations": "93", "year": "2022", "title":"Towards Unified Conversational Recommender Systems Via Knowledge-enhanced Prompt Learning", "abstract": "<p>Conversational recommender systems (CRS) aim to proactively elicit user\npreference and recommend high-quality items through natural language\nconversations. Typically, a CRS consists of a recommendation module to predict\npreferred items for users and a conversation module to generate appropriate\nresponses. To develop an effective CRS, it is essential to seamlessly integrate\nthe two modules. Existing works either design semantic alignment strategies, or\nshare knowledge resources and representations between the two modules. However,\nthese approaches still rely on different architectures or techniques to develop\nthe two modules, making it difficult for effective module integration.\n  To address this problem, we propose a unified CRS model named UniCRS based on\nknowledge-enhanced prompt learning. Our approach unifies the recommendation and\nconversation subtasks into the prompt learning paradigm, and utilizes\nknowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to\nfulfill both subtasks in a unified approach. In the prompt design, we include\nfused knowledge representations, task-specific soft tokens, and the dialogue\ncontext, which can provide sufficient contextual information to adapt the PLM\nfor the CRS task. Besides, for the recommendation subtask, we also incorporate\nthe generated response template as an important part of the prompt, to enhance\nthe information interaction between the two subtasks. Extensive experiments on\ntwo public CRS datasets have demonstrated the effectiveness of our approach.</p>\n", "tags": ["KDD","Prompting"] },
{"key": "xie2016neural", "citations": "132", "year": "2016", "title":"Neural Emoji Recommendation In Dialogue Systems", "abstract": "<p>Emoji is an essential component in dialogues which has been broadly utilized\non almost all social platforms. It could express more delicate feelings beyond\nplain texts and thus smooth the communications between users, making dialogue\nsystems more anthropomorphic and vivid. In this paper, we focus on\nautomatically recommending appropriate emojis given the contextual information\nin multi-turn dialogue systems, where the challenges locate in understanding\nthe whole conversations. More specifically, we propose the hierarchical long\nshort-term memory model (H-LSTM) to construct dialogue representations,\nfollowed by a softmax classifier for emoji classification. We evaluate our\nmodels on the task of emoji classification in a real-world dataset, with some\nfurther explorations on parameter sensitivity and case study. Experimental\nresults demonstrate that our method achieves the best performances on all\nevaluation metrics. It indicates that our method could well capture the\ncontextual information and emotion flow in dialogues, which is significant for\nemoji recommendation.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Model Architecture"] },
{"key": "xie2017large", "citations": "120", "year": "2018", "title":"Large-scale Cloze Test Dataset Created By Teachers", "abstract": "<p>Cloze tests are widely adopted in language exams to evaluate students’\nlanguage proficiency. In this paper, we propose the first large-scale\nhuman-created cloze test dataset CLOTH, containing questions used in\nmiddle-school and high-school language exams. With missing blanks carefully\ncreated by teachers and candidate choices purposely designed to be nuanced,\nCLOTH requires a deeper language understanding and a wider attention span than\npreviously automatically-generated cloze datasets. We test the performance of\ndedicatedly designed baseline models including a language model trained on the\nOne Billion Word Corpus and show humans outperform them by a significant\nmargin. We investigate the source of the performance gap, trace model\ndeficiencies to some distinct properties of CLOTH, and identify the limited\nability of comprehending the long-term context to be the key bottleneck.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "xie2019visual", "citations": "164", "year": "2019", "title":"Visual Entailment: A Novel Task For Fine-grained Image Understanding", "abstract": "<p>Existing visual reasoning datasets such as Visual Question Answering (VQA),\noften suffer from biases conditioned on the question, image or answer\ndistributions. The recently proposed CLEVR dataset addresses these limitations\nand requires fine-grained reasoning but the dataset is synthetic and consists\nof similar objects and sentence structures across the dataset.\n  In this paper, we introduce a new inference task, Visual Entailment (VE) -\nconsisting of image-sentence pairs whereby a premise is defined by an image,\nrather than a natural language sentence as in traditional Textual Entailment\ntasks. The goal of a trained VE model is to predict whether the image\nsemantically entails the text. To realize this task, we build a dataset SNLI-VE\nbased on the Stanford Natural Language Inference corpus and Flickr30k dataset.\nWe evaluate various existing VQA baselines and build a model called Explainable\nVisual Entailment (EVE) system to address the VE task. EVE achieves up to 71%\naccuracy and outperforms several other state-of-the-art VQA based models.\nFinally, we demonstrate the explainability of EVE through cross-modal attention\nvisualizations. The SNLI-VE dataset is publicly available at\nhttps://github.com/ necla-ml/SNLI-VE.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "xie2020contrastive", "citations": "401", "year": "2022", "title":"Contrastive Learning For Sequential Recommendation", "abstract": "<p>Sequential recommendation methods play a crucial role in modern recommender\nsystems because of their ability to capture a user’s dynamic interest from\nher/his historical interactions. Despite their success, we argue that these\napproaches usually rely on the sequential prediction task to optimize the huge\namounts of parameters. They usually suffer from the data sparsity problem,\nwhich makes it difficult for them to learn high-quality user representations.\nTo tackle that, inspired by recent advances of contrastive learning techniques\nin the computer version, we propose a novel multi-task model called\n\\textbf{C}ontrastive \\textbf{L}earning for \\textbf{S}equential\n\\textbf{Rec}ommendation~(\\textbf{CL4SRec}). CL4SRec not only takes advantage of\nthe traditional next item prediction task but also utilizes the contrastive\nlearning framework to derive self-supervision signals from the original user\nbehavior sequences. Therefore, it can extract more meaningful user patterns and\nfurther encode the user representation effectively. In addition, we propose\nthree data augmentation approaches to construct self-supervision signals.\nExtensive experiments on four public datasets demonstrate that CL4SRec achieves\nstate-of-the-art performance over existing baselines by inferring better user\nrepresentations.</p>\n", "tags": ["Datasets","Tools"] },
{"key": "xie2021explanation", "citations": "92", "year": "2021", "title":"An Explanation Of In-context Learning As Implicit Bayesian Inference", "abstract": "<p>Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.</p>\n", "tags": ["Datasets","Few-Shot","In Context Learning","Model Architecture","Prompting"] },
{"key": "xie2022decoupled", "citations": "82", "year": "2022", "title":"Decoupled Side Information Fusion For Sequential Recommendation", "abstract": "<p>Side information fusion for sequential recommendation (SR) aims to\neffectively leverage various side information to enhance the performance of\nnext-item prediction. Most state-of-the-art methods build on self-attention\nnetworks and focus on exploring various solutions to integrate the item\nembedding and side information embeddings before the attention layer. However,\nour analysis shows that the early integration of various types of embeddings\nlimits the expressiveness of attention matrices due to a rank bottleneck and\nconstrains the flexibility of gradients. Also, it involves mixed correlations\namong the different heterogeneous information resources, which brings extra\ndisturbance to attention calculation. Motivated by this, we propose Decoupled\nSide Information Fusion for Sequential Recommendation (DIF-SR), which moves the\nside information from the input to the attention layer and decouples the\nattention calculation of various side information and item representation. We\ntheoretically and empirically show that the proposed solution allows\nhigher-rank attention matrices and flexible gradients to enhance the modeling\ncapacity of side information fusion. Also, auxiliary attribute predictors are\nproposed to further activate the beneficial interaction between side\ninformation and item representation learning. Extensive experiments on four\nreal-world datasets demonstrate that our proposed solution stably outperforms\nstate-of-the-art SR models. Further studies show that our proposed solution can\nbe readily incorporated into current attention-based SR models and\nsignificantly boost performance. Our source code is available at\nhttps://github.com/AIM-SE/DIF-SR.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","SIGIR"] },
{"key": "xie2022discrimination", "citations": "60", "year": "2022", "title":"From Discrimination To Generation: Knowledge Graph Completion With Generative Transformer", "abstract": "<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.</p>\n", "tags": ["Datasets","Has Code","Model Architecture"] },
{"key": "xie2022unifiedskg", "citations": "185", "year": "2022", "title":"Unifiedskg: Unifying And Multi-tasking Structured Knowledge Grounding With Text-to-text Language Models", "abstract": "<p>Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the UnifiedSKG framework, which unifies\n21 SKG tasks into a text-to-text format, aiming to promote systematic SKG\nresearch, instead of being exclusive to a single task, domain, or dataset. We\nuse UnifiedSKG to benchmark T5 with different sizes and show that T5, with\nsimple modifications when necessary, achieves state-of-the-art performance on\nalmost all of the 21 tasks. We further demonstrate that multi-task\nprefix-tuning improves the performance on most tasks, largely improving the\noverall performance. UnifiedSKG also facilitates the investigation of zero-shot\nand few-shot learning, and we show that T0, GPT-3, and Codex struggle in\nzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a\nseries of controlled experiments on structured knowledge encoding variants\nacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is\nopen-sourced at https://github.com/hkunlp/unifiedskg.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Few-Shot","Has Code","Model Architecture","Tools"] },
{"key": "xin2020deebert", "citations": "253", "year": "2020", "title":"Deebert: Dynamic Early Exiting For Accelerating BERT Inference", "abstract": "<p>Large-scale pre-trained language models such as BERT have brought significant\nimprovements to NLP applications. However, they are also notorious for being\nslow in inference, which makes them difficult to deploy in real-time\napplications. We propose a simple but effective method, DeeBERT, to accelerate\nBERT inference. Our approach allows samples to exit earlier without passing\nthrough the entire model. Experiments show that DeeBERT is able to save up to\n~40% inference time with minimal degradation in model quality. Further analyses\nshow different behaviors in the BERT transformer layers and also reveal their\nredundancy. Our work provides new ideas to efficiently apply deep\ntransformer-based models to downstream tasks. Code is available at\nhttps://github.com/castorini/DeeBERT.</p>\n", "tags": ["Applications","Efficiency","Has Code","Model Architecture"] },
{"key": "xin2020self", "citations": "164", "year": "2020", "title":"Self-supervised Reinforcement Learning For Recommender Systems", "abstract": "<p>In session-based or sequential recommendation, it is important to consider a\nnumber of factors like long-term user engagement, multiple types of user-item\ninteractions such as clicks, purchases etc. The current state-of-the-art\nsupervised approaches fail to model them appropriately. Casting sequential\nrecommendation task as a reinforcement learning (RL) problem is a promising\ndirection. A major component of RL approaches is to train the agent through\ninteractions with the environment. However, it is often problematic to train a\nrecommender in an on-line fashion due to the requirement to expose users to\nirrelevant recommendations. As a result, learning the policy from logged\nimplicit feedback is of vital importance, which is challenging due to the pure\noff-policy setting and lack of negative rewards (feedback). In this paper, we\npropose self-supervised reinforcement learning for sequential recommendation\ntasks. Our approach augments standard recommendation models with two output\nlayers: one for self-supervised learning and the other for RL. The RL part acts\nas a regularizer to drive the supervised layer focusing on specific\nrewards(e.g., recommending items which may lead to purchases rather than\nclicks) while the self-supervised layer with cross-entropy loss provides strong\ngradient signals for parameter updates. Based on such an approach, we propose\ntwo frameworks namely Self-Supervised Q-learning(SQN) and Self-Supervised\nActor-Critic(SAC). We integrate the proposed frameworks with four\nstate-of-the-art recommendation models. Experimental results on two real-world\ndatasets demonstrate the effectiveness of our approach.</p>\n", "tags": ["Datasets","Reinforcement Learning","SIGIR","Training Techniques"] },
{"key": "xing2016topic", "citations": "331", "year": "2016", "title":"Topic Aware Neural Response Generation", "abstract": "<p>We consider incorporating topic information into the sequence-to-sequence\nframework to generate informative and interesting responses for chatbots. To\nthis end, we propose a topic aware sequence-to-sequence (TA-Seq2Seq) model. The\nmodel utilizes topics to simulate prior knowledge of human that guides them to\nform informative and interesting responses in conversation, and leverages the\ntopic information in generation by a joint attention mechanism and a biased\ngeneration probability. The joint attention mechanism summarizes the hidden\nvectors of an input message as context vectors by message attention,\nsynthesizes topic vectors by topic attention from the topic words of the\nmessage obtained from a pre-trained LDA model, and let these vectors jointly\naffect the generation of words in decoding. To increase the possibility of\ntopic words appearing in responses, the model modifies the generation\nprobability of topic words by adding an extra probability item to bias the\noverall distribution. Empirical study on both automatic evaluation metrics and\nhuman annotations shows that TA-Seq2Seq can generate more informative and\ninteresting responses, and significantly outperform the-state-of-the-art\nresponse generation models.</p>\n", "tags": ["Evaluation","Model Architecture","Tools"] },
{"key": "xing2017hierarchical", "citations": "151", "year": "2018", "title":"Hierarchical Recurrent Attention Network For Response Generation", "abstract": "<p>We study multi-turn response generation in chatbots where a response is\ngenerated according to a conversation context. Existing work has modeled the\nhierarchy of the context, but does not pay enough attention to the fact that\nwords and utterances in the context are differentially important. As a result,\nthey may lose important information in context and generate irrelevant\nresponses. We propose a hierarchical recurrent attention network (HRAN) to\nmodel both aspects in a unified framework. In HRAN, a hierarchical attention\nmechanism attends to important parts within and among utterances with word\nlevel attention and utterance level attention respectively. With the word level\nattention, hidden vectors of a word level encoder are synthesized as utterance\nvectors and fed to an utterance level encoder to construct hidden\nrepresentations of the context. The hidden vectors of the context are then\nprocessed by the utterance level attention and formed as context vectors for\ndecoding the response. Empirical studies on both automatic evaluation and human\njudgment show that HRAN can significantly outperform state-of-the-art models\nfor multi-turn response generation.</p>\n", "tags": ["AAAI","Evaluation","Model Architecture","Tools"] },
{"key": "xiong2016dynamic", "citations": "600", "year": "2016", "title":"Dynamic Memory Networks For Visual And Textual Question Answering", "abstract": "<p>Neural network architectures with memory and attention mechanisms exhibit\ncertain reasoning capabilities required for question answering. One such\narchitecture, the dynamic memory network (DMN), obtained high accuracy on a\nvariety of language tasks. However, it was not shown whether the architecture\nachieves strong results for question answering when supporting facts are not\nmarked during training or whether it could be applied to other modalities such\nas images. Based on an analysis of the DMN, we propose several improvements to\nits memory and input modules. Together with these changes we introduce a novel\ninput module for images in order to be able to answer visual questions. Our new\nDMN+ model improves the state of the art on both the Visual Question Answering\ndataset and the \\babi-10k text question-answering dataset without supporting\nfact supervision.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "xiong2016microsoft", "citations": "313", "year": "2017", "title":"The Microsoft 2016 Conversational Speech Recognition System", "abstract": "<p>We describe Microsoft’s conversational speech recognition system, in which we\ncombine recent developments in neural-network-based acoustic and language\nmodeling to advance the state of the art on the Switchboard recognition task.\nInspired by machine learning ensemble techniques, the system uses a range of\nconvolutional and recurrent neural networks. I-vector modeling and lattice-free\nMMI training provide significant gains for all acoustic model architectures.\nLanguage model rescoring with multiple forward and backward running RNNLMs, and\nword posterior-based system combination provide a 20% boost. The best single\nsystem uses a ResNet architecture acoustic model with RNNLM rescoring, and\nachieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The\ncombined system has an error rate of 6.2%, representing an improvement over\npreviously reported results on this benchmark task.</p>\n", "tags": ["Datasets","Evaluation","ICASSP","Model Architecture","Training Techniques"] },
{"key": "xiong2017dcn", "citations": "91", "year": "2017", "title":"DCN+: Mixed Objective And Deep Residual Coattention For Question Answering", "abstract": "<p>Traditional models for question answering optimize using cross entropy loss,\nwhich encourages exact answers at the cost of penalizing nearby or overlapping\nanswers that are sometimes equally accurate. We propose a mixed objective that\ncombines cross entropy loss with self-critical policy learning. The objective\nuses rewards derived from word overlap to solve the misalignment between\nevaluation metric and optimization objective. In addition to the mixed\nobjective, we improve dynamic coattention networks (DCN) with a deep residual\ncoattention encoder that is inspired by recent work in deep self-attention and\nresidual networks. Our proposals improve model performance across question\ntypes and input lengths, especially for long questions that requires the\nability to capture long-term dependencies. On the Stanford Question Answering\nDataset, our model achieves state-of-the-art results with 75.1% exact match\naccuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy\nand 86.0% F1.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "xiong2017microsoft", "citations": "459", "year": "2018", "title":"The Microsoft 2017 Conversational Speech Recognition System", "abstract": "<p>We describe the 2017 version of Microsoft’s conversational speech recognition\nsystem, in which we update our 2016 system with recent developments in\nneural-network-based acoustic and language modeling to further advance the\nstate of the art on the Switchboard speech recognition task. The system adds a\nCNN-BLSTM acoustic model to the set of model architectures we combined\npreviously, and includes character-based and dialog session aware LSTM language\nmodels in rescoring. For system combination we adopt a two-stage approach,\nwhereby subsets of acoustic models are first combined at the senone/frame\nlevel, followed by a word-level voting via confusion networks. We also added a\nconfusion network rescoring step after system combination. The resulting system\nyields a 5.1% word error rate on the 2000 Switchboard evaluation set.</p>\n", "tags": ["Evaluation","ICASSP","Model Architecture"] },
{"key": "xiong2018modeling", "citations": "84", "year": "2019", "title":"Modeling Coherence For Discourse Neural Machine Translation", "abstract": "<p>Discourse coherence plays an important role in the translation of one text.\nHowever, the previous reported models most focus on improving performance over\nindividual sentence while ignoring cross-sentence links and dependencies, which\naffects the coherence of the text. In this paper, we propose to use discourse\ncontext and reward to refine the translation quality from the discourse\nperspective. In particular, we generate the translation of individual sentences\nat first. Next, we deliberate the preliminary produced translations, and train\nthe model to learn the policy that produces discourse coherent text by a reward\nteacher. Practical results on multiple discourse test datasets indicate that\nour model significantly improves the translation quality over the\nstate-of-the-art baseline system by +1.23 BLEU score. Moreover, our model\ngenerates more discourse coherent text and obtains +2.2 BLEU improvements when\nevaluated by discourse metrics.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Reinforcement Learning"] },
{"key": "xiong2018move", "citations": "115", "year": "2018", "title":"Move Forward And Tell: A Progressive Generator Of Video Descriptions", "abstract": "<p>We present an efficient framework that can generate a coherent paragraph to\ndescribe a given video. Previous works on video captioning usually focus on\nvideo clips. They typically treat an entire video as a whole and generate the\ncaption conditioned on a single embedding. On the contrary, we consider videos\nwith rich temporal structures and aim to generate paragraph descriptions that\ncan preserve the story flow while being coherent and concise. Towards this\ngoal, we propose a new approach, which produces a descriptive paragraph by\nassembling temporally localized descriptions. Given a video, it selects a\nsequence of distinctive clips and generates sentences thereon in a coherent\nmanner. Particularly, the selection of clips and the production of sentences\nare done jointly and progressively driven by a recurrent network – what to\ndescribe next depends on what have been said before. Here, the recurrent\nnetwork is learned via self-critical sequence training with both sentence-level\nand paragraph-level rewards. On the ActivityNet Captions dataset, our method\ndemonstrated the capability of generating high-quality paragraph descriptions\nfor videos. Compared to those by other methods, the descriptions produced by\nour method are often more relevant, more coherent, and more concise.</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "xiong2019improving", "citations": "118", "year": "2019", "title":"Improving Question Answering Over Incomplete Kbs With Knowledge-aware Reader", "abstract": "<p>We propose a new end-to-end question answering model, which learns to\naggregate answer evidence from an incomplete knowledge base (KB) and a set of\nretrieved text snippets. Under the assumptions that the structured KB is easier\nto query and the acquired knowledge can help the understanding of unstructured\ntext, our model first accumulates knowledge of entities from a question-related\nKB subgraph; then reformulates the question in the latent space and reads the\ntexts with the accumulated entity knowledge at hand. The evidence from KB and\ntexts are finally aggregated to predict answers. On the widely-used KBQA\nbenchmark WebQSP, our model achieves consistent improvements across settings\nwith different extents of KB incompleteness.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "xiong2019pretrained", "citations": "164", "year": "2019", "title":"Pretrained Encyclopedia: Weakly Supervised Knowledge-pretrained Language Model", "abstract": "<p>Recent breakthroughs of pretrained language models have shown the\neffectiveness of self-supervised learning for a wide range of natural language\nprocessing (NLP) tasks. In addition to standard syntactic and semantic NLP\ntasks, pretrained models achieve strong improvements on tasks that involve\nreal-world knowledge, suggesting that large-scale language modeling could be an\nimplicit method to capture knowledge. In this work, we further investigate the\nextent to which pretrained models such as BERT capture knowledge using a\nzero-shot fact completion task. Moreover, we propose a simple yet effective\nweakly supervised pretraining objective, which explicitly forces the model to\nincorporate knowledge about real-world entities. Models trained with our new\nobjective yield significant improvements on the fact completion task. When\napplied to downstream tasks, our model consistently outperforms BERT on four\nentity-related question answering datasets (i.e., WebQuestions, TriviaQA,\nSearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard\nfine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "xiong2019tweetqa", "citations": "61", "year": "2019", "title":"TWEETQA: A Social Media Focused Question Answering Dataset", "abstract": "<p>With social media becoming increasingly pop-ular on which lots of news and\nreal-time eventsare reported, developing automated questionanswering systems is\ncritical to the effective-ness of many applications that rely on real-time\nknowledge. While previous datasets haveconcentrated on question answering (QA)\nforformal text like news and Wikipedia, wepresent the first large-scale dataset\nfor QA oversocial media data. To ensure that the tweetswe collected are useful,\nwe only gather tweetsused by journalists to write news articles. Wethen ask\nhuman annotators to write questionsand answers upon these tweets. Unlike\notherQA datasets like SQuAD in which the answersare extractive, we allow the\nanswers to be ab-stractive. We show that two recently proposedneural models\nthat perform well on formaltexts are limited in their performance when ap-plied\nto our dataset. In addition, even the fine-tuned BERT model is still lagging\nbehind hu-man performance with a large margin. Our re-sults thus point to the\nneed of improved QAsystems targeting social media text.</p>\n", "tags": ["Applications","Datasets"] },
{"key": "xu2016cached", "citations": "184", "year": "2016", "title":"Cached Long Short-term Memory Neural Networks For Document-level Sentiment Classification", "abstract": "<p>Recently, neural networks have achieved great success on sentiment\nclassification due to their ability to alleviate feature engineering. However,\none of the remaining challenges is to model long texts in document-level\nsentiment classification under a recurrent architecture because of the\ndeficiency of the memory unit. To address this problem, we present a Cached\nLong Short-Term Memory neural networks (CLSTM) to capture the overall semantic\ninformation in long texts. CLSTM introduces a cache mechanism, which divides\nmemory into several groups with different forgetting rates and thus enables the\nnetwork to keep sentiment information better within a recurrent unit. The\nproposed CLSTM outperforms the state-of-the-art models on three publicly\navailable document-level sentiment analysis datasets.</p>\n", "tags": ["Datasets","EMNLP","Memory & Context","Model Architecture"] },
{"key": "xu2016incorporating", "citations": "62", "year": "2017", "title":"Incorporating Loose-structured Knowledge Into Conversation Modeling Via Recall-gate LSTM", "abstract": "<p>Modeling human conversations is the essence for building satisfying chat-bots\nwith multi-turn dialog ability. Conversation modeling will notably benefit from\ndomain knowledge since the relationships between sentences can be clarified due\nto semantic hints introduced by knowledge. In this paper, a deep neural network\nis proposed to incorporate background knowledge for conversation modeling.\nThrough a specially designed Recall gate, domain knowledge can be transformed\ninto the extra global memory of Long Short-Term Memory (LSTM), so as to enhance\nLSTM by cooperating with its local memory to capture the implicit semantic\nrelevance between sentences within conversations. In addition, this paper\nintroduces the loose structured domain knowledge base, which can be built with\nslight amount of manual work and easily adopted by the Recall gate. Our model\nis evaluated on the context-oriented response selecting task, and experimental\nresults on both two datasets have shown that our approach is promising for\nmodeling human conversations and building key components of automatic chatting\nsystems.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "xu2017attngan", "citations": "1697", "year": "2018", "title":"Attngan: Fine-grained Text To Image Generation With Attentional Generative Adversarial Networks", "abstract": "<p>In this paper, we propose an Attentional Generative Adversarial Network\n(AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained\ntext-to-image generation. With a novel attentional generative network, the\nAttnGAN can synthesize fine-grained details at different subregions of the\nimage by paying attentions to the relevant words in the natural language\ndescription. In addition, a deep attentional multimodal similarity model is\nproposed to compute a fine-grained image-text matching loss for training the\ngenerator. The proposed AttnGAN significantly outperforms the previous state of\nthe art, boosting the best reported inception score by 14.14% on the CUB\ndataset and 170.25% on the more challenging COCO dataset. A detailed analysis\nis also performed by visualizing the attention layers of the AttnGAN. It for\nthe first time shows that the layered attentional GAN is able to automatically\nselect the condition at the word level for generating different parts of the\nimage.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "xu2017fooling", "citations": "72", "year": "2018", "title":"Fooling Vision And Language Models Despite Localization And Attention Mechanism", "abstract": "<p>Adversarial attacks are known to succeed on classifiers, but it has been an\nopen question whether more complex vision systems are vulnerable. In this\npaper, we study adversarial examples for vision and language models, which\nincorporate natural language understanding and complex structures such as\nattention, localization, and modular architectures. In particular, we\ninvestigate attacks on a dense captioning model and on two visual question\nanswering (VQA) models. Our evaluation shows that we can generate adversarial\nexamples with a high success rate (i.e., &gt; 90%) for these models. Our work\nsheds new light on understanding adversarial attacks on vision systems which\nhave a language component and shows that attention, bounding box localization,\nand compositional internal structures are vulnerable to adversarial attacks.\nThese observations will inform future work towards building effective defenses.</p>\n", "tags": ["CVPR","Evaluation","Model Architecture","Security"] },
{"key": "xu2018graph2seq", "citations": "163", "year": "2018", "title":"Graph2seq: Graph To Sequence Learning With Attention-based Neural Networks", "abstract": "<p>The celebrated Sequence to Sequence learning (Seq2Seq) technique and its\nnumerous variants achieve excellent performance on many tasks. However, many\nmachine learning tasks have inputs naturally represented as graphs; existing\nSeq2Seq models face a significant challenge in achieving accurate conversion\nfrom graph form to the appropriate sequence. To address this challenge, we\nintroduce a novel general end-to-end graph-to-sequence neural encoder-decoder\nmodel that maps an input graph to a sequence of vectors and uses an\nattention-based LSTM method to decode the target sequence from these vectors.\nOur method first generates the node and graph embeddings using an improved\ngraph-based neural network with a novel aggregation strategy to incorporate\nedge direction information in the node embeddings. We further introduce an\nattention mechanism that aligns node embeddings and the decoding sequence to\nbetter cope with large graphs. Experimental results on bAbI, Shortest Path, and\nNatural Language Generation tasks demonstrate that our model achieves\nstate-of-the-art performance and significantly outperforms existing graph\nneural networks, Seq2Seq, and Tree2Seq models; using the proposed\nbi-directional node embedding aggregation strategy, the model can converge\nrapidly to the optimal performance.</p>\n", "tags": ["Model Architecture"] },
{"key": "xu2018skeleton", "citations": "102", "year": "2018", "title":"A Skeleton-based Model For Promoting Coherence Among Sentences In Narrative Story Generation", "abstract": "<p>Narrative story generation is a challenging problem because it demands the\ngenerated sentences with tight semantic connections, which has not been well\nstudied by most existing generative models. To address this problem, we propose\na skeleton-based model to promote the coherence of generated stories. Different\nfrom traditional models that generate a complete sentence at a stroke, the\nproposed model first generates the most critical phrases, called skeleton, and\nthen expands the skeleton to a complete and fluent sentence. The skeleton is\nnot manually defined, but learned by a reinforcement learning method. Compared\nto the state-of-the-art models, our skeleton-based model can generate\nsignificantly more coherent text according to human evaluation and automatic\nevaluation. The G-score is improved by 20.1% in the human evaluation. The code\nis available at https://github.com/lancopku/Skeleton-Based-Generation-Model</p>\n", "tags": ["EMNLP","Evaluation","Has Code"] },
{"key": "xu2018sql", "citations": "71", "year": "2018", "title":"Sql-to-text Generation With Graph-to-sequence Model", "abstract": "<p>Previous work approaches the SQL-to-text generation task using vanilla\nSeq2Seq models, which may not fully capture the inherent graph-structured\ninformation in SQL query. In this paper, we first introduce a strategy to\nrepresent the SQL query as a directed graph and then employ a graph-to-sequence\nmodel to encode the global structure information into node embeddings. This\nmodel can effectively learn the correlation between the SQL query pattern and\nits interpretation. Experimental results on the WikiSQL dataset and\nStackoverflow dataset show that our model significantly outperforms the Seq2Seq\nand Tree2Seq baselines, achieving the state-of-the-art performance.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "xu2018unpaired", "citations": "208", "year": "2018", "title":"Unpaired Sentiment-to-sentiment Translation: A Cycled Reinforcement Learning Approach", "abstract": "<p>The goal of sentiment-to-sentiment “translation” is to change the underlying\nsentiment of a sentence while keeping its content. The main challenge is the\nlack of parallel data. To solve this problem, we propose a cycled reinforcement\nlearning method that enables training on unpaired data by collaboration between\na neutralization module and an emotionalization module. We evaluate our\napproach on two review datasets, Yelp and Amazon. Experimental results show\nthat our approach significantly outperforms the state-of-the-art systems.\nEspecially, the proposed method substantially improves the content preservation\nperformance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to\n14.06 on the two datasets, respectively.</p>\n", "tags": ["Datasets","Reinforcement Learning","Training Techniques"] },
{"key": "xu2019bert", "citations": "290", "year": "2019", "title":"BERT Post-training For Review Reading Comprehension And Aspect-based Sentiment Analysis", "abstract": "<p>Question-answering plays an important role in e-commerce as it allows\npotential customers to actively seek crucial information about products or\nservices to help their purchase decision making. Inspired by the recent success\nof machine reading comprehension (MRC) on formal documents, this paper explores\nthe potential of turning customer reviews into a large source of knowledge that\ncan be exploited to answer user questions.~We call this problem Review Reading\nComprehension (RRC). To the best of our knowledge, no existing work has been\ndone on RRC. In this work, we first build an RRC dataset called ReviewRC based\non a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has\nlimited training examples for RRC (and also for aspect-based sentiment\nanalysis), we then explore a novel post-training approach on the popular\nlanguage model BERT to enhance the performance of fine-tuning of BERT for RRC.\nTo show the generality of the approach, the proposed post-training is also\napplied to some other review-based tasks such as aspect extraction and aspect\nsentiment classification in aspect-based sentiment analysis. Experimental\nresults demonstrate that the proposed post-training is highly effective. The\ndatasets and code are available at https://www.cs.uic.edu/~hxu/.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "xu2019end", "citations": "142", "year": "2019", "title":"End-to-end Knowledge-routed Relational Dialogue System For Automatic Diagnosis", "abstract": "<p>Beyond current conversational chatbots or task-oriented dialogue systems that\nhave attracted increasing attention, we move forward to develop a dialogue\nsystem for automatic medical diagnosis that converses with patients to collect\nadditional symptoms beyond their self-reports and automatically makes a\ndiagnosis. Besides the challenges for conversational dialogue systems (e.g.\ntopic transition coherency and question understanding), automatic medical\ndiagnosis further poses more critical requirements for the dialogue rationality\nin the context of medical knowledge and symptom-disease relations. Existing\ndialogue systems (Madotto, Wu, and Fung 2018; Wei et al. 2018; Li et al. 2017)\nmostly rely on data-driven learning and cannot be able to encode extra expert\nknowledge graph. In this work, we propose an End-to-End Knowledge-routed\nRelational Dialogue System (KR-DS) that seamlessly incorporates rich medical\nknowledge graph into the topic transition in dialogue management, and makes it\ncooperative with natural language understanding and natural language\ngeneration. A novel Knowledge-routed Deep Q-network (KR-DQN) is introduced to\nmanage topic transitions, which integrates a relational refinement branch for\nencoding relations among different symptoms and symptom-disease pairs, and a\nknowledge-routed graph branch for topic decision-making. Extensive experiments\non a public medical dialogue dataset show our KR-DS significantly beats\nstate-of-the-art methods (by more than 8% in diagnosis accuracy). We further\nshow the superiority of our KR-DS on a newly collected medical dialogue system\ndataset, which is more challenging retaining original self-reports and\nconversational data between patients and doctors.</p>\n", "tags": ["AAAI","Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "xu2019layoutlm", "citations": "583", "year": "2020", "title":"Layoutlm: Pre-training Of Text And Layout For Document Image Understanding", "abstract": "<p>Pre-training techniques have been verified successfully in a variety of NLP\ntasks in recent years. Despite the widespread use of pre-training models for\nNLP applications, they almost exclusively focus on text-level manipulation,\nwhile neglecting layout and style information that is vital for document image\nunderstanding. In this paper, we propose the \\textbf{LayoutLM} to jointly model\ninteractions between text and layout information across scanned document\nimages, which is beneficial for a great number of real-world document image\nunderstanding tasks such as information extraction from scanned documents.\nFurthermore, we also leverage image features to incorporate words’ visual\ninformation into LayoutLM. To the best of our knowledge, this is the first time\nthat text and layout are jointly learned in a single framework for\ndocument-level pre-training. It achieves new state-of-the-art results in\nseveral downstream tasks, including form understanding (from 70.72 to 79.27),\nreceipt understanding (from 94.02 to 95.24) and document image classification\n(from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly\navailable at https://aka.ms/layoutlm.</p>\n", "tags": ["Applications","Has Code","KDD","Tools","Training Techniques"] },
{"key": "xu2020bert", "citations": "169", "year": "2020", "title":"Bert-of-theseus: Compressing BERT By Progressive Module Replacing", "abstract": "<p>In this paper, we propose a novel model compression approach to effectively\ncompress BERT by progressive module replacing. Our approach first divides the\noriginal BERT into several modules and builds their compact substitutes. Then,\nwe randomly replace the original modules with their substitutes to train the\ncompact modules to mimic the behavior of the original modules. We progressively\nincrease the probability of replacement through the training. In this way, our\napproach brings a deeper level of interaction between the original and compact\nmodels. Compared to the previous knowledge distillation approaches for BERT\ncompression, our approach does not introduce any additional loss function. Our\napproach outperforms existing knowledge distillation approaches on GLUE\nbenchmark, showing a new perspective of model compression.</p>\n", "tags": ["EMNLP","Efficiency","Evaluation","Model Architecture","Training Techniques"] },
{"key": "xu2020clue", "citations": "226", "year": "2020", "title":"CLUE: A Chinese Language Understanding Evaluation Benchmark", "abstract": "<p>The advent of natural language understanding (NLU) benchmarks for English,\nsuch as GLUE and SuperGLUE allows new NLU models to be evaluated across a\ndiverse set of tasks. These comprehensive benchmarks have facilitated a broad\nrange of research and applications in natural language processing (NLP). The\nproblem, however, is that most such benchmarks are limited to English, which\nhas made it difficult to replicate many of the successes in English NLU for\nother languages. To help remedy this issue, we introduce the first large-scale\nChinese Language Understanding Evaluation (CLUE) benchmark. CLUE is an\nopen-ended, community-driven project that brings together 9 tasks spanning\nseveral well-established single-sentence/sentence-pair classification tasks, as\nwell as machine reading comprehension, all on original Chinese text. To\nestablish results on these tasks, we report scores using an exhaustive set of\ncurrent state-of-the-art pre-trained Chinese models (9 in total). We also\nintroduce a number of supplementary datasets and additional tools to help\nfacilitate further progress on Chinese NLU. Our benchmark is released at\nhttps://www.CLUEbenchmarks.com</p>\n", "tags": ["COLING","Datasets","Evaluation","Tools"] },
{"key": "xu2020end", "citations": "110", "year": "2020", "title":"End-to-end Slot Alignment And Recognition For Cross-lingual NLU", "abstract": "<p>Natural language understanding (NLU) in the context of goal-oriented dialog\nsystems typically includes intent classification and slot labeling tasks.\nExisting methods to expand an NLU system to new languages use machine\ntranslation with slot label projection from source to the translated\nutterances, and thus are sensitive to projection errors. In this work, we\npropose a novel end-to-end model that learns to align and predict target slot\nlabels jointly for cross-lingual transfer. We introduce MultiATIS++, a new\nmultilingual NLU corpus that extends the Multilingual ATIS corpus to nine\nlanguages across four language families, and evaluate our method using the\ncorpus. Results show that our method outperforms a simple label projection\nmethod using fast-align on most languages, and achieves competitive performance\nto the more complex, state-of-the-art projection method with only half of the\ntraining time. We release our MultiATIS++ corpus to the community to continue\nfuture research on cross-lingual NLU.</p>\n", "tags": ["Datasets","EMNLP","Training Techniques"] },
{"key": "xu2020layoutlmv2", "citations": "330", "year": "2021", "title":"Layoutlmv2: Multi-modal Pre-training For Visually-rich Document Understanding", "abstract": "<p>Pre-training of text and layout has proved effective in a variety of\nvisually-rich document understanding tasks due to its effective model\narchitecture and the advantage of large-scale unlabeled scanned/digital-born\ndocuments. We propose LayoutLMv2 architecture with new pre-training tasks to\nmodel the interaction among text, layout, and image in a single multi-modal\nframework. Specifically, with a two-stream multi-modal Transformer encoder,\nLayoutLMv2 uses not only the existing masked visual-language modeling task but\nalso the new text-image alignment and text-image matching tasks, which make it\nbetter capture the cross-modality interaction in the pre-training stage.\nMeanwhile, it also integrates a spatial-aware self-attention mechanism into the\nTransformer architecture so that the model can fully understand the relative\npositional relationship among different text blocks. Experiment results show\nthat LayoutLMv2 outperforms LayoutLM by a large margin and achieves new\nstate-of-the-art results on a wide variety of downstream visually-rich document\nunderstanding tasks, including FUNSD (0.7895 \\(\\to\\) 0.8420), CORD (0.9493 \\(\\to\\)\n0.9601), SROIE (0.9524 \\(\\to\\) 0.9781), Kleister-NDA (0.8340 \\(\\to\\) 0.8520),\nRVL-CDIP (0.9443 \\(\\to\\) 0.9564), and DocVQA (0.7295 \\(\\to\\) 0.8672). We made our\nmodel and code publicly available at https://aka.ms/layoutlmv2.</p>\n", "tags": ["Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "xu2020megatron", "citations": "104", "year": "2020", "title":"MEGATRON-CNTRL: Controllable Story Generation With External Knowledge Using Large-scale Language Models", "abstract": "<p>Existing pre-trained large language models have shown unparalleled generative\ncapabilities. However, they are not controllable. In this paper, we propose\nMEGATRON-CNTRL, a novel framework that uses large-scale language models and\nadds control to text generation by incorporating an external knowledge base.\nOur framework consists of a keyword predictor, a knowledge retriever, a\ncontextual knowledge ranker, and a conditional text generator. As we do not\nhave access to ground-truth supervision for the knowledge ranker, we make use\nof weak supervision from sentence embedding. The empirical results show that\nour model generates more fluent, consistent, and coherent stories with less\nrepetition and higher diversity compared to prior work on the ROC story\ndataset. We showcase the controllability of our model by replacing the keywords\nused to generate stories and re-running the generation process. Human\nevaluation results show that 77.5% of these stories are successfully controlled\nby the new keywords. Furthermore, by scaling our model from 124 million to 8.3\nbillion parameters we demonstrate that larger models improve both the quality\nof generation (from 74.5% to 93.0% for consistency) and controllability (from\n77.5% to 91.5%).</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Retrieval Systems","Tools"] },
{"key": "xu2020recipes", "citations": "101", "year": "2020", "title":"Recipes For Safety In Open-domain Chatbots", "abstract": "<p>Models trained on large unlabeled corpora of human interactions will learn\npatterns and mimic behaviors therein, which include offensive or otherwise\ntoxic behavior and unwanted biases. We investigate a variety of methods to\nmitigate these issues in the context of open-domain generative dialogue models.\nWe introduce a new human-and-model-in-the-loop framework for both training\nsafer models and for evaluating them, as well as a novel method to distill\nsafety considerations inside generative models without the use of an external\nclassifier at deployment time. We conduct experiments comparing these methods\nand find our new techniques are (i) safer than existing models as measured by\nautomatic and human evaluations while (ii) maintaining usability metrics such\nas engagingness relative to the state of the art. We then discuss the\nlimitations of this work by analyzing failure cases of our models.</p>\n", "tags": ["Evaluation","Tools","Training Techniques"] },
{"key": "xu2021beyond", "citations": "81", "year": "2022", "title":"Beyond Goldfish Memory: Long-term Open-domain Conversation", "abstract": "<p>Despite recent improvements in open-domain dialogue models, state of the art\nmodels are trained and evaluated on short conversations with little context. In\ncontrast, the long-term conversation setting has hardly been studied. In this\nwork we collect and release a human-human dataset consisting of multiple chat\nsessions whereby the speaking partners learn about each other’s interests and\ndiscuss the things they have learnt from past sessions. We show how existing\nmodels trained on existing datasets perform poorly in this long-term\nconversation setting in both automatic and human evaluations, and we study\nlong-context models that can perform much better. In particular, we find\nretrieval-augmented methods and methods with an ability to summarize and recall\nprevious conversations outperform the standard encoder-decoder architectures\ncurrently considered state of the art.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","RAG"] },
{"key": "xu2021e2e", "citations": "75", "year": "2021", "title":"E2E-VLP: End-to-end Vision-language Pre-training Enhanced By Visual Learning", "abstract": "<p>Vision-language pre-training (VLP) on large-scale image-text pairs has\nachieved huge success for the cross-modal downstream tasks. The most existing\npre-training methods mainly adopt a two-step training procedure, which firstly\nemploys a pre-trained object detector to extract region-based visual features,\nthen concatenates the image representation and text embedding as the input of\nTransformer to train. However, these methods face problems of using\ntask-specific visual representation of the specific object detector for generic\ncross-modal understanding, and the computation inefficiency of two-stage\npipeline. In this paper, we propose the first end-to-end vision-language\npre-trained model for both V+L understanding and generation, namely E2E-VLP,\nwhere we build a unified Transformer framework to jointly learn visual\nrepresentation, and semantic alignments between image and text. We incorporate\nthe tasks of object detection and image captioning into pre-training with a\nunified Transformer encoder-decoder architecture for enhancing visual learning.\nAn extensive set of experiments have been conducted on well-established\nvision-language downstream tasks to demonstrate the effectiveness of this novel\nVLP paradigm.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "xu2021entity", "citations": "144", "year": "2021", "title":"Entity Structure Within And Throughout: Modeling Mention Dependencies For Document-level Relation Extraction", "abstract": "<p>Entities, as the essential elements in relation extraction tasks, exhibit\ncertain structure. In this work, we formulate such structure as distinctive\ndependencies between mention pairs. We then propose SSAN, which incorporates\nthese structural dependencies within the standard self-attention mechanism and\nthroughout the overall encoding stage. Specifically, we design two alternative\ntransformation modules inside each self-attention building block to produce\nattentive biases so as to adaptively regularize its attention flow. Our\nexperiments demonstrate the usefulness of the proposed entity structure and the\neffectiveness of SSAN. It significantly outperforms competitive baselines,\nachieving new state-of-the-art results on three popular document-level relation\nextraction datasets. We further provide ablation and visualization to show how\nthe entity structure guides the model for better relation extraction. Our code\nis publicly available.</p>\n", "tags": ["AAAI","Datasets","Model Architecture"] },
{"key": "xu2021ide", "citations": "102", "year": "2022", "title":"In-ide Code Generation From Natural Language: Promise And Challenges", "abstract": "<p>A great part of software development involves conceptualizing or\ncommunicating the underlying procedures and logic that needs to be expressed in\nprograms. One major difficulty of programming is turning concept into code,\nespecially when dealing with the APIs of unfamiliar libraries. Recently, there\nhas been a proliferation of machine learning methods for code generation and\nretrieval from natural language queries, but these have primarily been\nevaluated purely based on retrieval accuracy or overlap of generated code with\ndeveloper-written code, and the actual effect of these methods on the developer\nworkflow is surprisingly unattested. We perform the first comprehensive\ninvestigation of the promise and challenges of using such technology inside the\nIDE, asking “at the current state of technology does it improve developer\nproductivity or accuracy, how does it affect the developer experience, and what\nare the remaining gaps and challenges?” We first develop a plugin for the IDE\nthat implements a hybrid of code generation and code retrieval functionality,\nand orchestrate virtual environments to enable collection of many user events.\nWe ask developers with various backgrounds to complete 14 Python programming\ntasks ranging from basic file manipulation to machine learning or data\nvisualization, with or without the help of the plugin. While qualitative\nsurveys of developer experience are largely positive, quantitative results with\nregards to increased productivity, code quality, or program correctness are\ninconclusive. Analysis identifies several pain points that could improve the\neffectiveness of future machine learning based code generation/retrieval\ndeveloper assistants, and demonstrates when developers prefer code generation\nover code retrieval and vice versa. We release all data and software to pave\nthe road for future empirical studies and development of better models.</p>\n", "tags": ["Llm For Code","Tools"] },
{"key": "xu2021raise", "citations": "106", "year": "2021", "title":"Raise A Child In Large Language Model: Towards Effective And Generalizable Fine-tuning", "abstract": "<p>Recent pretrained language models extend from millions to billions of\nparameters. Thus the need to fine-tune an extremely large pretrained model with\na limited training corpus arises in various downstream tasks. In this paper, we\npropose a straightforward yet effective fine-tuning technique, Child-Tuning,\nwhich updates a subset of parameters (called child network) of large pretrained\nmodels via strategically masking out the gradients of the non-child network\nduring the backward process. Experiments on various downstream tasks in GLUE\nbenchmark show that Child-Tuning consistently outperforms the vanilla\nfine-tuning by 1.5~8.6 average score among four different pretrained models,\nand surpasses the prior fine-tuning techniques by 0.6~1.3 points. Furthermore,\nempirical results on domain transfer and task transfer show that Child-Tuning\ncan obtain better generalization performance by large margins.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "xu2021vlm", "citations": "69", "year": "2021", "title":"VLM: Task-agnostic Video-language Model Pre-training For Video Understanding", "abstract": "<p>We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "xu2022beyond", "citations": "81", "year": "2022", "title":"Beyond Preserved Accuracy: Evaluating Loyalty And Robustness Of BERT Compression", "abstract": "<p>Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.</p>\n", "tags": ["Datasets","Efficiency","Evaluation","Model Architecture"] },
{"key": "xu2022systematic", "citations": "314", "year": "2022", "title":"A Systematic Evaluation Of Large Language Models Of Code", "abstract": "<p>Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "xu2022transpolymer", "citations": "94", "year": "2023", "title":"Transpolymer: A Transformer-based Language Model For Polymer Property Predictions", "abstract": "<p>Accurate and efficient prediction of polymer properties is of great\nsignificance in polymer design. Conventionally, expensive and time-consuming\nexperiments or simulations are required to evaluate polymer functions.\nRecently, Transformer models, equipped with self-attention mechanisms, have\nexhibited superior performance in natural language processing. However, such\nmethods have not been investigated in polymer sciences. Herein, we report\nTransPolymer, a Transformer-based language model for polymer property\nprediction. Our proposed polymer tokenizer with chemical awareness enables\nlearning representations from polymer sequences. Rigorous experiments on ten\npolymer property prediction benchmarks demonstrate the superior performance of\nTransPolymer. Moreover, we show that TransPolymer benefits from pretraining on\nlarge unlabeled dataset via Masked Language Modeling. Experimental results\nfurther manifest the important role of self-attention in modeling polymer\nsequences. We highlight this model as a promising computational tool for\npromoting rational polymer design and understanding structure-property\nrelationships from a data science view.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "xu2023baize", "citations": "70", "year": "2023", "title":"Baize: An Open-source Chat Model With Parameter-efficient Tuning On Self-chat Data", "abstract": "<p>Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.</p>\n", "tags": ["Dialogue & Multi Turn","EMNLP","Has Code","Tools"] },
{"key": "xu2023drivegpt4", "citations": "92", "year": "2024", "title":"Drivegpt4: Interpretable End-to-end Autonomous Driving Via Large Language Model", "abstract": "<p>Multimodal large language models (MLLMs) have emerged as a prominent area of\ninterest within the research community, given their proficiency in handling and\nreasoning with non-textual data, including images and videos. This study seeks\nto extend the application of MLLMs to the realm of autonomous driving by\nintroducing DriveGPT4, a novel interpretable end-to-end autonomous driving\nsystem based on LLMs. Capable of processing multi-frame video inputs and\ntextual queries, DriveGPT4 facilitates the interpretation of vehicle actions,\noffers pertinent reasoning, and effectively addresses a diverse range of\nquestions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle\ncontrol signals in an end-to-end fashion.These advanced capabilities are\nachieved through the utilization of a bespoke visual instruction tuning\ndataset, specifically tailored for autonomous driving applications, in\nconjunction with a mix-finetuning training strategy. DriveGPT4 represents the\npioneering effort to leverage LLMs for the development of an interpretable\nend-to-end autonomous driving solution. Evaluations conducted on the BDD-X\ndataset showcase the superior qualitative and quantitative performance of\nDriveGPT4. Additionally, the fine-tuning of domain-specific data enables\nDriveGPT4 to yield close or even improved results in terms of autonomous\ndriving grounding when contrasted with GPT4-V.</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Training Techniques"] },
{"key": "xu2024hallucination", "citations": "73", "year": "2024", "title":"Hallucination Is Inevitable: An Innate Limitation Of Large Language Models", "abstract": "<p>Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs.</p>\n", "tags": ["Tools"] },
{"key": "xue2021advancing", "citations": "66", "year": "2022", "title":"Advancing High-resolution Video-language Representation With Large-scale Video Transcriptions", "abstract": "<p>We study joint video and language (VL) pre-training to enable cross-modality\nlearning and benefit plentiful downstream VL tasks. Existing works either\nextract low-quality video features or learn limited text embedding, while\nneglecting that high-resolution videos and diversified semantics can\nsignificantly improve cross-modality learning. In this paper, we propose a\nnovel High-resolution and Diversified VIdeo-LAnguage pre-training model\n(HD-VILA) for many visual tasks. In particular, we collect a large dataset with\ntwo distinct properties: 1) the first high-resolution dataset including 371.5k\nhours of 720p videos, and 2) the most diversified dataset covering 15 popular\nYouTube categories. To enable VL pre-training, we jointly optimize the HD-VILA\nmodel by a hybrid Transformer that learns rich spatiotemporal features, and a\nmultimodal Transformer that enforces interactions of the learned video features\nwith diversified texts. Our pre-training model achieves new state-of-the-art\nresults in 10 VL understanding tasks and 2 more novel text-to-visual generation\ntasks. For example, we outperform SOTA models with relative increases of 40.4%\nR@1 in zero-shot MSR-VTT text-to-video retrieval task and 55.4% in\nhigh-resolution dataset LSMDC. The learned VL embedding is also effective in\ngenerating visually pleasing and semantically relevant results in\ntext-to-visual editing and super-resolution tasks.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "xue2021byt5", "citations": "161", "year": "2022", "title":"Byt5: Towards A Token-free Future With Pre-trained Byte-to-byte Models", "abstract": "<p>Most widely-used pre-trained language models operate on sequences of tokens\ncorresponding to word or subword units. By comparison, token-free models that\noperate directly on raw text (bytes or characters) have many benefits: they can\nprocess text in any language out of the box, they are more robust to noise, and\nthey minimize technical debt by removing complex and error-prone text\npreprocessing pipelines. Since byte or character sequences are longer than\ntoken sequences, past work on token-free models has often introduced new model\narchitectures designed to amortize the cost of operating directly on raw text.\nIn this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte sequences. We characterize the\ntrade-offs in terms of parameter count, training FLOPs, and inference speed,\nand show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more\nrobust to noise and perform better on tasks that are sensitive to spelling and\npronunciation. As part of our contribution, we release a new set of pre-trained\nbyte-level Transformer models based on the T5 architecture, as well as all code\nand data used in our experiments.</p>\n", "tags": ["Model Architecture","TACL","Training Techniques"] },
{"key": "xue2022promptcast", "citations": "76", "year": "2023", "title":"Promptcast: A New Prompt-based Learning Paradigm For Time Series Forecasting", "abstract": "<p>This paper presents a new perspective on time series forecasting. In existing\ntime series forecasting methods, the models take a sequence of numerical values\nas input and yield numerical values as output. The existing SOTA models are\nlargely based on the Transformer architecture, modified with multiple encoding\nmechanisms to incorporate the context and semantics around the historical data.\nInspired by the successes of pre-trained language foundation models, we pose a\nquestion about whether these models can also be adapted to solve time-series\nforecasting. Thus, we propose a new forecasting paradigm: prompt-based time\nseries forecasting (PromptCast). In this novel task, the numerical input and\noutput are transformed into prompts and the forecasting task is framed in a\nsentence-to-sentence manner, making it possible to directly apply language\nmodels for forecasting purposes. To support and facilitate the research of this\ntask, we also present a large-scale dataset (PISA) that includes three\nreal-world forecasting scenarios. We evaluate different SOTA numerical-based\nforecasting methods and language generation models. The benchmark results with\nvarious forecasting settings demonstrate the proposed PromptCast with language\ngeneration models is a promising research direction. Additionally, in\ncomparison to conventional numerical-based forecasting, PromptCast shows a much\nbetter generalization ability under the zero-shot setting.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Prompting","Time Series"] },
{"key": "xuezhi2022self", "citations": "476", "year": "2022", "title":"Self-consistency Improves Chain Of Thought Reasoning In Language Models", "abstract": "<p>Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).</p>\n", "tags": ["Evaluation","Prompting"] },
{"key": "yagcioglu2018recipeqa", "citations": "122", "year": "2018", "title":"Recipeqa: A Challenge Dataset For Multimodal Comprehension Of Cooking Recipes", "abstract": "<p>Understanding and reasoning about cooking recipes is a fruitful research\ndirection towards enabling machines to interpret procedural text. In this work,\nwe introduce RecipeQA, a dataset for multimodal comprehension of cooking\nrecipes. It comprises of approximately 20K instructional recipes with multiple\nmodalities such as titles, descriptions and aligned set of images. With over\n36K automatically generated question-answer pairs, we design a set of\ncomprehension and reasoning tasks that require joint understanding of images\nand text, capturing the temporal flow of events and making sense of procedural\nknowledge. Our preliminary results indicate that RecipeQA will serve as a\nchallenging test bed and an ideal benchmark for evaluating machine\ncomprehension systems. The data and leaderboard are available at\nhttp://hucvl.github.io/recipeqa.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Has Code"] },
{"key": "yamada2020luke", "citations": "497", "year": "2020", "title":"LUKE: Deep Contextualized Entity Representations With Entity-aware Self-attention", "abstract": "<p>Entity representations are useful in natural language tasks involving\nentities. In this paper, we propose new pretrained contextualized\nrepresentations of words and entities based on the bidirectional transformer.\nThe proposed model treats words and entities in a given text as independent\ntokens, and outputs contextualized representations of them. Our model is\ntrained using a new pretraining task based on the masked language model of\nBERT. The task involves predicting randomly masked words and entities in a\nlarge entity-annotated corpus retrieved from Wikipedia. We also propose an\nentity-aware self-attention mechanism that is an extension of the\nself-attention mechanism of the transformer, and considers the types of tokens\n(words or entities) when computing attention scores. The proposed model\nachieves impressive empirical performance on a wide range of entity-related\ntasks. In particular, it obtains state-of-the-art results on five well-known\ndatasets: Open Entity (entity typing), TACRED (relation classification),\nCoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering),\nand SQuAD 1.1 (extractive question answering). Our source code and pretrained\nrepresentations are available at https://github.com/studio-ousia/luke.</p>\n", "tags": ["Datasets","EMNLP","Has Code","Model Architecture"] },
{"key": "yan2018deep", "citations": "61", "year": "2019", "title":"A Deep Cascade Model For Multi-document Reading Comprehension", "abstract": "<p>A fundamental trade-off between effectiveness and efficiency needs to be\nbalanced when designing an online question answering system. Effectiveness\ncomes from sophisticated functions such as extractive machine reading\ncomprehension (MRC), while efficiency is obtained from improvements in\npreliminary retrieval components such as candidate document selection and\nparagraph ranking. Given the complexity of the real-world multi-document MRC\nscenario, it is difficult to jointly optimize both in an end-to-end system. To\naddress this problem, we develop a novel deep cascade learning model, which\nprogressively evolves from the document-level and paragraph-level ranking of\ncandidate texts to more precise answer extraction with machine reading\ncomprehension. Specifically, irrelevant documents and paragraphs are first\nfiltered out with simple functions for efficiency consideration. Then we\njointly train three modules on the remaining texts for better tracking the\nanswer: the document extraction, the paragraph extraction and the answer\nextraction. Experiment results show that the proposed method outperforms the\nprevious state-of-the-art methods on two large-scale multi-document benchmark\ndatasets, i.e., TriviaQA and DuReader. In addition, our online system can\nstably serve typical scenarios with millions of daily requests in less than\n50ms.</p>\n", "tags": ["AAAI","Datasets","Evaluation"] },
{"key": "yan2019cosrec", "citations": "96", "year": "2019", "title":"Cosrec: 2D Convolutional Neural Networks For Sequential Recommendation", "abstract": "<p>Sequential patterns play an important role in building modern recommender\nsystems. To this end, several recommender systems have been built on top of\nMarkov Chains and Recurrent Models (among others). Although these sequential\nmodels have proven successful at a range of tasks, they still struggle to\nuncover complex relationships nested in user purchase histories. In this paper,\nwe argue that modeling pairwise relationships directly leads to an efficient\nrepresentation of sequential features and captures complex item correlations.\nSpecifically, we propose a 2D convolutional network for sequential\nrecommendation (CosRec). It encodes a sequence of items into a three-way\ntensor; learns local features using 2D convolutional filters; and aggregates\nhigh-order interactions in a feedforward manner. Quantitative results on two\npublic datasets show that our method outperforms both conventional methods and\nrecent sequence-based approaches, achieving state-of-the-art performance on\nvarious evaluation metrics.</p>\n", "tags": ["CIKM","Datasets","Evaluation"] },
{"key": "yan2019tener", "citations": "229", "year": "2019", "title":"TENER: Adapting Transformer Encoder For Named Entity Recognition", "abstract": "<p>The Bidirectional long short-term memory networks (BiLSTM) have been widely\nused as an encoder in models solving the named entity recognition (NER) task.\nRecently, the Transformer is broadly adopted in various Natural Language\nProcessing (NLP) tasks owing to its parallelism and advantageous performance.\nNevertheless, the performance of the Transformer in NER is not as good as it is\nin other NLP tasks. In this paper, we propose TENER, a NER architecture\nadopting adapted Transformer Encoder to model the character-level features and\nword-level features. By incorporating the direction and relative distance aware\nattention and the un-scaled attention, we prove the Transformer-like encoder is\njust as effective for NER as other NLP tasks.</p>\n", "tags": ["Model Architecture"] },
{"key": "yan2021consert", "citations": "424", "year": "2021", "title":"Consert: A Contrastive Framework For Self-supervised Sentence Representation Transfer", "abstract": "<p>Learning high-quality sentence representations benefits a wide range of\nnatural language processing tasks. Though BERT-based pre-trained language\nmodels achieve high performance on many downstream tasks, the native derived\nsentence representations are proved to be collapsed and thus produce a poor\nperformance on the semantic textual similarity (STS) tasks. In this paper, we\npresent ConSERT, a Contrastive Framework for Self-Supervised Sentence\nRepresentation Transfer, that adopts contrastive learning to fine-tune BERT in\nan unsupervised and effective way. By making use of unlabeled texts, ConSERT\nsolves the collapse issue of BERT-derived sentence representations and make\nthem more applicable for downstream tasks. Experiments on STS datasets\ndemonstrate that ConSERT achieves an 8% relative improvement over the previous\nstate-of-the-art, even comparable to the supervised SBERT-NLI. And when further\nincorporating NLI supervision, we achieve new state-of-the-art performance on\nSTS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples\navailable, showing its robustness in data scarcity scenarios.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "yan2021videogpt", "citations": "137", "year": "2021", "title":"Videogpt: Video Generation Using VQ-VAE And Transformers", "abstract": "<p>We present VideoGPT: a conceptually simple architecture for scaling\nlikelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE\nthat learns downsampled discrete latent representations of a raw video by\nemploying 3D convolutions and axial self-attention. A simple GPT-like\narchitecture is then used to autoregressively model the discrete latents using\nspatio-temporal position encodings. Despite the simplicity in formulation and\nease of training, our architecture is able to generate samples competitive with\nstate-of-the-art GAN models for video generation on the BAIR Robot dataset, and\ngenerate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset\n(TGIF). We hope our proposed architecture serves as a reproducible reference\nfor a minimalistic implementation of transformer based video generation models.\nSamples and code are available at\nhttps://wilson1yan.github.io/videogpt/index.html</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Training Techniques"] },
{"key": "yan2023practical", "citations": "290", "year": "2023", "title":"Practical And Ethical Challenges Of Large Language Models In Education: A Systematic Scoping Review", "abstract": "<p>Educational technology innovations leveraging large language models (LLMs)\nhave shown the potential to automate the laborious process of generating and\nanalysing textual content. While various innovations have been developed to\nautomate a range of educational tasks (e.g., question generation, feedback\nprovision, and essay grading), there are concerns regarding the practicality\nand ethicality of these innovations. Such concerns may hinder future research\nand the adoption of LLMs-based innovations in authentic educational contexts.\nTo address this, we conducted a systematic scoping review of 118 peer-reviewed\npapers published since 2017 to pinpoint the current state of research on using\nLLMs to automate and support educational tasks. The findings revealed 53 use\ncases for LLMs in automating education tasks, categorised into nine main\ncategories: profiling/labelling, detection, grading, teaching support,\nprediction, knowledge representation, feedback, content generation, and\nrecommendation. Additionally, we also identified several practical and ethical\nchallenges, including low technological readiness, lack of replicability and\ntransparency, and insufficient privacy and beneficence considerations. The\nfindings were summarised into three recommendations for future studies,\nincluding updating existing innovations with state-of-the-art models (e.g.,\nGPT-3/4), embracing the initiative of open-sourcing models/systems, and\nadopting a human-centred approach throughout the developmental process. As the\nintersection of AI and education is continuously evolving, the findings of this\nstudy can serve as an essential reference point for researchers, allowing them\nto leverage the strengths, learn from the limitations, and uncover potential\nresearch opportunities enabled by ChatGPT and other generative AI models.</p>\n", "tags": ["Ethics & Fairness","Privacy"] },
{"key": "yan2024promises", "citations": "72", "year": "2024", "title":"Promises And Challenges Of Generative Artificial Intelligence For Human Learning", "abstract": "<p>Generative artificial intelligence (GenAI) holds the potential to transform\nthe delivery, cultivation, and evaluation of human learning. This Perspective\nexamines the integration of GenAI as a tool for human learning, addressing its\npromises and challenges from a holistic viewpoint that integrates insights from\nlearning sciences, educational technology, and human-computer interaction.\nGenAI promises to enhance learning experiences by scaling personalised support,\ndiversifying learning materials, enabling timely feedback, and innovating\nassessment methods. However, it also presents critical issues such as model\nimperfections, ethical dilemmas, and the disruption of traditional assessments.\nCultivating AI literacy and adaptive skills is imperative for facilitating\ninformed engagement with GenAI technologies. Rigorous research across learning\ncontexts is essential to evaluate GenAI’s impact on human cognition,\nmetacognition, and creativity. Humanity must learn with and about GenAI,\nensuring it becomes a powerful ally in the pursuit of knowledge and innovation,\nrather than a crutch that undermines our intellectual abilities.</p>\n", "tags": ["Evaluation"] },
{"key": "yang2016end", "citations": "81", "year": "2017", "title":"End-to-end Joint Learning Of Natural Language Understanding And Dialogue Manager", "abstract": "<p>Natural language understanding and dialogue policy learning are both\nessential in conversational systems that predict the next system actions in\nresponse to a current user utterance. Conventional approaches aggregate\nseparate models of natural language understanding (NLU) and system action\nprediction (SAP) as a pipeline that is sensitive to noisy outputs of\nerror-prone NLU. To address the issues, we propose an end-to-end deep recurrent\nneural network with limited contextual dialogue memory by jointly training NLU\nand SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our\nproposed model significantly outperforms the state-of-the-art pipeline models\nfor both NLU and SAP, which indicates that our joint model is capable of\nmitigating the affects of noisy NLU outputs, and NLU model can be refined by\nerror flows backpropagating from the extra supervised signals of system\nactions.</p>\n", "tags": ["ICASSP","Training Techniques"] },
{"key": "yang2017improved", "citations": "262", "year": "2017", "title":"Improved Variational Autoencoders For Text Modeling Using Dilated Convolutions", "abstract": "<p>Recent work on generative modeling of text has found that variational\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\nunderstood, but has been attributed to the propensity of LSTM decoders to\nignore conditioning information from the encoder. In this paper, we experiment\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder’s\ndilation architecture, we control the effective context from previously\ngenerated words. In experiments, we find that there is a trade off between the\ncontextual capacity of the decoder and the amount of encoding information used.\nWe show that with the right decoder, VAE can outperform LSTM language models.\nWe demonstrate perplexity gains on two datasets, representing the first\npositive experimental result on the use VAE for generative modeling of text.\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\ndemonstrating gains over several strong baselines.</p>\n", "tags": ["Datasets","Model Architecture","Training Techniques"] },
{"key": "yang2017improving", "citations": "171", "year": "2018", "title":"Improving Neural Machine Translation With Conditional Sequence Generative Adversarial Nets", "abstract": "<p>This paper proposes an approach for applying GANs to NMT. We build a\nconditional sequence generative adversarial net which comprises of two\nadversarial sub models, a generator and a discriminator. The generator aims to\ngenerate sentences which are hard to be discriminated from human-translated\nsentences (i.e., the golden target sentences), And the discriminator makes\nefforts to discriminate the machine-generated sentences from human-translated\nones. The two sub models play a mini-max game and achieve the win-win situation\nwhen they reach a Nash Equilibrium. Additionally, the static sentence-level\nBLEU is utilized as the reinforced objective for the generator, which biases\nthe generation towards high BLEU points. During training, both the dynamic\ndiscriminator and the static BLEU objective are employed to evaluate the\ngenerated sentences and feedback the evaluations to guide the learning of the\ngenerator. Experimental results show that the proposed model consistently\noutperforms the traditional RNNSearch and the newly emerged state-of-the-art\nTransformer on English-German and Chinese-English translation tasks.</p>\n", "tags": ["Model Architecture","NAACL","Training Techniques"] },
{"key": "yang2018auto", "citations": "794", "year": "2019", "title":"Auto-encoding Scene Graphs For Image Captioning", "abstract": "<p>We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language\ninductive bias into the encoder-decoder image captioning framework for more\nhuman-like captions. Intuitively, we humans use the inductive bias to compose\ncollocations and contextual inference in discourse. For example, when we see\nthe relation <code class=\"language-plaintext highlighter-rouge\">person on bike', it is natural to replace </code>on’ with <code class=\"language-plaintext highlighter-rouge\">ride' and\ninfer </code>person riding bike on a road’ even the `road’ is not evident. Therefore,\nexploiting such bias as a language prior is expected to help the conventional\nencoder-decoder models less likely overfit to the dataset bias and focus on\nreasoning. Specifically, we use the scene graph — a directed graph\n(\\(\\mathcal{G}\\)) where an object node is connected by adjective nodes and\nrelationship nodes — to represent the complex structural layout of both image\n(\\(\\mathcal{I}\\)) and sentence (\\(\\mathcal{S}\\)). In the textual domain, we use\nSGAE to learn a dictionary (\\(\\mathcal{D}\\)) that helps to reconstruct sentences\nin the \\(\\mathcal{S}\\rightarrow \\mathcal{G} \\rightarrow \\mathcal{D} \\rightarrow\n\\mathcal{S}\\) pipeline, where \\(\\mathcal{D}\\) encodes the desired language prior;\nin the vision-language domain, we use the shared \\(\\mathcal{D}\\) to guide the\nencoder-decoder in the \\(\\mathcal{I}\\rightarrow \\mathcal{G}\\rightarrow\n\\mathcal{D} \\rightarrow \\mathcal{S}\\) pipeline. Thanks to the scene graph\nrepresentation and shared dictionary, the inductive bias is transferred across\ndomains in principle. We validate the effectiveness of SGAE on the challenging\nMS-COCO image captioning benchmark, e.g., our SGAE-based single-model achieves\na new state-of-the-art \\(127.8\\) CIDEr-D on the Karpathy split, and a competitive\n\\(125.5\\) CIDEr-D (c40) on the official server even compared to other ensemble\nmodels.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Tools"] },
{"key": "yang2018convolutional", "citations": "122", "year": "2019", "title":"Convolutional Self-attention Network", "abstract": "<p>Self-attention network (SAN) has recently attracted increasing interest due\nto its fully parallelized computation and flexibility in modeling dependencies.\nIt can be further enhanced with multi-headed attention mechanism by allowing\nthe model to jointly attend to information from different representation\nsubspaces at different positions (Vaswani et al., 2017). In this work, we\npropose a novel convolutional self-attention network (CSAN), which offers SAN\nthe abilities to 1) capture neighboring dependencies, and 2) model the\ninteraction between multiple attention heads. Experimental results on WMT14\nEnglish-to-German translation task demonstrate that the proposed approach\noutperforms both the strong Transformer baseline and other existing works on\nenhancing the locality of SAN. Comparing with previous work, our model does not\nintroduce any new parameters.</p>\n", "tags": ["Model Architecture"] },
{"key": "yang2018learning", "citations": "159", "year": "2018", "title":"Learning Semantic Textual Similarity From Conversations", "abstract": "<p>We present a novel approach to learn representations for sentence-level\nsemantic similarity using conversational data. Our method trains an\nunsupervised model to predict conversational input-response pairs. The\nresulting sentence embeddings perform well on the semantic textual similarity\n(STS) benchmark and SemEval 2017’s Community Question Answering (CQA) question\nsimilarity subtask. Performance is further improved by introducing multitask\ntraining combining the conversational input-response prediction task and a\nnatural language inference task. Extensive experiments show the proposed model\nachieves the best performance among all neural models on the STS benchmark and\nis competitive with the state-of-the-art feature engineered and mixed systems\nin both tasks.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Training Techniques"] },
{"key": "yang2018modeling", "citations": "216", "year": "2018", "title":"Modeling Localness For Self-attention Networks", "abstract": "<p>Self-attention networks have proven to be of profound value for its strength\nof capturing global dependencies. In this work, we propose to model localness\nfor self-attention networks, which enhances the ability of capturing useful\nlocal context. We cast localness modeling as a learnable Gaussian bias, which\nindicates the central and scope of the local region to be paid more attention.\nThe bias is then incorporated into the original attention distribution to form\na revised distribution. To maintain the strength of capturing long distance\ndependencies and enhance the ability of capturing short-range dependencies, we\nonly apply localness modeling to lower layers of self-attention networks.\nQuantitative and qualitative analyses on Chinese-English and English-German\ntranslation tasks demonstrate the effectiveness and universality of the\nproposed approach.</p>\n", "tags": ["EMNLP","Model Architecture"] },
{"key": "yang2018response", "citations": "114", "year": "2018", "title":"Response Ranking With Deep Matching Networks And External Knowledge In Information-seeking Conversation Systems", "abstract": "<p>Intelligent personal assistant systems with either text-based or voice-based\nconversational interfaces are becoming increasingly popular around the world.\nRetrieval-based conversation models have the advantages of returning fluent and\ninformative responses. Most existing studies in this area are on open domain\n“chit-chat” conversations or task / transaction oriented conversations. More\nresearch is needed for information-seeking conversations. There is also a lack\nof modeling external knowledge beyond the dialog utterances among current\nconversational models. In this paper, we propose a learning framework on the\ntop of deep neural matching networks that leverages external knowledge for\nresponse ranking in information-seeking conversation systems. We incorporate\nexternal knowledge into deep neural models with pseudo-relevance feedback and\nQA correspondence knowledge distillation. Extensive experiments with three\ninformation-seeking conversation data sets including both open benchmarks and\ncommercial data show that, our methods outperform various baseline methods\nincluding several deep text matching models and the state-of-the-art method on\nresponse selection in multi-turn conversations. We also perform analysis over\ndifferent response types, model variations and ranking examples. Our models and\nresearch findings provide new insights on how to utilize external knowledge\nwith deep neural models for response selection and have implications for the\ndesign of the next generation of information-seeking conversation systems.</p>\n", "tags": ["Efficiency","SIGIR","Tools"] },
{"key": "yang2018unsupervised", "citations": "137", "year": "2018", "title":"Unsupervised Neural Machine Translation With Weight Sharing", "abstract": "<p>Unsupervised neural machine translation (NMT) is a recently proposed approach\nfor machine translation which aims to train the model without using any labeled\ndata. The models proposed for unsupervised NMT often use only one shared\nencoder to map the pairs of sentences from different languages to a\nshared-latent space, which is weak in keeping the unique and internal\ncharacteristics of each language, such as the style, terminology, and sentence\nstructure. To address this issue, we introduce an extension by utilizing two\nindependent encoders but sharing some partial weights which are responsible for\nextracting high-level representations of the input sentences. Besides, two\ndifferent generative adversarial networks (GANs), namely the local GAN and\nglobal GAN, are proposed to enhance the cross-language translation. With this\nnew approach, we achieve significant improvements on English-German,\nEnglish-French and Chinese-to-English translation tasks.</p>\n", "tags": ["Security"] },
{"key": "yang2019context", "citations": "92", "year": "2019", "title":"Context-aware Self-attention Networks", "abstract": "<p>Self-attention model have shown its flexibility in parallel computation and\nthe effectiveness on modeling both long- and short-term dependencies. However,\nit calculates the dependencies between representations without considering the\ncontextual information, which have proven useful for modeling dependencies\namong neural representations in various natural language tasks. In this work,\nwe focus on improving self-attention networks through capturing the richness of\ncontext. To maintain the simplicity and flexibility of the self-attention\nnetworks, we propose to contextualize the transformations of the query and key\nlayers, which are used to calculates the relevance between elements.\nSpecifically, we leverage the internal representations that embed both global\nand deep contexts, thus avoid relying on external resources. Experimental\nresults on WMT14 English-German and WMT17 Chinese-English translation tasks\ndemonstrate the effectiveness and universality of the proposed methods.\nFurthermore, we conducted extensive analyses to quantity how the context\nvectors participate in the self-attention model.</p>\n", "tags": ["AAAI","Model Architecture"] },
{"key": "yang2019critically", "citations": "91", "year": "2019", "title":"Critically Examining The \"neural Hype\": Weak Baselines And The Additivity Of Effectiveness Gains From Neural Ranking Models", "abstract": "<p>Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed\nskepticism that neural ranking models were actually improving ad hoc retrieval\neffectiveness in limited data scenarios. He provided anecdotal evidence that\nauthors of neural IR papers demonstrate “wins” by comparing against weak\nbaselines. This paper provides a rigorous evaluation of those claims in two\nways: First, we conducted a meta-analysis of papers that have reported\nexperimental results on the TREC Robust04 test collection. We do not find\nevidence of an upward trend in effectiveness over time. In fact, the best\nreported results are from a decade ago and no recent neural approach comes\nclose. Second, we applied five recent neural models to rerank the strong\nbaselines that Lin used to make his arguments. A significant improvement was\nobserved for one of the models, demonstrating additivity in gains. While there\nappears to be merit to neural IR approaches, at least some of the gains\nreported in the literature appear illusory.</p>\n", "tags": ["Evaluation","Survey Paper"] },
{"key": "yang2019hybrid", "citations": "64", "year": "2019", "title":"A Hybrid Retrieval-generation Neural Conversation Model", "abstract": "<p>Intelligent personal assistant systems that are able to have multi-turn\nconversations with human users are becoming increasingly popular. Most previous\nresearch has been focused on using either retrieval-based or generation-based\nmethods to develop such systems. Retrieval-based methods have the advantage of\nreturning fluent and informative responses with great diversity. However, the\nperformance of the methods is limited by the size of the response repository.\nOn the other hand, generation-based methods can produce highly coherent\nresponses on any topics. But the generated responses are often generic and not\ninformative due to the lack of grounding knowledge. In this paper, we propose a\nhybrid neural conversation model that combines the merits of both response\nretrieval and generation methods. Experimental results on Twitter and\nFoursquare data show that the proposed model outperforms both retrieval-based\nmethods and generation-based methods (including a recently proposed\nknowledge-grounded neural conversation model) under both automatic evaluation\nmetrics and human evaluation. We hope that the findings in this study provide\nnew insights on how to integrate text retrieval and text generation models for\nbuilding conversation systems.</p>\n", "tags": ["CIKM","Evaluation","Has Code","RAG"] },
{"key": "yang2019learning", "citations": "94", "year": "2019", "title":"Learning To Prove Theorems Via Interacting With Proof Assistants", "abstract": "<p>Humans prove theorems by relying on substantial high-level reasoning and\nproblem-specific insights. Proof assistants offer a formalism that resembles\nhuman mathematical reasoning, representing theorems in higher-order logic and\nproofs as high-level tactics. However, human experts have to construct proofs\nmanually by entering tactics into the proof assistant. In this paper, we study\nthe problem of using machine learning to automate the interaction with proof\nassistants. We construct CoqGym, a large-scale dataset and learning environment\ncontaining 71K human-written proofs from 123 projects developed with the Coq\nproof assistant. We develop ASTactic, a deep learning-based model that\ngenerates tactics as programs in the form of abstract syntax trees (ASTs).\nExperiments show that ASTactic trained on CoqGym can generate effective tactics\nand can be used to prove new theorems not previously provable by automated\nmethods. Code is available at https://github.com/princeton-vl/CoqGym.</p>\n", "tags": ["Has Code","ICCV"] },
{"key": "yang2019making", "citations": "64", "year": "2019", "title":"Making History Matter: History-advantage Sequence Training For Visual Dialog", "abstract": "<p>We study the multi-round response generation in visual dialog, where a\nresponse is generated according to a visually grounded conversational history.\nGiven a triplet: an image, Q&amp;A history, and current question, all the\nprevailing methods follow a codec (i.e., encoder-decoder) fashion in a\nsupervised learning paradigm: a multimodal encoder encodes the triplet into a\nfeature vector, which is then fed into the decoder for the current answer\ngeneration, supervised by the ground-truth. However, this conventional\nsupervised learning does NOT take into account the impact of imperfect history,\nviolating the conversational nature of visual dialog and thus making the codec\nmore inclined to learn history bias but not contextual reasoning. To this end,\ninspired by the actor-critic policy gradient in reinforcement learning, we\npropose a novel training paradigm called History Advantage Sequence Training\n(HAST). Specifically, we intentionally impose wrong answers in the history,\nobtaining an adverse critic, and see how the historic error impacts the codec’s\nfuture behavior by History Advantage-a quantity obtained by subtracting the\nadverse critic from the gold reward of ground-truth history. Moreover, to make\nthe codec more sensitive to the history, we propose a novel attention network\ncalled History-Aware Co-Attention Network (HACAN) which can be effectively\ntrained by using HAST. Experimental results on three benchmarks: VisDial\nv0.9&amp;v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently\noutperforms the state-of-the-art supervised counterparts.</p>\n", "tags": ["ICCV","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "yang2019multilingual", "citations": "374", "year": "2020", "title":"Multilingual Universal Sentence Encoder For Semantic Retrieval", "abstract": "<p>We introduce two pre-trained retrieval focused multilingual sentence encoding\nmodels, respectively based on the Transformer and CNN model architectures. The\nmodels embed text from 16 languages into a single semantic space using a\nmulti-task trained dual-encoder that learns tied representations using\ntranslation based bridge tasks (Chidambaram al., 2018). The models provide\nperformance that is competitive with the state-of-the-art on: semantic\nretrieval (SR), translation pair bitext retrieval (BR) and retrieval question\nanswering (ReQA). On English transfer learning tasks, our sentence-level\nembeddings approach, and in some cases exceed, the performance of monolingual,\nEnglish only, sentence embedding models. Our models are made available for\ndownload on TensorFlow Hub.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Retrieval Systems"] },
{"key": "yang2019paws", "citations": "226", "year": "2019", "title":"PAWS-X: A Cross-lingual Adversarial Dataset For Paraphrase Identification", "abstract": "<p>Most existing work on adversarial data generation focuses on English. For\nexample, PAWS (Paraphrase Adversaries from Word Scrambling) consists of\nchallenging English paraphrase identification pairs from Wikipedia and Quora.\nWe remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS\nevaluation pairs in six typologically distinct languages: French, Spanish,\nGerman, Chinese, Japanese, and Korean. We provide baseline numbers for three\nmodels with different capacity to capture non-local context and sentence\nstructure, and using different multilingual training and evaluation regimes.\nMultilingual BERT fine-tuned on PAWS English plus machine-translated data\nperforms the best, with a range of 83.1-90.8 accuracy across the non-English\nlanguages and an average accuracy gain of 23% over the next best model. PAWS-X\nshows the effectiveness of deep, multilingual pre-training while also leaving\nconsiderable headroom as a new challenge to drive multilingual research that\nbetter captures structure and contextual information.</p>\n", "tags": ["Datasets","EMNLP","Evaluation"] },
{"key": "yang2019simple", "citations": "142", "year": "2019", "title":"Simple Applications Of BERT For Ad Hoc Document Retrieval", "abstract": "<p>Following recent successes in applying BERT to question answering, we explore\nsimple applications to ad hoc document retrieval. This required confronting the\nchallenge posed by documents that are typically longer than the length of input\nBERT was designed to handle. We address this issue by applying inference on\nsentences individually, and then aggregating sentence scores to produce\ndocument scores. Experiments on TREC microblog and newswire test collections\nshow that our approach is simple yet effective, as we report the highest\naverage precision on these datasets by neural approaches that we are aware of.</p>\n", "tags": ["Applications","Datasets","Model Architecture","Retrieval Systems"] },
{"key": "yang2019towards", "citations": "106", "year": "2020", "title":"Towards Making The Most Of BERT In Neural Machine Translation", "abstract": "<p>GPT-2 and BERT demonstrate the effectiveness of using pre-trained language\nmodels (LMs) on various natural language processing tasks. However, LM\nfine-tuning often suffers from catastrophic forgetting when applied to\nresource-rich tasks. In this work, we introduce a concerted training framework\n(CTNMT) that is the key to integrate the pre-trained LMs to neural machine\ntranslation (NMT). Our proposed CTNMT consists of three techniques: a)\nasymptotic distillation to ensure that the NMT model can retain the previous\npre-trained knowledge; b) a dynamic switching gate to avoid catastrophic\nforgetting of pre-trained knowledge; and c) a strategy to adjust the learning\npaces according to a scheduled policy. Our experiments in machine translation\nshow CTNMT gains of up to 3 BLEU score on the WMT14 English-German language\npair which even surpasses the previous state-of-the-art pre-training aided NMT\nby 1.4 BLEU score. While for the large WMT14 English-French task with 40\nmillions of sentence-pairs, our base model still significantly improves upon\nthe state-of-the-art Transformer big model by more than 1 BLEU score. The code\nand model can be downloaded from https://github.com/bytedance/neurst/\ntree/master/examples/ctnmt.</p>\n", "tags": ["AAAI","Fine-Tuning","Has Code","Model Architecture","Tools","Training Techniques"] },
{"key": "yang2019xlnet", "citations": "5686", "year": "2019", "title":"Xlnet: Generalized Autoregressive Pretraining For Language Understanding", "abstract": "<p>With the capability of modeling bidirectional contexts, denoising\nautoencoding based pretraining like BERT achieves better performance than\npretraining approaches based on autoregressive language modeling. However,\nrelying on corrupting the input with masks, BERT neglects dependency between\nthe masked positions and suffers from a pretrain-finetune discrepancy. In light\nof these pros and cons, we propose XLNet, a generalized autoregressive\npretraining method that (1) enables learning bidirectional contexts by\nmaximizing the expected likelihood over all permutations of the factorization\norder and (2) overcomes the limitations of BERT thanks to its autoregressive\nformulation. Furthermore, XLNet integrates ideas from Transformer-XL, the\nstate-of-the-art autoregressive model, into pretraining. Empirically, under\ncomparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a\nlarge margin, including question answering, natural language inference,\nsentiment analysis, and document ranking.</p>\n", "tags": ["Model Architecture"] },
{"key": "yang2020generative", "citations": "90", "year": "2020", "title":"Generative Data Augmentation For Commonsense Reasoning", "abstract": "<p>Recent advances in commonsense reasoning depend on large-scale\nhuman-annotated training data to achieve peak performance. However, manual\ncuration of training examples is expensive and has been shown to introduce\nannotation artifacts that neural models can readily exploit and overfit on. We\ninvestigate G-DAUG^C, a novel generative data augmentation method that aims to\nachieve more accurate and robust learning in the low-resource setting. Our\napproach generates synthetic examples using pretrained language models, and\nselects the most informative and diverse set of examples for data augmentation.\nIn experiments with multiple commonsense reasoning benchmarks, G-DAUG^C\nconsistently outperforms existing data augmentation methods based on\nback-translation, and establishes a new state-of-the-art on WinoGrande, CODAH,\nand CommonsenseQA. Further, in addition to improvements in in-distribution\naccuracy, G-DAUG^C-augmented training also enhances out-of-distribution\ngeneralization, showing greater robustness against adversarial or perturbed\nexamples. Our analysis demonstrates that G-DAUG^C produces a diverse set of\nfluent training examples, and that its selection and training approaches are\nimportant for performance. Our findings encourage future research toward\ngenerative data augmentation to enhance both in-distribution learning and\nout-of-distribution generalization.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "yang2020graph", "citations": "85", "year": "2020", "title":"Graph-structured Referring Expression Reasoning In The Wild", "abstract": "<p>Grounding referring expressions aims to locate in an image an object referred\nto by a natural language expression. The linguistic structure of a referring\nexpression provides a layout of reasoning over the visual contents, and it is\noften crucial to align and jointly understand the image and the referring\nexpression. In this paper, we propose a scene graph guided modular network\n(SGMN), which performs reasoning over a semantic graph and a scene graph with\nneural modules under the guidance of the linguistic structure of the\nexpression. In particular, we model the image as a structured semantic graph,\nand parse the expression into a language scene graph. The language scene graph\nnot only decodes the linguistic structure of the expression, but also has a\nconsistent representation with the image semantic graph. In addition to\nexploring structured solutions to grounding referring expressions, we also\npropose Ref-Reasoning, a large-scale real-world dataset for structured\nreferring expression reasoning. We automatically generate referring expressions\nover the scene graphs of images using diverse expression templates and\nfunctional programs. This dataset is equipped with real-world visual contents\nas well as semantically rich expressions with different reasoning layouts.\nExperimental results show that our SGMN not only significantly outperforms\nexisting state-of-the-art algorithms on the new Ref-Reasoning dataset, but also\nsurpasses state-of-the-art structured methods on commonly used benchmark\ndatasets. It can also provide interpretable visual evidences of reasoning. Data\nand code are available at https://github.com/sibeiyang/sgmn</p>\n", "tags": ["CVPR","Datasets","Evaluation","Has Code"] },
{"key": "yang2020just", "citations": "157", "year": "2021", "title":"Just Ask: Learning To Answer Questions From Millions Of Narrated Videos", "abstract": "<p>Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and show excellent\nresults, in particular for rare answers. Furthermore, we demonstrate our method\nto significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,\nActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce\niVQA, a new VideoQA dataset with reduced language biases and high-quality\nredundant manual annotations. Our code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html.</p>\n", "tags": ["Datasets","Evaluation","Has Code","ICCV","Model Architecture","Training Techniques"] },
{"key": "yang2020mtag", "citations": "65", "year": "2021", "title":"MTAG: Modal-temporal Attention Graph For Unaligned Human Multimodal Language Sequences", "abstract": "<p>Human communication is multimodal in nature; it is through multiple\nmodalities such as language, voice, and facial expressions, that opinions and\nemotions are expressed. Data in this domain exhibits complex multi-relational\nand temporal interactions. Learning from this data is a fundamentally\nchallenging research problem. In this paper, we propose Modal-Temporal\nAttention Graph (MTAG). MTAG is an interpretable graph-based neural model that\nprovides a suitable framework for analyzing multimodal sequential data. We\nfirst introduce a procedure to convert unaligned multimodal sequence data into\na graph with heterogeneous nodes and edges that captures the rich interactions\nacross modalities and through time. Then, a novel graph fusion operation,\ncalled MTAG fusion, along with a dynamic pruning and read-out technique, is\ndesigned to efficiently process this modal-temporal graph and capture various\ninteractions. By learning to focus only on the important interactions within\nthe graph, MTAG achieves state-of-the-art performance on multimodal sentiment\nanalysis and emotion recognition benchmarks, while utilizing significantly\nfewer model parameters.</p>\n", "tags": ["Model Architecture","NAACL","Tools"] },
{"key": "yang2020tap", "citations": "82", "year": "2021", "title":"TAP: Text-aware Pre-training For Text-vqa And Text-caption", "abstract": "<p>In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and\nText-Caption tasks. These two tasks aim at reading and understanding scene text\nin images for question answering and image caption generation, respectively. In\ncontrast to the conventional vision-language pre-training that fails to capture\nscene text and its relationship with the visual and text modalities, TAP\nexplicitly incorporates scene text (generated from OCR engines) in\npre-training. With three pre-training tasks, including masked language modeling\n(MLM), image-text (contrastive) matching (ITM), and relative (spatial) position\nprediction (RPP), TAP effectively helps the model learn a better aligned\nrepresentation among the three modalities: text word, visual object, and scene\ntext. Due to this aligned representation learning, even pre-trained on the same\ndownstream task dataset, TAP already boosts the absolute accuracy on the\nTextVQA dataset by +5.4%, compared with a non-TAP baseline. To further improve\nthe performance, we build a large-scale dataset based on the Conceptual Caption\ndataset, named OCR-CC, which contains 1.4 million scene text-related image-text\npairs. Pre-trained on this OCR-CC dataset, our approach outperforms the state\nof the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA,\n+8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps.</p>\n", "tags": ["CVPR","Datasets","Training Techniques"] },
{"key": "yang2020ubar", "citations": "99", "year": "2021", "title":"UBAR: Towards Fully End-to-end Task-oriented Dialog Systems With GPT-2", "abstract": "<p>This paper presents our task-oriented dialog system UBAR which models\ntask-oriented dialogs on a dialog session level. Specifically, UBAR is acquired\nby fine-tuning the large pre-trained unidirectional language model GPT-2 on the\nsequence of the entire dialog session which is composed of user utterance,\nbelief state, database result, system act, and system response of every dialog\nturn. Additionally, UBAR is evaluated in a more realistic setting, where its\ndialog context has access to user utterances and all content it generated such\nas belief states, system acts, and system responses. Experimental results on\nthe MultiWOZ datasets show that UBAR achieves state-of-the-art performances in\nmultiple settings, improving the combined score of response generation, policy\noptimization, and end-to-end modeling by 4.7, 3.5, and 9.4 points respectively.\nThorough analyses demonstrate that the session-level training sequence\nformulation and the generated dialog context are essential for UBAR to operate\nas a fully end-to-end task-oriented dialog system in real life. We also examine\nthe transfer ability of UBAR to new domains with limited data and provide\nvisualization and a case study to illustrate the advantages of UBAR in modeling\non a dialog session level.</p>\n", "tags": ["AAAI","Datasets","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "yang2021causal", "citations": "107", "year": "2021", "title":"Causal Attention For Vision-language Tasks", "abstract": "<p>We present a novel attention mechanism: Causal Attention (CATT), to remove\nthe ever-elusive confounding effect in existing attention-based vision-language\nmodels. This effect causes harmful bias that misleads the attention module to\nfocus on the spurious correlations in training data, damaging the model\ngeneralization. As the confounder is unobserved in general, we use the\nfront-door adjustment to realize the causal intervention, which does not\nrequire any knowledge on the confounder. Specifically, CATT is implemented as a\ncombination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention\n(CS-ATT), where the latter forcibly brings other samples into every IS-ATT,\nmimicking the causal intervention. CATT abides by the Q-K-V convention and\nhence can replace any attention module such as top-down attention and\nself-attention in Transformers. CATT improves various popular attention-based\nvision-language models by considerable margins. In particular, we show that\nCATT has great potential in large-scale pre-training, e.g., it can promote the\nlighter LXMERT~\\cite{tan2019lxmert}, which uses fewer data and less\ncomputational power, comparable to the heavier UNITER~\\cite{chen2020uniter}.\nCode is published in https://github.com/yangxuntu/catt.</p>\n", "tags": ["CVPR","Model Architecture","Training Techniques"] },
{"key": "yang2021empirical", "citations": "169", "year": "2022", "title":"An Empirical Study Of GPT-3 For Few-shot Knowledge-based VQA", "abstract": "<p>Knowledge-based visual question answering (VQA) involves answering questions\nthat require external knowledge not present in the image. Existing methods\nfirst retrieve knowledge from external resources, then reason over the selected\nknowledge, the input image, and question for answer prediction. However, this\ntwo-step approach could lead to mismatches that potentially limit the VQA\nperformance. For example, the retrieved knowledge might be noisy and irrelevant\nto the question, and the re-embedded knowledge features during reasoning might\ndeviate from their original meanings in the knowledge base (KB). To address\nthis challenge, we propose PICa, a simple yet effective method that Prompts\nGPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by\nGPT-3’s power in knowledge retrieval and question answering, instead of using\nstructured KBs as in previous work, we treat GPT-3 as an implicit and\nunstructured KB that can jointly acquire and process relevant knowledge.\nSpecifically, we first convert the image into captions (or tags) that GPT-3 can\nunderstand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just\nproviding a few in-context VQA examples. We further boost performance by\ncarefully investigating: (i) what text formats best describe the image content,\nand (ii) how in-context examples can be better selected and used. PICa unlocks\nthe first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa\nsurpasses the supervised state of the art by an absolute +8.6 points on the\nOK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent\nfew-shot performance.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Few-Shot","Model Architecture"] },
{"key": "yang2021fudge", "citations": "108", "year": "2021", "title":"FUDGE: Controlled Text Generation With Future Discriminators", "abstract": "<p>We propose Future Discriminators for Generation (FUDGE), a flexible and\nmodular method for controlled text generation. Given a pre-existing model G for\ngenerating text from a distribution of interest, FUDGE enables conditioning on\na desired attribute a (for example, formality) while requiring access only to\nG’s output logits. FUDGE learns an attribute predictor operating on a partial\nsequence, and uses this predictor’s outputs to adjust G’s original\nprobabilities. We show that FUDGE models terms corresponding to a Bayesian\ndecomposition of the conditional distribution of G given attribute a. Moreover,\nFUDGE can easily compose predictors for multiple desired attributes. We\nevaluate FUDGE on three tasks – couplet completion in poetry, topic control in\nlanguage generation, and formality change in machine translation – and observe\ngains in all three tasks.</p>\n", "tags": ["NAACL"] },
{"key": "yang2021superb", "citations": "473", "year": "2021", "title":"SUPERB: Speech Processing Universal Performance Benchmark", "abstract": "<p>Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of\nthe shared model, we especially focus on extracting the representation learned\nfrom SSL due to its preferable re-usability. We present a simple framework to\nsolve SUPERB tasks by learning task-specialized lightweight prediction heads on\ntop of the frozen shared model. Our results demonstrate that the framework is\npromising as SSL representations show competitive generalizability and\naccessibility across SUPERB tasks. We release SUPERB as a challenge with a\nleaderboard and a benchmark toolkit to fuel the research in representation\nlearning and general speech processing.</p>\n", "tags": ["Datasets","Evaluation","INTERSPEECH","Model Architecture","Tools","Training Techniques"] },
{"key": "yang2021survey", "citations": "60", "year": "2023", "title":"A Survey Of Knowledge Enhanced Pre-trained Models", "abstract": "<p>Pre-trained language models learn informative word representations on a\nlarge-scale text corpus through self-supervised learning, which has achieved\npromising performance in fields of natural language processing (NLP) after\nfine-tuning. These models, however, suffer from poor robustness and lack of\ninterpretability. We refer to pre-trained language models with knowledge\ninjection as knowledge-enhanced pre-trained language models (KEPLMs). These\nmodels demonstrate deep understanding and logical reasoning and introduce\ninterpretability. In this survey, we provide a comprehensive overview of KEPLMs\nin NLP. We first discuss the advancements in pre-trained language models and\nknowledge representation learning. Then we systematically categorize existing\nKEPLMs from three different perspectives. Finally, we outline some potential\ndirections of KEPLMs for future research.</p>\n", "tags": ["Datasets","Fine-Tuning","Security","Survey Paper","Training Techniques"] },
{"key": "yang2021taco", "citations": "87", "year": "2021", "title":"Taco: Token-aware Cascade Contrastive Learning For Video-text Alignment", "abstract": "<p>Contrastive learning has been widely used to train transformer-based\nvision-language models for video-text alignment and multi-modal representation\nlearning. This paper presents a new algorithm called Token-Aware Cascade\ncontrastive learning (TACo) that improves contrastive learning using two novel\ntechniques. The first is the token-aware contrastive loss which is computed by\ntaking into account the syntactic classes of words. This is motivated by the\nobservation that for a video-text pair, the content words in the text, such as\nnouns and verbs, are more likely to be aligned with the visual contents in the\nvideo than the function words. Second, a cascade sampling method is applied to\ngenerate a small set of hard negative examples for efficient loss estimation\nfor multi-modal fusion layers. To validate the effectiveness of TACo, in our\nexperiments we finetune pretrained models for a set of downstream tasks\nincluding text-video retrieval (YouCook2, MSR-VTT and ActivityNet), video\naction step localization (CrossTask), video action segmentation (COIN). The\nresults show that our models attain consistent improvements across different\nexperimental settings over previous methods, setting new state-of-the-art on\nthree public text-video retrieval benchmarks of YouCook2, MSR-VTT and\nActivityNet.</p>\n", "tags": ["ICCV","Model Architecture"] },
{"key": "yang2022improving", "citations": "89", "year": "2022", "title":"Improving Stability Of Fine-tuning Pretrained Language Models Via Component-wise Gradient Norm Clipping", "abstract": "<p>Fine-tuning over large pretrained language models (PLMs) has established many\nstate-of-the-art results. Despite its superior performance, such fine-tuning\ncan be unstable, resulting in significant variance in performance and potential\nrisks for practical applications. Previous works have attributed such\ninstability to the catastrophic forgetting problem in the top layers of PLMs,\nwhich indicates iteratively that fine-tuning layers in a top-down manner is a\npromising solution. In this paper, we first point out that this method does not\nalways work out due to the different convergence speeds of different\nlayers/modules. Inspired by this observation, we propose a simple\ncomponent-wise gradient norm clipping method to adjust the convergence speed\nfor different components. Experiment results demonstrate that our method\nachieves consistent improvements in terms of generalization performance,\nconvergence speed, and training stability. The codebase can be found at\nhttps://github.com/yangalan123/FineTuningStability.</p>\n", "tags": ["Applications","CVPR","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "yang2022vision", "citations": "191", "year": "2022", "title":"Vision-language Pre-training With Triple Contrastive Learning", "abstract": "<p>Vision-language representation learning largely benefits from image-text\nalignment through contrastive losses (e.g., InfoNCE loss). The success of this\nalignment strategy is attributed to its capability in maximizing the mutual\ninformation (MI) between an image and its matched text. However, simply\nperforming cross-modal alignment (CMA) ignores data potential within each\nmodality, which may result in degraded representations. For instance, although\nCMA-based models are able to map image-text pairs close together in the\nembedding space, they fail to ensure that similar inputs from the same modality\nstay close by. This problem can get even worse when the pre-training data is\nnoisy. In this paper, we propose triple contrastive learning (TCL) for\nvision-language pre-training by leveraging both cross-modal and intra-modal\nself-supervision. Besides CMA, TCL introduces an intra-modal contrastive\nobjective to provide complementary benefits in representation learning. To take\nadvantage of localized and structural information from image and text input,\nTCL further maximizes the average MI between local regions of image/text and\ntheir global summary. To the best of our knowledge, ours is the first work that\ntakes into account local structure information for multi-modality\nrepresentation learning. Experimental evaluations show that our approach is\ncompetitive and achieves the new state of the art on various common down-stream\nvision-language tasks such as image-text retrieval and visual question\nanswering.</p>\n", "tags": ["CVPR","Training Techniques"] },
{"key": "yang2023baichuan", "citations": "94", "year": "2023", "title":"Baichuan 2: Open Large-scale Language Models", "abstract": "<p>Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.</p>\n", "tags": ["Evaluation Frameworks","Training Techniques"] },
{"key": "yang2023dawn", "citations": "128", "year": "2023", "title":"The Dawn Of Lmms: Preliminary Explorations With Gpt-4v(ision)", "abstract": "<p>Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V’s capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V’s unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V’s unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI’s innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf</p>\n", "tags": ["Model Architecture","Prompting"] },
{"key": "yang2023exploring", "citations": "75", "year": "2023", "title":"Exploring An LM To Generate Prolog Predicates From Mathematics Questions", "abstract": "<p>Recently, there has been a surge in interest in NLP driven by ChatGPT.\nChatGPT, a transformer-based generative language model of substantial scale,\nexhibits versatility in performing various tasks based on natural language.\nNevertheless, large language models often exhibit poor performance in solving\nmathematics questions that require reasoning. Prior research has demonstrated\nthe effectiveness of chain-of-thought prompting in enhancing reasoning\ncapabilities. Now, we aim to investigate whether fine-tuning a model for the\ngeneration of Prolog codes, a logic language, and subsequently passing these\ncodes to a compiler can further improve accuracy. Consequently, we employ\nchain-of-thought to fine-tune LLaMA7B as a baseline model and develop other\nfine-tuned LLaMA7B models for the generation of Prolog code, Prolog code +\nchain-of-thought, and chain-of-thought + Prolog code, respectively. The results\nreveal that the Prolog generation model surpasses the baseline in performance,\nwhile the combination generation models do not yield significant improvements.\nThe Prolog corpus based on GSM8K and the correspondingly finetuned Prolog\ngeneration model based on LLaMA7B are released to the research community.</p>\n", "tags": ["Datasets","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "yang2023fingpt", "citations": "121", "year": "2023", "title":"Fingpt: Open-source Financial Large Language Models", "abstract": "<p>Large language models (LLMs) have shown the potential of revolutionizing\nnatural language processing tasks in diverse domains, sparking great interest\nin finance. Accessing high-quality financial data is the first challenge for\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\nadvantage of their unique data accumulation, such privileged access calls for\nan open-source alternative to democratize Internet-scale financial data.\n  In this paper, we present an open-source large language model, FinGPT, for\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\napproach, providing researchers and practitioners with accessible and\ntransparent resources to develop their FinLLMs. We highlight the importance of\nan automatic data curation pipeline and the lightweight low-rank adaptation\ntechnique in building FinGPT. Furthermore, we showcase several potential\napplications as stepping stones for users, such as robo-advising, algorithmic\ntrading, and low-code development. Through collaborative efforts within the\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\nassociated code repos are https://github.com/AI4Finance-Foundation/FinGPT\nand https://github.com/AI4Finance-Foundation/FinNLP</p>\n", "tags": ["Applications","Has Code"] },
{"key": "yang2023give", "citations": "63", "year": "2024", "title":"Give Us The Facts: Enhancing Large Language Models With Knowledge Graphs For Fact-aware Language Modeling", "abstract": "<p>Recently, ChatGPT, a representative large language model (LLM), has gained\nconsiderable attention due to its powerful emergent abilities. Some researchers\nsuggest that LLMs could potentially replace structured knowledge bases like\nknowledge graphs (KGs) and function as parameterized knowledge bases. However,\nwhile LLMs are proficient at learning probabilistic language patterns based on\nlarge corpus and engaging in conversations with humans, they, like previous\nsmaller pre-trained language models (PLMs), still have difficulty in recalling\nfacts while generating knowledge-grounded contents. To overcome these\nlimitations, researchers have proposed enhancing data-driven PLMs with\nknowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus\nimproving their performance to generate texts requiring factual knowledge and\nproviding more informed responses to user queries. This paper reviews the\nstudies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced\npre-trained language models (KGPLMs) as well as their applications. Inspired by\nexisting studies on KGPLM, this paper proposes to enhance LLMs with KGs by\ndeveloping knowledge graph-enhanced large language models (KGLLMs). KGLLM\nprovides a solution to enhance LLMs’ factual reasoning ability, opening up new\navenues for LLM research.</p>\n", "tags": ["Applications","Datasets","Emergent Abilities","Model Architecture"] },
{"key": "yang2023harnessing", "citations": "221", "year": "2024", "title":"Harnessing The Power Of Llms In Practice: A Survey On Chatgpt And Beyond", "abstract": "<p>This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\nhttps://github.com/Mooler0410/LLMsPracticalGuide.</p>\n", "tags": ["Applications","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "yang2023mm", "citations": "60", "year": "2023", "title":"Mm-bigbench: Evaluating Multimodal Models On Multimodal Content Comprehension Tasks", "abstract": "<p>The popularity of multimodal large language models (MLLMs) has triggered a\nrecent surge in research efforts dedicated to evaluating these models.\nNevertheless, existing evaluation studies of MLLMs primarily focus on the\ncomprehension and reasoning of unimodal (vision) content, neglecting\nperformance evaluations in the domain of multimodal (vision-language) content\nunderstanding. Beyond multimodal reasoning, tasks related to multimodal content\ncomprehension necessitate a profound understanding of multimodal contexts,\nachieved through the multimodal interaction to obtain a final answer. In this\npaper, we introduce a comprehensive assessment framework called MM-BigBench,\nwhich incorporates a diverse range of metrics to offer an extensive evaluation\nof the performance of various models and instructions across a wide spectrum of\ndiverse multimodal content comprehension tasks. Consequently, our work\ncomplements research on the performance of MLLMs in multimodal comprehension\ntasks, achieving a more comprehensive and holistic evaluation of MLLMs. To\nbegin, we employ the Best Performance metric to ascertain each model’s\nperformance upper bound on different datasets. Subsequently, the Mean Relative\nGain metric offers an assessment of the overall performance of various models\nand instructions, while the Stability metric measures their sensitivity.\nFurthermore, previous research centers on evaluating models independently or\nsolely assessing instructions, neglecting the adaptability between models and\ninstructions. We propose the Adaptability metric to quantify the adaptability\nbetween models and instructions. Our paper evaluates a total of 20 language\nmodels (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10\ninstructions for each task, and derives novel insights. Our code will be\nreleased at https://github.com/declare-lab/MM-BigBench.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Tools"] },
{"key": "yang2023towards", "citations": "61", "year": "2023", "title":"Towards Difficulty-agnostic Efficient Transfer Learning For Vision-language Models", "abstract": "<p>Vision-language models (VLMs) like CLIP have demonstrated remarkable\napplicability across a variety of downstream tasks, including zero-shot image\nclassification. Recently, the use of prompts or adapters for efficient transfer\nlearning (ETL) has gained significant attention for effectively adapting to\ndownstream tasks. However, previous studies have overlooked the challenge of\nvarying transfer difficulty of downstream tasks. In this paper, we empirically\nanalyze how each ETL method behaves with respect to transfer difficulty. Our\nobservations indicate that utilizing vision prompts and text adapters is\ncrucial for adaptability and generalizability in domains with high difficulty.\nAlso, by applying an adaptive ensemble approach that integrates task-adapted\nVLMs with pre-trained VLMs and strategically leverages more general knowledge\nin low-difficulty and less in high-difficulty domains, we consistently enhance\nperformance across both types of domains. Based on these observations, we\npropose an adaptive ensemble method that combines visual prompts and text\nadapters with pre-trained VLMs, tailored by transfer difficulty, to achieve\noptimal performance for any target domain. Upon experimenting with extensive\nbenchmarks, our method consistently outperforms all baselines, particularly on\nunseen tasks, demonstrating its effectiveness.</p>\n", "tags": ["EMNLP","Fine-Tuning","Model Architecture"] },
{"key": "yang2023vid2seq", "citations": "121", "year": "2023", "title":"Vid2seq: Large-scale Pretraining Of A Visual Language Model For Dense Video Captioning", "abstract": "<p>In this work, we introduce Vid2Seq, a multi-modal single-stage dense event\ncaptioning model pretrained on narrated videos which are readily-available at\nscale. The Vid2Seq architecture augments a language model with special time\ntokens, allowing it to seamlessly predict event boundaries and textual\ndescriptions in the same output sequence. Such a unified model requires\nlarge-scale training data, which is not available in current annotated\ndatasets. We show that it is possible to leverage unlabeled narrated videos for\ndense video captioning, by reformulating sentence boundaries of transcribed\nspeech as pseudo event boundaries, and using the transcribed speech sentences\nas pseudo event captions. The resulting Vid2Seq model pretrained on the\nYT-Temporal-1B dataset improves the state of the art on a variety of dense\nvideo captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.\nVid2Seq also generalizes well to the tasks of video paragraph captioning and\nvideo clip captioning, and to few-shot settings. Our code is publicly available\nat https://antoyang.github.io/vid2seq.html.</p>\n", "tags": ["CVPR","Datasets","Few-Shot","Has Code","Model Architecture","Training Techniques"] },
{"key": "yang2024harnessing", "citations": "221", "year": "2024", "title":"Harnessing The Power Of Large Language Models For Natural Language To First-order Logic Translation", "abstract": "<p>Translating natural language sentences to first-order logic (NL-FOL\ntranslation) is a longstanding challenge in the NLP and formal logic\nliterature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for\nNL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of\ndirectly translating natural language into FOL rules, which outperforms\nGPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,\nand can achieve similar performance as GPT-4 with a fraction of the cost. This\ncorrection ability was achieved by a novel supervised fine-tuning (SFT) +\nreinforcement learning with human feedback (RLHF) framework, which initially\ntrains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought\nreasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier\nas the reward model.\n  To train LogicLLaMA, we present MALLS (large language \\(\\textbf{M}\\)odel\ngener\\(\\textbf{A}\\)ted N\\(\\textbf{L}\\)-FO\\(\\textbf{L}\\) pair\\(\\textbf{S}\\)), a dataset\nof 34K high-quality and diverse sentence-level NL-FOL pairs collected from\nGPT-4. The dataset was created by implementing a pipeline that prompts GPT-4\nfor pairs, and dynamically adjusts the prompts to ensure the collection of\npairs with rich and diverse contexts at different levels of complexity, and\nverifies the validity of the generated FOL rules. Codes, weights, and data are\navailable at \\(\\href{https://github.com/gblackout/LogicLLaMA}{{\\small\n\\text{https://github.com/gblackout/LogicLLaMA}}}\\).</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Model Architecture","Prompting","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "yao2017incorporating", "citations": "152", "year": "2017", "title":"Incorporating Copying Mechanism In Image Captioning For Learning Novel Objects", "abstract": "<p>Image captioning often requires a large set of training image-sentence pairs.\nIn practice, however, acquiring sufficient training pairs is always expensive,\nmaking the recent captioning models limited in their ability to describe\nobjects outside of training corpora (i.e., novel objects). In this paper, we\npresent Long Short-Term Memory with Copying Mechanism (LSTM-C) — a new\narchitecture that incorporates copying into the Convolutional Neural Networks\n(CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for\ndescribing novel objects in captions. Specifically, freely available object\nrecognition datasets are leveraged to develop classifiers for novel objects.\nOur LSTM-C then nicely integrates the standard word-by-word sentence generation\nby a decoder RNN with copying mechanism which may instead select words from\nnovel objects at proper places in the output sentence. Extensive experiments\nare conducted on both MSCOCO image captioning and ImageNet datasets,\ndemonstrating the ability of our proposed LSTM-C architecture to describe novel\nobjects. Furthermore, superior results are reported when compared to\nstate-of-the-art deep models.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "yao2018plan", "citations": "282", "year": "2019", "title":"Plan-and-write: Towards Better Automatic Storytelling", "abstract": "<p>Automatic storytelling is challenging since it requires generating long,\ncoherent natural language to describes a sensible sequence of events. Despite\nconsiderable efforts on automatic story generation in the past, prior work\neither is restricted in plot planning, or can only generate stories in a narrow\ndomain. In this paper, we explore open-domain story generation that writes\nstories given a title (topic) as input. We propose a plan-and-write\nhierarchical generation framework that first plans a storyline, and then\ngenerates a story based on the storyline. We compare two planning strategies.\nThe dynamic schema interweaves story planning and its surface realization in\ntext, while the static schema plans out the entire storyline before generating\nstories. Experiments show that with explicit storyline planning, the generated\nstories are more diverse, coherent, and on topic than those generated without\ncreating a full plan, according to both automatic and human evaluations.</p>\n", "tags": ["AAAI","Tools"] },
{"key": "yao2020keep", "citations": "62", "year": "2020", "title":"Keep CALM And Explore: Language Models For Action Generation In Text-based Games", "abstract": "<p>Text-based games present a unique challenge for autonomous agents to operate\nin natural language and handle enormous action spaces. In this paper, we\npropose the Contextual Action Language Model (CALM) to generate a compact set\nof action candidates at each game state. Our key insight is to train language\nmodels on human gameplay, where people demonstrate linguistic priors and a\ngeneral game sense for promising actions conditioned on game history. We\ncombine CALM with a reinforcement learning agent which re-ranks the generated\naction candidates to maximize in-game rewards. We evaluate our approach using\nthe Jericho benchmark, on games unseen by CALM during training. Our method\nobtains a 69% relative improvement in average game score over the previous\nstate-of-the-art model. Surprisingly, on half of these games, CALM is\ncompetitive with or better than other models that have access to ground truth\nadmissible actions. Code and data are available at\nhttps://github.com/princeton-nlp/calm-textgame.</p>\n", "tags": ["Agentic","Datasets","EMNLP","Evaluation","Has Code","Reinforcement Learning","Training Techniques"] },
{"key": "yao2020self", "citations": "173", "year": "2021", "title":"Self-supervised Learning For Large-scale Item Recommendations", "abstract": "<p>Large scale recommender models find most relevant items from huge catalogs,\nand they play a critical role in modern search and recommendation systems. To\nmodel the input space with large-vocab categorical features, a typical\nrecommender model learns a joint embedding space through neural networks for\nboth queries and items from user feedback data. However, with millions to\nbillions of items in the corpus, users tend to provide feedback for a very\nsmall set of them, causing a power-law distribution. This makes the feedback\ndata for long-tail items extremely sparse.\n  Inspired by the recent success in self-supervised representation learning\nresearch in both computer vision and natural language understanding, we propose\na multi-task self-supervised learning (SSL) framework for large-scale item\nrecommendations. The framework is designed to tackle the label sparsity problem\nby learning better latent relationship of item features. Specifically, SSL\nimproves item representation learning as well as serving as additional\nregularization to improve generalization. Furthermore, we propose a novel data\naugmentation method that utilizes feature correlations within the proposed\nframework.\n  We evaluate our framework using two real-world datasets with 500M and 1B\ntraining examples respectively. Our results demonstrate the effectiveness of\nSSL regularization and show its superior performance over the state-of-the-art\nregularization techniques. We also have already launched the proposed\ntechniques to a web-scale commercial app-to-app recommendation system, with\nsignificant improvements top-tier business metrics demonstrated in A/B\nexperiments on live traffic. Our online results also verify our hypothesis that\nour framework indeed improves model performance even more on slices that lack\nsupervision.</p>\n", "tags": ["CIKM","Datasets","Evaluation","Tools","Training Techniques"] },
{"key": "yao2021cpt", "citations": "86", "year": "2021", "title":"CPT: Colorful Prompt Tuning For Pre-trained Vision-language Models", "abstract": "<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for large amounts of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, CPT\nenables strong few-shot and even zero-shot visual grounding capabilities of\nVL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs\noutperform their fine-tuned counterparts by a large margin (e.g., 17.3%\nabsolute accuracy improvement, and 73.8% relative standard deviation reduction\non average with one shot in RefCOCO evaluation). We make the data and code for\nthis paper publicly available at https://github.com/thunlp/CPT.</p>\n", "tags": ["Evaluation","Few-Shot","Fine-Tuning","Has Code","Prompting","Training Techniques"] },
{"key": "yao2021filip", "citations": "165", "year": "2021", "title":"FILIP: Fine-grained Interactive Language-image Pre-training", "abstract": "<p>Unsupervised large-scale vision-language pre-training has shown promising\nadvances on various downstream tasks. Existing methods often model the\ncross-modal interaction either via the similarity of the global feature of each\nmodality which misses sufficient information, or finer-grained interactions\nusing cross/self-attention upon visual and textual tokens. However,\ncross/self-attention suffers from inferior efficiency in both training and\ninference. In this paper, we introduce a large-scale Fine-grained Interactive\nLanguage-Image Pre-training (FILIP) to achieve finer-level alignment through a\ncross-modal late interaction mechanism, which uses a token-wise maximum\nsimilarity between visual and textual tokens to guide the contrastive\nobjective. FILIP successfully leverages the finer-grained expressiveness\nbetween image patches and textual words by modifying only contrastive loss,\nwhile simultaneously gaining the ability to pre-compute image and text\nrepresentations offline at inference, keeping both large-scale training and\ninference efficient. Furthermore, we construct a new large-scale image-text\npair dataset called FILIP300M for pre-training. Experiments show that FILIP\nachieves state-of-the-art performance on multiple downstream vision-language\ntasks including zero-shot image classification and image-text retrieval. The\nvisualization on word-patch alignment further shows that FILIP can learn\nmeaningful fine-grained features with promising localization ability.</p>\n", "tags": ["Datasets","Efficiency","Model Architecture","Training Techniques"] },
{"key": "yao2023visual", "citations": "78", "year": "2023", "title":"Visual-language Prompt Tuning With Knowledge-guided Context Optimization", "abstract": "<p>Prompt tuning is an effective way to adapt the pre-trained visual-language\nmodel (VLM) to the downstream task using task-related textual tokens.\nRepresentative CoOp-based work combines the learnable textual tokens with the\nclass tokens to obtain specific textual knowledge. However, the specific\ntextual knowledge is the worse generalization to the unseen classes because it\nforgets the essential general textual knowledge having a strong generalization\nability. To tackle this issue, we introduce a novel Knowledge-guided Context\nOptimization (KgCoOp) to enhance the generalization ability of the learnable\nprompt for unseen classes. The key insight of KgCoOp is that forgetting about\nessential knowledge can be alleviated by reducing the discrepancy between the\nlearnable prompt and the hand-crafted prompt. Especially, KgCoOp minimizes the\ndiscrepancy between the textual embeddings generated by learned prompts and the\nhand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can\nmake a discriminative prompt for both seen and unseen tasks. Extensive\nevaluation of several benchmarks demonstrates that the proposed\nKnowledge-guided Context Optimization is an efficient method for prompt tuning,\n<em>i.e.,</em> achieves better performance with less training time.</p>\n", "tags": ["CVPR","Efficiency","Evaluation","Prompting","Training Techniques"] },
{"key": "yasuda2018investigation", "citations": "76", "year": "2019", "title":"Investigation Of Enhanced Tacotron Text-to-speech Synthesis Systems With Self-attention For Pitch Accent Language", "abstract": "<p>End-to-end speech synthesis is a promising approach that directly converts\nraw text to speech. Although it was shown that Tacotron2 outperforms classical\npipeline systems with regards to naturalness in English, its applicability to\nother languages is still unknown. Japanese could be one of the most difficult\nlanguages for which to achieve end-to-end speech synthesis, largely due to its\ncharacter diversity and pitch accents. Therefore, state-of-the-art systems are\nstill based on a traditional pipeline framework that requires a separate text\nanalyzer and duration model. Towards end-to-end Japanese speech synthesis, we\nextend Tacotron to systems with self-attention to capture long-term\ndependencies related to pitch accents and compare their audio quality with\nclassical pipeline systems under various conditions to show their pros and\ncons. In a large-scale listening test, we investigated the impacts of the\npresence of accentual-type labels, the use of force or predicted alignments,\nand acoustic features used as local condition parameters of the Wavenet\nvocoder. Our results reveal that although the proposed systems still do not\nmatch the quality of a top-line pipeline system for Japanese, we show important\nstepping stones towards end-to-end Japanese speech synthesis.</p>\n", "tags": ["ICASSP","Model Architecture","Tools"] },
{"key": "yasunaga2021qa", "citations": "351", "year": "2021", "title":"QA-GNN: Reasoning With Language Models And Knowledge Graphs For Question Answering", "abstract": "<p>The problem of answering questions using knowledge from pre-trained language\nmodels (LMs) and knowledge graphs (KGs) presents two challenges: given a QA\ncontext (question and answer choice), methods need to (i) identify relevant\nknowledge from large KGs, and (ii) perform joint reasoning over the QA context\nand KG. In this work, we propose a new model, QA-GNN, which addresses the above\nchallenges through two key innovations: (i) relevance scoring, where we use LMs\nto estimate the importance of KG nodes relative to the given QA context, and\n(ii) joint reasoning, where we connect the QA context and KG to form a joint\ngraph, and mutually update their representations through graph neural networks.\nWe evaluate our model on QA benchmarks in the commonsense (CommonsenseQA,\nOpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing\nLM and LM+KG models, and exhibits capabilities to perform interpretable and\nstructured reasoning, e.g., correctly handling negation in questions.</p>\n", "tags": ["NAACL"] },
{"key": "yasunaga2022deep", "citations": "64", "year": "2022", "title":"Deep Bidirectional Language-knowledge Graph Pretraining", "abstract": "<p>Pretraining a language model (LM) on text has been shown to help various\ndownstream NLP tasks. Recent works show that a knowledge graph (KG) can\ncomplement text data, offering structured background knowledge that provides a\nuseful scaffold for reasoning. However, these works are not pretrained to learn\na deep fusion of the two modalities at scale, limiting the potential to acquire\nfully joint representations of text and KG. Here we propose DRAGON (Deep\nBidirectional Language-Knowledge Graph Pretraining), a self-supervised approach\nto pretraining a deeply joint language-knowledge foundation model from text and\nKG at scale. Specifically, our model takes pairs of text segments and relevant\nKG subgraphs as input and bidirectionally fuses information from both\nmodalities. We pretrain this model by unifying two self-supervised reasoning\ntasks, masked language modeling and KG link prediction. DRAGON outperforms\nexisting LM and LM+KG models on diverse downstream tasks including question\nanswering across general and biomedical domains, with +5% absolute gain on\naverage. In particular, DRAGON achieves notable performance on complex\nreasoning about language and knowledge (+10% on questions involving long\ncontexts or multi-step reasoning) and low-resource QA (+8% on OBQA and\nRiddleSense), and new state-of-the-art results on various BioNLP tasks. Our\ncode and trained models are available at\nhttps://github.com/michiyasunaga/dragon.</p>\n", "tags": ["Has Code","Training Techniques"] },
{"key": "yasunaga2022linkbert", "citations": "200", "year": "2022", "title":"Linkbert: Pretraining Language Models With Document Links", "abstract": "<p>Language model (LM) pretraining can learn various knowledge from text\ncorpora, helping downstream tasks. However, existing methods such as BERT model\na single document, and do not capture dependencies or knowledge that span\nacross documents. In this work, we propose LinkBERT, an LM pretraining method\nthat leverages links between documents, e.g., hyperlinks. Given a text corpus,\nwe view it as a graph of documents and create LM inputs by placing linked\ndocuments in the same context. We then pretrain the LM with two joint\nself-supervised objectives: masked language modeling and our new proposal,\ndocument relation prediction. We show that LinkBERT outperforms BERT on various\ndownstream tasks across two domains: the general domain (pretrained on\nWikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with\ncitation links). LinkBERT is especially effective for multi-hop reasoning and\nfew-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our\nbiomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on\nBioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,\nas well as code and data at https://github.com/michiyasunaga/LinkBERT.</p>\n", "tags": ["Datasets","Few-Shot","Has Code","Model Architecture","Training Techniques"] },
{"key": "yavuz2019deepcopy", "citations": "84", "year": "2019", "title":"Deepcopy: Grounded Response Generation With Hierarchical Pointer Networks", "abstract": "<p>Recent advances in neural sequence-to-sequence models have led to promising\nresults for several language generation-based tasks, including dialogue\nresponse generation, summarization, and machine translation. However, these\nmodels are known to have several problems, especially in the context of\nchit-chat based dialogue systems: they tend to generate short and dull\nresponses that are often too generic. Furthermore, these models do not ground\nconversational responses on knowledge and facts, resulting in turns that are\nnot accurate, informative and engaging for the users. In this paper, we propose\nand experiment with a series of response generation models that aim to serve in\nthe general scenario where in addition to the dialogue context, relevant\nunstructured external knowledge in the form of text is also assumed to be\navailable for models to harness. Our proposed approach extends\npointer-generator networks (See et al., 2017) by allowing the decoder to\nhierarchically attend and copy from external knowledge in addition to the\ndialogue context. We empirically show the effectiveness of the proposed model\ncompared to several baselines including (Ghazvininejad et al., 2018; Zhang et\nal., 2018) through both automatic evaluation metrics and human evaluation on\nCONVAI2 dataset.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation"] },
{"key": "ye2018interpretable", "citations": "130", "year": "2018", "title":"Interpretable Charge Predictions For Criminal Cases: Learning To Generate Court Views From Fact Descriptions", "abstract": "<p>In this paper, we propose to study the problem of COURT VIEW GENeration from\nthe fact description in a criminal case. The task aims to improve the\ninterpretability of charge prediction systems and help automatic legal document\ngeneration. We formulate this task as a text-to-text natural language\ngeneration (NLG) problem. Sequenceto-sequence model has achieved cutting-edge\nperformances in many NLG tasks. However, due to the non-distinctions of fact\ndescriptions, it is hard for Seq2Seq model to generate charge-discriminative\ncourt views. In this work, we explore charge labels to tackle this issue. We\npropose a label-conditioned Seq2Seq model with attention for this problem, to\ndecode court views conditioned on encoded charge labels. Experimental results\nshow the effectiveness of our method.</p>\n", "tags": ["Model Architecture"] },
{"key": "ye2019bp", "citations": "61", "year": "2019", "title":"Bp-transformer: Modelling Long-range Context Via Binary Partitioning", "abstract": "<p>The Transformer model is widely successful on many natural language\nprocessing tasks. However, the quadratic complexity of self-attention limit its\napplication on long text. In this paper, adopting a fine-to-coarse attention\nmechanism on multi-scale spans via binary partitioning (BP), we propose\nBP-Transformer (BPT for short). BPT yields \\(O(k\\cdot nlog (n/k))\\) connections\nwhere \\(k\\) is a hyperparameter to control the density of attention. BPT has a\ngood balance between computation complexity and model capacity. A series of\nexperiments on text classification, machine translation and language modeling\nshows BPT has a superior performance for long text than previous self-attention\nmodels. Our code, hyperparameters and CUDA kernels for sparse attention are\navailable in PyTorch.</p>\n", "tags": ["Memory & Context","Model Architecture"] },
{"key": "ye2020contrastive", "citations": "110", "year": "2021", "title":"Contrastive Triple Extraction With Generative Transformer", "abstract": "<p>Triple extraction is an essential task in information extraction for natural\nlanguage processing and knowledge graph construction. In this paper, we revisit\nthe end-to-end triple extraction task for sequence generation. Since generative\ntriple extraction may struggle to capture long-term dependencies and generate\nunfaithful triples, we introduce a novel model, contrastive triple extraction\nwith a generative transformer. Specifically, we introduce a single shared\ntransformer module for encoder-decoder-based generation. To generate faithful\nresults, we propose a novel triplet contrastive training object. Moreover, we\nintroduce two mechanisms to further improve model performance (i.e., batch-wise\ndynamic attention-masking and triple-wise calibration). Experimental results on\nthree datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves\nbetter performance than that of baselines.</p>\n", "tags": ["AAAI","Datasets","Model Architecture","Training Techniques"] },
{"key": "ye2020coreferential", "citations": "161", "year": "2020", "title":"Coreferential Reasoning Learning For Language Representation", "abstract": "<p>Language representation models such as BERT could effectively capture\ncontextual semantic information from plain text, and have been proved to\nachieve promising results in lots of downstream NLP tasks with appropriate\nfine-tuning. However, most existing language representation models cannot\nexplicitly handle coreference, which is essential to the coherent understanding\nof the whole discourse. To address this issue, we present CorefBERT, a novel\nlanguage representation model that can capture the coreferential relations in\ncontext. The experimental results show that, compared with existing baseline\nmodels, CorefBERT can achieve significant improvements consistently on various\ndownstream NLP tasks that require coreferential reasoning, while maintaining\ncomparable performance to previous models on other common NLP tasks. The source\ncode and experiment details of this paper can be obtained from\nhttps://github.com/thunlp/CorefBERT.</p>\n", "tags": ["EMNLP","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "ye2020leveraging", "citations": "65", "year": "2020", "title":"Leveraging Code Generation To Improve Code Retrieval And Summarization Via Dual Learning", "abstract": "<p>Code summarization generates brief natural language description given a\nsource code snippet, while code retrieval fetches relevant source code given a\nnatural language query. Since both tasks aim to model the association between\nnatural language and programming language, recent studies have combined these\ntwo tasks to improve their performance. However, researchers have yet been able\nto effectively leverage the intrinsic connection between the two tasks as they\ntrain these tasks in a separate or pipeline manner, which means their\nperformance can not be well balanced. In this paper, we propose a novel\nend-to-end model for the two tasks by introducing an additional code generation\ntask. More specifically, we explicitly exploit the probabilistic correlation\nbetween code summarization and code generation with dual learning, and utilize\nthe two encoders for code summarization and code generation to train the code\nretrieval task via multi-task learning. We have carried out extensive\nexperiments on an existing dataset of SQL and Python, and results show that our\nmodel can significantly improve the results of the code retrieval task over\nthe-state-of-art models, as well as achieve competitive performance in terms of\nBLEU score for the code summarization task.</p>\n", "tags": ["Datasets","Llm For Code"] },
{"key": "ye2021slot", "citations": "62", "year": "2021", "title":"Slot Self-attentive Dialogue State Tracking", "abstract": "<p>An indispensable component in task-oriented dialogue systems is the dialogue\nstate tracker, which keeps track of users’ intentions in the course of\nconversation. The typical approach towards this goal is to fill in multiple\npre-defined slots that are essential to complete the task. Although various\ndialogue state tracking methods have been proposed in recent years, most of\nthem predict the value of each slot separately and fail to consider the\ncorrelations among slots. In this paper, we propose a slot self-attention\nmechanism that can learn the slot correlations automatically. Specifically, a\nslot-token attention is first utilized to obtain slot-specific features from\nthe dialogue context. Then a stacked slot self-attention is applied on these\nfeatures to learn the correlations among slots. We conduct comprehensive\nexperiments on two multi-domain task-oriented dialogue datasets, including\nMultiWOZ 2.0 and MultiWOZ 2.1. The experimental results demonstrate that our\napproach achieves state-of-the-art performance on both datasets, verifying the\nnecessity and effectiveness of taking slot correlations into consideration.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "ye2022zerogen", "citations": "68", "year": "2022", "title":"Zerogen: Efficient Zero-shot Learning Via Dataset Generation", "abstract": "<p>There is a growing interest in dataset generation recently due to the\nsuperior generative capacity of large pre-trained language models (PLMs). In\nthis paper, we study a flexible and efficient zero-short learning method,\n\\textsc{ZeroGen}. Given a zero-shot task, we first generate a dataset from\nscratch using PLMs in an unsupervised manner. Then, we train a tiny task model\n(e.g., LSTM) under the supervision of the synthesized dataset. This approach\nallows highly efficient inference as the final task model only has orders of\nmagnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being\nannotation-free and efficient, we argue that \\textsc{ZeroGen} can also provide\nuseful insights from the perspective of data-free model-agnostic knowledge\ndistillation, and unreferenced text generation evaluation. Experiments and\nanalysis on different NLP tasks, namely, text classification, question\nanswering, and natural language inference, show the effectiveness of\n\\textsc{ZeroGen}.</p>\n", "tags": ["Datasets","EMNLP","Efficiency","Evaluation","Model Architecture"] },
{"key": "ye2023ip", "citations": "77", "year": "2023", "title":"Ip-adapter: Text Compatible Image Prompt Adapter For Text-to-image Diffusion Models", "abstract": "<p>Recent years have witnessed the strong power of large text-to-image diffusion\nmodels for the impressive generative capability to create high-fidelity images.\nHowever, it is very tricky to generate desired images using only text prompt as\nit often involves complex prompt engineering. An alternative to text prompt is\nimage prompt, as the saying goes: “an image is worth a thousand words”.\nAlthough existing methods of direct fine-tuning from pretrained models are\neffective, they require large computing resources and are not compatible with\nother base models, text prompt, and structural controls. In this paper, we\npresent IP-Adapter, an effective and lightweight adapter to achieve image\nprompt capability for the pretrained text-to-image diffusion models. The key\ndesign of our IP-Adapter is decoupled cross-attention mechanism that separates\ncross-attention layers for text features and image features. Despite the\nsimplicity of our method, an IP-Adapter with only 22M parameters can achieve\ncomparable or even better performance to a fully fine-tuned image prompt model.\nAs we freeze the pretrained diffusion model, the proposed IP-Adapter can be\ngeneralized not only to other custom models fine-tuned from the same base\nmodel, but also to controllable generation using existing controllable tools.\nWith the benefit of the decoupled cross-attention strategy, the image prompt\ncan also work well with the text prompt to achieve multimodal image generation.\nThe project page is available at https://ip-adapter.github.io.</p>\n", "tags": ["Fine-Tuning","Has Code","Model Architecture","Prompting","Tools","Training Techniques"] },
{"key": "ye2023mplug", "citations": "130", "year": "2023", "title":"Mplug-owl2: Revolutionizing Multi-modal Large Language Model With Modality Collaboration", "abstract": "<p>Multi-modal Large Language Models (MLLMs) have demonstrated impressive\ninstruction abilities across various open-ended tasks. However, previous\nmethods primarily focus on enhancing multi-modal capabilities. In this work, we\nintroduce a versatile multi-modal large language model, mPLUG-Owl2, which\neffectively leverages modality collaboration to improve performance in both\ntext and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design,\nwith the language decoder acting as a universal interface for managing\ndifferent modalities. Specifically, mPLUG-Owl2 incorporates shared functional\nmodules to facilitate modality collaboration and introduces a modality-adaptive\nmodule that preserves modality-specific features. Extensive experiments reveal\nthat mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal\ntasks and achieving state-of-the-art performances with a single generic model.\nNotably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality\ncollaboration phenomenon in both pure-text and multi-modal scenarios, setting a\npioneering path in the development of future multi-modal foundation models.</p>\n", "tags": [] },
{"key": "yi2018neural", "citations": "225", "year": "2018", "title":"Neural-symbolic VQA: Disentangling Reasoning From Vision And Language Understanding", "abstract": "<p>We marry two powerful ideas: deep representation learning for visual\nrecognition and language understanding, and symbolic program execution for\nreasoning. Our neural-symbolic visual question answering (NS-VQA) system first\nrecovers a structural scene representation from the image and a program trace\nfrom the question. It then executes the program on the scene representation to\nobtain an answer. Incorporating symbolic structure as prior knowledge offers\nthree unique advantages. First, executing programs on a symbolic space is more\nrobust to long program traces; our model can solve complex reasoning tasks\nbetter, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model\nis more data- and memory-efficient: it performs well after learning on a small\nnumber of training data; it can also encode an image into a compact\nrepresentation, requiring less storage than existing methods for offline\nquestion answering. Third, symbolic program execution offers full transparency\nto the reasoning process; we are thus able to interpret and diagnose each\nexecution step.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "yifan2023evaluating", "citations": "136", "year": "2023", "title":"Evaluating Object Hallucination In Large Vision-language Models", "abstract": "<p>Inspired by the superior language abilities of large language models (LLM),\nlarge vision-language models (LVLM) have been recently explored by integrating\npowerful LLMs for improving the performance on complex multimodal tasks.\nDespite the promising progress on LVLMs, we find that LVLMs suffer from the\nhallucination problem, i.e. they tend to generate objects that are inconsistent\nwith the target images in the descriptions. To investigate it, this work\npresents the first systematic study on object hallucination of LVLMs. We\nconduct the evaluation experiments on several representative LVLMs, and show\nthat they mostly suffer from severe object hallucination issue. We further\ndiscuss that the visual instructions may influence the hallucination, and find\nthat: objects that frequently occur in the visual instructions or co-occur with\nthe image objects, are obviously prone to be hallucinated by LVLMs. Besides, we\nfind that existing evaluation methods might be affected by the input\ninstructions and generation styles of LVLMs. Thus, we further design an\nimproved evaluation method for object hallucination by proposing a\npolling-based query method called POPE. Experiment results demonstrate that our\nPOPE can evaluate the object hallucination in a more stable and flexible way.\nOur codes and data are publicly available at https://github.com/RUCAIBox/POPE.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "yin2016attention", "citations": "92", "year": "2016", "title":"Attention-based Convolutional Neural Network For Machine Comprehension", "abstract": "<p>Understanding open-domain text is one of the primary challenges in natural\nlanguage processing (NLP). Machine comprehension benchmarks evaluate the\nsystem’s ability to understand text based on the text content only. In this\nwork, we investigate machine comprehension on MCTest, a question answering (QA)\nbenchmark. Prior work is mainly based on feature engineering approaches. We\ncome up with a neural network framework, named hierarchical attention-based\nconvolutional neural network (HABCNN), to address this task without any\nmanually designed features. Specifically, we explore HABCNN for this task by\ntwo routes, one is through traditional joint modeling of passage, question and\nanswer, one is through textual entailment. HABCNN employs an attention\nmechanism to detect key phrases, key sentences and key snippets that are\nrelevant to answering the question. Experiments show that HABCNN outperforms\nprior deep learning approaches by a big margin.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "yin2016simple", "citations": "134", "year": "2016", "title":"Simple Question Answering By Attentive Convolutional Neural Network", "abstract": "<p>This work focuses on answering single-relation factoid questions over\nFreebase. Each question can acquire the answer from a single fact of form\n(subject, predicate, object) in Freebase. This task, simple question answering\n(SimpleQA), can be addressed via a two-step pipeline: entity linking and fact\nselection. In fact selection, we match the subject entity in a fact candidate\nwith the entity mention in the question by a character-level convolutional\nneural network (char-CNN), and match the predicate in that fact with the\nquestion by a word-level CNN (word-CNN). This work makes two main\ncontributions. (i) A simple and effective entity linker over Freebase is\nproposed. Our entity linker outperforms the state-of-the-art entity linker over\nSimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so\nthat the predicate representation can be matched with the predicate-focused\nquestion representation more effectively. Experiments show that our system sets\nnew state-of-the-art in this task.</p>\n", "tags": [] },
{"key": "yin2017obj2text", "citations": "62", "year": "2017", "title":"OBJ2TEXT: Generating Visually Descriptive Language From Object Layouts", "abstract": "<p>Generating captions for images is a task that has recently received\nconsiderable attention. In this work we focus on caption generation for\nabstract scenes, or object layouts where the only information provided is a set\nof objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence\nmodel that encodes a set of objects and their locations as an input sequence\nusing an LSTM network, and decodes this representation using an LSTM language\nmodel. We show that our model, despite encoding object layouts as a sequence,\ncan represent spatial relationships between objects, and generate descriptions\nthat are globally coherent and semantically relevant. We test our approach in a\ntask of object-layout captioning by using only object annotations as inputs. We\nadditionally show that our model, combined with a state-of-the-art object\ndetector, improves an image captioning model from 0.863 to 0.950 (CIDEr score)\nin the test benchmark of the standard MS-COCO Captioning task.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture"] },
{"key": "yin2017syntactic", "citations": "543", "year": "2017", "title":"A Syntactic Neural Model For General-purpose Code Generation", "abstract": "<p>We consider the problem of parsing natural language descriptions into source\ncode written in a general-purpose programming language like Python. Existing\ndata-driven methods treat this problem as a language generation task without\nconsidering the underlying syntax of the target programming language. Informed\nby previous work in semantic parsing, in this paper we propose a novel neural\narchitecture powered by a grammar model to explicitly capture the target syntax\nas prior knowledge. Experiments find this an effective way to scale up to\ngeneration of complex programs from natural language descriptions, achieving\nstate-of-the-art results that well outperform previous code generation and\nsemantic parsing approaches.</p>\n", "tags": ["Llm For Code","Model Architecture"] },
{"key": "yin2018tranx", "citations": "191", "year": "2018", "title":"TRANX: A Transition-based Neural Abstract Syntax Parser For Semantic Parsing And Code Generation", "abstract": "<p>We present TRANX, a transition-based neural semantic parser that maps natural\nlanguage (NL) utterances into formal meaning representations (MRs). TRANX uses\na transition system based on the abstract syntax description language for the\ntarget MR, which gives it two major advantages: (1) it is highly accurate,\nusing information from the syntax of the target MR to constrain the output\nspace and model the information flow, and (2) it is highly generalizable, and\ncan easily be applied to new types of MR by just writing a new abstract syntax\ndescription corresponding to the allowable structures in the MR. Experiments on\nfour different semantic parsing and code generation tasks show that our system\nis generalizable, extensible, and effective, registering strong results\ncompared to existing neural semantic parsers.</p>\n", "tags": ["EMNLP","Llm For Code"] },
{"key": "yin2019context", "citations": "61", "year": "2019", "title":"Context And Attribute Grounded Dense Captioning", "abstract": "<p>Dense captioning aims at simultaneously localizing semantic regions and\ndescribing these regions-of-interest (ROIs) with short phrases or sentences in\nnatural language. Previous studies have shown remarkable progresses, but they\nare often vulnerable to the aperture problem that a caption generated by the\nfeatures inside one ROI lacks contextual coherence with its surrounding context\nin the input image. In this work, we investigate contextual reasoning based on\nmulti-scale message propagations from the neighboring contents to the target\nROIs. To this end, we design a novel end-to-end context and attribute grounded\ndense captioning framework consisting of 1) a contextual visual mining module\nand 2) a multi-level attribute grounded description generation module. Knowing\nthat captions often co-occur with the linguistic attributes (such as who, what\nand where), we also incorporate an auxiliary supervision from hierarchical\nlinguistic attributes to augment the distinctiveness of the learned captions.\nExtensive experiments and ablation studies on Visual Genome dataset demonstrate\nthe superiority of the proposed model in comparison to state-of-the-art\nmethods.</p>\n", "tags": ["CVPR","Datasets","Tools"] },
{"key": "yin2019semantics", "citations": "191", "year": "2019", "title":"Semantics Disentangling For Text-to-image Generation", "abstract": "<p>Synthesizing photo-realistic images from text descriptions is a challenging\nproblem. Previous studies have shown remarkable progresses on visual quality of\nthe generated images. In this paper, we consider semantics from the input text\ndescriptions in helping render photo-realistic images. However, diverse\nlinguistic expressions pose challenges in extracting consistent semantics even\nthey depict the same thing. To this end, we propose a novel photo-realistic\ntext-to-image generation model that implicitly disentangles semantics to both\nfulfill the high-level semantic consistency and low-level semantic diversity.\nTo be specific, we design (1) a Siamese mechanism in the discriminator to learn\nconsistent high-level semantics, and (2) a visual-semantic embedding strategy\nby semantic-conditioned batch normalization to find diverse low-level\nsemantics. Extensive experiments and ablation studies on CUB and MS-COCO\ndatasets demonstrate the superiority of the proposed method in comparison to\nstate-of-the-art methods.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "yin2020novel", "citations": "129", "year": "2020", "title":"A Novel Graph-based Multi-modal Fusion Encoder For Neural Machine Translation", "abstract": "<p>Multi-modal neural machine translation (NMT) aims to translate source\nsentences into a target language paired with images. However, dominant\nmulti-modal NMT models do not fully exploit fine-grained semantic\ncorrespondences between semantic units of different modalities, which have\npotential to refine multi-modal representation learning. To deal with this\nissue, in this paper, we propose a novel graph-based multi-modal fusion encoder\nfor NMT. Specifically, we first represent the input sentence and image using a\nunified multi-modal graph, which captures various semantic relationships\nbetween multi-modal semantic units (words and visual objects). We then stack\nmultiple graph-based multi-modal fusion layers that iteratively perform\nsemantic interactions to learn node representations. Finally, these\nrepresentations provide an attention-based context vector for the decoder. We\nevaluate our proposed encoder on the Multi30K datasets. Experimental results\nand in-depth analysis show the superiority of our multi-modal NMT model.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "yin2020tabert", "citations": "343", "year": "2020", "title":"Tabert: Pretraining For Joint Understanding Of Textual And Tabular Data", "abstract": "<p>Recent years have witnessed the burgeoning of pretrained language models\n(LMs) for text-based natural language (NL) understanding tasks. Such models are\ntypically trained on free-form NL text, hence may not be suitable for tasks\nlike semantic parsing over structured data, which require reasoning over both\nfree-form NL questions and structured tabular data (e.g., database tables). In\nthis paper we present TaBERT, a pretrained LM that jointly learns\nrepresentations for NL sentences and (semi-)structured tables. TaBERT is\ntrained on a large corpus of 26 million tables and their English contexts. In\nexperiments, neural semantic parsers using TaBERT as feature representation\nlayers achieve new best results on the challenging weakly-supervised semantic\nparsing benchmark WikiTableQuestions, while performing competitively on the\ntext-to-SQL dataset Spider. Implementation of the model will be available at\nhttp://fburl.com/TaBERT .</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "yin2020universal", "citations": "64", "year": "2020", "title":"Universal Natural Language Processing With Limited Annotations: Try Few-shot Textual Entailment As A Start", "abstract": "<p>A standard way to address different NLP problems is by first constructing a\nproblem-specific dataset, then building a model to fit this dataset. To build\nthe ultimate artificial intelligence, we desire a single machine that can\nhandle diverse new problems, for which task-specific annotations are limited.\nWe bring up textual entailment as a unified solver for such NLP problems.\nHowever, current research of textual entailment has not spilled much ink on the\nfollowing questions: (i) How well does a pretrained textual entailment system\ngeneralize across domains with only a handful of domain-specific examples? and\n(ii) When is it worth transforming an NLP task into textual entailment? We\nargue that the transforming is unnecessary if we can obtain rich annotations\nfor this task. Textual entailment really matters particularly when the target\nNLP task has insufficient annotations.\n  Universal NLP can be probably achieved through different routines. In this\nwork, we introduce Universal Few-shot textual Entailment (UFO-Entail). We\ndemonstrate that this framework enables a pretrained entailment model to work\nwell on new entailment domains in a few-shot setting, and show its\neffectiveness as a unified solver for several downstream NLP tasks such as\nquestion answering and coreference resolution when the end-task annotations are\nlimited. Code: https://github.com/salesforce/UniversalFewShotNLP</p>\n", "tags": ["Datasets","EMNLP","Few-Shot","Has Code","Tools"] },
{"key": "yogatama2019learning", "citations": "176", "year": "2019", "title":"Learning And Evaluating General Linguistic Intelligence", "abstract": "<p>We define general linguistic intelligence as the ability to reuse previously\nacquired knowledge about a language’s lexicon, syntax, semantics, and pragmatic\nconventions to adapt to new tasks quickly. Using this definition, we analyze\nstate-of-the-art natural language understanding models and conduct an extensive\nempirical investigation to evaluate them against these criteria through a\nseries of experiments that assess the task-independence of the knowledge being\nacquired by the learning process. In addition to task performance, we propose a\nnew evaluation metric based on an online encoding of the test data that\nquantifies how quickly an existing agent (model) learns a new task. Our results\nshow that while the field has made impressive progress in terms of model\narchitectures that generalize to many tasks, these models still require a lot\nof in-domain training examples (e.g., for fine tuning, training task-specific\nmodules), and are prone to catastrophic forgetting. Moreover, we find that far\nfrom solving general tasks (e.g., document question answering), our models are\noverfitting to the quirks of particular datasets (e.g., SQuAD). We discuss\nmissing components and conjecture on how to make progress toward general\nlinguistic intelligence.</p>\n", "tags": ["Agentic","Datasets","Evaluation","Training Techniques"] },
{"key": "yoo2018data", "citations": "92", "year": "2019", "title":"Data Augmentation For Spoken Language Understanding Via Joint Variational Generation", "abstract": "<p>Data scarcity is one of the main obstacles of domain adaptation in spoken\nlanguage understanding (SLU) due to the high cost of creating manually tagged\nSLU datasets. Recent works in neural text generative models, particularly\nlatent variable models such as variational autoencoder (VAE), have shown\npromising results in regards to generating plausible and natural sentences. In\nthis paper, we propose a novel generative architecture which leverages the\ngenerative power of latent variable models to jointly synthesize fully\nannotated utterances. Our experiments show that existing SLU models trained on\nthe additional synthetic examples achieve performance gains. Our approach not\nonly helps alleviate the data scarcity issue in the SLU task for many datasets\nbut also indiscriminately improves language understanding performances for\nvarious SLU models, supported by extensive experiments and rigorous statistical\ntesting.</p>\n", "tags": ["AAAI","Datasets","Fine-Tuning","Model Architecture"] },
{"key": "yoo2021gpt3mix", "citations": "108", "year": "2021", "title":"Gpt3mix: Leveraging Large-scale Language Models For Text Augmentation", "abstract": "<p>Large-scale language models such as GPT-3 are excellent few-shot learners,\nallowing them to be controlled via natural text prompts. Recent studies report\nthat prompt-based direct classification eliminates the need for fine-tuning but\nlacks data and inference scalability. This paper proposes a novel data\naugmentation technique that leverages large-scale language models to generate\nrealistic text samples from a mixture of real samples. We also propose\nutilizing soft-labels predicted by the language models, effectively distilling\nknowledge from the large-scale language models and creating textual\nperturbations simultaneously. We perform data augmentation experiments on\ndiverse classification tasks and show that our method hugely outperforms\nexisting text augmentation methods. Ablation studies and a qualitative analysis\nprovide more insights into our approach.</p>\n", "tags": ["EMNLP","Few-Shot","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "yoo2021towards", "citations": "73", "year": "2021", "title":"Towards Improving Adversarial Training Of NLP Models", "abstract": "<p>Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models’\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP models, which\nwe name Attacking to Training (A2T). The core part of A2T is a new and cheaper\nword substitution attack optimized for vanilla adversarial training. We use A2T\nto train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI\ndatasets. Our results empirically show that it is possible to train robust NLP\nmodels using a much cheaper adversary. We demonstrate that vanilla adversarial\ntraining with A2T can improve an NLP model’s robustness to the attack it was\noriginally trained with and also defend the model against other types of word\nsubstitution attacks. Furthermore, we show that A2T can improve NLP models’\nstandard accuracy, cross-domain generalization, and interpretability. Code is\navailable at https://github.com/QData/Textattack-A2T .</p>\n", "tags": ["EMNLP","Has Code","Training Techniques"] },
{"key": "yoon2019pre", "citations": "67", "year": "2020", "title":"Pre-trained Language Model For Biomedical Question Answering", "abstract": "<p>The recent success of question answering systems is largely attributed to\npre-trained language models. However, as language models are mostly pre-trained\non general domain corpora such as Wikipedia, they often have difficulty in\nunderstanding biomedical questions. In this paper, we investigate the\nperformance of BioBERT, a pre-trained biomedical language model, in answering\nbiomedical questions including factoid, list, and yes/no type questions.\nBioBERT uses almost the same structure across various question types and\nachieved the best performance in the 7th BioASQ Challenge (Task 7b, Phase B).\nBioBERT pre-trained on SQuAD or SQuAD 2.0 easily outperformed previous\nstate-of-the-art models. BioBERT obtains the best performance when it uses the\nappropriate pre-/post-processing strategies for questions, passages, and\nanswers.</p>\n", "tags": [] },
{"key": "you2019hierarchical", "citations": "98", "year": "2019", "title":"Hierarchical Temporal Convolutional Networks For Dynamic Recommender Systems", "abstract": "<p>Recommender systems that can learn from cross-session data to dynamically\npredict the next item a user will choose are crucial for online platforms.\nHowever, existing approaches often use out-of-the-box sequence models which are\nlimited by speed and memory consumption, are often infeasible for production\nenvironments, and usually do not incorporate cross-session information, which\nis crucial for effective recommendations. Here we propose Hierarchical Temporal\nConvolutional Networks (HierTCN), a hierarchical deep learning architecture\nthat makes dynamic recommendations based on users’ sequential multi-session\ninteractions with items. HierTCN is designed for web-scale systems with\nbillions of items and hundreds of millions of users. It consists of two levels\nof models: The high-level model uses Recurrent Neural Networks (RNN) to\naggregate users’ evolving long-term interests across different sessions, while\nthe low-level model is implemented with Temporal Convolutional Networks (TCN),\nutilizing both the long-term interests and the short-term interactions within\nsessions to predict the next interaction. We conduct extensive experiments on a\npublic XING dataset and a large-scale Pinterest dataset that contains 6 million\nusers with 1.6 billion interactions. We show that HierTCN is 2.5x faster than\nRNN-based models and uses 90% less data memory compared to TCN-based models. We\nfurther develop an effective data caching scheme and a queue-based mini-batch\ngenerator, enabling our model to be trained within 24 hours on a single GPU.\nOur model consistently outperforms state-of-the-art dynamic recommendation\nmethods, with up to 18% improvement in recall and 10% in mean reciprocal rank.</p>\n", "tags": ["Datasets","Model Architecture","Reinforcement Learning"] },
{"key": "you2022aligntransformer", "citations": "72", "year": "2021", "title":"Aligntransformer: Hierarchical Alignment Of Visual Regions And Disease Tags For Medical Report Generation", "abstract": "<p>Recently, medical report generation, which aims to automatically generate a\nlong and coherent descriptive paragraph of a given medical image, has received\ngrowing research interests. Different from the general image captioning tasks,\nmedical report generation is more challenging for data-driven neural models.\nThis is mainly due to 1) the serious data bias: the normal visual regions\ndominate the dataset over the abnormal visual regions, and 2) the very long\nsequence. To alleviate above two problems, we propose an AlignTransformer\nframework, which includes the Align Hierarchical Attention (AHA) and the\nMulti-Grained Transformer (MGT) modules: 1) AHA module first predicts the\ndisease tags from the input image and then learns the multi-grained visual\nfeatures by hierarchically aligning the visual regions and disease tags. The\nacquired disease-grounded visual features can better represent the abnormal\nregions of the input image, which could alleviate data bias problem; 2) MGT\nmodule effectively uses the multi-grained features and Transformer framework to\ngenerate the long medical report. The experiments on the public IU-Xray and\nMIMIC-CXR datasets show that the AlignTransformer can achieve results\ncompetitive with state-of-the-art methods on the two datasets. Moreover, the\nhuman evaluation conducted by professional radiologists further proves the\neffectiveness of our approach.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools"] },
{"key": "yu2016joint", "citations": "288", "year": "2017", "title":"A Joint Speaker-listener-reinforcer Model For Referring Expressions", "abstract": "<p>Referring expressions are natural language constructions used to identify\nparticular objects within a scene. In this paper, we propose a unified\nframework for the tasks of referring expression comprehension and generation.\nOur model is composed of three modules: speaker, listener, and reinforcer. The\nspeaker generates referring expressions, the listener comprehends referring\nexpressions, and the reinforcer introduces a reward function to guide sampling\nof more discriminative expressions. The listener-speaker modules are trained\njointly in an end-to-end learning framework, allowing the modules to be aware\nof one another during learning while also benefiting from the discriminative\nreinforcer’s feedback. We demonstrate that this unified framework and training\nachieves state-of-the-art results for both comprehension and generation on\nthree referring expression datasets. Project and demo page:\nhttps://vision.cs.unc.edu/refer</p>\n", "tags": ["CVPR","Datasets","Reinforcement Learning","Tools","Training Techniques"] },
{"key": "yu2016modeling", "citations": "821", "year": "2016", "title":"Modeling Context In Referring Expressions", "abstract": "<p>Humans refer to objects in their environments all the time, especially in\ndialogue with other people. We explore generating and comprehending natural\nlanguage referring expressions for objects in images. In particular, we focus\non incorporating better measures of visual context into referring expression\nmodels and find that visual comparison to other objects within an image helps\nimprove performance significantly. We also develop methods to tie the language\ngeneration process together, so that we generate expressions for all objects of\na particular category jointly. Evaluation on three recent datasets - RefCOCO,\nRefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring\nexpression generation and comprehension.</p>\n", "tags": ["Datasets","Evaluation","Memory & Context"] },
{"key": "yu2016online", "citations": "84", "year": "2016", "title":"Online Segment To Segment Neural Transduction", "abstract": "<p>We introduce an online neural sequence to sequence model that learns to\nalternate between encoding and decoding segments of the input as it is read. By\nindependently tracking the encoding and decoding representations our algorithm\npermits exact polynomial marginalization of the latent segmentation during\ntraining, and during decoding beam search is employed to find the best\nalignment path together with the predicted output sequence. Our model tackles\nthe bottleneck of vanilla encoder-decoders that have to read and memorize the\nentire input sequence in their fixed-length hidden states before producing any\noutput. It is different from previous attentive models in that, instead of\ntreating the attention weights as output of a deterministic function, our model\nassigns attention weights to a sequential latent variable which can be\nmarginalized out and permits online generation. Experiments on abstractive\nsentence summarization and morphological inflection show significant\nperformance gains over the baseline encoder-decoders.</p>\n", "tags": ["EMNLP","Model Architecture","Training Techniques"] },
{"key": "yu2017beyond", "citations": "516", "year": "2018", "title":"Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling For Visual Question Answering", "abstract": "<p>Visual question answering (VQA) is challenging because it requires a\nsimultaneous understanding of both visual content of images and textual content\nof questions. To support the VQA task, we need to find good solutions for the\nfollowing three issues: 1) fine-grained feature representations for both the\nimage and the question; 2) multi-modal feature fusion that is able to capture\nthe complex interactions between multi-modal features; 3) automatic answer\nprediction that is able to consider the complex correlations between multiple\ndiverse answers for the same question. For fine-grained image and question\nrepresentations, a `co-attention’ mechanism is developed by using a deep neural\nnetwork architecture to jointly learn the attentions for both the image and the\nquestion, which can allow us to reduce the irrelevant features effectively and\nobtain more discriminative features for image and question representations. For\nmulti-modal feature fusion, a generalized Multi-modal Factorized High-order\npooling approach (MFH) is developed to achieve more effective fusion of\nmulti-modal features by exploiting their correlations sufficiently, which can\nfurther result in superior VQA performance as compared with the\nstate-of-the-art approaches. For answer prediction, the KL (Kullback-Leibler)\ndivergence is used as the loss function to achieve precise characterization of\nthe complex correlations between multiple diverse answers with the same or\nsimilar meaning, which can allow us to achieve faster convergence rate and\nobtain slightly better accuracy on answer prediction. A deep neural network\narchitecture is designed to integrate all these aforementioned modules into a\nunified model for achieving superior VQA performance. With an ensemble of our\nMFH models, we achieve the state-of-the-art performance on the large-scale VQA\ndatasets and win the runner-up in VQA Challenge 2017.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "yu2017hierarchically", "citations": "74", "year": "2017", "title":"Hierarchically-attentive RNN For Album Summarization And Storytelling", "abstract": "<p>We address the problem of end-to-end visual storytelling. Given a photo\nalbum, our model first selects the most representative (summary) photos, and\nthen composes a natural language story for the album. For this task, we make\nuse of the Visual Storytelling dataset and a model composed of three\nhierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album\nphotos, select representative (summary) photos, and compose the story.\nAutomatic and human evaluations show our model achieves better performance on\nselection, generation, and retrieval than baselines.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "yu2017modelling", "citations": "106", "year": "2018", "title":"Modelling Domain Relationships For Transfer Learning On Retrieval-based Question Answering Systems In E-commerce", "abstract": "<p>In this paper, we study transfer learning for the PI and NLI problems, aiming\nto propose a general framework, which can effectively and efficiently adapt the\nshared knowledge learned from a resource-rich source domain to a resource- poor\ntarget domain. Specifically, since most existing transfer learning methods only\nfocus on learning a shared feature space across domains while ignoring the\nrelationship between the source and target domains, we propose to\nsimultaneously learn shared representations and domain relationships in a\nunified framework. Furthermore, we propose an efficient and effective hybrid\nmodel by combining a sentence encoding- based method and a sentence\ninteraction-based method as our base model. Extensive experiments on both\nparaphrase identification and natural language inference demonstrate that our\nbase model is efficient and has promising performance compared to the competing\nmodels, and our transfer learning method can help to significantly boost the\nperformance. Further analysis shows that the inter-domain and intra-domain\nrelationship captured by our model are insightful. Last but not least, we\ndeploy our transfer learning model for PI into our online chatbot system, which\ncan bring in significant improvements over our existing system. Finally, we\nlaunch our new system on the chatbot platform Eva in our E-commerce site\nAliExpress.</p>\n", "tags": ["Fine-Tuning","Tools"] },
{"key": "yu2017multi", "citations": "657", "year": "2017", "title":"Multi-modal Factorized Bilinear Pooling With Co-attention Learning For Visual Question Answering", "abstract": "<p>Visual question answering (VQA) is challenging because it requires a\nsimultaneous understanding of both the visual content of images and the textual\ncontent of questions. The approaches used to represent the images and questions\nin a fine-grained manner and questions and to fuse these multi-modal features\nplay key roles in performance. Bilinear pooling based models have been shown to\noutperform traditional linear models for VQA, but their high-dimensional\nrepresentations and high computational complexity may seriously limit their\napplicability in practice. For multi-modal feature fusion, here we develop a\nMulti-modal Factorized Bilinear (MFB) pooling approach to efficiently and\neffectively combine multi-modal features, which results in superior performance\nfor VQA compared with other bilinear pooling approaches. For fine-grained image\nand question representation, we develop a co-attention mechanism using an\nend-to-end deep network architecture to jointly learn both the image and\nquestion attentions. Combining the proposed MFB approach with co-attention\nlearning in a new network architecture provides a unified model for VQA. Our\nexperimental results demonstrate that the single MFB with co-attention model\nachieves new state-of-the-art performance on the real-world VQA dataset. Code\navailable at https://github.com/yuzcccc/mfb.</p>\n", "tags": ["Datasets","Has Code","ICCV","Model Architecture"] },
{"key": "yu2018joint", "citations": "323", "year": "2018", "title":"A Joint Sequence Fusion Model For Video Question Answering And Retrieval", "abstract": "<p>We present an approach named JSFusion (Joint Sequence Fusion) that can\nmeasure semantic similarity between any pairs of multimodal sequence data (e.g.\na video clip and a language sentence). Our multimodal matching network consists\nof two key components. First, the Joint Semantic Tensor composes a dense\npairwise representation of two sequence data into a 3D tensor. Then, the\nConvolutional Hierarchical Decoder computes their similarity score by\ndiscovering hidden hierarchical matches between the two sequence modalities.\nBoth modules leverage hierarchical attention mechanisms that learn to promote\nwell-matched representation patterns while prune out misaligned ones in a\nbottom-up manner. Although the JSFusion is a universal model to be applicable\nto any multimodal sequence data, this work focuses on video-language tasks\nincluding multimodal retrieval and video QA. We evaluate the JSFusion model in\nthree retrieval and VQA tasks in LSMDC, for which our model achieves the best\nperformance reported so far. We also perform multiple-choice and movie\nretrieval tasks for the MSR-VTT dataset, on which our approach outperforms many\nstate-of-the-art methods.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "yu2018mattnet", "citations": "750", "year": "2018", "title":"Mattnet: Modular Attention Network For Referring Expression Comprehension", "abstract": "<p>In this paper, we address referring expression comprehension: localizing an\nimage region described by a natural language expression. While most recent work\ntreats expressions as a single unit, we propose to decompose them into three\nmodular components related to subject appearance, location, and relationship to\nother objects. This allows us to flexibly adapt to expressions containing\ndifferent types of information in an end-to-end framework. In our model, which\nwe call the Modular Attention Network (MAttNet), two types of attention are\nutilized: language-based attention that learns the module weights as well as\nthe word/phrase attention that each module should focus on; and visual\nattention that allows the subject and relationship modules to focus on relevant\nimage components. Module weights combine scores from all three modules\ndynamically to output an overall score. Experiments show that MAttNet\noutperforms previous state-of-art methods by a large margin on both\nbounding-box-level and pixel-level comprehension tasks. Demo and code are\nprovided.</p>\n", "tags": ["CVPR","Model Architecture","Tools"] },
{"key": "yu2018qanet", "citations": "471", "year": "2018", "title":"Qanet: Combining Local Convolution With Global Self-attention For Reading Comprehension", "abstract": "<p>Current end-to-end machine reading and question answering (Q\\&amp;A) models are\nprimarily based on recurrent neural networks (RNNs) with attention. Despite\ntheir success, these models are often slow for both training and inference due\nto the sequential nature of RNNs. We propose a new Q\\&amp;A architecture called\nQANet, which does not require recurrent networks: Its encoder consists\nexclusively of convolution and self-attention, where convolution models local\ninteractions and self-attention models global interactions. On the SQuAD\ndataset, our model is 3x to 13x faster in training and 4x to 9x faster in\ninference, while achieving equivalent accuracy to recurrent models. The\nspeed-up gain allows us to train the model with much more data. We hence\ncombine our model with data generated by backtranslation from a neural machine\ntranslation model. On the SQuAD dataset, our single model, trained with\naugmented data, achieves 84.6 F1 score on the test set, which is significantly\nbetter than the best published F1 score of 81.8.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "yu2018typesql", "citations": "208", "year": "2018", "title":"Typesql: Knowledge-based Type-aware Neural Text-to-sql Generation", "abstract": "<p>Interacting with relational databases through natural language helps users of\nany background easily query and analyze a vast amount of data. This requires a\nsystem that understands users’ questions and converts them to SQL queries\nautomatically. In this paper we present a novel approach, TypeSQL, which views\nthis problem as a slot filling task. Additionally, TypeSQL utilizes type\ninformation to better understand rare entities and numbers in natural language\nquestions. We test this idea on the WikiSQL dataset and outperform the prior\nstate-of-the-art by 5.5% in much less time. We also show that accessing the\ncontent of databases can significantly improve the performance when users’\nqueries are not well-formed. TypeSQL gets 82.6% accuracy, a 17.5% absolute\nimprovement compared to the previous content-sensitive model.</p>\n", "tags": ["Datasets","NAACL"] },
{"key": "yu2019activitynet", "citations": "166", "year": "2019", "title":"Activitynet-qa: A Dataset For Understanding Complex Web Videos Via Question Answering", "abstract": "<p>Recent developments in modeling language and vision have been successfully\napplied to image question answering. It is both crucial and natural to extend\nthis research direction to the video domain for video question answering\n(VideoQA). Compared to the image domain where large scale and fully annotated\nbenchmark datasets exists, VideoQA datasets are limited to small scale and are\nautomatically generated, etc. These limitations restrict their applicability in\npractice. Here we introduce ActivityNet-QA, a fully annotated and large scale\nVideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web\nvideos derived from the popular ActivityNet dataset. We present a statistical\nanalysis of our ActivityNet-QA dataset and conduct extensive experiments on it\nby comparing existing VideoQA baselines. Moreover, we explore various video\nrepresentation strategies to improve VideoQA performance, especially for long\nvideos. The dataset is available at https://github.com/MILVLG/activitynet-qa</p>\n", "tags": ["AAAI","Datasets","Evaluation","Has Code"] },
{"key": "yu2019cosql", "citations": "103", "year": "2019", "title":"Cosql: A Conversational Text-to-sql Challenge Towards Cross-domain Natural Language Interfaces To Databases", "abstract": "<p>We present CoSQL, a corpus for building cross-domain, general-purpose\ndatabase (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+\nannotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k\ndialogues querying 200 complex DBs spanning 138 domains. Each dialogue\nsimulates a real-world DB query scenario with a crowd worker as a user\nexploring the DB and a SQL expert retrieving answers with SQL, clarifying\nambiguous questions, or otherwise informing of unanswerable questions. When\nuser questions are answerable by SQL, the expert describes the SQL and\nexecution results to the user, hence maintaining a natural interaction flow.\nCoSQL introduces new challenges compared to existing task-oriented dialogue\ndatasets:(1) the dialogue states are grounded in SQL, a domain-independent\nexecutable representation, instead of domain-specific slot-value pairs, and (2)\nbecause testing is done on unseen databases, success requires generalizing to\nnew domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking,\nresponse generation from query results, and user dialogue act prediction. We\nevaluate a set of strong baselines for each task and show that CoSQL presents\nsignificant challenges for future research. The dataset, baselines, and\nleaderboard will be released at https://yale-lily.github.io/cosql.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","EMNLP","Evaluation","Has Code"] },
{"key": "yu2019deep", "citations": "803", "year": "2019", "title":"Deep Modular Co-attention Networks For Visual Question Answering", "abstract": "<p>Visual Question Answering (VQA) requires a fine-grained and simultaneous\nunderstanding of both the visual content of images and the textual content of\nquestions. Therefore, designing an effective `co-attention’ model to associate\nkey words in questions with key objects in images is central to VQA\nperformance. So far, most successful attempts at co-attention learning have\nbeen achieved by using shallow models, and deep co-attention models show little\nimprovement over their shallow counterparts. In this paper, we propose a deep\nModular Co-Attention Network (MCAN) that consists of Modular Co-Attention (MCA)\nlayers cascaded in depth. Each MCA layer models the self-attention of questions\nand images, as well as the guided-attention of images jointly using a modular\ncomposition of two basic attention units. We quantitatively and qualitatively\nevaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive ablation\nstudies to explore the reasons behind MCAN’s effectiveness. Experimental\nresults demonstrate that MCAN significantly outperforms the previous\nstate-of-the-art. Our best single model delivers 70.63\\(%\\) overall accuracy on\nthe test-dev set. Code is available at https://github.com/MILVLG/mcan-vqa.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "yu2019multi", "citations": "70", "year": "2019", "title":"Multi-target Embodied Question Answering", "abstract": "<p>Embodied Question Answering (EQA) is a relatively new task where an agent is\nasked to answer questions about its environment from egocentric perception. EQA\nmakes the fundamental assumption that every question, e.g., “what color is the\ncar?”, has exactly one target (“car”) being inquired about. This assumption\nputs a direct limitation on the abilities of the agent. We present a\ngeneralization of EQA - Multi-Target EQA (MT-EQA). Specifically, we study\nquestions that have multiple targets in them, such as “Is the dresser in the\nbedroom bigger than the oven in the kitchen?”, where the agent has to navigate\nto multiple locations (“dresser in bedroom”, “oven in kitchen”) and perform\ncomparative reasoning (“dresser” bigger than “oven”) before it can answer a\nquestion. Such questions require the development of entirely new modules or\ncomponents in the agent. To address this, we propose a modular architecture\ncomposed of a program generator, a controller, a navigator, and a VQA module.\nThe program generator converts the given question into sequential executable\nsub-programs; the navigator guides the agent to multiple locations pertinent to\nthe navigation-related sub-programs; and the controller learns to select\nrelevant observations along its path. These observations are then fed to the\nVQA module to predict the answer. We perform detailed analysis for each of the\nmodel components and show that our joint model can outperform previous methods\nand strong baselines by a significant margin.</p>\n", "tags": ["Agentic","CVPR","Model Architecture"] },
{"key": "yu2019multimodal", "citations": "398", "year": "2019", "title":"Multimodal Unified Attention Networks For Vision-and-language Interactions", "abstract": "<p>Learning an effective attention mechanism for multimodal data is important in\nmany vision-and-language tasks that require a synergic understanding of both\nthe visual and textual contents. Existing state-of-the-art approaches use\nco-attention models to associate each visual object (e.g., image region) with\neach textual object (e.g., query word). Despite the success of these\nco-attention models, they only model inter-modal interactions while neglecting\nintra-modal interactions. Here we propose a general `unified attention’ model\nthat simultaneously captures the intra- and inter-modal interactions of\nmultimodal features and outputs their corresponding attended representations.\nBy stacking such unified attention blocks in depth, we obtain the deep\nMultimodal Unified Attention Network (MUAN), which can seamlessly be applied to\nthe visual question answering (VQA) and visual grounding tasks. We evaluate our\nMUAN models on two VQA datasets and three visual grounding datasets, and the\nresults show that MUAN achieves top-level performance on both tasks without\nbells and whistles.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "yu2019sparc", "citations": "111", "year": "2019", "title":"Sparc: Cross-domain Semantic Parsing In Context", "abstract": "<p>We present SParC, a dataset for cross-domainSemanticParsing inContext that\nconsists of 4,298 coherent question sequences (12k+ individual questions\nannotated with SQL queries). It is obtained from controlled user interactions\nwith 200 complex databases over 138 domains. We provide an in-depth analysis of\nSParC and show that it introduces new challenges compared to existing datasets.\nSParC demonstrates complex contextual dependencies, (2) has greater semantic\ndiversity, and (3) requires generalization to unseen domains due to its\ncross-domain nature and the unseen databases at test time. We experiment with\ntwo state-of-the-art text-to-SQL models adapted to the context-dependent,\ncross-domain setup. The best model obtains an exact match accuracy of 20.2%\nover all questions and less than10% over all interaction sequences, indicating\nthat the cross-domain setting and the con-textual phenomena of the dataset\npresent significant challenges for future research. The dataset, baselines, and\nleaderboard are released at https://yale-lily.github.io/sparc.</p>\n", "tags": ["Evaluation","Has Code"] },
{"key": "yu2020cross", "citations": "98", "year": "2020", "title":"Cross-modal Knowledge Reasoning For Knowledge-based Visual Question Answering", "abstract": "<p>Knowledge-based Visual Question Answering (KVQA) requires external knowledge\nbeyond the visible content to answer questions about an image. This ability is\nchallenging but indispensable to achieve general VQA. One limitation of\nexisting KVQA solutions is that they jointly embed all kinds of information\nwithout fine-grained selection, which introduces unexpected noises for\nreasoning the correct answer. How to capture the question-oriented and\ninformation-complementary evidence remains a key challenge to solve the\nproblem. Inspired by the human cognition theory, in this paper, we depict an\nimage by multiple knowledge graphs from the visual, semantic and factual views.\nThereinto, the visual graph and semantic graph are regarded as\nimage-conditioned instantiation of the factual graph. On top of these new\nrepresentations, we re-formulate Knowledge-based Visual Question Answering as a\nrecurrent reasoning process for obtaining complementary evidence from\nmultimodal information. To this end, we decompose the model into a series of\nmemory-based reasoning steps, each performed by a G raph-based R ead, U pdate,\nand C ontrol ( GRUC ) module that conducts parallel reasoning over both visual\nand semantic information. By stacking the modules multiple times, our model\nperforms transitive reasoning and obtains question-oriented concept\nrepresentations under the constrain of different modalities. Finally, we\nperform graph neural networks to infer the global-optimal answer by jointly\nconsidering all the concepts. We achieve a new state-of-the-art performance on\nthree popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and\ndemonstrate the effectiveness and interpretability of our model with extensive\nexperiments.</p>\n", "tags": ["CVPR","Datasets","Evaluation"] },
{"key": "yu2020few", "citations": "116", "year": "2020", "title":"Few-shot Generative Conversational Query Rewriting", "abstract": "<p>Conversational query rewriting aims to reformulate a concise conversational\nquery to a fully specified, context-independent query that can be effectively\nhandled by existing information retrieval systems. This paper presents a\nfew-shot generative approach to conversational query rewriting. We develop two\nmethods, based on rules and self-supervised learning, to generate weak\nsupervision data using large amounts of ad hoc search sessions, and to\nfine-tune GPT-2 to rewrite conversational queries. On the TREC Conversational\nAssistance Track, our weakly supervised GPT-2 rewriter improves the\nstate-of-the-art ranking accuracy by 12%, only using very limited amounts of\nmanual query rewrites. In the zero-shot learning setting, the rewriter still\ngives a comparable result to previous state-of-the-art systems. Our analyses\nreveal that GPT-2 effectively picks up the task syntax and learns to capture\ncontext dependencies, even for hard cases that involve group references and\nlong-turn dependencies.</p>\n", "tags": ["Few-Shot","Model Architecture","Retrieval Systems","SIGIR","Training Techniques"] },
{"key": "yu2020fine", "citations": "85", "year": "2021", "title":"Fine-tuning Pre-trained Language Model With Weak Supervision: A Contrastive-regularized Self-training Approach", "abstract": "<p>Fine-tuned pre-trained language models (LMs) have achieved enormous success\nin many natural language processing (NLP) tasks, but they still require\nexcessive labeled data in the fine-tuning stage. We study the problem of\nfine-tuning pre-trained LMs using only weak supervision, without any labeled\ndata. This problem is challenging because the high capacity of LMs makes them\nprone to overfitting the noisy labels generated by weak supervision. To address\nthis problem, we develop a contrastive self-training framework, COSINE, to\nenable fine-tuning LMs with weak supervision. Underpinned by contrastive\nregularization and confidence-based reweighting, this contrastive self-training\nframework can gradually improve model fitting while effectively suppressing\nerror propagation. Experiments on sequence, token, and sentence pair\nclassification tasks show that our model outperforms the strongest baseline by\nlarge margins on 7 benchmarks in 6 tasks, and achieves competitive performance\nwith fully-supervised fine-tuning methods.</p>\n", "tags": ["Fine-Tuning","NAACL","Training Techniques"] },
{"key": "yu2020grappa", "citations": "90", "year": "2020", "title":"Grappa: Grammar-augmented Pre-training For Table Semantic Parsing", "abstract": "<p>We present GraPPa, an effective pre-training approach for table semantic\nparsing that learns a compositional inductive bias in the joint representations\nof textual and tabular data. We construct synthetic question-SQL pairs over\nhigh-quality tables via a synchronous context-free grammar (SCFG) induced from\nexisting text-to-SQL datasets. We pre-train our model on the synthetic data\nusing a novel text-schema linking objective that predicts the syntactic role of\na table field in the SQL for each question-SQL pair. To maintain the model’s\nability to represent real-world data, we also include masked language modeling\n(MLM) over several existing table-and-language datasets to regularize the\npre-training process. On four popular fully supervised and weakly supervised\ntable semantic parsing benchmarks, GraPPa significantly outperforms\nRoBERTa-large as the feature representation layers and establishes new\nstate-of-the-art results on all of them.</p>\n", "tags": ["Datasets","Ethics & Fairness","Training Techniques"] },
{"key": "yu2020reclor", "citations": "128", "year": "2020", "title":"Reclor: A Reading Comprehension Dataset Requiring Logical Reasoning", "abstract": "<p>Recent powerful pre-trained language models have achieved remarkable\nperformance on most of the popular datasets for reading comprehension. It is\ntime to introduce more challenging datasets to push the development of this\nfield towards more comprehensive reasoning of text. In this paper, we introduce\na new Reading Comprehension dataset requiring logical reasoning (ReClor)\nextracted from standardized graduate admission examinations. As earlier studies\nsuggest, human-annotated datasets usually contain biases, which are often\nexploited by models to achieve high accuracy without truly understanding the\ntext. In order to comprehensively evaluate the logical reasoning ability of\nmodels on ReClor, we propose to identify biased data points and separate them\ninto EASY set while the rest as HARD set. Empirical results show that\nstate-of-the-art models have an outstanding ability to capture biases contained\nin the dataset with high accuracy on EASY set. However, they struggle on HARD\nset with poor performance near that of random guess, indicating more research\nis needed to essentially enhance the logical reasoning ability of current\nmodels.</p>\n", "tags": ["Datasets"] },
{"key": "yu2020survey", "citations": "199", "year": "2022", "title":"A Survey Of Knowledge-enhanced Text Generation", "abstract": "<p>The goal of text generation is to make machines express in human language. It\nis one of the most important yet challenging tasks in natural language\nprocessing (NLP). Since 2014, various neural encoder-decoder models pioneered\nby Seq2Seq have been proposed to achieve the goal by learning to map input text\nto output text. However, the input text alone often provides limited knowledge\nto generate the desired output, so the performance of text generation is still\nfar from satisfaction in many real-world scenarios. To address this issue,\nresearchers have considered incorporating various forms of knowledge beyond the\ninput text into the generation models. This research direction is known as\nknowledge-enhanced text generation. In this survey, we present a comprehensive\nreview of the research on knowledge enhanced text generation over the past five\nyears. The main content includes two parts: (i) general methods and\narchitectures for integrating knowledge into text generation; (ii) specific\ntechniques and applications according to different forms of knowledge data.\nThis survey can have broad audiences, researchers and practitioners, in\nacademia and industry.</p>\n", "tags": ["Applications","Survey Paper"] },
{"key": "yu2021adaptsum", "citations": "64", "year": "2021", "title":"Adaptsum: Towards Low-resource Domain Adaptation For Abstractive Summarization", "abstract": "<p>State-of-the-art abstractive summarization models generally rely on extensive\nlabeled data, which lowers their generalization ability on domains where such\ndata are not available. In this paper, we present a study of domain adaptation\nfor the abstractive summarization task across six diverse target domains in a\nlow-resource setting. Specifically, we investigate the second phase of\npre-training on large-scale generative models under three different settings:\n1) source domain pre-training; 2) domain-adaptive pre-training; and 3)\ntask-adaptive pre-training. Experiments show that the effectiveness of\npre-training is correlated with the similarity between the pre-training data\nand the target domain task. Moreover, we find that continuing pre-training\ncould lead to the pre-trained model’s catastrophic forgetting, and a learning\nmethod with less forgetting can alleviate this issue. Furthermore, results\nillustrate that a huge gap still exists between the low-resource and\nhigh-resource settings, which highlights the need for more advanced domain\nadaptation methods for the abstractive summarization task.</p>\n", "tags": ["Fine-Tuning","NAACL","Training Techniques"] },
{"key": "yu2021vector", "citations": "87", "year": "2021", "title":"Vector-quantized Image Modeling With Improved VQGAN", "abstract": "<p>Pretraining language models with next-token prediction on massive text\ncorpora has delivered phenomenal zero-shot, few-shot, transfer learning and\nmulti-tasking capabilities on both generative and discriminative language\ntasks. Motivated by this success, we explore a Vector-quantized Image Modeling\n(VIM) approach that involves pretraining a Transformer to predict rasterized\nimage tokens autoregressively. The discrete image tokens are encoded from a\nlearned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple\nimprovements over vanilla VQGAN from architecture to codebook learning,\nyielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN\nfurther improves vector-quantized image modeling tasks, including\nunconditional, class-conditioned image generation and unsupervised\nrepresentation learning. When trained on ImageNet at (256\\times256)\nresolution, we achieve Inception Score (IS) of 175.1 and Fr’echet Inception\nDistance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which\nobtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and\nunsupervised pretraining, we further evaluate the pretrained Transformer by\naveraging intermediate features, similar to Image GPT (iGPT). This\nImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy\nfrom 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL\nwhich is trained with extra web image data and larger model size.</p>\n", "tags": ["Efficiency","Few-Shot","Fine-Tuning","Model Architecture"] },
{"key": "yu2022coca", "citations": "466", "year": "2022", "title":"Coca: Contrastive Captioners Are Image-text Foundation Models", "abstract": "<p>Exploring large-scale pretrained foundation models is of significant interest\nin computer vision because these models can be quickly transferred to many\ndownstream tasks. This paper presents Contrastive Captioner (CoCa), a\nminimalist design to pretrain an image-text encoder-decoder foundation model\njointly with contrastive loss and captioning loss, thereby subsuming model\ncapabilities from contrastive approaches like CLIP and generative methods like\nSimVLM. In contrast to standard encoder-decoder transformers where all decoder\nlayers attend to encoder outputs, CoCa omits cross-attention in the first half\nof decoder layers to encode unimodal text representations, and cascades the\nremaining decoder layers which cross-attend to the image encoder for multimodal\nimage-text representations. We apply a contrastive loss between unimodal image\nand text embeddings, in addition to a captioning loss on the multimodal decoder\noutputs which predicts text tokens autoregressively. By sharing the same\ncomputational graph, the two training objectives are computed efficiently with\nminimal overhead. CoCa is pretrained end-to-end and from scratch on both\nweb-scale alt-text data and annotated images by treating all labels simply as\ntext, seamlessly unifying natural language supervision for representation\nlearning. Empirically, CoCa achieves state-of-the-art performance with\nzero-shot transfer or minimal task-specific adaptation on a broad range of\ndownstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700,\nMoments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal\nunderstanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps).\nNotably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1\naccuracy, 90.6% with a frozen encoder and learned classification head, and new\nstate-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "yu2022generate", "citations": "62", "year": "2022", "title":"Generate Rather Than Retrieve: Large Language Models Are Strong Context Generators", "abstract": "<p>Knowledge-intensive tasks, such as open-domain question answering (QA),\nrequire access to a large amount of world or domain knowledge. A common\napproach for knowledge-intensive tasks is to employ a retrieve-then-read\npipeline that first retrieves a handful of relevant contextual documents from\nan external corpus such as Wikipedia and then predicts an answer conditioned on\nthe retrieved documents. In this paper, we present a novel perspective for\nsolving knowledge-intensive tasks by replacing document retrievers with large\nlanguage model generators. We call our method generate-then-read (GenRead),\nwhich first prompts a large language model to generate contextutal documents\nbased on a given question, and then reads the generated documents to produce\nthe final answer. Furthermore, we propose a novel clustering-based prompting\nmethod that selects distinct prompts, resulting in the generated documents that\ncover different perspectives, leading to better recall over acceptable answers.\nWe conduct extensive experiments on three different knowledge-intensive tasks,\nincluding open-domain QA, fact checking, and dialogue system. Notably, GenRead\nachieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly\noutperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0\nand +3.9, without retrieving any documents from any external knowledge source.\nLastly, we demonstrate the model performance can be further improved by\ncombining retrieval and generation. Our code and generated documents can be\nfound at https://github.com/wyu97/GenRead.</p>\n", "tags": ["Dialogue & Multi Turn","Has Code","Memory & Context","Prompting"] },
{"key": "yu2022scaling", "citations": "280", "year": "2022", "title":"Scaling Autoregressive Models For Content-rich Text-to-image Generation", "abstract": "<p>We present the Pathways Autoregressive Text-to-Image (Parti) model, which\ngenerates high-fidelity photorealistic images and supports content-rich\nsynthesis involving complex compositions and world knowledge. Parti treats\ntext-to-image generation as a sequence-to-sequence modeling problem, akin to\nmachine translation, with sequences of image tokens as the target outputs\nrather than text tokens in another language. This strategy can naturally tap\ninto the rich body of prior work on large language models, which have seen\ncontinued advances in capabilities and performance through scaling data and\nmodel sizes. Our approach is simple: First, Parti uses a Transformer-based\nimage tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens.\nSecond, we achieve consistent quality improvements by scaling the\nencoder-decoder Transformer model up to 20B parameters, with a new\nstate-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on\nMS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts\n(P2), a new holistic benchmark of over 1600 English prompts, demonstrate the\neffectiveness of Parti across a wide variety of categories and difficulty\naspects. We also explore and highlight limitations of our models in order to\ndefine and exemplify key areas of focus for further improvements. See\nhttps://parti.research.google/ for high-resolution images.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "yuan2017machine", "citations": "166", "year": "2017", "title":"Machine Comprehension By Text-to-text Neural Question Generation", "abstract": "<p>We propose a recurrent neural model that generates natural-language questions\nfrom documents, conditioned on answers. We show how to train the model using a\ncombination of supervised and reinforcement learning. After teacher forcing for\nstandard maximum likelihood training, we fine-tune the model using policy\ngradient techniques to maximize several rewards that measure question quality.\nMost notably, one of these rewards is the performance of a question-answering\nsystem. We motivate question generation as a means to improve the performance\nof question answering systems. Our model is trained and evaluated on the recent\nquestion-answering dataset SQuAD.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning","Training Techniques"] },
{"key": "yuan2018one", "citations": "77", "year": "2020", "title":"One Size Does Not Fit All: Generating And Evaluating Variable Number Of Keyphrases", "abstract": "<p>Different texts shall by nature correspond to different number of keyphrases.\nThis desideratum is largely missing from existing neural keyphrase generation\nmodels. In this study, we address this problem from both modeling and\nevaluation perspectives.\n  We first propose a recurrent generative model that generates multiple\nkeyphrases as delimiter-separated sequences. Generation diversity is further\nenhanced with two novel techniques by manipulating decoder hidden states. In\ncontrast to previous approaches, our model is capable of generating diverse\nkeyphrases and controlling number of outputs.\n  We further propose two evaluation metrics tailored towards the\nvariable-number generation. We also introduce a new dataset StackEx that\nexpands beyond the only existing genre (i.e., academic writing) in keyphrase\ngeneration tasks. With both previous and new evaluation metrics, our model\noutperforms strong baselines on all datasets.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "yuan2018simple", "citations": "498", "year": "2019", "title":"A Simple Convolutional Generative Network For Next Item Recommendation", "abstract": "<p>Convolutional Neural Networks (CNNs) have been recently introduced in the\ndomain of session-based next item recommendation. An ordered collection of past\nitems the user has interacted with in a session (or sequence) are embedded into\na 2-dimensional latent matrix, and treated as an image. The convolution and\npooling operations are then applied to the mapped item embeddings. In this\npaper, we first examine the typical session-based CNN recommender and show that\nboth the generative model and network architecture are suboptimal when modeling\nlong-range dependencies in the item sequence. To address the issues, we\nintroduce a simple, but very effective generative model that is capable of\nlearning high-level representation from both short- and long-range item\ndependencies. The network architecture of the proposed model is formed of a\nstack of <em>holed</em> convolutional layers, which can efficiently increase the\nreceptive fields without relying on the pooling operation. Another contribution\nis the effective use of residual block structure in recommender systems, which\ncan ease the optimization for much deeper networks. The proposed generative\nmodel attains state-of-the-art accuracy with less training time in the next\nitem recommendation task. It accordingly can be used as a powerful\nrecommendation baseline to beat in future, especially when there are long\nsequences of user feedback.</p>\n", "tags": ["Efficiency","Model Architecture","Training Techniques"] },
{"key": "yuan2020parameter", "citations": "137", "year": "2020", "title":"Parameter-efficient Transfer From Sequential Behaviors For User Modeling And Recommendation", "abstract": "<p>Inductive transfer learning has had a big impact on computer vision and NLP\ndomains but has not been used in the area of recommender systems. Even though\nthere has been a large body of research on generating recommendations based on\nmodeling user-item interaction sequences, few of them attempt to represent and\ntransfer these models for serving downstream tasks where only limited data\nexists.\n  In this paper, we delve on the task of effectively learning a single user\nrepresentation that can be applied to a diversity of tasks, from cross-domain\nrecommendations to user profile predictions. Fine-tuning a large pre-trained\nnetwork and adapting it to downstream tasks is an effective way to solve such\ntasks. However, fine-tuning is parameter inefficient considering that an entire\nmodel needs to be re-trained for every new task. To overcome this issue, we\ndevelop a parameter efficient transfer learning architecture, termed as\nPeterRec, which can be configured on-the-fly to various downstream tasks.\nSpecifically, PeterRec allows the pre-trained parameters to remain unaltered\nduring fine-tuning by injecting a series of re-learned neural networks, which\nare small but as expressive as learning the entire network. We perform\nextensive experimental ablation to show the effectiveness of the learned user\nrepresentation in five downstream tasks. Moreover, we show that PeterRec\nperforms efficient transfer learning in multiple domains, where it achieves\ncomparable or sometimes better performance relative to fine-tuning the entire\nmodel parameters. Codes and datasets are available at\nhttps://github.com/fajieyuan/sigir2020_peterrec.</p>\n", "tags": ["Datasets","Fine-Tuning","Has Code","Model Architecture","SIGIR","Training Techniques"] },
{"key": "yuan2020reinforced", "citations": "91", "year": "2021", "title":"Reinforced Multi-teacher Selection For Knowledge Distillation", "abstract": "<p>In natural language processing (NLP) tasks, slow inference speed and huge\nfootprints in GPU usage remain the bottleneck of applying pre-trained deep\nmodels in production. As a popular method for model compression, knowledge\ndistillation transfers knowledge from one or multiple large (teacher) models to\na small (student) model. When multiple teacher models are available in\ndistillation, the state-of-the-art methods assign a fixed weight to a teacher\nmodel in the whole distillation. Furthermore, most of the existing methods\nallocate an equal weight to every teacher model. In this paper, we observe\nthat, due to the complexity of training examples and the differences in student\nmodel capability, learning differentially from teacher models can lead to\nbetter performance of student models distilled. We systematically develop a\nreinforced method to dynamically assign weights to teacher models for different\ntraining instances and optimize the performance of student model. Our extensive\nexperimental results on several NLP tasks clearly verify the feasibility and\neffectiveness of our approach.</p>\n", "tags": ["AAAI","Training Techniques"] },
{"key": "yuan2021bartscore", "citations": "287", "year": "2021", "title":"Bartscore: Evaluating Generated Text As Text Generation", "abstract": "<p>A wide variety of NLP applications, such as machine translation,\nsummarization, and dialog, involve text generation. One major challenge for\nthese applications is how to evaluate whether such generated texts are actually\nfluent, accurate, or effective. In this work, we conceptualize the evaluation\nof generated text as a text generation problem, modeled using pre-trained\nsequence-to-sequence models. The general idea is that models trained to convert\nthe generated text to/from a reference output or the source text will achieve\nhigher scores when the generated text is better. We operationalize this idea\nusing BART, an encoder-decoder based pre-trained model, and propose a metric\nBARTScore with a number of variants that can be flexibly applied in an\nunsupervised fashion to evaluation of text from different perspectives (e.g.\ninformativeness, fluency, or factuality). BARTScore is conceptually simple and\nempirically effective. It can outperform existing top-scoring metrics in 16 of\n22 test settings, covering evaluation of 16 datasets (e.g., machine\ntranslation, text summarization) and 7 different perspectives (e.g.,\ninformativeness, factuality). Code to calculate BARTScore is available at\nhttps://github.com/neulab/BARTScore, and we have released an interactive\nleaderboard for meta-evaluation at\nhttp://explainaboard.nlpedia.ai/leaderboard/task-meval/ on the ExplainaBoard\nplatform, which allows us to interactively understand the strengths,\nweaknesses, and complementarity of each metric.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Tools"] },
{"key": "yuan2021incorporating", "citations": "446", "year": "2021", "title":"Incorporating Convolution Designs Into Visual Transformers", "abstract": "<p>Motivated by the success of Transformers in natural language processing (NLP)\ntasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to\nthe vision domain. However, pure Transformer architectures often require a\nlarge amount of training data or extra supervision to obtain comparable\nperformance with convolutional neural networks (CNNs). To overcome these\nlimitations, we analyze the potential drawbacks when directly borrowing\nTransformer architectures from NLP. Then we propose a new\n\\textbf{Convolution-enhanced image Transformer (CeiT)} which combines the\nadvantages of CNNs in extracting low-level features, strengthening locality,\nand the advantages of Transformers in establishing long-range dependencies.\nThree modifications are made to the original Transformer: \\textbf{1)} instead\nof the straightforward tokenization from raw input images, we design an\n\\textbf{Image-to-Tokens (I2T)} module that extracts patches from generated\nlow-level features; \\textbf{2)} the feed-froward network in each encoder block\nis replaced with a \\textbf{Locally-enhanced Feed-Forward (LeFF)} layer that\npromotes the correlation among neighboring tokens in the spatial dimension;\n\\textbf{3)} a \\textbf{Layer-wise Class token Attention (LCA)} is attached at\nthe top of the Transformer that utilizes the multi-level representations.\n  Experimental results on ImageNet and seven downstream tasks show the\neffectiveness and generalization ability of CeiT compared with previous\nTransformers and state-of-the-art CNNs, without requiring a large amount of\ntraining data and extra CNN teachers. Besides, CeiT models also demonstrate\nbetter convergence with \\(3\\times\\) fewer training iterations, which can reduce\nthe training cost significantly\\footnote{Code and models will be released upon\nacceptance.}.</p>\n", "tags": ["ICCV","Model Architecture","Training Techniques"] },
{"key": "yuan2021tokens", "citations": "1720", "year": "2021", "title":"Tokens-to-token Vit: Training Vision Transformers From Scratch On Imagenet", "abstract": "<p>Transformers, which are popular for language modeling, have been explored for\nsolving vision tasks recently, e.g., the Vision Transformer (ViT) for image\nclassification. The ViT model splits each image into a sequence of tokens with\nfixed length and then applies multiple Transformer layers to model their global\nrelation for classification. However, ViT achieves inferior performance to CNNs\nwhen trained from scratch on a midsize dataset like ImageNet. We find it is\nbecause: 1) the simple tokenization of input images fails to model the\nimportant local structure such as edges and lines among neighboring pixels,\nleading to low training sample efficiency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for fixed computation budgets\nand limited training samples. To overcome such limitations, we propose a new\nTokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a\nlayer-wise Tokens-to-Token (T2T) transformation to progressively structurize\nthe image to tokens by recursively aggregating neighboring Tokens into one\nToken (Tokens-to-Token), such that local structure represented by surrounding\ntokens can be modeled and tokens length can be reduced; 2) an efficient\nbackbone with a deep-narrow structure for vision transformer motivated by CNN\narchitecture design after empirical study. Notably, T2T-ViT reduces the\nparameter count and MACs of vanilla ViT by half, while achieving more than\n3.0% improvement when trained from scratch on ImageNet. It also outperforms\nResNets and achieves comparable performance with MobileNets by directly\ntraining on ImageNet. For example, T2T-ViT with comparable size to ResNet50\n(21.5M parameters) can achieve 83.3% top1 accuracy in image resolution\n384\\(\\times\\)384 on ImageNet. (Code: https://github.com/yitu-opensource/T2T-ViT)</p>\n", "tags": ["Datasets","Has Code","ICCV","Model Architecture","Training Techniques"] },
{"key": "yuan2022biobart", "citations": "86", "year": "2022", "title":"Biobart: Pretraining And Evaluation Of A Biomedical Generative Language Model", "abstract": "<p>Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.</p>\n", "tags": ["Evaluation","Prompting"] },
{"key": "yuan2023where", "citations": "83", "year": "2023", "title":"Where To Go Next For Recommender Systems? ID- Vs. Modality-based Recommender Models Revisited", "abstract": "<p>Recommendation models that utilize unique identities (IDs) to represent\ndistinct users and items have been state-of-the-art (SOTA) and dominated the\nrecommender systems (RS) literature for over a decade. Meanwhile, the\npre-trained modality encoders, such as BERT and ViT, have become increasingly\npowerful in modeling the raw modality features of an item, such as text and\nimages. Given this, a natural question arises: can a purely modality-based\nrecommendation model (MoRec) outperforms or matches a pure ID-based model\n(IDRec) by replacing the itemID embedding with a SOTA modality encoder? In\nfact, this question was answered ten years ago when IDRec beats MoRec by a\nstrong margin in both recommendation accuracy and efficiency. We aim to revisit\nthis `old’ question and systematically study MoRec from several aspects.\nSpecifically, we study several sub-questions: (i) which recommendation\nparadigm, MoRec or IDRec, performs better in practical scenarios, especially in\nthe general setting and warm item scenarios where IDRec has a strong advantage?\ndoes this hold for items with different modality features? (ii) can the latest\ntechnical advances from other communities (i.e., natural language processing\nand computer vision) translate into accuracy improvement for MoRec? (iii) how\nto effectively utilize item modality representation, can we use it directly or\ndo we have to adjust it with new data? (iv) are there some key challenges for\nMoRec to be solved in practical applications? To answer them, we conduct\nrigorous experiments for item recommendations with two popular modalities,\ni.e., text and vision. We provide the first empirical evidence that MoRec is\nalready comparable to its IDRec counterpart with an expensive end-to-end\ntraining method, even for warm item recommendation. Our results potentially\nimply that the dominance of IDRec in the RS field may be greatly challenged in\nthe future.</p>\n", "tags": ["Applications","Model Architecture","SIGIR","Training Techniques"] },
{"key": "yun2019are", "citations": "72", "year": "2019", "title":"Are Transformers Universal Approximators Of Sequence-to-sequence Functions?", "abstract": "<p>Despite the widespread adoption of Transformer models for NLP tasks, the\nexpressive power of these models is not well-understood. In this paper, we\nestablish that Transformer models are universal approximators of continuous\npermutation equivariant sequence-to-sequence functions with compact support,\nwhich is quite surprising given the amount of shared parameters in these\nmodels. Furthermore, using positional encodings, we circumvent the restriction\nof permutation equivariance, and show that Transformer models can universally\napproximate arbitrary continuous sequence-to-sequence functions on a compact\ndomain. Interestingly, our proof techniques clearly highlight the different\nroles of the self-attention and the feed-forward layers in Transformers. In\nparticular, we prove that fixed width self-attention layers can compute\ncontextual mappings of the input sequences, playing a key role in the universal\napproximation property of Transformers. Based on this insight from our\nanalysis, we consider other simpler alternatives to self-attention layers and\nempirically evaluate them.</p>\n", "tags": ["Model Architecture"] },
{"key": "zadeh2020gobo", "citations": "105", "year": "2020", "title":"GOBO: Quantizing Attention-based NLP Models For Low Latency And Energy Efficient Inference", "abstract": "<p>Attention-based models have demonstrated remarkable success in various\nnatural language understanding tasks. However, efficient execution remains a\nchallenge for these models which are memory-bound due to their massive number\nof parameters. We present GOBO, a model quantization technique that compresses\nthe vast majority (typically 99.9%) of the 32-bit floating-point parameters of\nstate-of-the-art BERT models and their variants to 3 bits while maintaining\ntheir accuracy. Unlike other quantization methods, GOBO does not require\nfine-tuning nor retraining to compensate for the quantization error. We present\ntwo practical hardware applications of GOBO. In the first GOBO reduces memory\nstorage and traffic and as a result inference latency and energy consumption.\nThis GOBO memory compression mechanism is plug-in compatible with many\narchitectures; we demonstrate it with the TPU, Eyeriss, and an architecture\nusing Tensor Cores-like units. Second, we present a co-designed hardware\narchitecture that also reduces computation. Uniquely, the GOBO architecture\nmaintains most of the weights in 3b even during computation, a property that:\n(1) makes the processing elements area efficient, allowing us to pack more\ncompute power per unit area, (2) replaces most multiply-accumulations with\nadditions, and (3) reduces the off-chip traffic by amplifying on-chip memory\ncapacity.</p>\n", "tags": ["Applications","Efficiency","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "zafrir2019q8bert", "citations": "378", "year": "2019", "title":"Q8BERT: Quantized 8bit BERT", "abstract": "<p>Recently, pre-trained Transformer based language models such as BERT and GPT,\nhave shown great improvement in many Natural Language Processing (NLP) tasks.\nHowever, these models contain a large amount of parameters. The emergence of\neven larger and more accurate models such as GPT2 and Megatron, suggest a trend\nof large pre-trained Transformer models. However, using these large models in\nproduction environments is a complex task requiring a large amount of compute,\nmemory and power resources. In this work we show how to perform\nquantization-aware training during the fine-tuning phase of BERT in order to\ncompress BERT by \\(4\\times\\) with minimal accuracy loss. Furthermore, the\nproduced quantized model can accelerate inference speed if it is optimized for\n8bit Integer supporting hardware.</p>\n", "tags": ["Efficiency","Fine-Tuning","Model Architecture","NEURIPS","Training Techniques"] },
{"key": "zaheer2020big", "citations": "812", "year": "2020", "title":"Big Bird: Transformers For Longer Sequences", "abstract": "<p>Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having \\(O(1)\\) global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.</p>\n", "tags": ["Applications","Model Architecture","NEURIPS"] },
{"key": "zaib2021conversational", "citations": "83", "year": "2022", "title":"Conversational Question Answering: A Survey", "abstract": "<p>Question answering (QA) systems provide a way of querying the information\navailable in various formats including, but not limited to, unstructured and\nstructured data in natural languages. It constitutes a considerable part of\nconversational artificial intelligence (AI) which has led to the introduction\nof a special research topic on Conversational Question Answering (CQA), wherein\na system is required to understand the given context and then engages in\nmulti-turn QA to satisfy the user’s information needs. Whilst the focus of most\nof the existing research work is subjected to single-turn QA, the field of\nmulti-turn QA has recently grasped attention and prominence owing to the\navailability of large-scale, multi-turn QA datasets and the development of\npre-trained language models. With a good amount of models and research papers\nadding to the literature every year recently, there is a dire need of arranging\nand presenting the related work in a unified manner to streamline future\nresearch. This survey, therefore, is an effort to present a comprehensive\nreview of the state-of-the-art research trends of CQA primarily based on\nreviewed papers from 2016-2021. Our findings show that there has been a trend\nshift from single-turn to multi-turn QA which empowers the field of\nConversational AI from different perspectives. This survey is intended to\nprovide an epitome for the research community with the hope of laying a strong\nfoundation for the field of CQA.</p>\n", "tags": ["Datasets","Model Architecture","Survey Paper"] },
{"key": "zaken2021bitfit", "citations": "403", "year": "2022", "title":"Bitfit: Simple Parameter-efficient Fine-tuning For Transformer-based Masked Language-models", "abstract": "<p>We introduce BitFit, a sparse-finetuning method where only the bias-terms of\nthe model (or a subset of them) are being modified. We show that with\nsmall-to-medium training data, applying BitFit on pre-trained BERT models is\ncompetitive with (and sometimes better than) fine-tuning the entire model. For\nlarger data, the method is competitive with other sparse fine-tuning methods.\nBesides their practical utility, these findings are relevant for the question\nof understanding the commonly-used process of finetuning: they support the\nhypothesis that finetuning is mainly about exposing knowledge induced by\nlanguage-modeling training, rather than learning new task-specific linguistic\nknowledge.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "zakka2023almanac", "citations": "158", "year": "2024", "title":"Almanac: Retrieval-augmented Language Models For Clinical Medicine", "abstract": "<p>Large-language models have recently demonstrated impressive zero-shot\ncapabilities in a variety of natural language tasks such as summarization,\ndialogue generation, and question-answering. Despite many promising\napplications in clinical medicine, adoption of these models in real-world\nsettings has been largely limited by their tendency to generate incorrect and\nsometimes even toxic statements. In this study, we develop Almanac, a large\nlanguage model framework augmented with retrieval capabilities for medical\nguideline and treatment recommendations. Performance on a novel dataset of\nclinical scenarios (n = 130) evaluated by a panel of 5 board-certified and\nresident physicians demonstrates significant increases in factuality (mean of\n18% at p-value &lt; 0.05) across all specialties, with improvements in\ncompleteness and safety. Our results demonstrate the potential for large\nlanguage models to be effective tools in the clinical decision-making process,\nwhile also emphasizing the importance of careful testing and deployment to\nmitigate their shortcomings.</p>\n", "tags": ["Applications","Datasets","RAG","Tools"] },
{"key": "zan2022large", "citations": "60", "year": "2023", "title":"Large Language Models Meet Nl2code: A Survey", "abstract": "<p>The task of generating code from a natural language description, or NL2Code,\nis considered a pressing and significant challenge in code intelligence. Thanks\nto the rapid development of pre-training techniques, surging large language\nmodels are being proposed for code, sparking the advances in NL2Code. To\nfacilitate further research and applications in this field, in this paper, we\npresent a comprehensive survey of 27 existing large language models for\nNL2Code, and also review benchmarks and metrics. We provide an intuitive\ncomparison of all existing models on the HumanEval benchmark. Through in-depth\nobservation and analysis, we provide some insights and conclude that the key\nfactors contributing to the success of large language models for NL2Code are\n“Large Size, Premium Data, Expert Tuning”. In addition, we discuss challenges\nand opportunities regarding the gap between models and humans. We also create a\nwebsite https://nl2code.github.io to track the latest progress through\ncrowd-sourcing. To the best of our knowledge, this is the first survey of large\nlanguage models for NL2Code, and we believe it will contribute to the ongoing\ndevelopment of the field.</p>\n", "tags": ["Applications","Datasets","Evaluation","Has Code","Llm For Code","Survey Paper","Training Techniques"] },
{"key": "zang2020multiwoz", "citations": "166", "year": "2020", "title":"Multiwoz 2.2 : A Dialogue Dataset With Additional Annotation Corrections And State Tracking Baselines", "abstract": "<p>MultiWOZ is a well-known task-oriented dialogue dataset containing over\n10,000 annotated dialogues spanning 8 domains. It is extensively used as a\nbenchmark for dialogue state tracking. However, recent works have reported\npresence of substantial noise in the dialogue state annotations. MultiWOZ 2.1\nidentified and fixed many of these erroneous annotations and user utterances,\nresulting in an improved version of this dataset. This work introduces MultiWOZ\n2.2, which is a yet another improved version of this dataset. Firstly, we\nidentify and fix dialogue state annotation errors across 17.3% of the\nutterances on top of MultiWOZ 2.1. Secondly, we redefine the ontology by\ndisallowing vocabularies of slots with a large number of possible values (e.g.,\nrestaurant name, time of booking). In addition, we introduce slot span\nannotations for these slots to standardize them across recent models, which\npreviously used custom string matching heuristics to generate them. We also\nbenchmark a few state of the art dialogue state tracking models on the\ncorrected dataset to facilitate comparison for future work. In the end, we\ndiscuss best practices for dialogue data collection that can help avoid\nannotation errors.</p>\n", "tags": ["Datasets","Evaluation"] },
{"key": "zastudil2023generative", "citations": "62", "year": "2023", "title":"Generative AI In Computing Education: Perspectives Of Students And Instructors", "abstract": "<p>Generative models are now capable of producing natural language text that is,\nin some cases, comparable in quality to the text produced by people. In the\ncomputing education context, these models are being used to generate code, code\nexplanations, and programming exercises. The rapid adoption of these models has\nprompted multiple position papers and workshops which discuss the implications\nof these models for computing education, both positive and negative. This paper\npresents results from a series of semi-structured interviews with 12 students\nand 6 instructors about their awareness, experiences, and preferences regarding\nthe use of tools powered by generative AI in computing classrooms. The results\nsuggest that Generative AI (GAI) tools will play an increasingly significant\nrole in computing education. However, students and instructors also raised\nnumerous concerns about how these models should be integrated to best support\nthe needs and learning goals of students. We also identified interesting\ntensions and alignments that emerged between how instructors and students\nprefer to engage with these models. We discuss these results and provide\nrecommendations related to curriculum development, assessment methods, and\npedagogical practice. As GAI tools become increasingly prevalent, it’s\nimportant to understand educational stakeholders’ preferences and values to\nensure that these tools can be used for good and that potential harms can be\nmitigated.</p>\n", "tags": ["Tools"] },
{"key": "zeghidour2018fully", "citations": "90", "year": "2018", "title":"Fully Convolutional Speech Recognition", "abstract": "<p>Current state-of-the-art speech recognition systems build on recurrent neural\nnetworks for acoustic and/or language modeling, and rely on feature extraction\npipelines to extract mel-filterbanks or cepstral coefficients. In this paper we\npresent an alternative approach based solely on convolutional neural networks,\nleveraging recent advances in acoustic models from the raw waveform and\nlanguage modeling. This fully convolutional approach is trained end-to-end to\npredict characters from the raw waveform, removing the feature extraction step\naltogether. An external convolutional language model is used to decode words.\nOn Wall Street Journal, our model matches the current state-of-the-art. On\nLibrispeech, we report state-of-the-art performance among end-to-end models,\nincluding Deep Speech 2 trained with 12 times more acoustic data and\nsignificantly more linguistic data.</p>\n", "tags": [] },
{"key": "zellers2018recognition", "citations": "726", "year": "2019", "title":"From Recognition To Cognition: Visual Commonsense Reasoning", "abstract": "<p>Visual understanding goes well beyond object recognition. With one glance at\nan image, we can effortlessly imagine the world beyond the pixels: for\ninstance, we can infer people’s actions, goals, and mental states. While this\ntask is easy for humans, it is tremendously difficult for today’s vision\nsystems, requiring higher-order cognition and commonsense reasoning about the\nworld. We formalize this task as Visual Commonsense Reasoning. Given a\nchallenging question about an image, a machine must answer correctly and then\nprovide a rationale justifying its answer.\n  Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA\nproblems derived from 110k movie scenes. The key recipe for generating\nnon-trivial and high-quality problems at scale is Adversarial Matching, a new\napproach to transform rich annotations into multiple choice questions with\nminimal bias. Experimental results show that while humans find VCR easy (over\n90% accuracy), state-of-the-art vision models struggle (~45%).\n  To move towards cognition-level understanding, we present a new reasoning\nengine, Recognition to Cognition Networks (R2C), that models the necessary\nlayered inferences for grounding, contextualization, and reasoning. R2C helps\nnarrow the gap between humans and machines (~65%); still, the challenge is far\nfrom solved, and we provide analysis that suggests avenues for future work.</p>\n", "tags": ["CVPR","Datasets","Ethics & Fairness"] },
{"key": "zellers2018swag", "citations": "712", "year": "2018", "title":"SWAG: A Large-scale Adversarial Dataset For Grounded Commonsense Inference", "abstract": "<p>Given a partial description like “she opened the hood of the car,” humans can\nreason about the situation and anticipate what might come next (“then, she\nexamined the engine”). In this paper, we introduce the task of grounded\ncommonsense inference, unifying natural language inference and commonsense\nreasoning.\n  We present SWAG, a new dataset with 113k multiple choice questions about a\nrich spectrum of grounded situations. To address the recurring challenges of\nthe annotation artifacts and human biases found in many existing datasets, we\npropose Adversarial Filtering (AF), a novel procedure that constructs a\nde-biased dataset by iteratively training an ensemble of stylistic classifiers,\nand using them to filter the data. To account for the aggressive adversarial\nfiltering, we use state-of-the-art language models to massively oversample a\ndiverse set of potential counterfactuals. Empirical results demonstrate that\nwhile humans can solve the resulting inference problems with high accuracy\n(88%), various competitive models struggle on our task. We provide\ncomprehensive analysis that indicates significant opportunities for future\nresearch.</p>\n", "tags": ["Datasets","EMNLP","Training Techniques"] },
{"key": "zellers2019defending", "citations": "396", "year": "2019", "title":"Defending Against Neural Fake News", "abstract": "<p>Recent progress in natural language generation has raised dual-use concerns.\nWhile applications like summarization and translation are positive, the\nunderlying technology also might enable adversaries to generate neural fake\nnews: targeted propaganda that closely mimics the style of real news.\n  Modern computer security relies on careful threat modeling: identifying\npotential threats and vulnerabilities from an adversary’s point of view, and\nexploring potential mitigations to these threats. Likewise, developing robust\ndefenses against neural fake news requires us first to carefully investigate\nand characterize the risks of these models. We thus present a model for\ncontrollable text generation called Grover. Given a headline like `Link Found\nBetween Vaccines and Autism,’ Grover can generate the rest of the article;\nhumans find these generations to be more trustworthy than human-written\ndisinformation.\n  Developing robust verification techniques against generators like Grover is\ncritical. We find that best current discriminators can classify neural fake\nnews from real, human-written, news with 73% accuracy, assuming access to a\nmoderate level of training data. Counterintuitively, the best defense against\nGrover turns out to be Grover itself, with 92% accuracy, demonstrating the\nimportance of public release of strong generators. We investigate these results\nfurther, showing that exposure bias – and sampling strategies that alleviate\nits effects – both leave artifacts that similar discriminators can pick up on.\nWe conclude by discussing ethical issues regarding the technology, and plan to\nrelease Grover publicly, helping pave the way for better detection of neural\nfake news.</p>\n", "tags": ["Applications","Ethics & Fairness","Security","Training Techniques"] },
{"key": "zellers2019hellaswag", "citations": "467", "year": "2019", "title":"Hellaswag: Can A Machine Really Finish Your Sentence?", "abstract": "<p>Recent work by Zellers et al. (2018) introduced a new task of commonsense\nnatural language inference: given an event description such as “A woman sits at\na piano,” a machine must select the most likely followup: “She sets her fingers\non the keys.” With the introduction of BERT, near human-level performance was\nreached. Does this mean that machines can perform human level commonsense\ninference?\n  In this paper, we show that commonsense inference still proves difficult for\neven state-of-the-art models, by presenting HellaSwag, a new challenge dataset.\nThough its questions are trivial for humans (&gt;95% accuracy), state-of-the-art\nmodels struggle (&lt;48%). We achieve this via Adversarial Filtering (AF), a data\ncollection paradigm wherein a series of discriminators iteratively select an\nadversarial set of machine-generated wrong answers. AF proves to be\nsurprisingly robust. The key insight is to scale up the length and complexity\nof the dataset examples towards a critical ‘Goldilocks’ zone wherein generated\ntext is ridiculous to humans, yet often misclassified by state-of-the-art\nmodels.\n  Our construction of HellaSwag, and its resulting difficulty, sheds light on\nthe inner workings of deep pretrained models. More broadly, it suggests a new\npath forward for NLP research, in which benchmarks co-evolve with the evolving\nstate-of-the-art in an adversarial way, so as to present ever-harder\nchallenges.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "zellers2021merlot", "citations": "148", "year": "2021", "title":"MERLOT: Multimodal Neural Script Knowledge Models", "abstract": "<p>As humans, we understand events in the visual world contextually, performing\nmultimodal reasoning across time to make inferences about the past, present,\nand future. We introduce MERLOT, a model that learns multimodal script\nknowledge by watching millions of YouTube videos with transcribed speech – in\nan entirely label-free, self-supervised manner. By pretraining with a mix of\nboth frame-level (spatial) and video-level (temporal) objectives, our model not\nonly learns to match images to temporally corresponding words, but also to\ncontextualize what is happening globally over time. As a result, MERLOT\nexhibits strong out-of-the-box representations of temporal commonsense, and\nachieves state-of-the-art performance on 12 different video QA datasets when\nfinetuned. It also transfers well to the world of static images, allowing\nmodels to reason about the dynamic context behind visual scenes. On Visual\nCommonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,\noutperforming state-of-the-art models of similar size by over 3%, even those\nthat make heavy use of auxiliary supervised data (like object bounding boxes).\n  Ablation analyses demonstrate the complementary importance of: 1) training on\nvideos versus static images; 2) scaling the magnitude and diversity of the\npretraining video corpus; and 3) using diverse objectives that encourage\nfull-stack multimodal reasoning, from the recognition to cognition level.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "zellers2022merlot", "citations": "106", "year": "2022", "title":"MERLOT Reserve: Neural Script Knowledge Through Vision And Language And Sound", "abstract": "<p>As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time – through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n  Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining – even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n  We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Training Techniques"] },
{"key": "zeng2016leveraging", "citations": "169", "year": "2017", "title":"Leveraging Video Descriptions To Learn Video Question Answering", "abstract": "<p>We propose a scalable approach to learn video-based question answering (QA):\nanswer a “free-form natural language question” about a video content. Our\napproach automatically harvests a large number of videos and descriptions\nfreely available online. Then, a large number of candidate QA pairs are\nautomatically generated from descriptions rather than manually annotated. Next,\nwe use these candidate QA pairs to train a number of video-based QA methods\nextended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et\nal. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect\ncandidate QA pairs, we propose a self-paced learning procedure to iteratively\nidentify them and mitigate their effects in training. Finally, we evaluate\nperformance on manually generated video-based QA pairs. The results show that\nour self-paced learning procedure is effective, and the extended SS model\noutperforms various baselines.</p>\n", "tags": ["AAAI","Training Techniques"] },
{"key": "zeng2016title", "citations": "73", "year": "2016", "title":"Title Generation For User Generated Videos", "abstract": "<p>A great video title describes the most salient event compactly and captures\nthe viewer’s attention. In contrast, video captioning tends to generate\nsentences that describe the video as a whole. Although generating a video title\nautomatically is a very useful task, it is much less addressed than video\ncaptioning. We address video title generation for the first time by proposing\ntwo methods that extend state-of-the-art video captioners to this new task.\nFirst, we make video captioners highlight sensitive by priming them with a\nhighlight detector. Our framework allows for jointly training a model for title\ngeneration and video highlight localization. Second, we induce high sentence\ndiversity in video captioners, so that the generated titles are also diverse\nand catchy. This means that a large number of sentences might be required to\nlearn the sentence structure of titles. Hence, we propose a novel sentence\naugmentation method to train a captioner with additional sentence-only examples\nthat come without corresponding videos. We collected a large-scale Video Titles\nin the Wild (VTW) dataset of 18100 automatically crawled user-generated videos\nand titles. On VTW, our methods consistently improve title prediction accuracy,\nand achieve the best performance in both automatic and human evaluation.\nFinally, our sentence augmentation method also outperforms the baselines on the\nM-VAD dataset.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Tools","Training Techniques"] },
{"key": "zeng2019copymtl", "citations": "181", "year": "2020", "title":"Copymtl: Copy Mechanism For Joint Extraction Of Entities And Relations With Multi-task Learning", "abstract": "<p>Joint extraction of entities and relations has received significant attention\ndue to its potential of providing higher performance for both tasks. Among\nexisting methods, CopyRE is effective and novel, which uses a\nsequence-to-sequence framework and copy mechanism to directly generate the\nrelation triplets. However, it suffers from two fatal problems. The model is\nextremely weak at differing the head and tail entity, resulting in inaccurate\nentity extraction. It also cannot predict multi-token entities (e.g.\n\\textit{Steven Jobs}). To address these problems, we give a detailed analysis\nof the reasons behind the inaccurate entity extraction problem, and then\npropose a simple but extremely effective model structure to solve this problem.\nIn addition, we propose a multi-task learning framework equipped with copy\nmechanism, called CopyMTL, to allow the model to predict multi-token entities.\nExperiments reveal the problems of CopyRE and show that our model achieves\nsignificant improvement over the current state-of-the-art method by 9% in NYT\nand 16% in WebNLG (F1 score). Our code is available at\nhttps://github.com/WindChimeRan/CopyMTL</p>\n", "tags": ["AAAI","Has Code","Model Architecture","Tools"] },
{"key": "zeng2020aligntts", "citations": "60", "year": "2020", "title":"Aligntts: Efficient Feed-forward Text-to-speech System Without Explicit Alignment", "abstract": "<p>Targeting at both high efficiency and performance, we propose AlignTTS to\npredict the mel-spectrum in parallel. AlignTTS is based on a Feed-Forward\nTransformer which generates mel-spectrum from a sequence of characters, and the\nduration of each character is determined by a duration predictor.Instead of\nadopting the attention mechanism in Transformer TTS to align text to\nmel-spectrum, the alignment loss is presented to consider all possible\nalignments in training by use of dynamic programming. Experiments on the\nLJSpeech dataset show that our model achieves not only state-of-the-art\nperformance which outperforms Transformer TTS by 0.03 in mean option score\n(MOS), but also a high efficiency which is more than 50 times faster than\nreal-time.</p>\n", "tags": ["Datasets","Efficiency","ICASSP","Model Architecture","Training Techniques"] },
{"key": "zeng2021multi", "citations": "90", "year": "2021", "title":"Multi-grained Vision Language Pre-training: Aligning Texts With Visual Concepts", "abstract": "<p>Most existing methods in vision language pre-training rely on object-centric\nfeatures extracted through object detection and make fine-grained alignments\nbetween the extracted features and texts. It is challenging for these methods\nto learn relations among multiple objects. To this end, we propose a new method\ncalled X-VLM to perform `multi-grained vision language pre-training.’ The key\nto learning multi-grained alignments is to locate visual concepts in the image\ngiven the associated texts, and in the meantime align the texts with the visual\nconcepts, where the alignments are in multi-granularity. Experimental results\nshow that X-VLM effectively leverages the learned multi-grained alignments to\nmany downstream vision language tasks and consistently outperforms\nstate-of-the-art methods.</p>\n", "tags": ["Training Techniques"] },
{"key": "zenkel2019adding", "citations": "85", "year": "2019", "title":"Adding Interpretable Attention To Neural Translation Models Improves Word Alignment", "abstract": "<p>Multi-layer models with multiple attention heads per layer provide superior\ntranslation quality compared to simpler and shallower models, but determining\nwhat source context is most relevant to each target word is more challenging as\na result. Therefore, deriving high-accuracy word alignments from the\nactivations of a state-of-the-art neural machine translation model is an open\nchallenge. We propose a simple model extension to the Transformer architecture\nthat makes use of its hidden representations and is restricted to attend solely\non encoder information to predict the next word. It can be trained on bilingual\ndata without word-alignment information. We further introduce a novel alignment\ninference procedure which applies stochastic gradient descent to directly\noptimize the attention activations towards a given target word. The resulting\nalignments dramatically outperform the naive approach to interpreting\nTransformer attention activations, and are comparable to Giza++ on two publicly\navailable data sets.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "zenkel2020end", "citations": "61", "year": "2020", "title":"End-to-end Neural Word Alignment Outperforms GIZA++", "abstract": "<p>Word alignment was once a core unsupervised learning task in natural language\nprocessing because of its essential role in training statistical machine\ntranslation (MT) models. Although unnecessary for training neural MT models,\nword alignment still plays an important role in interactive applications of\nneural machine translation, such as annotation transfer and lexicon injection.\nWhile statistical MT methods have been replaced by neural approaches with\nsuperior performance, the twenty-year-old GIZA++ toolkit remains a key\ncomponent of state-of-the-art word alignment systems. Prior work on neural word\nalignment has only been able to outperform GIZA++ by using its output during\ntraining. We present the first end-to-end neural word alignment method that\nconsistently outperforms GIZA++ on three data sets. Our approach repurposes a\nTransformer model trained for supervised translation to also serve as an\nunsupervised word alignment model in a manner that is tightly integrated and\ndoes not affect translation quality.</p>\n", "tags": ["Applications","Model Architecture","Training Techniques"] },
{"key": "zeyer2018improved", "citations": "291", "year": "2018", "title":"Improved Training Of End-to-end Attention Models For Speech Recognition", "abstract": "<p>Sequence-to-sequence attention-based models on subword units allow simple\nopen-vocabulary end-to-end speech recognition. In this work, we show that such\nmodels can achieve competitive results on the Switchboard 300h and LibriSpeech\n1000h tasks. In particular, we report the state-of-the-art word error rates\n(WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets\nof LibriSpeech. We introduce a new pretraining scheme by starting with a high\ntime reduction factor and lowering it during training, which is crucial both\nfor convergence and final performance. In some experiments, we also use an\nauxiliary CTC loss function to help the convergence. In addition, we train long\nshort-term memory (LSTM) language models on subword units. By shallow fusion,\nwe report up to 27% relative improvements in WER over the attention baseline\nwithout a language model.</p>\n", "tags": ["Evaluation","INTERSPEECH","Model Architecture","Training Techniques"] },
{"key": "zeyer2018returnn", "citations": "72", "year": "2018", "title":"RETURNN As A Generic Flexible Neural Toolkit With Application To Translation And Speech Recognition", "abstract": "<p>We compare the fast training and decoding speed of RETURNN of attention\nmodels for translation, due to fast CUDA LSTM kernels, and a fast pure\nTensorFlow beam search decoder. We show that a layer-wise pretraining scheme\nfor recurrent attention models gives over 1% BLEU improvement absolute and it\nallows to train deeper recurrent encoder networks. Promising preliminary\nresults on max. expected BLEU training are presented. We are able to train\nstate-of-the-art models for translation and end-to-end models for speech\nrecognition and show results on WMT 2017 and Switchboard. The flexibility of\nRETURNN allows a fast research feedback loop to experiment with alternative\narchitectures, and its generality allows to use it on a wide range of\napplications.</p>\n", "tags": ["Applications","Model Architecture","Training Techniques"] },
{"key": "zha2019context", "citations": "144", "year": "2019", "title":"Context-aware Visual Policy Network For Fine-grained Image Captioning", "abstract": "<p>With the maturity of visual detection techniques, we are more ambitious in\ndescribing visual content with open-vocabulary, fine-grained and free-form\nlanguage, i.e., the task of image captioning. In particular, we are interested\nin generating longer, richer and more fine-grained sentences and paragraphs as\nimage descriptions. Image captioning can be translated to the task of\nsequential language prediction given visual content, where the output sequence\nforms natural language description with plausible grammar. However, existing\nimage captioning methods focus only on language policy while not visual policy,\nand thus fail to capture visual context that are crucial for compositional\nreasoning such as object relationships (e.g., “man riding horse”) and visual\ncomparisons (e.g., “small(er) cat”). This issue is especially severe when\ngenerating longer sequences such as a paragraph. To fill the gap, we propose a\nContext-Aware Visual Policy network (CAVP) for fine-grained image-to-language\ngeneration: image sentence captioning and image paragraph captioning. During\ncaptioning, CAVP explicitly considers the previous visual attentions as\ncontext, and decides whether the context is used for the current word/sentence\ngeneration given the current visual attention. Compared against traditional\nvisual attention mechanism that only fixes a single visual region at each step,\nCAVP can attend to complex visual compositions over time. The whole image\ncaptioning model – CAVP and its subsequent language policy network – can be\nefficiently optimized end-to-end by using an actor-critic policy gradient\nmethod. We have demonstrated the effectiveness of CAVP by state-of-the-art\nperformances on MS-COCO and Stanford captioning datasets, using various metrics\nand sensible visualizations of qualitative visual context.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Reinforcement Learning"] },
{"key": "zhai2021lit", "citations": "237", "year": "2022", "title":"Lit: Zero-shot Transfer With Locked-image Text Tuning", "abstract": "<p>This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning “Locked-image Tuning” (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%\nzero-shot transfer accuracy on the ImageNet test set, and 82.5% on the\nchallenging out-of-distribution ObjectNet test set.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "zhai2023sigmoid", "citations": "123", "year": "2023", "title":"Sigmoid Loss For Language Image Pre-training", "abstract": "<p>We propose a simple pairwise Sigmoid loss for Language-Image Pre-training\n(SigLIP). Unlike standard contrastive learning with softmax normalization, the\nsigmoid loss operates solely on image-text pairs and does not require a global\nview of the pairwise similarities for normalization. The sigmoid loss\nsimultaneously allows further scaling up the batch size, while also performing\nbetter at smaller batch sizes. Combined with Locked-image Tuning, with only\nfour TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet\nzero-shot accuracy in two days. The disentanglement of the batch size from the\nloss further allows us to study the impact of examples vs pairs and negative to\npositive ratio. Finally, we push the batch size to the extreme, up to one\nmillion, and find that the benefits of growing batch size quickly diminish,\nwith a more reasonable batch size of 32k being sufficient. We release our\nmodels at https://github.com/google-research/big_vision and hope our research\nmotivates further explorations in improving the quality and efficiency of\nlanguage-image pre-training.</p>\n", "tags": ["Has Code","ICCV","Training Techniques"] },
{"key": "zhang2016automatic", "citations": "72", "year": "2017", "title":"Automatic Generation Of Grounded Visual Questions", "abstract": "<p>In this paper, we propose the first model to be able to generate visually\ngrounded questions with diverse types for a single image. Visual question\ngeneration is an emerging topic which aims to ask questions in natural language\nbased on visual input. To the best of our knowledge, it lacks automatic methods\nto generate meaningful questions with various types for the same visual input.\nTo circumvent the problem, we propose a model that automatically generates\nvisually grounded questions with varying types. Our model takes as input both\nimages and the captions generated by a dense caption model, samples the most\nprobable question types, and generates the questions in sequel. The\nexperimental results on two real world datasets show that our model outperforms\nthe strongest baseline in terms of both correctness and diversity with a wide\nmargin.</p>\n", "tags": ["Datasets","IJCAI"] },
{"key": "zhang2016rationale", "citations": "259", "year": "2016", "title":"Rationale-augmented Convolutional Neural Networks For Text Classification", "abstract": "<p>We present a new Convolutional Neural Network (CNN) model for text\nclassification that jointly exploits labels on documents and their component\nsentences. Specifically, we consider scenarios in which annotators explicitly\nmark sentences (or snippets) that support their overall document\ncategorization, i.e., they provide rationales. Our model exploits such\nsupervision via a hierarchical approach in which each document is represented\nby a linear combination of the vector representations of its component\nsentences. We propose a sentence-level convolutional model that estimates the\nprobability that a given sentence is a rationale, and we then scale the\ncontribution of each sentence to the aggregate document representation in\nproportion to these estimates. Experiments on five classification datasets that\nhave document labels and associated rationales demonstrate that our approach\nconsistently outperforms strong baselines. Moreover, our model naturally\nprovides explanations for its predictions.</p>\n", "tags": ["Datasets","EMNLP"] },
{"key": "zhang2016variational", "citations": "198", "year": "2016", "title":"Variational Neural Machine Translation", "abstract": "<p>Models of neural machine translation are often from a discriminative family\nof encoderdecoders that learn a conditional distribution of a target sentence\ngiven a source sentence. In this paper, we propose a variational model to learn\nthis conditional distribution for neural machine translation: a variational\nencoderdecoder model that can be trained end-to-end. Different from the vanilla\nencoder-decoder model that generates target translations from hidden\nrepresentations of source sentences alone, the variational model introduces a\ncontinuous latent variable to explicitly model underlying semantics of source\nsentences and to guide the generation of target translations. In order to\nperform efficient posterior inference and large-scale training, we build a\nneural posterior approximator conditioned on both the source and the target\nsides, and equip it with a reparameterization technique to estimate the\nvariational lower bound. Experiments on both Chinese-English and English-\nGerman translation tasks show that the proposed variational neural machine\ntranslation achieves significant improvements over the vanilla neural machine\ntranslation baselines.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "zhang2017actor", "citations": "110", "year": "2017", "title":"Actor-critic Sequence Training For Image Captioning", "abstract": "<p>Generating natural language descriptions of images is an important capability\nfor a robot or other visual-intelligence driven AI agent that may need to\ncommunicate with human users about what it is seeing. Such image captioning\nmethods are typically trained by maximising the likelihood of ground-truth\nannotated caption given the image. While simple and easy to implement, this\napproach does not directly maximise the language quality metrics we care about\nsuch as CIDEr. In this paper we investigate training image captioning methods\nbased on actor-critic reinforcement learning in order to directly optimise\nnon-differentiable quality metrics of interest. By formulating a per-token\nadvantage and value computation strategy in this novel reinforcement learning\nbased captioning model, we show that it is possible to achieve the state of the\nart performance on the widely used MSCOCO benchmark.</p>\n", "tags": ["Datasets","Evaluation","Reinforcement Learning","Training Techniques"] },
{"key": "zhang2017flexible", "citations": "80", "year": "2017", "title":"Flexible And Creative Chinese Poetry Generation Using Neural Memory", "abstract": "<p>It has been shown that Chinese poems can be successfully generated by\nsequence-to-sequence neural models, particularly with the attention mechanism.\nA potential problem of this approach, however, is that neural models can only\nlearn abstract rules, while poem generation is a highly creative process that\ninvolves not only rules but also innovations for which pure statistical models\nare not appropriate in principle. This work proposes a memory-augmented neural\nmodel for Chinese poem generation, where the neural model and the augmented\nmemory work together to balance the requirements of linguistic accordance and\naesthetic innovation, leading to innovative generations that are still\nrule-compliant. In addition, it is found that the memory mechanism provides\ninteresting flexibility that can be used to generate poems with different\nstyles.</p>\n", "tags": ["Memory & Context","Model Architecture"] },
{"key": "zhang2017neural", "citations": "92", "year": "2018", "title":"Neural Personalized Response Generation As Domain Adaptation", "abstract": "<p>In this paper, we focus on the personalized response generation for\nconversational systems. Based on the sequence to sequence learning, especially\nthe encoder-decoder framework, we propose a two-phase approach, namely\ninitialization then adaptation, to model the responding style of human and then\ngenerate personalized responses. For evaluation, we propose a novel human aided\nmethod to evaluate the performance of the personalized response generation\nmodels by online real-time conversation and offline human judgement. Moreover,\nthe lexical divergence of the responses generated by the 5 personalized models\nindicates that the proposed two-phase approach achieves good results on\nmodeling the responding style of human and generating personalized responses\nfor the conversational systems.</p>\n", "tags": ["Evaluation","Fine-Tuning","Reinforcement Learning","Tools"] },
{"key": "zhang2017sentence", "citations": "330", "year": "2017", "title":"Sentence Simplification With Deep Reinforcement Learning", "abstract": "<p>Sentence simplification aims to make sentences easier to read and understand.\nMost recent approaches draw on insights from machine translation to learn\nsimplification rewrites from monolingual corpora of complex and simple\nsentences. We address the simplification problem with an encoder-decoder model\ncoupled with a deep reinforcement learning framework. Our model, which we call\n{\\sc Dress} (as shorthand for {\\bf D}eep {\\bf RE}inforcement {\\bf S}entence\n{\\bf S}implification), explores the space of possible simplifications while\nlearning to optimize a reward function that encourages outputs which are\nsimple, fluent, and preserve the meaning of the input. Experiments on three\ndatasets demonstrate that our model outperforms competitive simplification\nsystems.</p>\n", "tags": ["Agentic","Datasets","EMNLP","Reinforcement Learning","Tools"] },
{"key": "zhang2017thumt", "citations": "88", "year": "2017", "title":"THUMT: An Open Source Toolkit For Neural Machine Translation", "abstract": "<p>This paper introduces THUMT, an open-source toolkit for neural machine\ntranslation (NMT) developed by the Natural Language Processing Group at\nTsinghua University. THUMT implements the standard attention-based\nencoder-decoder framework on top of Theano and supports three training\ncriteria: maximum likelihood estimation, minimum risk training, and\nsemi-supervised training. It features a visualization tool for displaying the\nrelevance between hidden states in neural networks and contextual words, which\nhelps to analyze the internal workings of NMT. Experiments on Chinese-English\ndatasets show that THUMT using minimum risk training significantly outperforms\nGroundHog, a state-of-the-art toolkit for NMT.</p>\n", "tags": ["Datasets","Model Architecture","Tools","Training Techniques"] },
{"key": "zhang2017variational", "citations": "335", "year": "2018", "title":"Variational Reasoning For Question Answering With Knowledge Graph", "abstract": "<p>Knowledge graph (KG) is known to be helpful for the task of question\nanswering (QA), since it provides well-structured relational information\nbetween entities, and allows one to further infer indirect facts. However, it\nis challenging to build QA systems which can learn to reason over knowledge\ngraphs based on question-answer pairs alone. First, when people ask questions,\ntheir expressions are noisy (for example, typos in texts, or variations in\npronunciations), which is non-trivial for the QA system to match those\nmentioned entities to the knowledge graph. Second, many questions require\nmulti-hop logic reasoning over the knowledge graph to retrieve the answers. To\naddress these challenges, we propose a novel and unified deep learning\narchitecture, and an end-to-end variational learning algorithm which can handle\nnoise in questions, and learn multi-hop reasoning simultaneously. Our method\nachieves state-of-the-art performance on a recent benchmark dataset in the\nliterature. We also derive a series of new benchmark datasets, including\nquestions for multi-hop reasoning, questions paraphrased by neural translation\nmodel, and questions in human voice. Our method yields very promising results\non all these challenging datasets.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Model Architecture"] },
{"key": "zhang2017visual", "citations": "549", "year": "2017", "title":"Visual Translation Embedding Network For Visual Relation Detection", "abstract": "<p>Visual relations, such as “person ride bike” and “bike next to car”, offer a\ncomprehensive scene understanding of an image, and have already shown their\ngreat utility in connecting computer vision and natural language. However, due\nto the challenging combinatorial complexity of modeling\nsubject-predicate-object relation triplets, very little work has been done to\nlocalize and predict visual relations. Inspired by the recent advances in\nrelational representation learning of knowledge bases and convolutional object\ndetection networks, we propose a Visual Translation Embedding network (VTransE)\nfor visual relation detection. VTransE places objects in a low-dimensional\nrelation space where a relation can be modeled as a simple vector translation,\ni.e., subject + predicate \\(\\approx\\) object. We propose a novel feature\nextraction layer that enables object-relation knowledge transfer in a\nfully-convolutional fashion that supports training and inference in a single\nforward/backward pass. To the best of our knowledge, VTransE is the first\nend-to-end relation detection network. We demonstrate the effectiveness of\nVTransE over other state-of-the-art methods on two large-scale datasets: Visual\nRelationship and Visual Genome. Note that even though VTransE is a purely\nvisual model, it is still competitive to the Lu’s multi-modal model with\nlanguage priors.</p>\n", "tags": ["CVPR","Datasets","Training Techniques"] },
{"key": "zhang2018accelerating", "citations": "116", "year": "2018", "title":"Accelerating Neural Transformer Via An Average Attention Network", "abstract": "<p>With parallelizable attention networks, the neural Transformer is very fast\nto train. However, due to the auto-regressive architecture and self-attention\nin the decoder, the decoding procedure becomes slow. To alleviate this issue,\nwe propose an average attention network as an alternative to the self-attention\nnetwork in the decoder of the neural Transformer. The average attention network\nconsists of two layers, with an average layer that models dependencies on\nprevious positions and a gating layer that is stacked over the average layer to\nenhance the expressiveness of the proposed attention network. We apply this\nnetwork on the decoder part of the neural Transformer to replace the original\ntarget-side self-attention model. With masking tricks and dynamic programming,\nour model enables the neural Transformer to decode sentences over four times\nfaster than its original version with almost no loss in training time and\ntranslation performance. We conduct a series of experiments on WMT17\ntranslation tasks, where on 6 different language pairs, we obtain robust and\nconsistent speed-ups in decoding.</p>\n", "tags": ["Model Architecture","Training Techniques"] },
{"key": "zhang2018asynchronous", "citations": "117", "year": "2018", "title":"Asynchronous Bidirectional Decoding For Neural Machine Translation", "abstract": "<p>The dominant neural machine translation (NMT) models apply unified\nattentional encoder-decoder neural networks for translation. Traditionally, the\nNMT decoders adopt recurrent neural networks (RNNs) to perform translation in a\nleft-toright manner, leaving the target-side contexts generated from right to\nleft unexploited during translation. In this paper, we equip the conventional\nattentional encoder-decoder NMT framework with a backward decoder, in order to\nexplore bidirectional decoding for NMT. Attending to the hidden state sequence\nproduced by the encoder, our backward decoder first learns to generate the\ntarget-side hidden state sequence from right to left. Then, the forward decoder\nperforms translation in the forward direction, while in each translation\nprediction timestep, it simultaneously applies two attention models to consider\nthe source-side and reverse target-side hidden states, respectively. With this\nnew architecture, our model is able to fully exploit source- and target-side\ncontexts to improve translation quality altogether. Experimental results on\nNIST Chinese-English and WMT English-German translation tasks demonstrate that\nour model achieves substantial improvements over the conventional NMT by 3.14\nand 1.38 BLEU points, respectively. The source code of this work can be\nobtained from https://github.com/DeepLearnXMU/ABDNMT.</p>\n", "tags": ["AAAI","Has Code","Model Architecture","Tools"] },
{"key": "zhang2018explainable", "citations": "679", "year": "2020", "title":"Explainable Recommendation: A Survey And New Perspectives", "abstract": "<p>Explainable recommendation attempts to develop models that generate not only\nhigh-quality recommendations but also intuitive explanations. The explanations\nmay either be post-hoc or directly come from an explainable model (also called\ninterpretable or transparent model in some contexts). Explainable\nrecommendation tries to address the problem of why: by providing explanations\nto users or system designers, it helps humans to understand why certain items\nare recommended by the algorithm, where the human can either be users or system\ndesigners. Explainable recommendation helps to improve the transparency,\npersuasiveness, effectiveness, trustworthiness, and satisfaction of\nrecommendation systems. It also facilitates system designers for better system\ndebugging. In recent years, a large number of explainable recommendation\napproaches – especially model-based methods – have been proposed and applied\nin real-world systems.\n  In this survey, we provide a comprehensive review for the explainable\nrecommendation research. We first highlight the position of explainable\nrecommendation in recommender system research by categorizing recommendation\nproblems into the 5W, i.e., what, when, who, where, and why. We then conduct a\ncomprehensive survey of explainable recommendation on three perspectives: 1) We\nprovide a chronological research timeline of explainable recommendation. 2) We\nprovide a two-dimensional taxonomy to classify existing explainable\nrecommendation research. 3) We summarize how explainable recommendation applies\nto different recommendation tasks. We also devote a chapter to discuss the\nexplanation perspectives in broader IR and AI/ML research. We end the survey by\ndiscussing potential future directions to promote the explainable\nrecommendation research area and beyond.</p>\n", "tags": ["Ethics & Fairness","Survey Paper"] },
{"key": "zhang2018generating", "citations": "174", "year": "2018", "title":"Generating Informative And Diverse Conversational Responses Via Adversarial Information Maximization", "abstract": "<p>Responses generated by neural conversational models tend to lack\ninformativeness and diversity. We present Adversarial Information Maximization\n(AIM), an adversarial learning strategy that addresses these two related but\ndistinct problems. To foster response diversity, we leverage adversarial\ntraining that allows distributional matching of synthetic and real responses.\nTo improve informativeness, our framework explicitly optimizes a variational\nlower bound on pairwise mutual information between query and response.\nEmpirical results from automatic and human evaluations demonstrate that our\nmethods significantly boost informativeness and diversity.</p>\n", "tags": ["Dialogue & Multi Turn","Tools","Training Techniques"] },
{"key": "zhang2018guiding", "citations": "117", "year": "2018", "title":"Guiding Neural Machine Translation With Retrieved Translation Pieces", "abstract": "<p>One of the difficulties of neural machine translation (NMT) is the recall and\nappropriate translation of low-frequency words or phrases. In this paper, we\npropose a simple, fast, and effective method for recalling previously seen\ntranslation examples and incorporating them into the NMT decoding process.\nSpecifically, for an input sentence, we use a search engine to retrieve\nsentence pairs whose source sides are similar with the input sentence, and then\ncollect \\(n\\)-grams that are both in the retrieved target sentences and aligned\nwith words that match in the source sentences, which we call “translation\npieces”. We compute pseudo-probabilities for each retrieved sentence based on\nsimilarities between the input sentence and the retrieved source sentences, and\nuse these to weight the retrieved translation pieces. Finally, an existing NMT\nmodel is used to translate the input sentence, with an additional bonus given\nto outputs that contain the collected translation pieces. We show our method\nimproves NMT translation results up to 6 BLEU points on three narrow domain\ntranslation tasks where repetitiveness of the target sentences is particularly\nsalient. It also causes little increase in the translation time, and compares\nfavorably to another alternative retrieval-based method with respect to\naccuracy, speed, and simplicity of implementation.</p>\n", "tags": ["NAACL"] },
{"key": "zhang2018improving", "citations": "262", "year": "2018", "title":"Improving The Transformer Translation Model With Document-level Context", "abstract": "<p>Although the Transformer translation model (Vaswani et al., 2017) has\nachieved state-of-the-art performance in a variety of translation tasks, how to\nuse document-level context to deal with discourse phenomena problematic for\nTransformer still remains a challenge. In this work, we extend the Transformer\nmodel with a new context encoder to represent document-level context, which is\nthen incorporated into the original encoder and decoder. As large-scale\ndocument-level parallel corpora are usually not available, we introduce a\ntwo-step training method to take full advantage of abundant sentence-level\nparallel corpora and limited document-level parallel corpora. Experiments on\nthe NIST Chinese-English datasets and the IWSLT French-English datasets show\nthat our approach improves over Transformer significantly.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture","Training Techniques"] },
{"key": "zhang2018interpretable", "citations": "68", "year": "2019", "title":"Interpretable Visual Question Answering By Visual Grounding From Attention Supervision Mining", "abstract": "<p>A key aspect of VQA models that are interpretable is their ability to ground\ntheir answers to relevant regions in the image. Current approaches with this\ncapability rely on supervised learning and human annotated groundings to train\nattention mechanisms inside the VQA architecture. Unfortunately, obtaining\nhuman annotations specific for visual grounding is difficult and expensive. In\nthis work, we demonstrate that we can effectively train a VQA architecture with\ngrounding supervision that can be automatically obtained from available region\ndescriptions and object annotations. We also show that our model trained with\nthis mined supervision generates visual groundings that achieve a higher\ncorrelation with respect to manually-annotated groundings, meanwhile achieving\nstate-of-the-art VQA accuracy.</p>\n", "tags": ["Applications","Model Architecture"] },
{"key": "zhang2018joint", "citations": "107", "year": "2018", "title":"Joint Training For Neural Machine Translation Models With Monolingual Data", "abstract": "<p>Monolingual data have been demonstrated to be helpful in improving\ntranslation quality of both statistical machine translation (SMT) systems and\nneural machine translation (NMT) systems, especially in resource-poor or domain\nadaptation tasks where parallel data are not rich enough. In this paper, we\npropose a novel approach to better leveraging monolingual data for neural\nmachine translation by jointly learning source-to-target and target-to-source\nNMT models for a language pair with a joint EM optimization method. The\ntraining process starts with two initial NMT models pre-trained on parallel\ndata for each direction, and these two models are iteratively updated by\nincrementally decreasing translation losses on training data. In each iteration\nstep, both NMT models are first used to translate monolingual data from one\nlanguage to the other, forming pseudo-training data of the other NMT model.\nThen two new NMT models are learnt from parallel data together with the pseudo\ntraining data. Both NMT models are expected to be improved and better\npseudo-training data can be generated in next step. Experiment results on\nChinese-English and English-German translation tasks show that our approach can\nsimultaneously improve translation quality of source-to-target and\ntarget-to-source models, significantly outperforming strong baseline systems\nwhich are enhanced with monolingual data for model training including\nback-translation.</p>\n", "tags": ["AAAI","Training Techniques"] },
{"key": "zhang2018learning", "citations": "117", "year": "2018", "title":"Learning Universal Sentence Representations With Mean-max Attention Autoencoder", "abstract": "<p>In order to learn universal sentence representations, previous methods focus\non complex recurrent neural networks or supervised learning. In this paper, we\npropose a mean-max attention autoencoder (mean-max AAE) within the\nencoder-decoder framework. Our autoencoder rely entirely on the MultiHead\nself-attention mechanism to reconstruct the input sequence. In the encoding we\npropose a mean-max strategy that applies both mean and max pooling operations\nover the hidden vectors to capture diverse information of the input. To enable\nthe information to steer the reconstruction process dynamically, the decoder\nperforms attention over the mean-max representation. By training our model on a\nlarge collection of unlabelled data, we obtain high-quality representations of\nsentences. Experimental results on a broad range of 10 transfer tasks\ndemonstrate that our model outperforms the state-of-the-art unsupervised single\nmethods, including the classical skip-thoughts and the advanced\nskip-thoughts+LN model. Furthermore, compared with the traditional recurrent\nneural network, our mean-max AAE greatly reduce the training time.</p>\n", "tags": ["Model Architecture","Tools","Training Techniques"] },
{"key": "zhang2018modeling", "citations": "157", "year": "2018", "title":"Modeling Multi-turn Conversation With Deep Utterance Aggregation", "abstract": "<p>Multi-turn conversation understanding is a major challenge for building\nintelligent dialogue systems. This work focuses on retrieval-based response\nmatching for multi-turn conversation whose related work simply concatenates the\nconversation utterances, ignoring the interactions among previous utterances\nfor context modeling. In this paper, we formulate previous utterances into\ncontext using a proposed deep utterance aggregation model to form a\nfine-grained context representation. In detail, a self-matching attention is\nfirst introduced to route the vital information in each utterance. Then the\nmodel matches a response with each refined utterance and the final matching\nscore is obtained after attentive turns aggregation. Experimental results show\nour model outperforms the state-of-the-art methods on three multi-turn\nconversation benchmarks, including a newly introduced e-commerce dialogue\ncorpus.</p>\n", "tags": ["COLING","Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "zhang2018next", "citations": "70", "year": "2018", "title":"Next Item Recommendation With Self-attention", "abstract": "<p>In this paper, we propose a novel sequence-aware recommendation model. Our\nmodel utilizes self-attention mechanism to infer the item-item relationship\nfrom user’s historical interactions. With self-attention, it is able to\nestimate the relative weights of each item in user interaction trajectories to\nlearn better representations for user’s transient interests. The model is\nfinally trained in a metric learning framework, taking both short-term and\nlong-term intentions into consideration. Experiments on a wide range of\ndatasets on different domains demonstrate that our approach outperforms the\nstate-of-the-art by a wide margin.</p>\n", "tags": ["Datasets","Model Architecture","Tools"] },
{"key": "zhang2018record", "citations": "238", "year": "2018", "title":"Record: Bridging The Gap Between Human And Machine Commonsense Reading Comprehension", "abstract": "<p>We present a large-scale dataset, ReCoRD, for machine reading comprehension\nrequiring commonsense reasoning. Experiments on this dataset demonstrate that\nthe performance of state-of-the-art MRC systems fall far behind human\nperformance. ReCoRD represents a challenge for future research to bridge the\ngap between human and machine commonsense reading comprehension. ReCoRD is\navailable at http://nlp.jhu.edu/record.</p>\n", "tags": ["Datasets"] },
{"key": "zhang2018regularizing", "citations": "64", "year": "2019", "title":"Regularizing Neural Machine Translation By Target-bidirectional Agreement", "abstract": "<p>Although Neural Machine Translation (NMT) has achieved remarkable progress in\nthe past several years, most NMT systems still suffer from a fundamental\nshortcoming as in other sequence generation tasks: errors made early in\ngeneration process are fed as inputs to the model and can be quickly amplified,\nharming subsequent sequence generation. To address this issue, we propose a\nnovel model regularization method for NMT training, which aims to improve the\nagreement between translations generated by left-to-right (L2R) and\nright-to-left (R2L) NMT decoders. This goal is achieved by introducing two\nKullback-Leibler divergence regularization terms into the NMT training\nobjective to reduce the mismatch between output probabilities of L2R and R2L\nmodels. In addition, we also employ a joint training strategy to allow L2R and\nR2L models to improve each other in an interactive update process. Experimental\nresults show that our proposed method significantly outperforms\nstate-of-the-art baselines on Chinese-English and English-German translation\ntasks.</p>\n", "tags": ["AAAI","Training Techniques"] },
{"key": "zhang2019addressing", "citations": "113", "year": "2019", "title":"Addressing Semantic Drift In Question Generation For Semi-supervised Question Answering", "abstract": "<p>Text-based Question Generation (QG) aims at generating natural and relevant\nquestions that can be answered by a given answer in some context. Existing QG\nmodels suffer from a “semantic drift” problem, i.e., the semantics of the\nmodel-generated question drifts away from the given context and answer. In this\npaper, we first propose two semantics-enhanced rewards obtained from downstream\nquestion paraphrasing and question answering tasks to regularize the QG model\nto generate semantically valid questions. Second, since the traditional\nevaluation metrics (e.g., BLEU) often fall short in evaluating the quality of\ngenerated questions, we propose a QA-based evaluation method which measures the\nQG model’s ability to mimic human annotators in generating QA training data.\nExperiments show that our method achieves the new state-of-the-art performance\nw.r.t. traditional metrics, and also performs best on our QA-based evaluation\nmetrics. Further, we investigate how to use our QG model to augment QA datasets\nand enable semi-supervised QA. We propose two ways to generate synthetic QA\npairs: generate new questions from existing articles or collect QA pairs from\nnew articles. We also propose two empirically effective strategies, a data\nfilter and mixing mini-batch training, to properly use the QG-generated data\nfor QA. Experiments show that our method improves over both BiDAF and BERT QA\nbaselines, even without introducing new articles.</p>\n", "tags": ["Datasets","EMNLP","Evaluation","Model Architecture","Training Techniques"] },
{"key": "zhang2019amr", "citations": "114", "year": "2019", "title":"AMR Parsing As Sequence-to-graph Transduction", "abstract": "<p>We propose an attention-based model that treats AMR parsing as\nsequence-to-graph transduction. Unlike most AMR parsers that rely on\npre-trained aligners, external semantic resources, or data augmentation, our\nproposed parser is aligner-free, and it can be effectively trained with limited\namounts of labeled AMR data. Our experimental results outperform all previously\nreported SMATCH scores, on both AMR 2.0 (76.3% F1 on LDC2017T10) and AMR 1.0\n(70.2% F1 on LDC2014T12).</p>\n", "tags": ["Model Architecture"] },
{"key": "zhang2019bertscore", "citations": "1961", "year": "2019", "title":"Bertscore: Evaluating Text Generation With BERT", "abstract": "<p>We propose BERTScore, an automatic evaluation metric for text generation.\nAnalogously to common metrics, BERTScore computes a similarity score for each\ntoken in the candidate sentence with each token in the reference sentence.\nHowever, instead of exact matches, we compute token similarity using contextual\nembeddings. We evaluate using the outputs of 363 machine translation and image\ncaptioning systems. BERTScore correlates better with human judgments and\nprovides stronger model selection performance than existing metrics. Finally,\nwe use an adversarial paraphrase detection task to show that BERTScore is more\nrobust to challenging examples when compared to existing metrics.</p>\n", "tags": ["Evaluation","Model Architecture"] },
{"key": "zhang2019bridging", "citations": "213", "year": "2019", "title":"Bridging The Gap Between Training And Inference For Neural Machine Translation", "abstract": "<p>Neural Machine Translation (NMT) generates target words sequentially in the\nway of predicting the next word conditioned on the context words. At training\ntime, it predicts with the ground truth words as context while at inference it\nhas to generate the entire sequence from scratch. This discrepancy of the fed\ncontext leads to error accumulation among the way. Furthermore, word-level\ntraining requires strict matching between the generated sequence and the ground\ntruth sequence which leads to overcorrection over different but reasonable\ntranslations. In this paper, we address these issues by sampling context words\nnot only from the ground truth sequence but also from the predicted sequence by\nthe model during training, where the predicted sequence is selected with a\nsentence-level optimum. Experiment results on Chinese-&gt;English and WMT’14\nEnglish-&gt;German translation tasks demonstrate that our approach can achieve\nsignificant improvements on multiple datasets.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "zhang2019broad", "citations": "83", "year": "2019", "title":"Broad-coverage Semantic Parsing As Transduction", "abstract": "<p>We unify different broad-coverage semantic parsing tasks under a transduction\nparadigm, and propose an attention-based neural framework that incrementally\nbuilds a meaning representation via a sequence of semantic relations. By\nleveraging multiple attention mechanisms, the transducer can be effectively\ntrained without relying on a pre-trained aligner. Experiments conducted on\nthree separate broad-coverage semantic parsing tasks – AMR, SDP and UCCA –\ndemonstrate that our attention-based neural transducer improves the state of\nthe art on both AMR and UCCA, and is competitive with the state of the art on\nSDP.</p>\n", "tags": ["EMNLP","Model Architecture","Tools"] },
{"key": "zhang2019curriculum", "citations": "120", "year": "2019", "title":"Curriculum Learning For Domain Adaptation In Neural Machine Translation", "abstract": "<p>We introduce a curriculum learning approach to adapt generic neural machine\ntranslation models to a specific domain. Samples are grouped by their\nsimilarities to the domain of interest and each group is fed to the training\nalgorithm with a particular schedule. This approach is simple to implement on\ntop of any neural framework or architecture, and consistently outperforms both\nunadapted and adapted baselines in experiments with two distinct domains and\ntwo language pairs.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Tools","Training Techniques"] },
{"key": "zhang2019dialogpt", "citations": "997", "year": "2020", "title":"Dialogpt: Large-scale Generative Pre-training For Conversational Response Generation", "abstract": "<p>We present a large, tunable neural conversational response generation model,\nDialoGPT (dialogue generative pre-trained transformer). Trained on 147M\nconversation-like exchanges extracted from Reddit comment chains over a period\nspanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch\ntransformer to attain a performance close to human both in terms of automatic\nand human evaluation in single-turn dialogue settings. We show that\nconversational systems that leverage DialoGPT generate more relevant,\ncontentful and context-consistent responses than strong baseline systems. The\npre-trained model and training pipeline are publicly released to facilitate\nresearch into neural response generation and the development of more\nintelligent open-domain dialogue systems.</p>\n", "tags": ["Dialogue & Multi Turn","Evaluation","Model Architecture","Training Techniques"] },
{"key": "zhang2019dual", "citations": "96", "year": "2020", "title":"Dual Co-matching Network For Multi-choice Reading Comprehension", "abstract": "<p>Multi-choice reading comprehension is a challenging task that requires\ncomplex reasoning procedure. Given passage and question, a correct answer need\nto be selected from a set of candidate answers. In this paper, we propose\n\\textbf{D}ual \\textbf{C}o-\\textbf{M}atching \\textbf{N}etwork (\\textbf{DCMN})\nwhich model the relationship among passage, question and answer\nbidirectionally. Different from existing approaches which only calculate\nquestion-aware or option-aware passage representation, we calculate\npassage-aware question representation and passage-aware answer representation\nat the same time. To demonstrate the effectiveness of our model, we evaluate\nour model on a large-scale multiple choice machine reading comprehension\ndataset (i.e. RACE). Experimental result show that our proposed model achieves\nnew state-of-the-art results.</p>\n", "tags": ["AAAI","Datasets"] },
{"key": "zhang2019editing", "citations": "114", "year": "2019", "title":"Editing-based SQL Query Generation For Cross-domain Context-dependent Questions", "abstract": "<p>We focus on the cross-domain context-dependent text-to-SQL generation task.\nBased on the observation that adjacent natural language questions are often\nlinguistically dependent and their corresponding SQL queries tend to overlap,\nwe utilize the interaction history by editing the previous predicted query to\nimprove the generation quality. Our editing mechanism views SQL as sequences\nand reuses generation results at the token level in a simple manner. It is\nflexible to change individual tokens and robust to error propagation.\nFurthermore, to deal with complex table structures in different domains, we\nemploy an utterance-table encoder and a table-aware decoder to incorporate the\ncontext of the user utterance and the table schema. We evaluate our approach on\nthe SParC dataset and demonstrate the benefit of editing compared with the\nstate-of-the-art baselines which generate SQL from scratch. Our code is\navailable at https://github.com/ryanzhumich/sparc_atis_pytorch.</p>\n", "tags": ["Datasets","EMNLP","Has Code"] },
{"key": "zhang2019effect", "citations": "73", "year": "2019", "title":"The Effect Of Translationese In Machine Translation Test Sets", "abstract": "<p>The effect of translationese has been studied in the field of machine\ntranslation (MT), mostly with respect to training data. We study in depth the\neffect of translationese on test data, using the test sets from the last three\neditions of WMT’s news shared task, containing 17 translation directions. We\nshow evidence that (i) the use of translationese in test sets results in\ninflated human evaluation scores for MT systems; (ii) in some cases system\nrankings do change and (iii) the impact translationese has on a translation\ndirection is inversely correlated to the translation quality attainable by\nstate-of-the-art MT systems for that direction.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "zhang2019ernie", "citations": "1308", "year": "2019", "title":"ERNIE: Enhanced Language Representation With Informative Entities", "abstract": "<p>Neural language representation models such as BERT pre-trained on large-scale\ncorpora can well capture rich semantic patterns from plain text, and be\nfine-tuned to consistently improve the performance of various NLP tasks.\nHowever, the existing pre-trained language models rarely consider incorporating\nknowledge graphs (KGs), which can provide rich structured knowledge facts for\nbetter language understanding. We argue that informative entities in KGs can\nenhance language representation with external knowledge. In this paper, we\nutilize both large-scale textual corpora and KGs to train an enhanced language\nrepresentation model (ERNIE), which can take full advantage of lexical,\nsyntactic, and knowledge information simultaneously. The experimental results\nhave demonstrated that ERNIE achieves significant improvements on various\nknowledge-driven tasks, and meanwhile is comparable with the state-of-the-art\nmodel BERT on other common NLP tasks. The source code of this paper can be\nobtained from https://github.com/thunlp/ERNIE.</p>\n", "tags": ["Has Code","Model Architecture"] },
{"key": "zhang2019find", "citations": "104", "year": "2019", "title":"Find Or Classify? Dual Strategy For Slot-value Predictions On Multi-domain Dialog State Tracking", "abstract": "<p>Dialog state tracking (DST) is a core component in task-oriented dialog\nsystems. Existing approaches for DST mainly fall into one of two categories,\nnamely, ontology-based and ontology-free methods. An ontology-based method\nselects a value from a candidate-value list for each target slot, while an\nontology-free method extracts spans from dialog contexts. Recent work\nintroduced a BERT-based model to strike a balance between the two methods by\npre-defining categorical and non-categorical slots. However, it is not clear\nenough which slots are better handled by either of the two slot types, and the\nway to use the pre-trained model has not been well investigated. In this paper,\nwe propose a simple yet effective dual-strategy model for DST, by adapting a\nsingle BERT-style reading comprehension model to jointly handle both the\ncategorical and non-categorical slots. Our experiments on the MultiWOZ datasets\nshow that our method significantly outperforms the BERT-based counterpart,\nfinding that the key is a deep interaction between the domain-slot and context\ninformation. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1)\nsettings, our method performs competitively and robustly across the two\ndifferent settings. Our method sets the new state of the art in the noisy\nsetting, while performing more robustly than the best model in the cleaner\nsetting. We also conduct a comprehensive error analysis on the dataset,\nincluding the effects of the dual strategy for each slot, to facilitate future\nresearch.</p>\n", "tags": ["Datasets","Model Architecture"] },
{"key": "zhang2019generating", "citations": "103", "year": "2019", "title":"Generating Adequate Distractors For Multiple-choice Questions", "abstract": "<p>This paper presents a novel approach to automatic generation of adequate\ndistractors for a given question-answer pair (QAP) generated from a given\narticle to form an adequate multiple-choice question (MCQ). Our method is a\ncombination of part-of-speech tagging, named-entity tagging, semantic-role\nlabeling, regular expressions, domain knowledge bases, word embeddings, word\nedit distance, WordNet, and other algorithms. We use the US SAT (Scholastic\nAssessment Test) practice reading tests as a dataset to produce QAPs and\ngenerate three distractors for each QAP to form an MCQ. We show that, via\nexperiments and evaluations by human judges, each MCQ has at least one adequate\ndistractor and 84% of MCQs have three adequate distractors.</p>\n", "tags": ["Datasets"] },
{"key": "zhang2019grounded", "citations": "123", "year": "2020", "title":"Grounded Conversation Generation As Guided Traverses In Commonsense Knowledge Graphs", "abstract": "<p>Human conversations naturally evolve around related concepts and scatter to\nmulti-hop concepts. This paper presents a new conversation generation model,\nConceptFlow, which leverages commonsense knowledge graphs to explicitly model\nconversation flows. By grounding conversations to the concept space,\nConceptFlow represents the potential conversation flow as traverses in the\nconcept space along commonsense relations. The traverse is guided by graph\nattentions in the concept graph, moving towards more meaningful directions in\nthe concept space, in order to generate more semantic and informative\nresponses. Experiments on Reddit conversations demonstrate ConceptFlow’s\neffectiveness over previous knowledge-aware conversation models and GPT-2 based\nmodels while using 70% fewer parameters, confirming the advantage of explicit\nmodeling conversation structures. All source codes of this work are available\nat https://github.com/thunlp/ConceptFlow.</p>\n", "tags": ["Dialogue & Multi Turn","Has Code","Model Architecture"] },
{"key": "zhang2019learning", "citations": "149", "year": "2019", "title":"Learning To Order Sub-questions For Complex Question Answering", "abstract": "<p>Answering complex questions involving multiple entities and relations is a\nchallenging task. Logically, the answer to a complex question should be derived\nby decomposing the complex question into multiple simple sub-questions and then\nanswering those sub-questions. Existing work has followed this strategy but has\nnot attempted to optimize the order of how those sub-questions are answered. As\na result, the sub-questions are answered in an arbitrary order, leading to\nlarger search space and a higher risk of missing an answer. In this paper, we\npropose a novel reinforcement learning(RL) approach to answering complex\nquestions that can learn a policy to dynamically decide which sub-question\nshould be answered at each stage of reasoning. We lever-age the expected\nvalue-variance criterion to enable the learned policy to balance between the\nrisk and utility of answering a sub-question. Experiment results show that the\nRL approach can substantially improve the optimality of ordering the\nsub-questions, leading to improved accuracy of question answering. The proposed\nmethod for learning to order sub-questions is general and can thus be\npotentially combined with many existing ideas for answering complex questions\nto enhance their performance.</p>\n", "tags": ["Agentic","INTERSPEECH","Reinforcement Learning"] },
{"key": "zhang2019paws", "citations": "136", "year": "2019", "title":"PAWS: Paraphrase Adversaries From Word Scrambling", "abstract": "<p>Existing paraphrase identification datasets lack sentence pairs that have\nhigh lexical overlap without being paraphrases. Models trained on such data\nfail to distinguish pairs like flights from New York to Florida and flights\nfrom Florida to New York. This paper introduces PAWS (Paraphrase Adversaries\nfrom Word Scrambling), a new dataset with 108,463 well-formed paraphrase and\nnon-paraphrase pairs with high lexical overlap. Challenging pairs are generated\nby controlled word swapping and back translation, followed by fluency and\nparaphrase judgments by human raters. State-of-the-art models trained on\nexisting datasets have dismal performance on PAWS (&lt;40% accuracy); however,\nincluding PAWS training data for these models improves their accuracy to 85%\nwhile maintaining performance on existing tasks. In contrast, models that do\nnot capture non-local contextual information fail even with PAWS training\nexamples. As such, PAWS provides an effective instrument for driving further\nprogress on models that better exploit structure, context, and pairwise\ncomparisons.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "zhang2019pegasus", "citations": "892", "year": "2019", "title":"PEGASUS: Pre-training With Extracted Gap-sentences For Abstractive Summarization", "abstract": "<p>Recent work pre-training Transformers with self-supervised objectives on\nlarge text corpora has shown great success when fine-tuned on downstream NLP\ntasks including text summarization. However, pre-training objectives tailored\nfor abstractive text summarization have not been explored. Furthermore there is\na lack of systematic evaluation across diverse domains. In this work, we\npropose pre-training large Transformer-based encoder-decoder models on massive\ntext corpora with a new self-supervised objective. In PEGASUS, important\nsentences are removed/masked from an input document and are generated together\nas one output sequence from the remaining sentences, similar to an extractive\nsummary. We evaluated our best PEGASUS model on 12 downstream summarization\ntasks spanning news, science, stories, instructions, emails, patents, and\nlegislative bills. Experiments demonstrate it achieves state-of-the-art\nperformance on all 12 downstream datasets measured by ROUGE scores. Our model\nalso shows surprising performance on low-resource summarization, surpassing\nprevious state-of-the-art results on 6 datasets with only 1000 examples.\nFinally we validated our results using human evaluation and show that our model\nsummaries achieve human performance on multiple datasets.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Training Techniques"] },
{"key": "zhang2019pretraining", "citations": "183", "year": "2019", "title":"Pretraining-based Natural Language Generation For Text Summarization", "abstract": "<p>In this paper, we propose a novel pretraining-based encoder-decoder\nframework, which can generate the output sequence based on the input sequence\nin a two-stage manner. For the encoder of our model, we encode the input\nsequence into context representations using BERT. For the decoder, there are\ntwo stages in our model, in the first stage, we use a Transformer-based decoder\nto generate a draft output sequence. In the second stage, we mask each word of\nthe draft sequence and feed it to BERT, then by combining the input sequence\nand the draft representation generated by BERT, we use a Transformer-based\ndecoder to predict the refined word for each masked position. To the best of\nour knowledge, our approach is the first method which applies the BERT into\ntext generation tasks. As the first step in this direction, we evaluate our\nproposed method on the text summarization task. Experimental results show that\nour model achieves new state-of-the-art on both CNN/Daily Mail and New York\nTimes datasets.</p>\n", "tags": ["Datasets","Model Architecture","Tools"] },
{"key": "zhang2019raven", "citations": "157", "year": "2019", "title":"RAVEN: A Dataset For Relational And Analogical Visual Reasoning", "abstract": "<p>Dramatic progress has been witnessed in basic vision tasks involving\nlow-level perception, such as object recognition, detection, and tracking.\nUnfortunately, there is still an enormous performance gap between artificial\nvision systems and human intelligence in terms of higher-level vision problems,\nespecially ones involving reasoning. Earlier attempts in equipping machines\nwith high-level reasoning have hovered around Visual Question Answering (VQA),\none typical task associating vision and language understanding. In this work,\nwe propose a new dataset, built in the context of Raven’s Progressive Matrices\n(RPM) and aimed at lifting machine intelligence by associating vision with\nstructural, relational, and analogical reasoning in a hierarchical\nrepresentation. Unlike previous works in measuring abstract reasoning using\nRPM, we establish a semantic link between vision and reasoning by providing\nstructure representation. This addition enables a new type of abstract\nreasoning by jointly operating on the structure representation. Machine\nreasoning ability using modern computer vision is evaluated in this newly\nproposed dataset. Additionally, we also provide human performance as a\nreference. Finally, we show consistent improvement across all models by\nincorporating a simple neural module that combines visual understanding and\nstructure reasoning.</p>\n", "tags": ["CVPR","Datasets"] },
{"key": "zhang2019recosa", "citations": "120", "year": "2019", "title":"Recosa: Detecting The Relevant Contexts With Self-attention For Multi-turn Dialogue Generation", "abstract": "<p>In multi-turn dialogue generation, response is usually related with only a\nfew contexts. Therefore, an ideal model should be able to detect these relevant\ncontexts and produce a suitable response accordingly. However, the widely used\nhierarchical recurrent encoderdecoder models just treat all the contexts\nindiscriminately, which may hurt the following response generation process.\nSome researchers try to use the cosine similarity or the traditional attention\nmechanism to find the relevant contexts, but they suffer from either\ninsufficient relevance assumption or position bias problem. In this paper, we\npropose a new model, named ReCoSa, to tackle this problem. Firstly, a word\nlevel LSTM encoder is conducted to obtain the initial representation of each\ncontext. Then, the self-attention mechanism is utilized to update both the\ncontext and masked response representation. Finally, the attention weights\nbetween each context and response representations are computed and used in the\nfurther decoding process. Experimental results on both Chinese customer\nservices dataset and English Ubuntu dialogue dataset show that ReCoSa\nsignificantly outperforms baseline models, in terms of both metric-based and\nhuman evaluations. Further analysis on attention shows that the detected\nrelevant contexts by ReCoSa are highly coherent with human’s understanding,\nvalidating the correctness and interpretability of ReCoSa.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "zhang2019semantics", "citations": "319", "year": "2020", "title":"Semantics-aware BERT For Language Understanding", "abstract": "<p>The latest work on language representations carefully integrates\ncontextualized features into language model training, which enables a series of\nsuccess especially in various machine reading comprehension and natural\nlanguage inference tasks. However, the existing language representation models\nincluding ELMo, GPT and BERT only exploit plain context-sensitive features such\nas character or word embeddings. They rarely consider incorporating structured\nsemantic information which can provide rich semantics for language\nrepresentation. To promote natural language understanding, we propose to\nincorporate explicit contextual semantics from pre-trained semantic role\nlabeling, and introduce an improved language representation model,\nSemantics-aware BERT (SemBERT), which is capable of explicitly absorbing\ncontextual semantics over a BERT backbone. SemBERT keeps the convenient\nusability of its BERT precursor in a light fine-tuning way without substantial\ntask-specific modifications. Compared with BERT, semantics-aware BERT is as\nsimple in concept but more powerful. It obtains new state-of-the-art or\nsubstantially improves results on ten reading comprehension and language\ninference tasks.</p>\n", "tags": ["AAAI","Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "zhang2019sg", "citations": "195", "year": "2020", "title":"Sg-net: Syntax-guided Machine Reading Comprehension", "abstract": "<p>For machine reading comprehension, the capacity of effectively modeling the\nlinguistic knowledge from the detail-riddled and lengthy passages and getting\nride of the noises is essential to improve its performance. Traditional\nattentive models attend to all words without explicit constraint, which results\nin inaccurate concentration on some dispensable words. In this work, we propose\nusing syntax to guide the text modeling by incorporating explicit syntactic\nconstraints into attention mechanism for better linguistically motivated word\nrepresentations. In detail, for self-attention network (SAN) sponsored\nTransformer-based encoder, we introduce syntactic dependency of interest (SDOI)\ndesign into the SAN to form an SDOI-SAN with syntax-guided self-attention.\nSyntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the\nSAN from the original Transformer encoder through a dual contextual\narchitecture for better linguistics inspired representation. To verify its\neffectiveness, the proposed SG-Net is applied to typical pre-trained language\nmodel BERT which is right based on a Transformer encoder. Extensive experiments\non popular benchmarks including SQuAD 2.0 and RACE show that the proposed\nSG-Net design helps achieve substantial performance improvement over strong\nbaselines.</p>\n", "tags": ["AAAI","Model Architecture"] },
{"key": "zhang2019task", "citations": "129", "year": "2020", "title":"Task-oriented Dialog Systems That Consider Multiple Appropriate Responses Under The Same Context", "abstract": "<p>Conversations have an intrinsic one-to-many property, which means that\nmultiple responses can be appropriate for the same dialog context. In\ntask-oriented dialogs, this property leads to different valid dialog policies\ntowards task completion. However, none of the existing task-oriented dialog\ngeneration approaches takes this property into account. We propose a\nMulti-Action Data Augmentation (MADA) framework to utilize the one-to-many\nproperty to generate diverse appropriate dialog responses. Specifically, we\nfirst use dialog states to summarize the dialog history, and then discover all\npossible mappings from every dialog state to its different valid system\nactions. During dialog system training, we enable the current dialog state to\nmap to all valid system actions discovered in the previous process to create\nadditional state-action pairs. By incorporating these additional pairs, the\ndialog policy learns a balanced action distribution, which further guides the\ndialog model to generate diverse responses. Experimental results show that the\nproposed framework consistently improves dialog policy diversity, and results\nin improved response diversity and appropriateness. Our model obtains\nstate-of-the-art results on MultiWOZ.</p>\n", "tags": ["AAAI","Dialogue & Multi Turn","Training Techniques"] },
{"key": "zhang2020cpm", "citations": "85", "year": "2021", "title":"CPM: A Large-scale Generative Chinese Pre-trained Language Model", "abstract": "<p>Pre-trained Language Models (PLMs) have proven to be beneficial for various\ndownstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB\ntraining data, drew a lot of attention due to the capacity of few-shot (even\nzero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is\nstill challenging, as the training corpus of GPT-3 is primarily English, and\nthe parameters are not publicly available. In this technical report, we release\nthe Chinese Pre-trained Language Model (CPM) with generative pre-training on\nlarge-scale Chinese training data. To the best of our knowledge, CPM, with 2.6\nbillion parameters and 100GB Chinese training data, is the largest Chinese\npre-trained language model, which could facilitate several downstream Chinese\nNLP tasks, such as conversation, essay generation, cloze test, and language\nunderstanding. Extensive experiments demonstrate that CPM achieves strong\nperformance on many NLP tasks in the settings of few-shot (even zero-shot)\nlearning. The code and parameters are available at\nhttps://github.com/TsinghuaAI/CPM-Generate.</p>\n", "tags": ["Datasets","Few-Shot","Has Code","Model Architecture","Training Techniques"] },
{"key": "zhang2020discriminative", "citations": "61", "year": "2020", "title":"Discriminative Nearest Neighbor Few-shot Intent Detection By Transferring Natural Language Inference", "abstract": "<p>Intent detection is one of the core components of goal-oriented dialog\nsystems, and detecting out-of-scope (OOS) intents is also a practically\nimportant skill. Few-shot learning is attracting much attention to mitigate\ndata scarcity, but OOS detection becomes even more challenging. In this paper,\nwe present a simple yet effective approach, discriminative nearest neighbor\nclassification with deep self-attention. Unlike softmax classifiers, we\nleverage BERT-style pairwise encoding to train a binary classifier that\nestimates the best matched training example for a user input. We propose to\nboost the discriminative ability by transferring a natural language inference\n(NLI) model. Our extensive experiments on a large-scale multi-domain intent\ndetection task show that our method achieves more stable and accurate in-domain\nand OOS detection accuracy than RoBERTa-based classifiers and embedding-based\nnearest neighbor approaches. More notably, the NLI transfer enables our 10-shot\nmodel to perform competitively with 50-shot or even full-shot classifiers,\nwhile we can keep the inference time constant by leveraging a faster embedding\nretrieval model.</p>\n", "tags": ["EMNLP","Few-Shot","Model Architecture","Training Techniques"] },
{"key": "zhang2020evaluating", "citations": "85", "year": "2020", "title":"Evaluating Conversational Recommender Systems Via User Simulation", "abstract": "<p>Conversational information access is an emerging research area. Currently,\nhuman evaluation is used for end-to-end system evaluation, which is both very\ntime and resource intensive at scale, and thus becomes a bottleneck of\nprogress. As an alternative, we propose automated evaluation by means of\nsimulating users. Our user simulator aims to generate responses that a real\nhuman would give by considering both individual preferences and the general\nflow of interaction with the system. We evaluate our simulation approach on an\nitem recommendation task by comparing three existing conversational recommender\nsystems. We show that preference modeling and task-specific interaction models\nboth contribute to more realistic simulations, and can help achieve high\ncorrelation between automatic evaluation measures and manual human assessments.</p>\n", "tags": ["Evaluation","KDD"] },
{"key": "zhang2020generating", "citations": "103", "year": "2019", "title":"Generating Fluent Adversarial Examples For Natural Languages", "abstract": "<p>Efficiently building an adversarial attacker for natural language processing\n(NLP) tasks is a real challenge. Firstly, as the sentence space is discrete, it\nis difficult to make small perturbations along the direction of gradients.\nSecondly, the fluency of the generated examples cannot be guaranteed. In this\npaper, we propose MHA, which addresses both problems by performing\nMetropolis-Hastings sampling, whose proposal is designed with the guidance of\ngradients. Experiments on IMDB and SNLI show that our proposed MHA outperforms\nthe baseline model on attacking capability. Adversarial training with MAH also\nleads to better robustness and performance.</p>\n", "tags": ["Security","Training Techniques"] },
{"key": "zhang2020improving", "citations": "201", "year": "2020", "title":"Improving Adversarial Text Generation By Modeling The Distant Future", "abstract": "<p>Auto-regressive text generation models usually focus on local fluency, and\nmay cause inconsistent semantic meaning in long text generation. Further,\nautomatically generating words with similar semantics is challenging, and\nhand-crafted linguistic rules are difficult to apply. We consider a text\nplanning scheme and present a model-based imitation-learning approach to\nalleviate the aforementioned issues. Specifically, we propose a novel guider\nnetwork to focus on the generative process over a longer horizon, which can\nassist next-word prediction and provide intermediate rewards for generator\noptimization. Extensive experiments demonstrate that the proposed method leads\nto improved performance.</p>\n", "tags": ["Efficiency","Security"] },
{"key": "zhang2020model", "citations": "77", "year": "2021", "title":"A Model Of Two Tales: Dual Transfer Learning Framework For Improved Long-tail Item Recommendation", "abstract": "<p>Highly skewed long-tail item distribution is very common in recommendation\nsystems. It significantly hurts model performance on tail items. To improve\ntail-item recommendation, we conduct research to transfer knowledge from head\nitems to tail items, leveraging the rich user feedback in head items and the\nsemantic connections between head and tail items. Specifically, we propose a\nnovel dual transfer learning framework that jointly learns the knowledge\ntransfer from both model-level and item-level: 1. The model-level knowledge\ntransfer builds a generic meta-mapping of model parameters from few-shot to\nmany-shot model. It captures the implicit data augmentation on the model-level\nto improve the representation learning of tail items. 2. The item-level\ntransfer connects head and tail items through item-level features, to ensure a\nsmooth transfer of meta-mapping from head items to tail items. The two types of\ntransfers are incorporated to ensure the learned knowledge from head items can\nbe well applied for tail item representation learning in the long-tail\ndistribution settings. Through extensive experiments on two benchmark datasets,\nresults show that our proposed dual transfer learning framework significantly\noutperforms other state-of-the-art methods for tail item recommendation in hit\nratio and NDCG. It is also very encouraging that our framework further improves\nhead items and overall performance on top of the gains on tail items.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Fine-Tuning","Tools"] },
{"key": "zhang2020pointer", "citations": "66", "year": "2020", "title":"POINTER: Constrained Progressive Text Generation Via Insertion-based Generative Pre-training", "abstract": "<p>Large-scale pre-trained language models, such as BERT and GPT-2, have\nachieved excellent performance in language representation learning and\nfree-form text generation. However, these models cannot be directly employed to\ngenerate text under specified lexical constraints. To address this challenge,\nwe present POINTER (PrOgressive INsertion-based TransformER), a simple yet\nnovel insertion-based approach for hard-constrained text generation. The\nproposed method operates by progressively inserting new tokens between existing\ntokens in a parallel manner. This procedure is recursively applied until a\nsequence is completed. The resulting coarse-to-fine hierarchy makes the\ngeneration process intuitive and interpretable. We pre-train our model with the\nproposed progressive insertion-based objective on a 12GB Wikipedia dataset, and\nfine-tune it on downstream hard-constrained generation tasks.\nNon-autoregressive decoding yields an empirically logarithmic time complexity\nduring inference time. Experimental results on both News and Yelp datasets\ndemonstrate that POINTER achieves state-of-the-art performance on constrained\ntext generation. We released the pre-trained models and the source code to\nfacilitate future research (https://github.com/dreasysnail/POINTER).</p>\n", "tags": ["Datasets","EMNLP","Has Code","Model Architecture","Training Techniques"] },
{"key": "zhang2020relex", "citations": "65", "year": "2021", "title":"Relex: A Model-agnostic Relational Model Explainer", "abstract": "<p>In recent years, considerable progress has been made on improving the\ninterpretability of machine learning models. This is essential, as complex deep\nlearning models with millions of parameters produce state of the art results,\nbut it can be nearly impossible to explain their predictions. While various\nexplainability techniques have achieved impressive results, nearly all of them\nassume each data instance to be independent and identically distributed (iid).\nThis excludes relational models, such as Statistical Relational Learning (SRL),\nand the recently popular Graph Neural Networks (GNNs), resulting in few options\nto explain them. While there does exist one work on explaining GNNs,\nGNN-Explainer, they assume access to the gradients of the model to learn\nexplanations, which is restrictive in terms of its applicability across\nnon-differentiable relational models and practicality. In this work, we develop\nRelEx, a model-agnostic relational explainer to explain black-box relational\nmodels with only access to the outputs of the black-box. RelEx is able to\nexplain any relational model, including SRL models and GNNs. We compare RelEx\nto the state-of-the-art relational explainer, GNN-Explainer, and relational\nextensions of iid explanation models and show that RelEx achieves comparable or\nbetter performance, while remaining model-agnostic.</p>\n", "tags": ["AAAI","Ethics & Fairness"] },
{"key": "zhang2020retrospective", "citations": "187", "year": "2021", "title":"Retrospective Reader For Machine Reading Comprehension", "abstract": "<p>Machine reading comprehension (MRC) is an AI challenge that requires machine\nto determine the correct answers to questions based on a given passage. MRC\nsystems must not only answer question when necessary but also distinguish when\nno answer is available according to the given passage and then tactfully\nabstain from answering. When unanswerable questions are involved in the MRC\ntask, an essential verification module called verifier is especially required\nin addition to the encoder, though the latest practice on MRC modeling still\nmost benefits from adopting well pre-trained language models as the encoder\nblock by only focusing on the “reading”. This paper devotes itself to exploring\nbetter verifier design for the MRC task with unanswerable questions. Inspired\nby how humans solve reading comprehension questions, we proposed a\nretrospective reader (Retro-Reader) that integrates two stages of reading and\nverification strategies: 1) sketchy reading that briefly investigates the\noverall interactions of passage and question, and yield an initial judgment; 2)\nintensive reading that verifies the answer and gives the final prediction. The\nproposed reader is evaluated on two benchmark MRC challenge datasets SQuAD2.0\nand NewsQA, achieving new state-of-the-art results. Significance tests show\nthat our model is significantly better than the strong ELECTRA and ALBERT\nbaselines. A series of analysis is also conducted to interpret the\neffectiveness of the proposed reader.</p>\n", "tags": ["AAAI","Datasets","Evaluation"] },
{"key": "zhang2020revisiting", "citations": "188", "year": "2021", "title":"Revisiting Few-sample BERT Fine-tuning", "abstract": "<p>This paper is a study of fine-tuning of BERT contextual representations, with\nfocus on commonly observed instabilities in few-sample scenarios. We identify\nseveral factors that cause this instability: the common use of a non-standard\noptimization method with biased gradient estimation; the limited applicability\nof significant parts of the BERT network for down-stream tasks; and the\nprevalent practice of using a pre-determined, and small number of training\niterations. We empirically test the impact of these factors, and identify\nalternative practices that resolve the commonly observed instability of the\nprocess. In light of these observations, we re-visit recently proposed methods\nto improve few-sample fine-tuning with BERT and re-evaluate their\neffectiveness. Generally, we observe the impact of these methods diminishes\nsignificantly with our modified process.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "zhang2020ternarybert", "citations": "135", "year": "2020", "title":"Ternarybert: Distillation-aware Ultra-low Bit BERT", "abstract": "<p>Transformer-based pre-training models like BERT have achieved remarkable\nperformance in many natural language processing tasks.However, these models are\nboth computation and memory expensive, hindering their deployment to\nresource-constrained devices. In this work, we propose TernaryBERT, which\nternarizes the weights in a fine-tuned BERT model. Specifically, we use both\napproximation-based and loss-aware ternarization methods and empirically\ninvestigate the ternarization granularity of different parts of BERT. Moreover,\nto reduce the accuracy degradation caused by the lower capacity of low bits, we\nleverage the knowledge distillation technique in the training process.\nExperiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT\noutperforms the other BERT quantization methods, and even achieves comparable\nperformance as the full-precision model while being 14.9x smaller.</p>\n", "tags": ["EMNLP","Efficiency","Evaluation","Model Architecture","Training Techniques"] },
{"key": "zhang2020trie", "citations": "94", "year": "2020", "title":"TRIE: End-to-end Text Reading And Information Extraction For Document Understanding", "abstract": "<p>Since real-world ubiquitous documents (e.g., invoices, tickets, resumes and\nleaflets) contain rich information, automatic document image understanding has\nbecome a hot topic. Most existing works decouple the problem into two separate\ntasks, (1) text reading for detecting and recognizing texts in images and (2)\ninformation extraction for analyzing and extracting key elements from\npreviously extracted plain text. However, they mainly focus on improving\ninformation extraction task, while neglecting the fact that text reading and\ninformation extraction are mutually correlated. In this paper, we propose a\nunified end-to-end text reading and information extraction network, where the\ntwo tasks can reinforce each other. Specifically, the multimodal visual and\ntextual features of text reading are fused for information extraction and in\nturn, the semantics in information extraction contribute to the optimization of\ntext reading. On three real-world datasets with diverse document images (from\nfixed layout to variable layout, from structured text to semi-structured text),\nour proposed method significantly outperforms the state-of-the-art methods in\nboth efficiency and accuracy.</p>\n", "tags": ["Datasets","Efficiency"] },
{"key": "zhang2020trojaning", "citations": "87", "year": "2021", "title":"Trojaning Language Models For Fun And Profit", "abstract": "<p>Recent years have witnessed the emergence of a new paradigm of building\nnatural language processing (NLP) systems: general-purpose, pre-trained\nlanguage models (LMs) are composed with simple downstream models and fine-tuned\nfor a variety of NLP tasks. This paradigm shift significantly simplifies the\nsystem development cycles. However, as many LMs are provided by untrusted third\nparties, their lack of standardization or regulation entails profound security\nimplications, which are largely unexplored.\n  To bridge this gap, this work studies the security threats posed by malicious\nLMs to NLP systems. Specifically, we present TROJAN-LM, a new class of\ntrojaning attacks in which maliciously crafted LMs trigger host NLP systems to\nmalfunction in a highly predictable manner. By empirically studying three\nstate-of-the-art LMs (BERT, GPT-2, XLNet) in a range of security-critical NLP\ntasks (toxic comment detection, question answering, text completion) as well as\nuser studies on crowdsourcing platforms, we demonstrate that TROJAN-LM\npossesses the following properties: (i) flexibility - the adversary is able to\nflexibly dene logical combinations (e.g., ‘and’, ‘or’, ‘xor’) of arbitrary\nwords as triggers, (ii) efficacy - the host systems misbehave as desired by the\nadversary with high probability when trigger-embedded inputs are present, (iii)\nspecificity - the trojan LMs function indistinguishably from their benign\ncounterparts on clean inputs, and (iv) fluency - the trigger-embedded inputs\nappear as fluent natural language and highly relevant to their surrounding\ncontexts. We provide analytical justification for the practicality of\nTROJAN-LM, and further discuss potential countermeasures and their challenges,\nwhich lead to several promising research directions.</p>\n", "tags": ["Model Architecture","Privacy","Security"] },
{"key": "zhang2020when", "citations": "83", "year": "2021", "title":"When Do You Need Billions Of Words Of Pretraining Data?", "abstract": "<p>NLP is currently dominated by general-purpose pretrained language models like\nRoBERTa, which achieve strong performance on NLU tasks through pretraining on\nbillions of words. But what exact knowledge or skills do Transformer LMs learn\nfrom large-scale pretraining that they cannot learn from less data? We adopt\nfour probing methods—classifier probing, information-theoretic probing,\nunsupervised relative acceptability judgment, and fine-tuning on NLU\ntasks—and draw learning curves that track the growth of these different\nmeasures of linguistic ability with respect to pretraining data volume using\nthe MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B\nwords. We find that LMs require only about 10M or 100M words to learn\nrepresentations that reliably encode most syntactic and semantic features we\ntest. A much larger quantity of data is needed in order to acquire enough\ncommonsense knowledge and other skills required to master typical downstream\nNLU tasks. The results suggest that, while the ability to encode linguistic\nfeatures is almost certainly necessary for language understanding, it is likely\nthat other forms of knowledge are the major drivers of recent improvements in\nlanguage understanding among large pretrained models.</p>\n", "tags": ["Fine-Tuning","Model Architecture","Training Techniques"] },
{"key": "zhang2021cross", "citations": "257", "year": "2021", "title":"Cross-modal Contrastive Learning For Text-to-image Generation", "abstract": "<p>The output of text-to-image synthesis systems should be coherent, clear,\nphoto-realistic scenes with high semantic fidelity to their conditioned text\ndescriptions. Our Cross-Modal Contrastive Generative Adversarial Network\n(XMC-GAN) addresses this challenge by maximizing the mutual information between\nimage and text. It does this via multiple contrastive losses which capture\ninter-modality and intra-modality correspondences. XMC-GAN uses an attentional\nself-modulation generator, which enforces strong text-image correspondence, and\na contrastive discriminator, which acts as a critic as well as a feature\nencoder for contrastive learning. The quality of XMC-GAN’s output is a major\nstep up from previous models, as we show on three challenging datasets. On\nMS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,\nbut–more importantly–people prefer XMC-GAN by 77.3 for image quality and 74.1\nfor image-text alignment, compared to three other recent models. XMC-GAN also\ngeneralizes to the challenging Localized Narratives dataset (which has longer,\nmore detailed descriptions), improving state-of-the-art FID from 48.70 to\n14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images\ndata, establishing a strong benchmark FID score of 26.91.</p>\n", "tags": ["CVPR","Datasets","Evaluation"] },
{"key": "zhang2021differentiable", "citations": "65", "year": "2021", "title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners", "abstract": "<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.</p>\n", "tags": ["Applications","Evaluation","Few-Shot","Has Code","Prompting"] },
{"key": "zhang2021tip", "citations": "113", "year": "2021", "title":"Tip-adapter: Training-free Clip-adapter For Better Vision-language Modeling", "abstract": "<p>Contrastive Vision-Language Pre-training, known as CLIP, has provided a new\nparadigm for learning visual representations by using large-scale contrastive\nimage-text pairs. It shows impressive performance on zero-shot knowledge\ntransfer to downstream tasks. To further enhance CLIP’s few-shot capability,\nCLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and\nsignificantly improves the performance for few-shot classification. However,\nsuch a process still needs extra training and computational resources. In this\npaper, we propose \\textbf{T}raining-Free CL\\textbf{IP}-\\textbf{Adapter}\n(\\textbf{Tip-Adapter}), which not only inherits CLIP’s training-free advantage\nbut also performs comparably or even better than CLIP-Adapter. Tip-Adapter does\nnot require any back propagation for training the adapter, but creates the\nweights by a key-value cache model constructed from the few-shot training set.\nIn this non-parametric manner, Tip-Adapter acquires well-performed adapter\nweights without any training, which is both efficient and effective. Moreover,\nthe performance of Tip-Adapter can be further boosted by fine-tuning such\nproperly initialized adapter for only a few epochs with super-fast convergence\nspeed. We conduct extensive experiments of few-shot classification on ImageNet\nand other 10 datasets to demonstrate the superiority of proposed Tip-Adapter.\nThe code will be released at https://github.com/gaopengcuhk/Tip-Adapter.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Has Code","Memory & Context","Training Techniques"] },
{"key": "zhang2021vinvl", "citations": "683", "year": "2021", "title":"Vinvl: Revisiting Visual Representations In Vision-language Models", "abstract": "<p>This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused <em>bottom-up and top-down</em> model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.</p>\n", "tags": ["CVPR","Datasets","Model Architecture","Training Techniques"] },
{"key": "zhang2022glipv2", "citations": "85", "year": "2022", "title":"Glipv2: Unifying Localization And Vision-language Understanding", "abstract": "<p>We present GLIPv2, a grounded VL understanding model, that serves both\nlocalization tasks (e.g., object detection, instance segmentation) and\nVision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2\nelegantly unifies localization pre-training and Vision-Language Pre-training\n(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of\nthe detection task, region-word contrastive learning as a novel region-word\nlevel contrastive learning task, and the masked language modeling. This\nunification not only simplifies the previous multi-stage VLP procedure but also\nachieves mutual benefits between localization and understanding tasks.\nExperimental results show that a single GLIPv2 model (all model weights are\nshared) achieves near SoTA performance on various localization and\nunderstanding tasks. The model also shows (1) strong zero-shot and few-shot\nadaption performance on open-vocabulary object detection tasks and (2) superior\ngrounding capability on VL understanding tasks. Code will be released at\nhttps://github.com/microsoft/GLIP.</p>\n", "tags": ["Few-Shot","Has Code","Training Techniques"] },
{"key": "zhang2022sine", "citations": "84", "year": "2023", "title":"SINE: Single Image Editing With Text-to-image Diffusion Models", "abstract": "<p>Recent works on diffusion models have demonstrated a strong capability for\nconditioning image generation, e.g., text-guided image synthesis. Such success\ninspires many efforts trying to use large-scale pre-trained diffusion models\nfor tackling a challenging problem–real image editing. Works conducted in this\narea learn a unique textual token corresponding to several images containing\nthe same object. However, under many circumstances, only one image is\navailable, such as the painting of the Girl with a Pearl Earring. Using\nexisting works on fine-tuning the pre-trained diffusion models with a single\nimage causes severe overfitting issues. The information leakage from the\npre-trained diffusion models makes editing can not keep the same content as the\ngiven image while creating new features depicted by the language guidance. This\nwork aims to address the problem of single-image editing. We propose a novel\nmodel-based guidance built upon the classifier-free guidance so that the\nknowledge from the model trained on a single image can be distilled into the\npre-trained diffusion model, enabling content creation even with one given\nimage. Additionally, we propose a patch-based fine-tuning that can effectively\nhelp the model generate images of arbitrary resolution. We provide extensive\nexperiments to validate the design choices of our approach and show promising\nediting capabilities, including changing style, content addition, and object\nmanipulation. The code is available for research purposes at\nhttps://github.com/zhang-zx/SINE.git .</p>\n", "tags": ["CVPR","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "zhang2022storybuddy", "citations": "100", "year": "2022", "title":"Storybuddy: A Human-ai Collaborative Chatbot For Parent-child Interactive Storytelling With Flexible Parental Involvement", "abstract": "<p>Despite its benefits for children’s skill development and parent-child\nbonding, many parents do not often engage in interactive storytelling by having\nstory-related dialogues with their child due to limited availability or\nchallenges in coming up with appropriate questions. While recent advances made\nAI generation of questions from stories possible, the fully-automated approach\nexcludes parent involvement, disregards educational goals, and underoptimizes\nfor child engagement. Informed by need-finding interviews and participatory\ndesign (PD) results, we developed StoryBuddy, an AI-enabled system for parents\nto create interactive storytelling experiences. StoryBuddy’s design highlighted\nthe need for accommodating dynamic user needs between the desire for parent\ninvolvement and parent-child bonding and the goal of minimizing parent\nintervention when busy. The PD revealed varied assessment and educational goals\nof parents, which StoryBuddy addressed by supporting configuring question types\nand tracking child progress. A user study validated StoryBuddy’s usability and\nsuggested design insights for future parent-AI collaboration systems.</p>\n", "tags": [] },
{"key": "zhang2022survey", "citations": "172", "year": "2023", "title":"A Survey Of Controllable Text Generation Using Transformer-based Pre-trained Language Models", "abstract": "<p>Controllable Text Generation (CTG) is emerging area in the field of natural\nlanguage generation (NLG). It is regarded as crucial for the development of\nadvanced text generation technologies that better meet the specific constraints\nin practical applications. In recent years, methods using large-scale\npre-trained language models (PLMs), in particular the widely used\ntransformer-based PLMs, have become a new paradigm of NLG, allowing generation\nof more diverse and fluent text. However, due to the limited level of\ninterpretability of deep neural networks, the controllability of these methods\nneed to be guaranteed. To this end, controllable text generation using\ntransformer-based PLMs has become a rapidly growing yet challenging new\nresearch hotspot. A diverse range of approaches have emerged in the recent 3-4\nyears, targeting different CTG tasks that require different types of controlled\nconstraints. In this paper, we present a systematic critical review on the\ncommon tasks, main approaches, and evaluation methods in this area. Finally, we\ndiscuss the challenges that the field is facing, and put forward various\npromising future directions. To the best of our knowledge, this is the first\nsurvey paper to summarize the state-of-the-art CTG techniques from the\nperspective of Transformer-based PLMs. We hope it can help researchers and\npractitioners in the related fields to quickly track the academic and\ntechnological frontier, providing them with a landscape of the area and a\nroadmap for future research.</p>\n", "tags": ["Applications","Evaluation","Model Architecture","Survey Paper"] },
{"key": "zhang2022tip", "citations": "144", "year": "2022", "title":"Tip-adapter: Training-free Adaption Of CLIP For Few-shot Classification", "abstract": "<p>Contrastive Vision-Language Pre-training, known as CLIP, has provided a new\nparadigm for learning visual representations using large-scale image-text\npairs. It shows impressive performance on downstream tasks by zero-shot\nknowledge transfer. To further enhance CLIP’s adaption capability, existing\nmethods proposed to fine-tune additional learnable modules, which significantly\nimproves the few-shot performance but introduces extra training time and\ncomputational resources. In this paper, we propose a training-free adaption\nmethod for CLIP to conduct few-shot classification, termed as Tip-Adapter,\nwhich not only inherits the training-free advantage of zero-shot CLIP but also\nperforms comparably to those training-required approaches. Tip-Adapter\nconstructs the adapter via a key-value cache model from the few-shot training\nset, and updates the prior knowledge encoded in CLIP by feature retrieval. On\ntop of that, the performance of Tip-Adapter can be further boosted to be\nstate-of-the-art on ImageNet by fine-tuning the cache model for 10\\(\\times\\)\nfewer epochs than existing methods, which is both effective and efficient. We\nconduct extensive experiments of few-shot classification on 11 datasets to\ndemonstrate the superiority of our proposed methods. Code is released at\nhttps://github.com/gaopengcuhk/Tip-Adapter.</p>\n", "tags": ["Datasets","Few-Shot","Fine-Tuning","Has Code","Training Techniques"] },
{"key": "zhang2022wenet", "citations": "65", "year": "2022", "title":"Wenet 2.0: More Productive End-to-end Speech Recognition Toolkit", "abstract": "<p>Recently, we made available WeNet, a production-oriented end-to-end speech\nrecognition toolkit, which introduces a unified two-pass (U2) framework and a\nbuilt-in runtime to address the streaming and non-streaming decoding modes in a\nsingle model. To further improve ASR performance and facilitate various\nproduction requirements, in this paper, we present WeNet 2.0 with four\nimportant updates. (1) We propose U2++, a unified two-pass framework with\nbidirectional attention decoders, which includes the future contextual\ninformation by a right-to-left attention decoder to improve the representative\nability of the shared encoder and the performance during the rescoring stage.\n(2) We introduce an n-gram based language model and a WFST-based decoder into\nWeNet 2.0, promoting the use of rich text data in production scenarios. (3) We\ndesign a unified contextual biasing framework, which leverages user-specific\ncontext (e.g., contact lists) to provide rapid adaptation ability for\nproduction and improves ASR accuracy in both with-LM and without-LM scenarios.\n(4) We design a unified IO to support large-scale data for effective model\ntraining. In summary, the brand-new WeNet 2.0 achieves up to 10% relative\nrecognition performance improvement over the original WeNet on various corpora\nand makes available several important production-oriented features.</p>\n", "tags": ["INTERSPEECH","Model Architecture","Tools","Training Techniques"] },
{"key": "zhang2023benchmarking", "citations": "130", "year": "2024", "title":"Benchmarking Large Language Models For News Summarization", "abstract": "<p>Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM’s zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","TACL"] },
{"key": "zhang2023complete", "citations": "81", "year": "2023", "title":"A Complete Survey On Generative AI (AIGC): Is Chatgpt From GPT-4 To GPT-5 All You Need?", "abstract": "<p>As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT’s future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.</p>\n", "tags": ["Applications","Model Architecture","Survey Paper","Training Techniques"] },
{"key": "zhang2023google", "citations": "97", "year": "2023", "title":"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages", "abstract": "<p>We introduce the Universal Speech Model (USM), a single large model that\nperforms automatic speech recognition (ASR) across 100+ languages. This is\nachieved by pre-training the encoder of the model on a large unlabeled\nmultilingual dataset of 12 million (M) hours spanning over 300 languages, and\nfine-tuning on a smaller labeled dataset. We use multilingual pre-training with\nrandom-projection quantization and speech-text modality matching to achieve\nstate-of-the-art performance on downstream multilingual ASR and speech-to-text\ntranslation tasks. We also demonstrate that despite using a labeled training\nset 1/7-th the size of that used for the Whisper model, our model exhibits\ncomparable or better performance on both in-domain and out-of-domain speech\nrecognition tasks across many languages.</p>\n", "tags": ["Datasets","Fine-Tuning","Training Techniques"] },
{"key": "zhang2023instruction", "citations": "70", "year": "2023", "title":"Instruction Tuning For Large Language Models: A Survey", "abstract": "<p>This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users’ objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey</p>\n", "tags": ["Applications","Datasets","Fine-Tuning","Survey Paper","Training Techniques"] },
{"key": "zhang2023multimodal", "citations": "68", "year": "2023", "title":"Multimodal Pre-training Framework For Sequential Recommendation Via Contrastive Learning", "abstract": "<p>Current multimodal sequential recommendation models are often unable to\neffectively explore and capture correlations among behavior sequences of users\nand items across different modalities, either neglecting correlations among\nsequence representations or inadequately capturing associations between\nmultimodal data and sequence data in their representations. To address this\nproblem, we explore multimodal pre-training in the context of sequential\nrecommendation, with the aim of enhancing fusion and utilization of multimodal\ninformation. We propose a novel Multimodal Pre-training for Sequential\nRecommendation (MP4SR) framework, which utilizes contrastive losses to capture\nthe correlation among different modality sequences of users, as well as the\ncorrelation among different modality sequences of users and items. MP4SR\nconsists of three key components: 1) multimodal feature extraction, 2) a\nbackbone network, Multimodal Mixup Sequence Encoder (M2SE), and 3) pre-training\ntasks. After utilizing pre-trained encoders to generate initial multimodal\nfeatures of items, M2SE adopts a complementary sequence mixup strategy to fuse\ndifferent modality sequences, and leverages contrastive learning to capture\nmodality interactions at the sequence-to-sequence and sequence-to-item levels.\nExtensive experiments on four real-world datasets demonstrate that MP4SR\noutperforms state-of-the-art approaches in both normal and cold-start settings.\nWe further highlight the efficacy of incorporating multimodal pre-training in\nsequential recommendation representation learning, serving as an effective\nregularizer and optimizing the parameter space for the recommendation task.</p>\n", "tags": ["Datasets","Tools","Training Techniques"] },
{"key": "zhang2023sentiment", "citations": "86", "year": "2024", "title":"Sentiment Analysis In The Era Of Large Language Models: A Reality Check", "abstract": "<p>Sentiment analysis (SA) has been a long-standing research area in natural\nlanguage processing. It can offer rich insights into human sentiments and\nopinions and has thus seen considerable interest from both academia and\nindustry. With the advent of large language models (LLMs) such as ChatGPT,\nthere is a great potential for their employment on SA problems. However, the\nextent to which existing LLMs can be leveraged for different sentiment analysis\ntasks remains unclear. This paper aims to provide a comprehensive investigation\ninto the capabilities of LLMs in performing various sentiment analysis tasks,\nfrom conventional sentiment classification to aspect-based sentiment analysis\nand multifaceted analysis of subjective texts. We evaluate performance across\n13 tasks on 26 datasets and compare the results against small language models\n(SLMs) trained on domain-specific datasets. Our study reveals that while LLMs\ndemonstrate satisfactory performance in simpler tasks, they lag behind in more\ncomplex tasks requiring deeper understanding or structured sentiment\ninformation. However, LLMs significantly outperform SLMs in few-shot learning\nsettings, suggesting their potential when annotation resources are limited. We\nalso highlight the limitations of current evaluation practices in assessing\nLLMs’ SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a\nmore comprehensive and realistic evaluation. Data and code during our\ninvestigations are available at\nhttps://github.com/DAMO-NLP-SG/LLM-Sentiment.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Has Code","NAACL"] },
{"key": "zhang2023siren", "citations": "159", "year": "2023", "title":"Siren's Song In The AI Ocean: A Survey On Hallucination In Large Language Models", "abstract": "<p>While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.</p>\n", "tags": ["Evaluation","Survey Paper"] },
{"key": "zhang2023video", "citations": "182", "year": "2023", "title":"Video-llama: An Instruction-tuned Audio-visual Language Model For Video Understanding", "abstract": "<p>We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM’s embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.</p>\n", "tags": ["Datasets","EMNLP","Tools","Training Techniques"] },
{"key": "zhang2024benchmarking", "citations": "130", "year": "2024", "title":"Benchmarking LLM Code Generation For Audio Programming With Visual Dataflow Languages", "abstract": "<p>Node-based programming languages are increasingly popular in media arts\ncoding domains. These languages are designed to be accessible to users with\nlimited coding experience, allowing them to achieve creative output without an\nextensive programming background. Using LLM-based code generation to further\nlower the barrier to creative output is an exciting opportunity. However, the\nbest strategy for code generation for visual node-based programming languages\nis still an open question. In particular, such languages have multiple levels\nof representation in text, each of which may be used for code generation. In\nthis work, we explore the performance of LLM code generation in audio\nprogramming tasks in visual programming languages at multiple levels of\nrepresentation. We explore code generation through metaprogramming code\nrepresentations for these languages (i.e., coding the language using a\ndifferent high-level text-based programming language), as well as through\ndirect node generation with JSON. We evaluate code generated in this way for\ntwo visual languages for audio programming on a benchmark set of coding\nproblems. We measure both correctness and complexity of the generated code. We\nfind that metaprogramming results in more semantically correct generated code,\ngiven that the code is well-formed (i.e., is syntactically correct and runs).\nWe also find that prompting for richer metaprogramming using randomness and\nloops led to more complex code.</p>\n", "tags": ["Datasets","Evaluation","Llm For Code","Prompting","TACL"] },
{"key": "zhang2024mm", "citations": "67", "year": "2024", "title":"Mm-eval: A Hierarchical Benchmark For Modern Mongolian Evaluation In Llms", "abstract": "<p>Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture"] },
{"key": "zhankui2023large", "citations": "63", "year": "2023", "title":"Large Language Models As Zero-shot Conversational Recommenders", "abstract": "<p>In this paper, we present empirical studies on conversational recommendation\ntasks using representative large language models in a zero-shot setting with\nthree primary contributions. (1) Data: To gain insights into model behavior in\n“in-the-wild” conversational recommendation scenarios, we construct a new\ndataset of recommendation-related conversations by scraping a popular\ndiscussion website. This is the largest public real-world conversational\nrecommendation dataset to date. (2) Evaluation: On the new dataset and two\nexisting conversational recommendation datasets, we observe that even without\nfine-tuning, large language models can outperform existing fine-tuned\nconversational recommendation models. (3) Analysis: We propose various probing\ntasks to investigate the mechanisms behind the remarkable performance of large\nlanguage models in conversational recommendation. We analyze both the large\nlanguage models’ behaviors and the characteristics of the datasets, providing a\nholistic understanding of the models’ effectiveness, limitations and suggesting\ndirections for the design of future conversational recommenders</p>\n", "tags": ["CIKM","Datasets","Evaluation","Fine-Tuning","Training Techniques"] },
{"key": "zhao2016towards", "citations": "187", "year": "2016", "title":"Towards End-to-end Learning For Dialog State Tracking And Management Using Deep Reinforcement Learning", "abstract": "<p>This paper presents an end-to-end framework for task-oriented dialog systems\nusing a variant of Deep Recurrent Q-Networks (DRQN). The model is able to\ninterface with a relational database and jointly learn policies for both\nlanguage understanding and dialog strategy. Moreover, we propose a hybrid\nalgorithm that combines the strength of reinforcement learning and supervised\nlearning to achieve faster learning speed. We evaluated the proposed model on a\n20 Question Game conversational game simulator. Results show that the proposed\nmethod outperforms the modular-based baseline and learns a distributed\nrepresentation of the latent dialog state.</p>\n", "tags": ["Dialogue & Multi Turn","Reinforcement Learning","Tools"] },
{"key": "zhao2017deep", "citations": "109", "year": "2018", "title":"Deep Reinforcement Learning For List-wise Recommendations", "abstract": "<p>Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users’ personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users’ feedbacks. In particular, we introduce an online user-agent\ninteracting environment simulator, which can pre-train and evaluate model\nparameters offline before applying the model online. Moreover, we validate the\nimportance of list-wise recommendations during the interactions between users\nand agent, and develop a novel approach to incorporate them into the proposed\nframework LIRD for list-wide recommendations. The experimental results based on\na real-world e-commerce dataset demonstrate the effectiveness of the proposed\nframework.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning","Tools"] },
{"key": "zhao2017learning", "citations": "727", "year": "2017", "title":"Learning Discourse-level Diversity For Neural Dialog Models Using Conditional Variational Autoencoders", "abstract": "<p>While recent neural encoder-decoder models have shown great promise in\nmodeling open-domain conversations, they often generate dull and generic\nresponses. Unlike past work that has focused on diversifying the output of the\ndecoder at word-level to alleviate this problem, we present a novel framework\nbased on conditional variational autoencoders that captures the discourse-level\ndiversity in the encoder. Our model uses latent variables to learn a\ndistribution over potential conversational intents and generates diverse\nresponses using only greedy decoders. We have further developed a novel variant\nthat is integrated with linguistic prior knowledge for better performance.\nFinally, the training procedure is improved by introducing a bag-of-word loss.\nOur proposed models have been validated to generate significantly more diverse\nresponses than baseline approaches and exhibit competence in discourse-level\ndecision-making.</p>\n", "tags": ["Dialogue & Multi Turn","Tools","Training Techniques"] },
{"key": "zhao2018deep", "citations": "182", "year": "2018", "title":"Deep Reinforcement Learning For Page-wise Recommendations", "abstract": "<p>Recommender systems can mitigate the information overload problem by\nsuggesting users’ personalized items. In real-world recommendations such as\ne-commerce, a typical interaction between the system and its users is – users\nare recommended a page of items and provide feedback; and then the system\nrecommends a new page of items. To effectively capture such interaction for\nrecommendations, we need to solve two key problems – (1) how to update\nrecommending strategy according to user’s \\textit{real-time feedback}, and 2)\nhow to generate a page of items with proper display, which pose tremendous\nchallenges to traditional recommender systems. In this paper, we study the\nproblem of page-wise recommendations aiming to address aforementioned two\nchallenges simultaneously. In particular, we propose a principled approach to\njointly generate a set of complementary items and the corresponding strategy to\ndisplay them in a 2-D page; and propose a novel page-wise recommendation\nframework based on deep reinforcement learning, DeepPage, which can optimize a\npage of items with proper display based on real-time feedback from users. The\nexperimental results based on a real-world e-commerce dataset demonstrate the\neffectiveness of the proposed framework.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning","Tools"] },
{"key": "zhao2018integrating", "citations": "107", "year": "2018", "title":"Integrating Transformer And Paraphrase Rules For Sentence Simplification", "abstract": "<p>Sentence simplification aims to reduce the complexity of a sentence while\nretaining its original meaning. Current models for sentence simplification\nadopted ideas from ma- chine translation studies and implicitly learned\nsimplification mapping rules from normal- simple sentence pairs. In this paper,\nwe explore a novel model based on a multi-layer and multi-head attention\narchitecture and we pro- pose two innovative approaches to integrate the Simple\nPPDB (A Paraphrase Database for Simplification), an external paraphrase\nknowledge base for simplification that covers a wide range of real-world\nsimplification rules. The experiments show that the integration provides two\nmajor benefits: (1) the integrated model outperforms multiple state- of-the-art\nbaseline models for sentence simplification in the literature (2) through\nanalysis of the rule utilization, the model seeks to select more accurate\nsimplification rules. The code and models used in the paper are available at\nhttps://github.com/ Sanqiang/text_simplification.</p>\n", "tags": ["EMNLP","Has Code","Model Architecture"] },
{"key": "zhao2018unsupervised", "citations": "139", "year": "2018", "title":"Unsupervised Discrete Sentence Representation Learning For Interpretable Neural Dialog Generation", "abstract": "<p>The encoder-decoder dialog model is one of the most prominent methods used to\nbuild dialog systems in complex domains. Yet it is limited because it cannot\noutput interpretable actions as in traditional systems, which hinders humans\nfrom understanding its generation process. We present an unsupervised discrete\nsentence representation learning method that can integrate with any existing\nencoder-decoder dialog models for interpretable response generation. Building\nupon variational autoencoders (VAEs), we present two novel models, DI-VAE and\nDI-VST that improve VAEs and can discover interpretable semantics via either\nauto encoding or context predicting. Our methods have been validated on\nreal-world dialog datasets to discover semantic representations and enhance\nencoder-decoder models with interpretable generation.</p>\n", "tags": ["Datasets"] },
{"key": "zhao2019explicit", "citations": "80", "year": "2019", "title":"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection", "abstract": "<p>Self-attention based Transformer has demonstrated the state-of-the-art\nperformances in a number of natural language processing tasks. Self-attention\nis able to model long-term dependencies, but it may suffer from the extraction\nof irrelevant information in the context. To tackle the problem, we propose a\nnovel model called \\textbf{Explicit Sparse Transformer}. Explicit Sparse\nTransformer is able to improve the concentration of attention on the global\ncontext through an explicit selection of the most relevant segments. Extensive\nexperimental results on a series of natural language processing and computer\nvision tasks, including neural machine translation, image captioning, and\nlanguage modeling, all demonstrate the advantages of Explicit Sparse\nTransformer in model performance. We also show that our proposed sparse\nattention method achieves comparable or better results than the previous sparse\nattention method, but significantly reduces training and testing time. For\nexample, the inference speed is twice that of sparsemax in Transformer model.\nCode will be available at\nhttps://github.com/lancopku/Explicit-Sparse-Transformer</p>\n", "tags": ["Has Code","Model Architecture","Training Techniques"] },
{"key": "zhao2019moverscore", "citations": "437", "year": "2019", "title":"Moverscore: Text Generation Evaluating With Contextualized Embeddings And Earth Mover Distance", "abstract": "<p>A robust evaluation metric has a profound impact on the development of text\ngeneration systems. A desirable metric compares system output against\nreferences based on their semantics rather than surface forms. In this paper we\ninvestigate strategies to encode system and reference texts to devise a metric\nthat shows a high correlation with human judgment of text quality. We validate\nour new metric, namely MoverScore, on a number of text generation tasks\nincluding summarization, machine translation, image captioning, and\ndata-to-text generation, where the outputs are produced by a variety of neural\nand non-neural systems. Our findings suggest that metrics combining\ncontextualized representations with a distance measure perform the best. Such\nmetrics also demonstrate strong generalization capability across tasks. For\nease-of-use we make our metrics available as web service.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "zhao2019rethinking", "citations": "126", "year": "2019", "title":"Rethinking Action Spaces For Reinforcement Learning In End-to-end Dialog Agents With Latent Variable Models", "abstract": "<p>Defining action spaces for conversational agents and optimizing their\ndecision-making process with reinforcement learning is an enduring challenge.\nCommon practice has been to use handcrafted dialog acts, or the output\nvocabulary, e.g. in neural encoder decoders, as the action spaces. Both have\ntheir own limitations. This paper proposes a novel latent action framework that\ntreats the action spaces of an end-to-end dialog agent as latent variables and\ndevelops unsupervised methods in order to induce its own action space from the\ndata. Comprehensive experiments are conducted examining both continuous and\ndiscrete action types and two different optimization methods based on\nstochastic variational inference. Results show that the proposed latent actions\nachieve superior empirical performance improvement over previous word-level\npolicy gradient methods on both DealOrNoDeal and MultiWoz dialogs. Our detailed\nanalysis also provides insights about various latent variable approaches for\npolicy learning and can serve as a foundation for developing better latent\nactions in future research.</p>\n", "tags": ["Agentic","Dialogue & Multi Turn","Reinforcement Learning","Tools"] },
{"key": "zhao2019uer", "citations": "71", "year": "2019", "title":"UER: An Open-source Toolkit For Pre-training Models", "abstract": "<p>Existing works, including ELMO and BERT, have revealed the importance of\npre-training for NLP tasks. While there does not exist a single pre-training\nmodel that works best in all cases, it is of necessity to develop a framework\nthat is able to deploy various pre-training models efficiently. For this\npurpose, we propose an assemble-on-demand pre-training toolkit, namely\nUniversal Encoder Representations (UER). UER is loosely coupled, and\nencapsulated with rich modules. By assembling modules on demand, users can\neither reproduce a state-of-the-art pre-training model or develop a\npre-training model that remains unexplored. With UER, we have built a model\nzoo, which contains pre-trained models based on different corpora, encoders,\nand targets (objectives). With proper pre-trained models, we could achieve new\nstate-of-the-art results on a range of downstream datasets.</p>\n", "tags": ["EMNLP","Model Architecture","Tools","Training Techniques"] },
{"key": "zhao2020gender", "citations": "61", "year": "2020", "title":"Gender Bias In Multilingual Embeddings And Cross-lingual Transfer", "abstract": "<p>Multilingual representations embed words from many languages into a single\nsemantic space such that words with similar meanings are close to each other\nregardless of the language. These embeddings have been widely used in various\nsettings, such as cross-lingual transfer, where a natural language processing\n(NLP) model trained on one language is deployed to another language. While the\ncross-lingual transfer techniques are powerful, they carry gender bias from the\nsource to target languages. In this paper, we study gender bias in multilingual\nembeddings and how it affects transfer learning for NLP applications. We create\na multilingual dataset for bias analysis and propose several ways for\nquantifying bias in multilingual representations from both the intrinsic and\nextrinsic perspectives. Experimental results show that the magnitude of bias in\nthe multilingual representations changes differently when we align the\nembeddings to different target spaces and that the alignment direction can also\nhave an influence on the bias in transfer learning. We further provide\nrecommendations for using the multilingual word representations for downstream\ntasks.</p>\n", "tags": ["Applications","Datasets","Ethics & Fairness","Fine-Tuning"] },
{"key": "zhao2020knowledge", "citations": "149", "year": "2020", "title":"Knowledge-grounded Dialogue Generation With Pre-trained Language Models", "abstract": "<p>We study knowledge-grounded dialogue generation with pre-trained language\nmodels. To leverage the redundant external knowledge under capacity constraint,\nwe propose equipping response generation defined by a pre-trained language\nmodel with a knowledge selection module, and an unsupervised approach to\njointly optimizing knowledge selection and response generation with unlabeled\ndialogues. Empirical results on two benchmarks indicate that our model can\nsignificantly outperform state-of-the-art methods in both automatic evaluation\nand human judgment.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "zhao2020low", "citations": "86", "year": "2020", "title":"Low-resource Knowledge-grounded Dialogue Generation", "abstract": "<p>Responding with knowledge has been recognized as an important capability for\nan intelligent conversational agent. Yet knowledge-grounded dialogues, as\ntraining data for learning such a response generation model, are difficult to\nobtain. Motivated by the challenge in practice, we consider knowledge-grounded\ndialogue generation under a natural assumption that only limited training\nexamples are available. In such a low-resource setting, we devise a\ndisentangled response decoder in order to isolate parameters that depend on\nknowledge-grounded dialogues from the entire generation model. By this means,\nthe major part of the model can be learned from a large number of ungrounded\ndialogues and unstructured documents, while the remaining small parameters can\nbe well fitted using the limited training examples. Evaluation results on two\nbenchmarks indicate that with only 1/8 training data, our model can achieve the\nstate-of-the-art performance and generalize well on out-of-domain knowledge.</p>\n", "tags": ["Evaluation","Training Techniques"] },
{"key": "zhao2020reducing", "citations": "86", "year": "2020", "title":"Reducing Quantity Hallucinations In Abstractive Summarization", "abstract": "<p>It is well-known that abstractive summaries are subject to\nhallucination—including material that is not supported by the original text.\nWhile summaries can be made hallucination-free by limiting them to general\nphrases, such summaries would fail to be very informative. Alternatively, one\ncan try to avoid hallucinations by verifying that any specific entities in the\nsummary appear in the original text in a similar context. This is the approach\ntaken by our system, Herman. The system learns to recognize and verify quantity\nentities (dates, numbers, sums of money, etc.) in a beam-worth of abstractive\nsummaries produced by state-of-the-art models, in order to up-rank those\nsummaries whose quantity terms are supported by the original text. Experimental\nresults demonstrate that the ROUGE scores of such up-ranked summaries have a\nhigher Precision than summaries that have not been up-ranked, without a\ncomparable loss in Recall, resulting in higher F\\(_1\\). Preliminary human\nevaluation of up-ranked vs. original summaries shows people’s preference for\nthe former.</p>\n", "tags": ["EMNLP","Evaluation"] },
{"key": "zhao2021calibrate", "citations": "341", "year": "2021", "title":"Calibrate Before Use: Improving Few-shot Performance Of Language Models", "abstract": "<p>GPT-3 can perform numerous tasks when provided a natural language prompt that\ncontains a few training examples. We show that this type of few-shot learning\ncan be unstable: the choice of prompt format, training examples, and even the\norder of the training examples can cause accuracy to vary from near chance to\nnear state-of-the-art. We demonstrate that this instability arises from the\nbias of language models towards predicting certain answers, e.g., those that\nare placed near the end of the prompt or are common in the pre-training data.\nTo mitigate this, we first estimate the model’s bias towards each answer by\nasking for its prediction when given the training prompt and a content-free\ntest input such as “N/A”. We then fit calibration parameters that cause the\nprediction for this input to be uniform across answers. On a diverse set of\ntasks, this contextual calibration procedure substantially improves GPT-3 and\nGPT-2’s average accuracy (up to 30.0% absolute) and reduces variance across\ndifferent choices of the prompt.</p>\n", "tags": ["Few-Shot","Model Architecture","Prompting","Training Techniques"] },
{"key": "zhao2022dense", "citations": "66", "year": "2023", "title":"Dense Text Retrieval Based On Pretrained Language Models: A Survey", "abstract": "<p>Text retrieval is a long-standing research topic on information seeking,\nwhere a system is required to return relevant information resources to user’s\nqueries in natural language. From classic retrieval methods to learning-based\nranking functions, the underlying retrieval models have been continually\nevolved with the ever-lasting technical innovation. To design effective\nretrieval models, a key point lies in how to learn the text representation and\nmodel the relevance matching. The recent success of pretrained language models\n(PLMs) sheds light on developing more capable text retrieval approaches by\nleveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can\neffectively learn the representations of queries and texts in the latent\nrepresentation space, and further construct the semantic matching function\nbetween the dense vectors for relevance modeling. Such a retrieval approach is\nreferred to as dense retrieval, since it employs dense vectors (a.k.a.,\nembeddings) to represent the texts. Considering the rapid progress on dense\nretrieval, in this survey, we systematically review the recent advances on\nPLM-based dense retrieval. Different from previous surveys on dense retrieval,\nwe take a new perspective to organize the related work by four major aspects,\nincluding architecture, training, indexing and integration, and summarize the\nmainstream techniques for each aspect. We thoroughly survey the literature, and\ninclude 300+ related reference papers on dense retrieval. To support our\nsurvey, we create a website for providing useful resources, and release a code\nrepertory and toolkit for implementing dense retrieval models. This survey aims\nto provide a comprehensive, practical reference focused on the major progress\nfor dense text retrieval.</p>\n", "tags": ["Model Architecture","Survey Paper","Training Techniques"] },
{"key": "zhao2023explainability", "citations": "214", "year": "2024", "title":"Explainability For Large Language Models: A Survey", "abstract": "<p>Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language processing. However, their internal mechanisms are still\nunclear and this lack of transparency poses unwanted risks for downstream\napplications. Therefore, understanding and explaining these models is crucial\nfor elucidating their behaviors, limitations, and social impacts. In this\npaper, we introduce a taxonomy of explainability techniques and provide a\nstructured overview of methods for explaining Transformer-based language\nmodels. We categorize techniques based on the training paradigms of LLMs:\ntraditional fine-tuning-based paradigm and prompting-based paradigm. For each\nparadigm, we summarize the goals and dominant approaches for generating local\nexplanations of individual predictions and global explanations of overall model\nknowledge. We also discuss metrics for evaluating generated explanations, and\ndiscuss how explanations can be leveraged to debug models and improve\nperformance. Lastly, we examine key challenges and emerging opportunities for\nexplanation techniques in the era of LLMs in comparison to conventional machine\nlearning models.</p>\n", "tags": ["Applications","Ethics & Fairness","Evaluation","Survey Paper"] },
{"key": "zhao2023recommender", "citations": "100", "year": "2024", "title":"Recommender Systems In The Era Of Large Language Models (llms)", "abstract": "<p>With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users’ interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.</p>\n", "tags": ["Applications","Fine-Tuning","Prompting","Training Techniques"] },
{"key": "zheng2018intention", "citations": "60", "year": "2019", "title":"Intention Oriented Image Captions With Guiding Objects", "abstract": "<p>Although existing image caption models can produce promising results using\nrecurrent neural networks (RNNs), it is difficult to guarantee that an object\nwe care about is contained in generated descriptions, for example in the case\nthat the object is inconspicuous in the image. Problems become even harder when\nthese objects did not appear in training stage. In this paper, we propose a\nnovel approach for generating image captions with guiding objects (CGO). The\nCGO constrains the model to involve a human-concerned object when the object is\nin the image. CGO ensures that the object is in the generated description while\nmaintaining fluency. Instead of generating the sequence from left to right, we\nstart the description with a selected object and generate other parts of the\nsequence based on this object. To achieve this, we design a novel framework\ncombining two LSTMs in opposite directions. We demonstrate the characteristics\nof our method on MSCOCO where we generate descriptions for each detected object\nin the images. With CGO, we can extend the ability of description to the\nobjects being neglected in image caption labels and provide a set of more\ncomprehensive and diverse descriptions for an image. CGO shows advantages when\napplied to the task of describing novel objects. We show experimental results\non both MSCOCO and ImageNet datasets. Evaluations show that our method\noutperforms the state-of-the-art models in the task with average F1 75.8,\nleading to better descriptions in terms of both content accuracy and fluency.</p>\n", "tags": ["CVPR","Datasets","Tools","Training Techniques"] },
{"key": "zheng2019personalized", "citations": "87", "year": "2019", "title":"Personalized Dialogue Generation With Diversified Traits", "abstract": "<p>Endowing a dialogue system with particular personality traits is essential to\ndeliver more human-like conversations. However, due to the challenge of\nembodying personality via language expression and the lack of large-scale\npersona-labeled dialogue data, this research problem is still far from\nwell-studied. In this paper, we investigate the problem of incorporating\nexplicit personality traits in dialogue generation to deliver personalized\ndialogues.\n  To this end, firstly, we construct PersonalDialog, a large-scale multi-turn\ndialogue dataset containing various traits from a large number of speakers. The\ndataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers.\nEach utterance is associated with a speaker who is marked with traits like Age,\nGender, Location, Interest Tags, etc. Several anonymization schemes are\ndesigned to protect the privacy of each speaker. This large-scale dataset will\nfacilitate not only the study of personalized dialogue generation, but also\nother researches on sociolinguistics or social science.\n  Secondly, to study how personality traits can be captured and addressed in\ndialogue generation, we propose persona-aware dialogue generation models within\nthe sequence to sequence learning framework. Explicit personality traits\n(structured by key-value pairs) are embedded using a trait fusion module.\nDuring the decoding process, two techniques, namely persona-aware attention and\npersona-aware bias, are devised to capture and address trait-related\ninformation. Experiments demonstrate that our model is able to address proper\ntraits in different contexts. Case studies also show interesting results for\nthis challenging research problem.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Ethics & Fairness","Model Architecture","Privacy","Tools"] },
{"key": "zheng2019simpler", "citations": "85", "year": "2019", "title":"Simpler And Faster Learning Of Adaptive Policies For Simultaneous Translation", "abstract": "<p>Simultaneous translation is widely useful but remains challenging. Previous\nwork falls into two main categories: (a) fixed-latency policies such as Ma et\nal. (2019) and (b) adaptive policies such as Gu et al. (2017). The former are\nsimple and effective, but have to aggressively predict future content due to\ndiverging source-target word order; the latter do not anticipate, but suffer\nfrom unstable and inefficient training. To combine the merits of both\napproaches, we propose a simple supervised-learning framework to learn an\nadaptive policy from oracle READ/WRITE sequences generated from parallel text.\nAt each step, such an oracle sequence chooses to WRITE the next target word if\nthe available source sentence context provides enough information to do so,\notherwise READ the next source word. Experiments on German&lt;-&gt;English show that\nour method, without retraining the underlying NMT model, can learn flexible\npolicies with better BLEU scores and similar latencies compared to previous\nwork.</p>\n", "tags": ["EMNLP","Tools","Training Techniques"] },
{"key": "zheng2019simultaneous", "citations": "74", "year": "2019", "title":"Simultaneous Translation With Flexible Policy Via Restricted Imitation Learning", "abstract": "<p>Simultaneous translation is widely useful but remains one of the most\ndifficult tasks in NLP. Previous work either uses fixed-latency policies, or\ntrain a complicated two-staged model using reinforcement learning. We propose a\nmuch simpler single model that adds a `delay’ token to the target vocabulary,\nand design a restricted dynamic oracle to greatly simplify training.\nExperiments on Chinese&lt;-&gt;English simultaneous translation show that our work\nleads to flexible policies that achieve better BLEU scores and lower latencies\ncompared to both fixed and RL-learned policies.</p>\n", "tags": ["Agentic","Reinforcement Learning","Training Techniques"] },
{"key": "zheng2020towards", "citations": "68", "year": "2020", "title":"Towards Making The Most Of Context In Neural Machine Translation", "abstract": "<p>Document-level machine translation manages to outperform sentence level\nmodels by a small margin, but have failed to be widely adopted. We argue that\nprevious research did not make a clear use of the global context, and propose a\nnew document-level NMT framework that deliberately models the local context of\neach sentence with the awareness of the global context of the document in both\nsource and target languages. We specifically design the model to be able to\ndeal with documents containing any number of sentences, including single\nsentences. This unified approach allows our model to be trained elegantly on\nstandard datasets without needing to train on sentence and document level data\nseparately. Experimental results demonstrate that our model outperforms\nTransformer baselines and previous document-level NMT models with substantial\nmargins of up to 2.1 BLEU on state-of-the-art baselines. We also provide\nanalyses which show the benefit of context far beyond the neighboring two or\nthree sentences, which previous studies have typically incorporated.</p>\n", "tags": ["Datasets","IJCAI","Model Architecture","Tools"] },
{"key": "zheng2022disentangling", "citations": "82", "year": "2022", "title":"Disentangling Long And Short-term Interests For Recommendation", "abstract": "<p>Modeling user’s long-term and short-term interests is crucial for accurate\nrecommendation. However, since there is no manually annotated label for user\ninterests, existing approaches always follow the paradigm of entangling these\ntwo aspects, which may lead to inferior recommendation accuracy and\ninterpretability. In this paper, to address it, we propose a Contrastive\nlearning framework to disentangle Long and Short-term interests for\nRecommendation (CLSR) with self-supervision. Specifically, we first propose two\nseparate encoders to independently capture user interests of different time\nscales. We then extract long-term and short-term interests proxies from the\ninteraction sequences, which serve as pseudo labels for user interests. Then\npairwise contrastive tasks are designed to supervise the similarity between\ninterest representations and their corresponding interest proxies. Finally,\nsince the importance of long-term and short-term interests is dynamically\nchanging, we propose to adaptively aggregate them through an attention-based\nnetwork for prediction. We conduct experiments on two large-scale real-world\ndatasets for e-commerce and short-video recommendation. Empirical results show\nthat our CLSR consistently outperforms all state-of-the-art models with\nsignificant improvements: GAUC is improved by over 0.01, and NDCG is improved\nby over 4%. Further counterfactual evaluations demonstrate that stronger\ndisentanglement of long and short-term interests is successfully achieved by\nCLSR. The code and data are available at\nhttps://github.com/tsinghua-fib-lab/CLSR.</p>\n", "tags": ["Datasets","Has Code","Model Architecture","Tools"] },
{"key": "zheng2023gpt", "citations": "88", "year": "2023", "title":"Gpt-fathom: Benchmarking Large Language Models To Decipher The Evolutionary Path Towards GPT-4 And Beyond", "abstract": "<p>With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI’s legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI’s earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM’s reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Model Architecture","Tools"] },
{"key": "zheng2023judging", "citations": "315", "year": "2023", "title":"Judging Llm-as-a-judge With Mt-bench And Chatbot Arena", "abstract": "<p>Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Tools"] },
{"key": "zhengyuan2023mm", "citations": "60", "year": "2023", "title":"MM-REACT: Prompting Chatgpt For Multimodal Reasoning And Action", "abstract": "<p>We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT’s prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT’s effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT’s system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/</p>\n", "tags": ["Has Code","Prompting"] },
{"key": "zhong2017seq2sql", "citations": "798", "year": "2017", "title":"Seq2sql: Generating Structured Queries From Natural Language Using Reinforcement Learning", "abstract": "<p>A significant amount of the world’s knowledge is stored in relational\ndatabases. However, the ability for users to retrieve facts from a database is\nlimited due to a lack of understanding of query languages such as SQL. We\npropose Seq2SQL, a deep neural network for translating natural language\nquestions to corresponding SQL queries. Our model leverages the structure of\nSQL queries to significantly reduce the output space of generated queries.\nMoreover, we use rewards from in-the-loop query execution over the database to\nlearn a policy to generate unordered parts of the query, which we show are less\nsuitable for optimization via cross entropy loss. In addition, we will publish\nWikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL\nqueries distributed across 24241 tables from Wikipedia. This dataset is\nrequired to train our model and is an order of magnitude larger than comparable\ndatasets. By applying policy-based reinforcement learning with a query\nexecution environment to WikiSQL, our model Seq2SQL outperforms attentional\nsequence to sequence models, improving execution accuracy from 35.9% to 59.4%\nand logical form accuracy from 23.4% to 48.3%.</p>\n", "tags": ["Agentic","Datasets","Reinforcement Learning"] },
{"key": "zhong2018affect", "citations": "85", "year": "2019", "title":"An Affect-rich Neural Conversational Model With Biased Attention And Weighted Cross-entropy Loss", "abstract": "<p>Affect conveys important implicit information in human communication. Having\nthe capability to correctly express affect during human-machine conversations\nis one of the major milestones in artificial intelligence. In recent years,\nextensive research on open-domain neural conversational models has been\nconducted. However, embedding affect into such models is still under explored.\nIn this paper, we propose an end-to-end affect-rich open-domain neural\nconversational model that produces responses not only appropriate in syntax and\nsemantics, but also with rich affect. Our model extends the Seq2Seq model and\nadopts VAD (Valence, Arousal and Dominance) affective notations to embed each\nword with affects. In addition, our model considers the effect of negators and\nintensifiers via a novel affective attention mechanism, which biases attention\ntowards affect-rich words in input sentences. Lastly, we train our model with\nan affect-incorporated objective function to encourage the generation of\naffect-rich words in the output responses. Evaluations based on both perplexity\nand human evaluations show that our model outperforms the state-of-the-art\nbaseline model of comparable size in producing natural and affect-rich\nresponses.</p>\n", "tags": ["AAAI","Model Architecture"] },
{"key": "zhong2018global", "citations": "76", "year": "2018", "title":"Global-locally Self-attentive Dialogue State Tracker", "abstract": "<p>Dialogue state tracking, which estimates user goals and requests given the\ndialogue context, is an essential part of task-oriented dialogue systems. In\nthis paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker\n(GLAD), which learns representations of the user utterance and previous system\nactions with global-local modules. Our model uses global modules to share\nparameters between estimators for different types (called slots) of dialogue\nstates, and uses local modules to learn slot-specific features. We show that\nthis significantly improves tracking of rare states and achieves\nstate-of-the-art performance on the WoZ and DSTC2 state tracking tasks. GLAD\nobtains 88.1% joint goal accuracy and 97.1% request accuracy on WoZ,\noutperforming prior work by 3.7% and 5.5%. On DSTC2, our model obtains 74.5%\njoint goal accuracy and 97.5% request accuracy, outperforming prior work by\n1.1% and 1.0%.</p>\n", "tags": ["Dialogue & Multi Turn"] },
{"key": "zhong2019jec", "citations": "91", "year": "2020", "title":"JEC-QA: A Legal-domain Question Answering Dataset", "abstract": "<p>We present JEC-QA, the largest question answering dataset in the legal\ndomain, collected from the National Judicial Examination of China. The\nexamination is a comprehensive evaluation of professional skills for legal\npractitioners. College students are required to pass the examination to be\ncertified as a lawyer or a judge. The dataset is challenging for existing\nquestion answering methods, because both retrieving relevant materials and\nanswering questions require the ability of logic reasoning. Due to the high\ndemand of multiple reasoning abilities to answer legal questions, the\nstate-of-the-art models can only achieve about 28% accuracy on JEC-QA, while\nskilled humans and unskilled humans can reach 81% and 64% accuracy\nrespectively, which indicates a huge gap between humans and machines on this\ntask. We will release JEC-QA and our baselines to help improve the reasoning\nability of machine comprehension models. You can access the dataset from\nhttp://jecqa.thunlp.org/.</p>\n", "tags": ["AAAI","Datasets","Evaluation"] },
{"key": "zhong2019knowledge", "citations": "265", "year": "2019", "title":"Knowledge-enriched Transformer For Emotion Detection In Textual Conversations", "abstract": "<p>Messages in human conversations inherently convey emotions. The task of\ndetecting emotions in textual conversations leads to a wide range of\napplications such as opinion mining in social networks. However, enabling\nmachines to analyze emotions in conversations is challenging, partly because\nhumans often rely on the context and commonsense knowledge to express emotions.\nIn this paper, we address these challenges by proposing a Knowledge-Enriched\nTransformer (KET), where contextual utterances are interpreted using\nhierarchical self-attention and external commonsense knowledge is dynamically\nleveraged using a context-aware affective graph attention mechanism.\nExperiments on multiple textual conversation datasets demonstrate that both\ncontext and commonsense knowledge are consistently beneficial to the emotion\ndetection performance. In addition, the experimental results show that our KET\nmodel outperforms the state-of-the-art models on most of the tested datasets in\nF1 score.</p>\n", "tags": ["Applications","Datasets","EMNLP","Model Architecture"] },
{"key": "zhong2020grounded", "citations": "79", "year": "2020", "title":"Grounded Adaptation For Zero-shot Executable Semantic Parsing", "abstract": "<p>We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing\n(GAZP) to adapt an existing semantic parser to new environments (e.g. new\ndatabase schemas). GAZP combines a forward semantic parser with a backward\nutterance generator to synthesize data (e.g. utterances and SQL queries) in the\nnew environment, then selects cycle-consistent examples to adapt the parser.\nUnlike data-augmentation, which typically synthesizes unverified examples in\nthe training environment, GAZP synthesizes examples in the new environment\nwhose input-output consistency are verified. On the Spider, Sparc, and CoSQL\nzero-shot semantic parsing tasks, GAZP improves logical form and execution\naccuracy of the baseline parser. Our analyses show that GAZP outperforms\ndata-augmentation in the training environment, performance increases with the\namount of GAZP-synthesized data, and cycle-consistency is central to successful\nadaptation.</p>\n", "tags": ["EMNLP","Training Techniques"] },
{"key": "zhong2020towards", "citations": "100", "year": "2020", "title":"Towards Persona-based Empathetic Conversational Models", "abstract": "<p>Empathetic conversational models have been shown to improve user satisfaction\nand task outcomes in numerous domains. In Psychology, persona has been shown to\nbe highly correlated to personality, which in turn influences empathy. In\naddition, our empirical analysis also suggests that persona plays an important\nrole in empathetic conversations. To this end, we propose a new task towards\npersona-based empathetic conversations and present the first empirical study on\nthe impact of persona on empathetic responding. Specifically, we first present\na novel large-scale multi-domain dataset for persona-based empathetic\nconversations. We then propose CoBERT, an efficient BERT-based response\nselection model that obtains the state-of-the-art performance on our dataset.\nFinally, we conduct extensive experiments to investigate the impact of persona\non empathetic responding. Notably, our results show that persona improves\nempathetic responding more when CoBERT is trained on empathetic conversations\nthan non-empathetic ones, establishing an empirical link between persona and\nempathy in human conversations.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "zhong2021adapting", "citations": "88", "year": "2021", "title":"Adapting Language Models For Zero-shot Learning By Meta-tuning On Dataset And Prompt Collections", "abstract": "<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can “prompt” the LM with the review\nand the label description “Does the user like this movie?”, and ask whether the\nnext word is “yes” or “no”. However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.</p>\n", "tags": ["Datasets","EMNLP","Fine-Tuning","Model Architecture","Prompting","Training Techniques"] },
{"key": "zhong2021factual", "citations": "216", "year": "2021", "title":"Factual Probing Is [MASK]: Learning Vs. Learning To Recall", "abstract": "<p>Petroni et al. (2019) demonstrated that it is possible to retrieve world\nfacts from a pre-trained language model by expressing them as cloze-style\nprompts and interpret the model’s prediction accuracy as a lower bound on the\namount of factual information it encodes. Subsequent work has attempted to\ntighten the estimate by searching for better prompts, using a disjoint set of\nfacts as training data. In this work, we make two complementary contributions\nto better understand these factual probing techniques. First, we propose\nOptiPrompt, a novel and efficient method which directly optimizes in continuous\nembedding space. We find this simple method is able to predict an additional\n6.4% of facts in the LAMA benchmark. Second, we raise a more important\nquestion: Can we really interpret these probing results as a lower bound? Is it\npossible that these prompt-search methods learn from the training data too? We\nfind, somewhat surprisingly, that the training data used by these methods\ncontains certain regularities of the underlying fact distribution, and all the\nexisting prompt methods, including ours, are able to exploit them for better\nfact prediction. We conduct a set of control experiments to disentangle\n“learning” from “learning to recall”, providing a more detailed picture of what\ndifferent prompts can reveal about pre-trained language models.</p>\n", "tags": ["Datasets","Evaluation","NAACL","Prompting","Training Techniques"] },
{"key": "zhong2021regionclip", "citations": "297", "year": "2022", "title":"Regionclip: Region-based Language-image Pretraining", "abstract": "<p>Contrastive language-image pretraining (CLIP) using image-text pairs has\nachieved impressive results on image classification in both zero-shot and\ntransfer learning settings. However, we show that directly applying such models\nto recognize image regions for object detection leads to poor performance due\nto a domain shift: CLIP was trained to match an image as a whole to a text\ndescription, without capturing the fine-grained alignment between image regions\nand text spans. To mitigate this issue, we propose a new method called\nRegionCLIP that significantly extends CLIP to learn region-level visual\nrepresentations, thus enabling fine-grained alignment between image regions and\ntextual concepts. Our method leverages a CLIP model to match image regions with\ntemplate captions and then pretrains our model to align these region-text pairs\nin the feature space. When transferring our pretrained model to the\nopen-vocabulary object detection tasks, our method significantly outperforms\nthe state of the art by 3.8 AP50 and 2.2 AP for novel categories on COCO and\nLVIS datasets, respectively. Moreoever, the learned region representations\nsupport zero-shot inference for object detection, showing promising results on\nboth COCO and LVIS datasets. Our code is available at\nhttps://github.com/microsoft/RegionCLIP.</p>\n", "tags": ["CVPR","Datasets","Fine-Tuning","Has Code"] },
{"key": "zhong2023can", "citations": "105", "year": "2023", "title":"Can Chatgpt Replace Stackoverflow? A Study On Robustness And Reliability Of Large Language Model Code Generation", "abstract": "<p>Recently, the large language models (LLMs) have shown extraordinary ability\nin understanding natural language and generating programming code. It has been\na common practice of software engineers to consult LLMs when encountering\ncoding questions. Although efforts have been made to avoid syntax errors and\nalign the code with the intended semantics, the reliability and robustness of\nthe code generationfrom LLMs have not yet been thoroughly studied. The\nexecutable code is not equivalent to the reliable and robust code, especially\nin the context of real-world software development. The misuse of APIs in the\ngenerated code could lead to severe problem, such as resource leaks, program\ncrashes. To make things worse, the users of LLM code generation services are\nactually the developers that are most vulnerable to these code that seems right\n– They are always novice developers that are not familiar with the APIs that\nLLMs generate code for them. Therefore, they could hardly tell the misuse in\nthe code generated by LLMs, which further facilitates the incorrect code\napplied in real-world software. Existing code evaluation benchmark and datasets\nfocus on crafting small tasks such as programming questions in coding\ninterviews, which however deviates from the problem that developers would ask\nLLM for real-world coding help. To fill the missing piece, in this work, we\npropose a dataset RobustAPI for evaluating the reliability and robustness of\ncode generated by LLMs. We collect 1208 coding questions from StackOverflow on\n24 representative Java APIs. We summarize thecommon misuse patterns of these\nAPIs and evaluate them oncurrent popular LLMs. The evaluation results show that\nevenfor GPT-4, 62% of the generated code contains API misuses,which would cause\nunexpected consequences if the code isintroduced into real-world software.</p>\n", "tags": ["Evaluation","Llm For Code","Tools"] },
{"key": "zhou2016deep", "citations": "228", "year": "2016", "title":"Deep Recurrent Models With Fast-forward Connections For Neural Machine Translation", "abstract": "<p>Neural machine translation (NMT) aims at solving machine translation (MT)\nproblems using neural networks and has exhibited promising results in recent\nyears. However, most of the existing NMT models are shallow and there is still\na performance gap between a single NMT model and the best conventional MT\nsystem. In this work, we introduce a new type of linear connections, named\nfast-forward connections, based on deep Long Short-Term Memory (LSTM) networks,\nand an interleaved bi-directional architecture for stacking the LSTM layers.\nFast-forward connections play an essential role in propagating the gradients\nand building a deep topology of depth 16. On the WMT’14 English-to-French task,\nwe achieve BLEU=37.7 with a single attention model, which outperforms the\ncorresponding single shallow model by 6.2 BLEU points. This is the first time\nthat a single NMT model achieves state-of-the-art performance and outperforms\nthe best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3\neven without using an attention mechanism. After special handling of unknown\nwords and model ensembling, we obtain the best score reported to date on this\ntask with BLEU=40.4. Our models are also validated on the more difficult WMT’14\nEnglish-to-German task.</p>\n", "tags": ["Model Architecture","TACL"] },
{"key": "zhou2017emotional", "citations": "721", "year": "2018", "title":"Emotional Chatting Machine: Emotional Conversation Generation With Internal And External Memory", "abstract": "<p>Perception and expression of emotion are key factors to the success of\ndialogue systems or conversational agents. However, this problem has not been\nstudied in large-scale conversation generation so far. In this paper, we\npropose Emotional Chatting Machine (ECM) that can generate appropriate\nresponses not only in content (relevant and grammatical) but also in emotion\n(emotionally consistent). To the best of our knowledge, this is the first work\nthat addresses the emotion factor in large-scale conversation generation. ECM\naddresses the factor using three new mechanisms that respectively (1) models\nthe high-level abstraction of emotion expressions by embedding emotion\ncategories, (2) captures the change of implicit internal emotion states, and\n(3) uses explicit emotion expressions with an external emotion vocabulary.\nExperiments show that the proposed model can generate responses appropriate not\nonly in content but also in emotion.</p>\n", "tags": ["AAAI","Dialogue & Multi Turn","Memory & Context"] },
{"key": "zhou2017mojitalk", "citations": "211", "year": "2018", "title":"Mojitalk: Generating Emotional Responses At Scale", "abstract": "<p>Generating emotional language is a key step towards building empathetic\nnatural language processing agents. However, a major challenge for this line of\nresearch is the lack of large-scale labeled training data, and previous studies\nare limited to only small sets of human annotated sentiment labels.\nAdditionally, explicitly controlling the emotion and sentiment of generated\ntext is also difficult. In this paper, we take a more radical approach: we\nexploit the idea of leveraging Twitter data that are naturally labeled with\nemojis. More specifically, we collect a large corpus of Twitter conversations\nthat include emojis in the response, and assume the emojis convey the\nunderlying emotions of the sentence. We then introduce a reinforced conditional\nvariational encoder approach to train a deep generative model on these\nconversations, which allows us to use emojis to control the emotion of the\ngenerated text. Experimentally, we show in our quantitative and qualitative\nanalyses that the proposed models can successfully generate high-quality\nabstractive conversation responses in accordance with designated emotions.</p>\n", "tags": ["Datasets","Training Techniques"] },
{"key": "zhou2017neural", "citations": "334", "year": "2018", "title":"Neural Question Generation From Text: A Preliminary Study", "abstract": "<p>Automatic question generation aims to generate questions from a text passage\nwhere the generated questions can be answered by certain sub-spans of the given\npassage. Traditional methods mainly use rigid heuristic rules to transform a\nsentence into related questions. In this work, we propose to apply the neural\nencoder-decoder model to generate meaningful and diverse questions from natural\nlanguage sentences. The encoder reads the input text and the answer position,\nto produce an answer-aware input representation, which is fed to the decoder to\ngenerate an answer focused question. We conduct a preliminary study on neural\nquestion generation from text with the SQuAD dataset, and the experiment\nresults show that our method can produce fluent and diverse questions.</p>\n", "tags": ["Datasets"] },
{"key": "zhou2017selective", "citations": "233", "year": "2017", "title":"Selective Encoding For Abstractive Sentence Summarization", "abstract": "<p>We propose a selective encoding model to extend the sequence-to-sequence\nframework for abstractive sentence summarization. It consists of a sentence\nencoder, a selective gate network, and an attention equipped decoder. The\nsentence encoder and decoder are built with recurrent neural networks. The\nselective gate network constructs a second level sentence representation by\ncontrolling the information flow from encoder to decoder. The second level\nrepresentation is tailored for sentence summarization task, which leads to\nbetter performance. We evaluate our model on the English Gigaword, DUC 2004 and\nMSR abstractive sentence summarization datasets. The experimental results show\nthat the proposed selective encoding model outperforms the state-of-the-art\nbaseline models.</p>\n", "tags": ["Datasets","Model Architecture","Tools"] },
{"key": "zhou2018dataset", "citations": "207", "year": "2018", "title":"A Dataset For Document Grounded Conversations", "abstract": "<p>This paper introduces a document grounded dataset for text conversations. We\ndefine “Document Grounded Conversations” as conversations that are about the\ncontents of a specified document. In this dataset the specified documents were\nWikipedia articles about popular movies. The dataset contains 4112\nconversations with an average of 21.43 turns per conversation. This positions\nthis dataset to not only provide a relevant chat history while generating\nresponses but also provide a source of information that the models could use.\nWe describe two neural architectures that provide benchmark performance on the\ntask of generating the next response. We also evaluate our models for\nengagement and fluency, and find that the information from the document helps\nin generating more engaging and fluent responses.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","EMNLP","Evaluation"] },
{"key": "zhou2018interpretable", "citations": "69", "year": "2018", "title":"An Interpretable Reasoning Network For Multi-relation Question Answering", "abstract": "<p>Multi-relation Question Answering is a challenging task, due to the\nrequirement of elaborated analysis on questions and reasoning over multiple\nfact triples in knowledge base. In this paper, we present a novel model called\nInterpretable Reasoning Network that employs an interpretable, hop-by-hop\nreasoning process for question answering. The model dynamically decides which\npart of an input question should be analyzed at each hop; predicts a relation\nthat corresponds to the current parsed results; utilizes the predicted relation\nto update the question representation and the state of the reasoning process;\nand then drives the next-hop reasoning. Experiments show that our model yields\nstate-of-the-art results on two datasets. More interestingly, the model can\noffer traceable and observable intermediate predictions for reasoning analysis\nand failure diagnosis, thereby allowing manual manipulation in predicting the\nfinal answer.</p>\n", "tags": ["Datasets"] },
{"key": "zhou2018neural", "citations": "334", "year": "2018", "title":"Neural System Combination For Machine Translation", "abstract": "<p>Neural machine translation (NMT) becomes a new approach to machine\ntranslation and generates much more fluent results compared to statistical\nmachine translation (SMT).\n  However, SMT is usually better than NMT in translation adequacy. It is\ntherefore a promising direction to combine the advantages of both NMT and SMT.\n  In this paper, we propose a neural system combination framework leveraging\nmulti-source NMT, which takes as input the outputs of NMT and SMT systems and\nproduces the final translation.\n  Extensive experiments on the Chinese-to-English translation task show that\nour model archives significant improvement by 5.3 BLEU points over the best\nsingle system output and 3.4 BLEU points over the state-of-the-art traditional\nsystem combination methods.</p>\n", "tags": ["Tools"] },
{"key": "zhou2018syllable", "citations": "97", "year": "2018", "title":"Syllable-based Sequence-to-sequence Speech Recognition With The Transformer In Mandarin Chinese", "abstract": "<p>Sequence-to-sequence attention-based models have recently shown very\npromising results on automatic speech recognition (ASR) tasks, which integrate\nan acoustic, pronunciation and language model into a single neural network. In\nthese models, the Transformer, a new sequence-to-sequence attention-based model\nrelying entirely on self-attention without using RNNs or convolutions, achieves\na new single-model state-of-the-art BLEU on neural machine translation (NMT)\ntasks. Since the outstanding performance of the Transformer, we extend it to\nspeech and concentrate on it as the basic architecture of sequence-to-sequence\nattention-based model on Mandarin Chinese ASR tasks. Furthermore, we\ninvestigate a comparison between syllable based model and context-independent\nphoneme (CI-phoneme) based model with the Transformer in Mandarin Chinese.\nAdditionally, a greedy cascading decoder with the Transformer is proposed for\nmapping CI-phoneme sequences and syllable sequences into word sequences.\nExperiments on HKUST datasets demonstrate that syllable based model with the\nTransformer performs better than CI-phoneme based counterpart, and achieves a\ncharacter error rate (CER) of <em>\\(28.77%\\)</em>, which is competitive to the\nstate-of-the-art CER of \\(28.0%\\) by the joint CTC-attention based\nencoder-decoder network.</p>\n", "tags": ["Datasets","INTERSPEECH","Model Architecture"] },
{"key": "zhou2018visual", "citations": "90", "year": "2018", "title":"A Visual Attention Grounding Neural Model For Multimodal Machine Translation", "abstract": "<p>We introduce a novel multimodal machine translation model that utilizes\nparallel visual and textual information. Our model jointly optimizes the\nlearning of a shared visual-language embedding and a translator. The model\nleverages a visual attention grounding mechanism that links the visual\nsemantics with the corresponding textual semantics. Our approach achieves\ncompetitive state-of-the-art results on the Multi30K and the Ambiguous COCO\ndatasets. We also collected a new multilingual multimodal product description\ndataset to simulate a real-world international online shopping scenario. On\nthis dataset, our visual attention grounding model outperforms other methods by\na large margin.</p>\n", "tags": ["Datasets","EMNLP","Model Architecture"] },
{"key": "zhou2019evaluating", "citations": "133", "year": "2020", "title":"Evaluating Commonsense In Pre-trained Language Models", "abstract": "<p>Contextualized representations trained over large raw text data have given\nremarkable improvements for NLP tasks including question answering and reading\ncomprehension. There have been works showing that syntactic, semantic and word\nsense knowledge are contained in such representations, which explains why they\nbenefit such tasks. However, relatively little work has been done investigating\ncommonsense knowledge contained in contextualized representations, which is\ncrucial for human question answering and reading comprehension. We study the\ncommonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven\nchallenging benchmarks, finding that language modeling and its variants are\neffective objectives for promoting models’ commonsense ability while\nbi-directional context and larger training set are bonuses. We additionally\nfind that current models do poorly on tasks require more necessary inference\nsteps. Finally, we test the robustness of models by making dual test cases,\nwhich are correlated so that the correct prediction of one sample should lead\nto correct prediction of the other. Interestingly, the models show confusion on\nthese test cases, which suggests that they learn commonsense at the surface\nrather than the deep level. We release a test set, named CATs publicly, for\nfuture research.</p>\n", "tags": ["AAAI","Evaluation","Model Architecture","Training Techniques"] },
{"key": "zhou2019synchronous", "citations": "117", "year": "2019", "title":"Synchronous Bidirectional Neural Machine Translation", "abstract": "<p>Existing approaches to neural machine translation (NMT) generate the target\nlanguage sequence token by token from left to right. However, this kind of\nunidirectional decoding framework cannot make full use of the target-side\nfuture contexts which can be produced in a right-to-left decoding direction,\nand thus suffers from the issue of unbalanced outputs. In this paper, we\nintroduce a synchronous bidirectional neural machine translation (SB-NMT) that\npredicts its outputs using left-to-right and right-to-left decoding\nsimultaneously and interactively, in order to leverage both of the history and\nfuture information at the same time. Specifically, we first propose a new\nalgorithm that enables synchronous bidirectional decoding in a single model.\nThen, we present an interactive decoding model in which left-to-right\n(right-to-left) generation does not only depend on its previously generated\noutputs, but also relies on future contexts predicted by right-to-left\n(left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on\nlarge-scale NIST Chinese-English, WMT14 English-German, and WMT18\nRussian-English translation tasks. Experimental results demonstrate that our\nmodel achieves significant improvements over the strong Transformer model by\n3.92, 1.49 and 1.04 BLEU points respectively, and obtains the state-of-the-art\nperformance on Chinese-English and English-German translation tasks.</p>\n", "tags": ["Model Architecture","TACL","Tools"] },
{"key": "zhou2019unified", "citations": "776", "year": "2020", "title":"Unified Vision-language Pre-training For Image Captioning And VQA", "abstract": "<p>This paper presents a unified Vision-Language Pre-training (VLP) model. The\nmodel is unified in that (1) it can be fine-tuned for either vision-language\ngeneration (e.g., image captioning) or understanding (e.g., visual question\nanswering) tasks, and (2) it uses a shared multi-layer transformer network for\nboth encoding and decoding, which differs from many existing methods where the\nencoder and decoder are implemented using separate models. The unified VLP\nmodel is pre-trained on a large amount of image-text pairs using the\nunsupervised learning objectives of two tasks: bidirectional and\nsequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks\ndiffer solely in what context the prediction conditions on. This is controlled\nby utilizing specific self-attention masks for the shared transformer network.\nTo the best of our knowledge, VLP is the first reported model that achieves\nstate-of-the-art results on both vision-language generation and understanding\ntasks, as disparate as image captioning and visual question answering, across\nthree challenging benchmark datasets: COCO Captions, Flickr30k Captions, and\nVQA 2.0. The code and the pre-trained models are available at\nhttps://github.com/LuoweiZhou/VLP.</p>\n", "tags": ["AAAI","Datasets","Evaluation","Has Code","Model Architecture","Training Techniques"] },
{"key": "zhou2020bert", "citations": "161", "year": "2020", "title":"BERT Loses Patience: Fast And Robust Inference With Early Exit", "abstract": "<p>In this paper, we propose Patience-based Early Exit, a straightforward yet\neffective inference method that can be used as a plug-and-play technique to\nsimultaneously improve the efficiency and robustness of a pretrained language\nmodel (PLM). To achieve this, our approach couples an internal-classifier with\neach layer of a PLM and dynamically stops inference when the intermediate\npredictions of the internal classifiers remain unchanged for a pre-defined\nnumber of steps. Our approach improves inference efficiency as it allows the\nmodel to make a prediction with fewer layers. Meanwhile, experimental results\nwith an ALBERT model show that our method can improve the accuracy and\nrobustness of the model by preventing it from overthinking and exploiting\nmultiple classifiers for prediction, yielding a better accuracy-speed trade-off\ncompared to existing early exit methods.</p>\n", "tags": ["Efficiency","Model Architecture","Security"] },
{"key": "zhou2020contrastive", "citations": "114", "year": "2021", "title":"Contrastive Learning For Debiased Candidate Generation In Large-scale Recommender Systems", "abstract": "<p>Deep candidate generation (DCG) that narrows down the collection of relevant\nitems from billions to hundreds via representation learning has become\nprevalent in industrial recommender systems. Standard approaches approximate\nmaximum likelihood estimation (MLE) through sampling for better scalability and\naddress the problem of DCG in a way similar to language modeling. However, live\nrecommender systems face severe exposure bias and have a vocabulary several\norders of magnitude larger than that of natural language, implying that MLE\nwill preserve and even exacerbate the exposure bias in the long run in order to\nfaithfully fit the observed samples. In this paper, we theoretically prove that\na popular choice of contrastive loss is equivalent to reducing the exposure\nbias via inverse propensity weighting, which provides a new perspective for\nunderstanding the effectiveness of contrastive learning. Based on the\ntheoretical discovery, we design CLRec, a contrastive learning method to\nimprove DCG in terms of fairness, effectiveness and efficiency in recommender\nsystems with extremely large candidate size. We further improve upon CLRec and\npropose Multi-CLRec, for accurate multi-intention aware bias reduction. Our\nmethods have been successfully deployed in Taobao, where at least four-month\nonline A/B tests and offline analyses demonstrate its substantial improvements,\nincluding a dramatic reduction in the Matthew effect.</p>\n", "tags": ["Ethics & Fairness","KDD"] },
{"key": "zhou2020detecting", "citations": "93", "year": "2021", "title":"Detecting Hallucinated Content In Conditional Neural Sequence Generation", "abstract": "<p>Neural sequence models can generate highly fluent sentences, but recent\nstudies have also shown that they are also prone to hallucinate additional\ncontent not supported by the input. These variety of fluent but wrong outputs\nare particularly problematic, as it will not be possible for users to tell they\nare being presented incorrect content. To detect these errors, we propose a\ntask to predict whether each token in the output sequence is hallucinated (not\ncontained in the input) and collect new manually annotated evaluation sets for\nthis task. We also introduce a method for learning to detect hallucinations\nusing pretrained language models fine tuned on synthetic data that includes\nautomatically inserted hallucinations Experiments on machine translation (MT)\nand abstractive summarization demonstrate that our proposed approach\nconsistently outperforms strong baselines on all benchmark datasets. We further\ndemonstrate how to use the token-level hallucination labels to define a\nfine-grained loss over the target sequence in low-resource MT and achieve\nsignificant improvements over strong baseline methods. We also apply our method\nto word-level quality estimation for MT and show its effectiveness in both\nsupervised and unsupervised settings. Codes and data available at\nhttps://github.com/violet-zct/fairseq-detect-hallucination.</p>\n", "tags": ["Evaluation","Has Code"] },
{"key": "zhou2020more", "citations": "122", "year": "2020", "title":"More Grounded Image Captioning By Distilling Image-text Matching Model", "abstract": "<p>Visual attention not only improves the performance of image captioners, but\nalso serves as a visual interpretation to qualitatively measure the caption\nrationality and model transparency. Specifically, we expect that a captioner\ncan fix its attentive gaze on the correct objects while generating the\ncorresponding words. This ability is also known as grounded image captioning.\nHowever, the grounding accuracy of existing captioners is far from\nsatisfactory. To improve the grounding accuracy while retaining the captioning\nquality, it is expensive to collect the word-region alignment as strong\nsupervision. To this end, we propose a Part-of-Speech (POS) enhanced image-text\nmatching model (SCAN \\cite{lee2018stacked}): POS-SCAN, as the effective\nknowledge distillation for more grounded image captioning. The benefits are\ntwo-fold: 1) given a sentence and an image, POS-SCAN can ground the objects\nmore accurately than SCAN; 2) POS-SCAN serves as a word-region alignment\nregularization for the captioner’s visual attention module. By showing\nbenchmark experimental results, we demonstrate that conventional image\ncaptioners equipped with POS-SCAN can significantly improve the grounding\naccuracy without strong supervision. Last but not the least, we explore the\nindispensable Self-Critical Sequence Training (SCST) \\cite{Rennie_2017_CVPR} in\nthe context of grounded image captioning and show that the image-text matching\nscore can serve as a reward for more grounded captioning\n\\footnote{https://github.com/YuanEZhou/Grounded-Image-Captioning}.</p>\n", "tags": ["CVPR","Datasets","Evaluation","Has Code","Model Architecture","Reinforcement Learning","Training Techniques"] },
{"key": "zhou2021melm", "citations": "67", "year": "2022", "title":"MELM: Data Augmentation With Masked Entity Language Modeling For Low-resource NER", "abstract": "<p>Data augmentation is an effective solution to data scarcity in low-resource\nscenarios. However, when applied to token-level tasks such as NER, data\naugmentation methods often suffer from token-label misalignment, which leads to\nunsatsifactory performance. In this work, we propose Masked Entity Language\nModeling (MELM) as a novel data augmentation framework for low-resource NER. To\nalleviate the token-label misalignment issue, we explicitly inject NER labels\ninto sentence context, and thus the fine-tuned MELM is able to predict masked\nentity tokens by explicitly conditioning on their labels. Thereby, MELM\ngenerates high-quality augmented data with novel entities, which provides rich\nentity regularity knowledge and boosts NER performance. When training data from\nmultiple languages are available, we also integrate MELM with code-mixing for\nfurther improvement. We demonstrate the effectiveness of MELM on monolingual,\ncross-lingual and multilingual NER across various low-resource levels.\nExperimental results show that our MELM presents substantial improvement over\nthe baseline methods.</p>\n", "tags": ["Tools","Training Techniques"] },
{"key": "zhou2022bootstrap", "citations": "113", "year": "2023", "title":"Bootstrap Latent Representations For Multi-modal Recommendation", "abstract": "<p>This paper studies the multi-modal recommendation problem, where the item\nmulti-modality information (e.g., images and textual descriptions) is exploited\nto improve the recommendation accuracy. Besides the user-item interaction\ngraph, existing state-of-the-art methods usually use auxiliary graphs (e.g.,\nuser-user or item-item relation graph) to augment the learned representations\nof users and/or items. These representations are often propagated and\naggregated on auxiliary graphs using graph convolutional networks, which can be\nprohibitively expensive in computation and memory, especially for large graphs.\nMoreover, existing multi-modal recommendation methods usually leverage randomly\nsampled negative examples in Bayesian Personalized Ranking (BPR) loss to guide\nthe learning of user/item representations, which increases the computational\ncost on large graphs and may also bring noisy supervision signals into the\ntraining process. To tackle the above issues, we propose a novel\nself-supervised multi-modal recommendation model, dubbed BM3, which requires\nneither augmentations from auxiliary graphs nor negative samples. Specifically,\nBM3 first bootstraps latent contrastive views from the representations of users\nand items with a simple dropout augmentation. It then jointly optimizes three\nmulti-modal objectives to learn the representations of users and items by\nreconstructing the user-item interaction graph and aligning modality features\nunder both inter- and intra-modality perspectives. BM3 alleviates both the need\nfor contrasting with negative examples and the complex graph augmentation from\nan additional target network for contrastive view generation. We show BM3\noutperforms prior recommendation models on three datasets with number of nodes\nranging from 20K to 200K, while achieving a 2-9X reduction in training time.\nOur code is available at https://github.com/enoche/BM3.</p>\n", "tags": ["Datasets","Has Code","Training Techniques"] },
{"key": "zhou2022conditional", "citations": "753", "year": "2022", "title":"Conditional Prompt Learning For Vision-language Models", "abstract": "<p>With the rise of powerful pre-trained vision-language models like CLIP, it\nbecomes essential to investigate ways to adapt these models to downstream\ndatasets. A recently proposed method named Context Optimization (CoOp)\nintroduces the concept of prompt learning – a recent trend in NLP – to the\nvision domain for adapting pre-trained vision-language models. Specifically,\nCoOp turns context words in a prompt into a set of learnable vectors and, with\nonly a few labeled images for learning, can achieve huge improvements over\nintensively-tuned manual prompts. In our study we identify a critical problem\nof CoOp: the learned context is not generalizable to wider unseen classes\nwithin the same dataset, suggesting that CoOp overfits base classes observed\nduring training. To address the problem, we propose Conditional Context\nOptimization (CoCoOp), which extends CoOp by further learning a lightweight\nneural network to generate for each image an input-conditional token (vector).\nCompared to CoOp’s static prompts, our dynamic prompts adapt to each instance\nand are thus less sensitive to class shift. Extensive experiments show that\nCoCoOp generalizes much better than CoOp to unseen classes, even showing\npromising transferability beyond a single dataset; and yields stronger domain\ngeneralization performance as well. Code is available at\nhttps://github.com/KaiyangZhou/CoOp.</p>\n", "tags": ["CVPR","Datasets","Efficiency","Has Code","Prompting","Training Techniques"] },
{"key": "zhou2022large", "citations": "195", "year": "2022", "title":"Large Language Models Are Human-level Prompt Engineers", "abstract": "<p>By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the “program,” optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.</p>\n", "tags": ["Few-Shot","In Context Learning","Prompting"] },
{"key": "zhou2023comprehensive", "citations": "160", "year": "2024", "title":"A Comprehensive Survey On Pretrained Foundation Models: A History From BERT To Chatgpt", "abstract": "<p>Pretrained Foundation Models (PFMs) are regarded as the foundation for\nvarious downstream tasks with different data modalities. A PFM (e.g., BERT,\nChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable\nparameter initialization for a wide range of downstream applications. BERT\nlearns bidirectional encoder representations from Transformers, which are\ntrained on large datasets as contextual language models. Similarly, the\ngenerative pretrained transformer (GPT) method employs Transformers as the\nfeature extractor and is trained using an autoregressive paradigm on large\ndatasets. Recently, ChatGPT shows promising success on large language models,\nwhich applies an autoregressive language model with zero shot or few shot\nprompting. The remarkable achievements of PFM have brought significant\nbreakthroughs to various fields of AI. Numerous studies have proposed different\nmethods, raising the demand for an updated survey. This study provides a\ncomprehensive review of recent research advancements, challenges, and\nopportunities for PFMs in text, image, graph, as well as other data modalities.\nThe review covers the basic components and existing pretraining methods used in\nnatural language processing, computer vision, and graph learning. Additionally,\nit explores advanced PFMs used for different data modalities and unified PFMs\nthat consider data quality and quantity. The review also discusses research\nrelated to the fundamentals of PFMs, such as model efficiency and compression,\nsecurity, and privacy. Finally, the study provides key implications, future\nresearch directions, challenges, and open problems in the field of PFMs.\nOverall, this survey aims to shed light on the research of the PFMs on\nscalability, security, logical reasoning ability, cross-domain learning\nability, and the user-friendly interactive ability for artificial general\nintelligence.</p>\n", "tags": ["Applications","Datasets","Efficiency","Ethics & Fairness","Model Architecture","Prompting","Security","Survey Paper"] },
{"key": "zhu2017flexible", "citations": "89", "year": "2017", "title":"Flexible End-to-end Dialogue System For Knowledge Grounded Conversation", "abstract": "<p>In knowledge grounded conversation, domain knowledge plays an important role\nin a special domain such as Music. The response of knowledge grounded\nconversation might contain multiple answer entities or no entity at all.\nAlthough existing generative question answering (QA) systems can be applied to\nknowledge grounded conversation, they either have at most one entity in a\nresponse or cannot deal with out-of-vocabulary entities. We propose a fully\ndata-driven generative dialogue system GenDS that is capable of generating\nresponses based on input message and related knowledge base (KB). To generate\narbitrary number of answer entities even when these entities never appear in\nthe training set, we design a dynamic knowledge enquirer which selects\ndifferent answer entities at different positions in a single response,\naccording to different local context. It does not rely on the representations\nof entities, enabling our model deal with out-of-vocabulary entities. We\ncollect a human-human conversation data (ConversMusic) with knowledge\nannotations. The proposed method is evaluated on CoversMusic and a public\nquestion answering dataset. Our proposed GenDS system outperforms baseline\nmethods significantly in terms of the BLEU, entity accuracy, entity recall and\nhuman evaluation. Moreover,the experiments also demonstrate that GenDS works\nbetter even on small datasets.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","Training Techniques"] },
{"key": "zhu2017structured", "citations": "110", "year": "2017", "title":"Structured Attentions For Visual Question Answering", "abstract": "<p>Visual attention, which assigns weights to image regions according to their\nrelevance to a question, is considered as an indispensable part by most Visual\nQuestion Answering models. Although the questions may involve complex relations\namong multiple regions, few attention models can effectively encode such\ncross-region relations. In this paper, we demonstrate the importance of\nencoding such relations by showing the limited effective receptive field of\nResNet on two datasets, and propose to model the visual attention as a\nmultivariate distribution over a grid-structured Conditional Random Field on\nimage regions. We demonstrate how to convert the iterative inference\nalgorithms, Mean Field and Loopy Belief Propagation, as recurrent layers of an\nend-to-end neural network. We empirically evaluated our model on 3 datasets, in\nwhich it surpasses the best baseline model of the newly released CLEVR dataset\nby 9.5%, and the best published model on the VQA dataset by 1.25%. Source code\nis available at https: //github.com/zhuchen03/vqa-sva.</p>\n", "tags": ["Datasets","Has Code","ICCV","Model Architecture"] },
{"key": "zhu2018sdnet", "citations": "124", "year": "2018", "title":"Sdnet: Contextualized Attention-based Deep Network For Conversational Question Answering", "abstract": "<p>Conversational question answering (CQA) is a novel QA task that requires\nunderstanding of dialogue context. Different from traditional single-turn\nmachine reading comprehension (MRC) tasks, CQA includes passage comprehension,\ncoreference resolution, and contextual understanding. In this paper, we propose\nan innovated contextualized attention-based deep neural network, SDNet, to fuse\ncontext into traditional MRC models. Our model leverages both inter-attention\nand self-attention to comprehend conversation context and extract relevant\ninformation from passage. Furthermore, we demonstrated a novel method to\nintegrate the latest BERT contextual model. Empirical results show the\neffectiveness of our model, which sets the new state of the art result in CoQA\nleaderboard, outperforming the previous best model by 1.6% F1. Our ensemble\nmodel further improves the result by 2.7% F1.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture"] },
{"key": "zhu2018texygen", "citations": "164", "year": "2018", "title":"Texygen: A Benchmarking Platform For Text Generation Models", "abstract": "<p>We introduce Texygen, a benchmarking platform to support research on\nopen-domain text generation models. Texygen has not only implemented a majority\nof text generation models, but also covered a set of metrics that evaluate the\ndiversity, the quality and the consistency of the generated texts. The Texygen\nplatform could help standardize the research on text generation and facilitate\nthe sharing of fine-tuned open-source implementations among researchers for\ntheir work. As a consequence, this would help in improving the reproductivity\nand reliability of future research work in text generation.</p>\n", "tags": ["Datasets","Evaluation","Tools"] },
{"key": "zhu2019freelb", "citations": "185", "year": "2019", "title":"Freelb: Enhanced Adversarial Training For Natural Language Understanding", "abstract": "<p>Adversarial training, which minimizes the maximal risk for label-preserving\ninput perturbations, has proved to be effective for improving the\ngeneralization of language models. In this work, we propose a novel adversarial\ntraining algorithm, FreeLB, that promotes higher invariance in the embedding\nspace, by adding adversarial perturbations to word embeddings and minimizing\nthe resultant adversarial risk inside different regions around input samples.\nTo validate the effectiveness of the proposed approach, we apply it to\nTransformer-based models for natural language understanding and commonsense\nreasoning tasks. Experiments on the GLUE benchmark show that when applied only\nto the finetuning stage, it is able to improve the overall test scores of\nBERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8.\nIn addition, the proposed approach achieves state-of-the-art single-model test\naccuracies of 85.44% and 67.75% on ARC-Easy and ARC-Challenge. Experiments on\nCommonsenseQA benchmark further demonstrate that FreeLB can be generalized and\nboost the performance of RoBERTa-large model on other tasks as well. Code is\navailable at \\url{https://github.com/zhuchen03/FreeLB .</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Security","Training Techniques"] },
{"key": "zhu2019modeling", "citations": "97", "year": "2019", "title":"Modeling Graph Structure In Transformer For Better Amr-to-text Generation", "abstract": "<p>Recent studies on AMR-to-text generation often formalize the task as a\nsequence-to-sequence (seq2seq) learning problem by converting an Abstract\nMeaning Representation (AMR) graph into a word sequence. Graph structures are\nfurther modeled into the seq2seq framework in order to utilize the structural\ninformation in the AMR graphs. However, previous approaches only consider the\nrelations between directly connected concepts while ignoring the rich structure\nin AMR graphs. In this paper we eliminate such a strong limitation and propose\na novel structure-aware self-attention approach to better modeling the\nrelations between indirectly connected concepts in the state-of-the-art seq2seq\nmodel, i.e., the Transformer. In particular, a few different methods are\nexplored to learn structural representations between two concepts. Experimental\nresults on English AMR benchmark datasets show that our approach significantly\noutperforms the state of the art with 29.66 and 31.82 BLEU scores on LDC2015E86\nand LDC2017T10, respectively. To the best of our knowledge, these are the best\nresults achieved so far by supervised models on the benchmarks.</p>\n", "tags": ["EMNLP","Evaluation","Model Architecture","Tools"] },
{"key": "zhu2019soft", "citations": "112", "year": "2019", "title":"Soft Contextual Data Augmentation For Neural Machine Translation", "abstract": "<p>While data augmentation is an important trick to boost the accuracy of deep\nlearning methods in computer vision tasks, its study in natural language tasks\nis still very limited. In this paper, we present a novel data augmentation\nmethod for neural machine translation. Different from previous augmentation\nmethods that randomly drop, swap or replace words with other words in a\nsentence, we softly augment a randomly chosen word in a sentence by its\ncontextual mixture of multiple related words. More accurately, we replace the\none-hot representation of a word by a distribution (provided by a language\nmodel) over the vocabulary, i.e., replacing the embedding of this word by a\nweighted combination of multiple semantically similar words. Since the weights\nof those words depend on the contextual information of the word to be replaced,\nthe newly generated sentences capture much richer information than previous\naugmentation methods. Experimental results on both small scale and large scale\nmachine translation datasets demonstrate the superiority of our method over\nstrong baselines.</p>\n", "tags": ["Datasets"] },
{"key": "zhu2019vision", "citations": "189", "year": "2020", "title":"Vision-language Navigation With Self-supervised Auxiliary Reasoning Tasks", "abstract": "<p>Vision-Language Navigation (VLN) is a task where agents learn to navigate\nfollowing natural language instructions. The key to this task is to perceive\nboth the visual scene and natural language sequentially. Conventional\napproaches exploit the vision and language features in cross-modal grounding.\nHowever, the VLN task remains challenging, since previous works have neglected\nthe rich semantic information contained in the environment (such as implicit\nnavigation graphs or sub-trajectory semantics). In this paper, we introduce\nAuxiliary Reasoning Navigation (AuxRN), a framework with four self-supervised\nauxiliary reasoning tasks to take advantage of the additional training signals\nderived from the semantic information. The auxiliary tasks have four reasoning\nobjectives: explaining the previous actions, estimating the navigation\nprogress, predicting the next orientation, and evaluating the trajectory\nconsistency. As a result, these additional training signals help the agent to\nacquire knowledge of semantic representations in order to reason about its\nactivity and build a thorough perception of the environment. Our experiments\nindicate that auxiliary reasoning tasks improve both the performance of the\nmain task and the model generalizability by a large margin. Empirically, we\ndemonstrate that an agent trained with self-supervised auxiliary reasoning\ntasks substantially outperforms the previous state-of-the-art method, being the\nbest existing approach on the standard benchmark.</p>\n", "tags": ["Agentic","CVPR","Datasets","Evaluation","Tools","Training Techniques"] },
{"key": "zhu2020crosswoz", "citations": "91", "year": "2020", "title":"Crosswoz: A Large-scale Chinese Cross-domain Task-oriented Dialogue Dataset", "abstract": "<p>To advance multi-domain (cross-domain) dialogue modeling as well as alleviate\nthe shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first\nlarge-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It\ncontains 6K dialogue sessions and 102K utterances for 5 domains, including\nhotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains\nrich annotation of dialogue states and dialogue acts at both user and system\nsides. About 60% of the dialogues have cross-domain user goals that favor\ninter-domain dependency and encourage natural transition across domains in\nconversation. We also provide a user simulator and several benchmark models for\npipelined task-oriented dialogue systems, which will facilitate researchers to\ncompare and evaluate their models on this corpus. The large size and rich\nannotation of CrossWOZ make it suitable to investigate a variety of tasks in\ncross-domain dialogue modeling, such as dialogue state tracking, policy\nlearning, user simulation, etc.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Evaluation","TACL"] },
{"key": "zhu2020incorporating", "citations": "223", "year": "2020", "title":"Incorporating BERT Into Neural Machine Translation", "abstract": "<p>The recently proposed BERT has shown great power on a variety of natural\nlanguage understanding tasks, such as text classification, reading\ncomprehension, etc. However, how to effectively apply BERT to neural machine\ntranslation (NMT) lacks enough exploration. While BERT is more commonly used as\nfine-tuning instead of contextual embedding for downstream language\nunderstanding tasks, in NMT, our preliminary exploration of using BERT as\ncontextual embedding is better than using for fine-tuning. This motivates us to\nthink how to better leverage BERT for NMT along this direction. We propose a\nnew algorithm named BERT-fused model, in which we first use BERT to extract\nrepresentations for an input sequence, and then the representations are fused\nwith each layer of the encoder and decoder of the NMT model through attention\nmechanisms. We conduct experiments on supervised (including sentence-level and\ndocument-level translations), semi-supervised and unsupervised machine\ntranslation, and achieve state-of-the-art results on seven benchmark datasets.\nOur code is available at https://github.com/bert-nmt/bert-nmt.</p>\n", "tags": ["Datasets","Evaluation","Fine-Tuning","Has Code","Model Architecture","Training Techniques"] },
{"key": "zhu2020mucko", "citations": "103", "year": "2020", "title":"Mucko: Multi-layer Cross-modal Knowledge Reasoning For Fact-based Visual Question Answering", "abstract": "<p>Fact-based Visual Question Answering (FVQA) requires external knowledge\nbeyond visible content to answer questions about an image, which is challenging\nbut indispensable to achieve general VQA. One limitation of existing FVQA\nsolutions is that they jointly embed all kinds of information without\nfine-grained selection, which introduces unexpected noises for reasoning the\nfinal answer. How to capture the question-oriented and\ninformation-complementary evidence remains a key challenge to solve the\nproblem. In this paper, we depict an image by a multi-modal heterogeneous\ngraph, which contains multiple layers of information corresponding to the\nvisual, semantic and factual features. On top of the multi-layer graph\nrepresentations, we propose a modality-aware heterogeneous graph convolutional\nnetwork to capture evidence from different layers that is most relevant to the\ngiven question. Specifically, the intra-modal graph convolution selects\nevidence from each modality and cross-modal graph convolution aggregates\nrelevant information across different modalities. By stacking this process\nmultiple times, our model performs iterative reasoning and predicts the optimal\nanswer by analyzing all question-oriented evidence. We achieve a new\nstate-of-the-art performance on the FVQA task and demonstrate the effectiveness\nand interpretability of our model with extensive experiments.</p>\n", "tags": ["IJCAI"] },
{"key": "zhu2020overcoming", "citations": "91", "year": "2020", "title":"Overcoming Language Priors With Self-supervised Learning For Visual Question Answering", "abstract": "<p>Most Visual Question Answering (VQA) models suffer from the language prior\nproblem, which is caused by inherent data biases. Specifically, VQA models tend\nto answer questions (e.g., what color is the banana?) based on the\nhigh-frequency answers (e.g., yellow) ignoring image contents. Existing\napproaches tackle this problem by creating delicate models or introducing\nadditional visual annotations to reduce question dependency while strengthening\nimage dependency. However, they are still subject to the language prior problem\nsince the data biases have not been even alleviated. In this paper, we\nintroduce a self-supervised learning framework to solve this problem.\nConcretely, we first automatically generate labeled data to balance the biased\ndata, and propose a self-supervised auxiliary task to utilize the balanced data\nto assist the base VQA model to overcome language priors. Our method can\ncompensate for the data biases by generating balanced data without introducing\nexternal annotations. Experimental results show that our method can\nsignificantly outperform the state-of-the-art, improving the overall accuracy\nfrom 49.50% to 57.59% on the most commonly used benchmark VQA-CP v2. In other\nwords, we can increase the performance of annotation-based methods by 16%\nwithout using external annotations.</p>\n", "tags": ["Datasets","Evaluation","IJCAI","Tools","Training Techniques"] },
{"key": "zhu2021mediasum", "citations": "82", "year": "2021", "title":"Mediasum: A Large-scale Media Interview Dataset For Dialogue Summarization", "abstract": "<p>MediaSum, a large-scale media interview dataset consisting of 463.6K\ntranscripts with abstractive summaries. To create this dataset, we collect\ninterview transcripts from NPR and CNN and employ the overview and topic\ndescriptions as summaries. Compared with existing public corpora for dialogue\nsummarization, our dataset is an order of magnitude larger and contains complex\nmulti-party conversations from multiple domains. We conduct statistical\nanalysis to demonstrate the unique positional bias exhibited in the transcripts\nof televised and radioed interviews. We also show that MediaSum can be used in\ntransfer learning to improve a model’s performance on other dialogue\nsummarization tasks.</p>\n", "tags": ["Datasets","Fine-Tuning","NAACL"] },
{"key": "zhu2021retrieving", "citations": "146", "year": "2021", "title":"Retrieving And Reading: A Comprehensive Survey On Open-domain Question Answering", "abstract": "<p>Open-domain Question Answering (OpenQA) is an important task in Natural\nLanguage Processing (NLP), which aims to answer a question in the form of\nnatural language based on large-scale unstructured documents. Recently, there\nhas been a surge in the amount of research literature on OpenQA, particularly\non techniques that integrate with neural Machine Reading Comprehension (MRC).\nWhile these research works have advanced performance to new heights on\nbenchmark datasets, they have been rarely covered in existing surveys on QA\nsystems. In this work, we review the latest research trends in OpenQA, with\nparticular attention to systems that incorporate neural MRC techniques.\nSpecifically, we begin with revisiting the origin and development of OpenQA\nsystems. We then introduce modern OpenQA architecture named “Retriever-Reader”\nand analyze the various systems that follow this architecture as well as the\nspecific techniques adopted in each of the components. We then discuss key\nchallenges to developing OpenQA systems and offer an analysis of benchmarks\nthat are commonly used. We hope our work would enable researchers to be\ninformed of the recent advancement and also the open challenges in OpenQA\nresearch, so as to stimulate further progress in this field.</p>\n", "tags": ["Datasets","Evaluation","Model Architecture","Retrieval Systems","Survey Paper"] },
{"key": "zhu2021topic", "citations": "107", "year": "2021", "title":"Topic-driven And Knowledge-aware Transformer For Dialogue Emotion Detection", "abstract": "<p>Emotion detection in dialogues is challenging as it often requires the\nidentification of thematic topics underlying a conversation, the relevant\ncommonsense knowledge, and the intricate transition patterns between the\naffective states. In this paper, we propose a Topic-Driven Knowledge-Aware\nTransformer to handle the challenges above. We firstly design a topic-augmented\nlanguage model (LM) with an additional layer specialized for topic detection.\nThe topic-augmented LM is then combined with commonsense statements derived\nfrom a knowledge base based on the dialogue contextual information. Finally, a\ntransformer-based encoder-decoder architecture fuses the topical and\ncommonsense information, and performs the emotion label sequence prediction.\nThe model has been experimented on four datasets in dialogue emotion detection,\ndemonstrating its superiority empirically over the existing state-of-the-art\napproaches. Quantitative and qualitative results show that the model can\ndiscover topics which help in distinguishing emotion categories.</p>\n", "tags": ["Datasets","Dialogue & Multi Turn","Model Architecture"] },
{"key": "zhu2022pointclip", "citations": "77", "year": "2023", "title":"Pointclip V2: Prompting CLIP And GPT For Powerful 3D Open-world Learning", "abstract": "<p>Large-scale pre-trained models have shown promising open-world performance\nfor both vision and language tasks. However, their transferred capacity on 3D\npoint clouds is still limited and only constrained to the classification task.\nIn this paper, we first collaborate CLIP and GPT to be a unified 3D open-world\nlearner, named as PointCLIP V2, which fully unleashes their potential for\nzero-shot 3D classification, segmentation, and detection. To better align 3D\ndata with the pre-trained language knowledge, PointCLIP V2 contains two key\ndesigns. For the visual end, we prompt CLIP via a shape projection module to\ngenerate more realistic depth maps, narrowing the domain gap between projected\npoint clouds with natural images. For the textual end, we prompt the GPT model\nto generate 3D-specific text as the input of CLIP’s textual encoder. Without\nany training in 3D domains, our approach significantly surpasses PointCLIP by\n+42.90%, +40.44%, and +28.75% accuracy on three datasets for zero-shot 3D\nclassification. On top of that, V2 can be extended to few-shot 3D\nclassification, zero-shot 3D part segmentation, and 3D object detection in a\nsimple manner, demonstrating our generalization ability for unified 3D\nopen-world learning.</p>\n", "tags": ["Datasets","Few-Shot","ICCV","Model Architecture","Prompting","Training Techniques"] },
{"key": "zhu2023llms", "citations": "77", "year": "2024", "title":"Llms For Knowledge Graph Construction And Reasoning: Recent Capabilities And Future Opportunities", "abstract": "<p>This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs’ performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG.</p>\n", "tags": ["Datasets","Evaluation","Few-Shot","Has Code","Model Architecture"] },
{"key": "zhuang2017parallel", "citations": "142", "year": "2018", "title":"Parallel Attention: A Unified Framework For Visual Object Discovery Through Dialogs And Queries", "abstract": "<p>Recognising objects according to a pre-defined fixed set of class labels has\nbeen well studied in the Computer Vision. There are a great many practical\napplications where the subjects that may be of interest are not known\nbeforehand, or so easily delineated, however. In many of these cases natural\nlanguage dialog is a natural way to specify the subject of interest, and the\ntask achieving this capability (a.k.a, Referring Expression Comprehension) has\nrecently attracted attention. To this end we propose a unified framework, the\nParalleL AttentioN (PLAN) network, to discover the object in an image that is\nbeing referred to in variable length natural expression descriptions, from\nshort phrases query to long multi-round dialogs. The PLAN network has two\nattention mechanisms that relate parts of the expressions to both the global\nvisual content and also directly to object candidates. Furthermore, the\nattention mechanisms are recurrent, making the referring process visualizable\nand explainable. The attended information from these dual sources are combined\nto reason about the referred object. These two attention mechanisms can be\ntrained in parallel and we find the combined system outperforms the\nstate-of-art on several benchmarked datasets with different length language\ninput, such as RefCOCO, RefCOCO+ and GuessWhat?!.</p>\n", "tags": ["Applications","CVPR","Datasets","Model Architecture","Tools"] },
{"key": "zhuge2021kaleido", "citations": "99", "year": "2021", "title":"Kaleido-bert: Vision-language Pre-training On Fashion Domain", "abstract": "<p>We present a new vision-language (VL) pre-training model dubbed Kaleido-BERT,\nwhich introduces a novel kaleido strategy for fashion cross-modality\nrepresentations from transformers. In contrast to random masking strategy of\nrecent VL models, we design alignment guided masking to jointly focus more on\nimage-text semantic relations. To this end, we carry out five novel tasks,\ni.e., rotation, jigsaw, camouflage, grey-to-color, and blank-to-color for\nself-supervised VL pre-training at patches of different scale. Kaleido-BERT is\nconceptually simple and easy to extend to the existing BERT framework, it\nattains new state-of-the-art results by large margins on four downstream tasks,\nincluding text retrieval (R@1: 4.03% absolute improvement), image retrieval\n(R@1: 7.13% abs imv.), category recognition (ACC: 3.28% abs imv.), and fashion\ncaptioning (Bleu4: 1.2 abs imv.). We validate the efficiency of Kaleido-BERT on\na wide range of e-commerical websites, demonstrating its broader potential in\nreal-world applications.</p>\n", "tags": ["Applications","CVPR","Model Architecture","Tools","Training Techniques"] },
{"key": "zhuosheng2022automatic", "citations": "174", "year": "2022", "title":"Automatic Chain Of Thought Prompting In Large Language Models", "abstract": "<p>Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like “Let’s think step by\nstep” to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the “Let’s think step by step” prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let’s think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot</p>\n", "tags": ["Datasets","Evaluation","Has Code","Model Architecture","Prompting"] },
{"key": "zhuosheng2023multimodal", "citations": "68", "year": "2023", "title":"Multimodal Chain-of-thought Reasoning In Language Models", "abstract": "<p>Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies have primarily focused on the language modality. We\npropose Multimodal-CoT that incorporates language (text) and vision (images)\nmodalities into a two-stage framework that separates rationale generation and\nanswer inference. In this way, answer inference can leverage better generated\nrationales that are based on multimodal information. Experimental results on\nScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed\napproach. With Multimodal-CoT, our model under 1 billion parameters achieves\nstate-of-the-art performance on the ScienceQA benchmark. Our analysis indicates\nthat Multimodal-CoT offers the advantages of mitigating hallucination and\nenhancing convergence speed. Code is publicly available at\nhttps://github.com/amazon-science/mm-cot.</p>\n", "tags": ["Datasets","Evaluation","Has Code","Prompting","Tools"] },
{"key": "ziegler2019fine", "citations": "355", "year": "2019", "title":"Fine-tuning Language Models From Human Preferences", "abstract": "<p>Reward learning enables the application of reinforcement learning (RL) to\ntasks where reward is defined by human judgment, building a model of reward by\nasking humans questions. Most work on reward learning has used simulated\nenvironments, but complex information about values is often expressed in\nnatural language, and we believe reward learning for language is a key to\nmaking RL practical and safe for real-world tasks. In this paper, we build on\nadvances in generative pretraining of language models to apply reward learning\nto four natural language tasks: continuing text with positive sentiment or\nphysically descriptive language, and summarization tasks on the TL;DR and\nCNN/Daily Mail datasets. For stylistic continuation we achieve good results\nwith only 5,000 comparisons evaluated by humans. For summarization, models\ntrained with 60,000 comparisons copy whole sentences from the input but skip\nirrelevant preamble; this leads to reasonable ROUGE scores and very good\nperformance according to our human labelers, but may be exploiting the fact\nthat labelers rely on simple heuristics.</p>\n", "tags": ["Agentic","Datasets","Fine-Tuning","Reinforcement Learning","Training Techniques"] },
{"key": "ziems2023can", "citations": "246", "year": "2023", "title":"Can Large Language Models Transform Computational Social Science?", "abstract": "<p>Large Language Models (LLMs) are capable of successfully performing many\nlanguage processing tasks zero-shot (without training data). If zero-shot LLMs\ncan also reliably classify and explain social phenomena like persuasiveness and\npolitical ideology, then LLMs could augment the Computational Social Science\n(CSS) pipeline in important ways. This work provides a road map for using LLMs\nas CSS tools. Towards this end, we contribute a set of prompting best practices\nand an extensive evaluation pipeline to measure the zero-shot performance of 13\nlanguage models on 25 representative English CSS benchmarks. On taxonomic\nlabeling tasks (classification), LLMs fail to outperform the best fine-tuned\nmodels but still achieve fair levels of agreement with humans. On free-form\ncoding tasks (generation), LLMs produce explanations that often exceed the\nquality of crowdworkers’ gold references. We conclude that the performance of\ntoday’s LLMs can augment the CSS research pipeline in two ways: (1) serving as\nzero-shot data annotators on human annotation teams, and (2) bootstrapping\nchallenging creative generation tasks (e.g., explaining the underlying\nattributes of a text). In summary, LLMs are posed to meaningfully participate\nin social science analysis in partnership with humans.</p>\n", "tags": ["Evaluation","Prompting","Tools","Training Techniques"] },
{"key": "zini2022explainability", "citations": "69", "year": "2022", "title":"On The Explainability Of Natural Language Processing Deep Models", "abstract": "<p>While there has been a recent explosion of work on ExplainableAI ExAI on deep\nmodels that operate on imagery and tabular data, textual datasets present new\nchallenges to the ExAI community. Such challenges can be attributed to the lack\nof input structure in textual data, the use of word embeddings that add to the\nopacity of the models and the difficulty of the visualization of the inner\nworkings of deep models when they are trained on textual data.\n  Lately, methods have been developed to address the aforementioned challenges\nand present satisfactory explanations on Natural Language Processing (NLP)\nmodels. However, such methods are yet to be studied in a comprehensive\nframework where common challenges are properly stated and rigorous evaluation\npractices and metrics are proposed. Motivated to democratize ExAI methods in\nthe NLP field, we present in this work a survey that studies model-agnostic as\nwell as model-specific explainability methods on NLP models. Such methods can\neither develop inherently interpretable NLP models or operate on pre-trained\nmodels in a post-hoc manner. We make this distinction and we further decompose\nthe methods into three categories according to what they explain: (1) word\nembeddings (input-level), (2) inner workings of NLP models (processing-level)\nand (3) models’ decisions (output-level). We also detail the different\nevaluation approaches interpretability methods in the NLP field. Finally, we\npresent a case-study on the well-known neural machine translation in an\nappendix and we propose promising future research directions for ExAI in the\nNLP field.</p>\n", "tags": ["Datasets","Evaluation","Survey Paper","Tools"] },
{"key": "zintgraf2018fast", "citations": "184", "year": "2018", "title":"Fast Context Adaptation Via Meta-learning", "abstract": "<p>We propose CAVIA for meta-learning, a simple extension to MAML that is less\nprone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA\npartitions the model parameters into two parts: context parameters that serve\nas additional input to the model and are adapted on individual tasks, and\nshared parameters that are meta-trained and shared across tasks. At test time,\nonly the context parameters are updated, leading to a low-dimensional task\nrepresentation. We show empirically that CAVIA outperforms MAML for regression,\nclassification, and reinforcement learning. Our experiments also highlight\nweaknesses in current benchmarks, in that the amount of adaptation needed in\nsome cases is small.</p>\n", "tags": ["Agentic","Reinforcement Learning"] },
{"key": "zoph2016multi", "citations": "292", "year": "2016", "title":"Multi-source Neural Translation", "abstract": "<p>We build a multi-source machine translation model and train it to maximize\nthe probability of a target English string given French and German sources.\nUsing the neural encoder-decoder framework, we explore several combination\nmethods and report up to +4.8 Bleu increases on top of a very strong\nattention-based neural translation model.</p>\n", "tags": ["Model Architecture","NAACL","Tools"] },
{"key": "zoph2016transfer", "citations": "740", "year": "2016", "title":"Transfer Learning For Low-resource Neural Machine Translation", "abstract": "<p>The encoder-decoder framework for neural machine translation (NMT) has been\nshown effective in large data scenarios, but is much less effective for\nlow-resource languages. We present a transfer learning method that\nsignificantly improves Bleu scores across a range of low-resource languages.\nOur key idea is to first train a high-resource language pair (the parent\nmodel), then transfer some of the learned parameters to the low-resource pair\n(the child model) to initialize and constrain training. Using our transfer\nlearning method we improve baseline NMT models by an average of 5.6 Bleu on\nfour low-resource language pairs. Ensembling and unknown word replacement add\nanother 2 Bleu which brings the NMT performance on low-resource machine\ntranslation close to a strong syntax based machine translation (SBMT) system,\nexceeding its performance on one language pair. Additionally, using the\ntransfer learning model for re-scoring, we can improve the SBMT system by an\naverage of 1.3 Bleu, improving the state-of-the-art on low-resource machine\ntranslation.</p>\n", "tags": ["EMNLP","Fine-Tuning","Tools","Training Techniques"] },
{"key": "zou2019reinforced", "citations": "60", "year": "2020", "title":"A Reinforced Generation Of Adversarial Examples For Neural Machine Translation", "abstract": "<p>Neural machine translation systems tend to fail on less decent inputs despite\nits significant efficacy, which may significantly harm the credibility of this\nsystems-fathoming how and when neural-based systems fail in such cases is\ncritical for industrial maintenance. Instead of collecting and analyzing bad\ncases using limited handcrafted error features, here we investigate this issue\nby generating adversarial examples via a new paradigm based on reinforcement\nlearning. Our paradigm could expose pitfalls for a given performance metric,\ne.g., BLEU, and could target any given neural machine translation architecture.\nWe conduct experiments of adversarial attacks on two mainstream neural machine\ntranslation architectures, RNN-search, and Transformer. The results show that\nour method efficiently produces stable attacks with meaning-preserving\nadversarial examples. We also present a qualitative and quantitative analysis\nfor the preference pattern of the attack, demonstrating its capability of\npitfall exposure.</p>\n", "tags": ["Model Architecture","Reinforcement Learning","Security"] },
{"key": "zou2023segment", "citations": "134", "year": "2023", "title":"Segment Everything Everywhere All At Once", "abstract": "<p>In this work, we present SEEM, a promptable and interactive model for\nsegmenting everything everywhere all at once in an image, as shown in Fig.1. In\nSEEM, we propose a novel decoding mechanism that enables diverse prompting for\nall types of segmentation tasks, aiming at a universal segmentation interface\nthat behaves like large language models (LLMs). More specifically, SEEM is\ndesigned with four desiderata: i) Versatility. We introduce a new visual prompt\nto unify different spatial queries including points, boxes, scribbles and\nmasks, which can further generalize to a different referring image; ii)\nCompositionality. We learn a joint visual-semantic space between text and\nvisual prompts, which facilitates the dynamic composition of two prompt types\nrequired for various segmentation tasks; iii) Interactivity. We further\nincorporate learnable memory prompts into the decoder to retain segmentation\nhistory through mask-guided cross-attention from decoder to image features; and\niv) Semantic-awareness. We use a text encoder to encode text queries and mask\nlabels into the same semantic space for open-vocabulary segmentation. We\nconduct a comprehensive empirical study to validate the effectiveness of SEEM\nacross diverse segmentation tasks. Notably, our single SEEM model achieves\ncompetitive performance across interactive segmentation, generic segmentation,\nreferring segmentation, and video object segmentation on 9 datasets with\nminimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity\nfor generalization to novel prompts or their combinations, rendering it a\nreadily universal image segmentation interface.</p>\n", "tags": ["Datasets","Model Architecture","Prompting"] },
{"key": "zou2023universal", "citations": "105", "year": "2023", "title":"Universal And Transferable Adversarial Attacks On Aligned Language Models", "abstract": "<p>Because “out-of-the-box” large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures – so-called “jailbreaks” against\nLLMs – these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.</p>\n", "tags": ["Has Code","Prompting","Security"] },
{"key": "östling2017neural", "citations": "171", "year": "2022", "title":"Neural Machine Translation For Low-resource Languages", "abstract": "<p>Neural machine translation (NMT) approaches have improved the state of the\nart in many machine translation settings over the last couple of years, but\nthey require large amounts of training data to produce sensible output. We\ndemonstrate that NMT can be used for low-resource languages as well, by\nintroducing more local dependencies and using word alignments to learn sentence\nreordering during translation. In addition to our novel model, we also present\nan empirical evaluation of low-resource phrase-based statistical machine\ntranslation (SMT) and NMT to investigate the lower limits of the respective\ntechnologies. We find that while SMT remains the best option for low-resource\nsettings, our method can produce acceptable translations with only 70000 tokens\nof training data, a level where the baseline NMT system fails completely.</p>\n", "tags": ["Evaluation","Survey Paper","Training Techniques"] },
{"key": "üstün2020udapter", "citations": "100", "year": "2020", "title":"Udapter: Language Adaptation For Truly Universal Dependency Parsing", "abstract": "<p>Recent advances in multilingual dependency parsing have brought the idea of a\ntruly universal parser closer to reality. However, cross-language interference\nand restrained model capacity remain major obstacles. To address this, we\npropose a novel multilingual task adaptation approach based on contextual\nparameter generation and adapter modules. This approach enables to learn\nadapters via language embeddings while sharing model parameters across\nlanguages. It also allows for an easy but effective integration of existing\nlinguistic typology features into the parsing network. The resulting parser,\nUDapter, outperforms strong monolingual and multilingual baselines on the\nmajority of both high-resource and low-resource (zero-shot) languages, showing\nthe success of the proposed adaptation approach. Our in-depth analyses show\nthat soft parameter sharing via typological features is key to this success.</p>\n", "tags": ["EMNLP"] },
{"key": "čyras2021argumentative", "citations": "77", "year": "2021", "title":"Argumentative XAI: A Survey", "abstract": "<p>Explainable AI (XAI) has been investigated for decades and, together with AI\nitself, has witnessed unprecedented growth in recent years. Among various\napproaches to XAI, argumentative models have been advocated in both the AI and\nsocial science literature, as their dialectical nature appears to match some\nbasic desirable features of the explanation activity. In this survey we\noverview XAI approaches built using methods from the field of computational\nargumentation, leveraging its wide array of reasoning abstractions and\nexplanation delivery methods. We overview the literature focusing on different\ntypes of explanation (intrinsic and post-hoc), different models with which\nargumentation-based explanations are deployed, different forms of delivery, and\ndifferent argumentation frameworks they use. We also lay out a roadmap for\nfuture work.</p>\n", "tags": ["IJCAI","Survey Paper"] },
{"key": "łańcucki2020fastpitch", "citations": "184", "year": "2021", "title":"Fastpitch: Parallel Text-to-speech With Pitch Prediction", "abstract": "<p>We present FastPitch, a fully-parallel text-to-speech model based on\nFastSpeech, conditioned on fundamental frequency contours. The model predicts\npitch contours during inference. By altering these predictions, the generated\nspeech can be more expressive, better match the semantic of the utterance, and\nin the end more engaging to the listener. Uniformly increasing or decreasing\npitch with FastPitch generates speech that resembles the voluntary modulation\nof voice. Conditioning on frequency contours improves the overall quality of\nsynthesized speech, making it comparable to state-of-the-art. It does not\nintroduce an overhead, and FastPitch retains the favorable, fully-parallel\nTransformer architecture, with over 900x real-time factor for mel-spectrogram\nsynthesis of a typical utterance.</p>\n", "tags": ["ICASSP","Model Architecture"] },
{"key": "šuster2018clicr", "citations": "87", "year": "2018", "title":"Clicr: A Dataset Of Clinical Case Reports For Machine Reading Comprehension", "abstract": "<p>We present a new dataset for machine comprehension in the medical domain. Our\ndataset uses clinical case reports with around 100,000 gap-filling queries\nabout these cases. We apply several baselines and state-of-the-art neural\nreaders to the dataset, and observe a considerable gap in performance (20% F1)\nbetween the best human and machine readers. We analyze the skills required for\nsuccessful answering and show how reader performance varies depending on the\napplicable skills. We find that inferences using domain knowledge and object\ntracking are the most frequently required skills, and that recognizing omitted\ninformation and spatio-temporal reasoning are the most difficult for the\nmachines.</p>\n", "tags": ["Datasets","NAACL"] }

]

